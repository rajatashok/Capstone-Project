563d3dec1c488238ac27de6e	X	Thanks! How to fix this problem - goo.gl/QTdm4
563d3dec1c488238ac27de6f	X	@roman-nazarkin So the issue was the bucket location?
563d3dec1c488238ac27de70	X	in my case there was some extra whitespace in the config file/setting which had the secret key, so it was transmitting an extra tab character in the "password", thus invalidating the signature.
563d3dec1c488238ac27de71	X	I had this same issue. It turned out the debugging tool I was using was mistakingly sending GET requests when the signature specified POST. This threw me off to thinking something was wrong with my signature encodings. Dumb mistake took up almost a day of trial and error.
563d3dec1c488238ac27de72	X	ditto - when adding Metadata with a key 'Cache-Control' onto an object that already has a metadata key 'cache-control' I get this error.
563d3dec1c488238ac27de73	X	I am using a PHP class for Amazon S3 and CloudFront - Link. But when I try to upload a file into a bucket, I get this error: [SignatureDoesNotMatch] The request signature we calculated does not match the signature you provided. Check your key and signing method. How to fix it? Thanks.
563d3dec1c488238ac27de74	X	When you sign up for Amazon, you can create yourself a key pair (Amazon calls those access key ID and secret access key). Those two are used to sign requests to Amazon's webservices. Amazon re-calculates the signature and compares if it matches the one that was contained in your request. That way the secret access key never needs to be transmitted over the network. If you get "Signature does not match", it's highly likely you used a wrong secret access key. Can you double-check access key and secret access key to make sure they're correct?
563d3ded1c488238ac27de75	X	Personally I received this error because of the characters that were in my meta data. The problematic character was the "–" chracter which is "\u2013" in unicode and different to "-". A note from the documentation http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#UserMetadata... Amazon S3 stores user-defined metadata in lowercase. Each name, value pair must conform to US-ASCII when using REST and UTF-8 when using SOAP or browser-based uploads via POST.
563d43c9a4387b6f44e92357	X	thanks, for some reason when I googles I found other libs, that wasn't suitable, but not this one. So I'm going to try this one..
563d43c9a4387b6f44e92358	X	If there someone who worked with Amazon S3 API in C? I can't manage to sign my REST request proper. Can someone share his successful experience in that?
563d43c9a4387b6f44e92359	X	I've never tried it, but a quick Google turned up the libs3 C library API for Amazon S3. That might make things easier, so you don't have to deal with raw HTTP requests via curl.
563d43caa4387b6f44e9235a	X	Did you apply both code for same Bucket or for live server you did use the bucket which is already in lower case ?
563d43caa4387b6f44e9235b	X	Same bucket name on both the servers. Why we should append the key with the bucket name?
563d43caa4387b6f44e9235c	X	I do not have knowledge of the language (php ) you are working on. BUT Yes, you do not need to append key with bucket name. You are already passing key in create object method.
563d43caa4387b6f44e9235d	X	The following is sample code from Amazon S3 API Documentation. This works on live site but on the localhost the latter gives an error saying no bucket found   // Success? but removing the . strtolower($s3->key); works
563d43caa4387b6f44e9235e	X	Amazon S3 is case sensitive. So for Bucket as well as Object if you changes the Name to Upper or Lower Case, It will give you different result. Means if Bucket Name has some Capital Laters and your code make changed the it name to lower case then It will returns you Bucket Does No Exist like message. So make sure that what actually bucket as well as object name exist at Amazon S3.
563d43caa4387b6f44e9235f	X	I am trying to use Amazon S3 API for uploading images to bucket. But I can't create a bucket. It shows "Access Denied " error. My code is: Is it required any permission? Anyone please help me.
563d43caa4387b6f44e92360	X	possible duplicate of privacy on Amazon S3
563d43caa4387b6f44e92361	X	I'm using amazon S3 php to upload and download files. for exemple to get a private file from amazon s3 I'm using: and TO download it but this is very heavy for the server, is there a solution to directly download private objects from amazon s3 using a link, a little like for public objects with a security. Thanks
563d43caa4387b6f44e92362	X	You can create pre-signed URLs for objects that have an expiration date. You can use this feature to allow people to download private objects directly from Amazon S3. The AWS SDK for PHP has an easy S3Client::getObjectUrl() method that can help you do this.
563d43cba4387b6f44e92363	X	So I setup another S3 account and use it's credentials (key/secret) then?
563d43cba4387b6f44e92364	X	That's correct.
563d43cba4387b6f44e92365	X	That would limit them (meaning one who has this other account credientials) from manipulating that shared bucket, but wouldn't they have unfettered access to that S3 account and store? Meaning, they could create bucket(s) via the API and upload stuff to their hearts content? I'm looking specifically for a way to have a client app that can talk to S3 with the restful API but is restricted in what can be done with those credentials. Namely read-only. Is that possible?
563d43cba4387b6f44e92366	X	You're right that using another S3 account gives that other account the ability to create new buckets. The only way I can think of to do what you suggest is to use anonymous access to your S3 bucket. If you choose random enough object names, then people aren't likely to guess the names of your objects. However, you are then responsible for bandwidth costs incurred by the anonymous downloads, and access to your objects aren't limited to authenticated accounts.
563d43cba4387b6f44e92367	X	I'm not concerned about people downloading stuff... just don't want them doing anything else. Read only, as it were. So the REST API, if used, always applies to a user with full access to the store? The only way to do something like this is to use a normal HTTP downloading through the object's public URL?
563d43cba4387b6f44e92368	X	Is there a way to create a different identity to (access key / secret key) to access Amazon S3 buckets via the REST API where I can restrict access (read only for example)?
563d43cba4387b6f44e92369	X	Yes, you can. The S3 API documentation describes the Authentication and Access Control services available to you. You can set up a bucket so that another Amazon S3 account can read but not modify items in the bucket.
563d43cba4387b6f44e9236a	X	The recommended way is to use IAM to create a new user, then apply a policy to that user.
563d43cba4387b6f44e9236b	X	Check out the details at http://docs.amazonwebservices.com/AmazonS3/2006-03-01/dev/index.html?UsingAuthAccess.html (follow the link to "Using Query String Authentication")- this is a subdocument to the one Greg Posted, and describes how to generate access URLs on the fly. This uses a hashed form of the private key and allows expiration, so you can give brief access to files in a bucket without allowed unfettered access to the rest of the S3 store. Constructing the REST URL is quite difficult, it took me about 3 hours of coding to get it right, but this is a very powerful access technique.
563d43cca4387b6f44e9236c	X	I'm trying to make an AutoIT script interface with the Amazon S3 API. I've been trying both SOAP and REST, although no success. This is the SOAP code I'm working with (modified example from Ptrex on the AutoIT forums), however I get the following response: "soapenv:Client.badRequest Missing SOAPAction header" To be honest, the code doesn't make that much sense to me and I'm really just tinkering around. Any examples or pointers to get me going in the right direction on how to properly interface with the Amazon S3 API would be greatly appreciated!
563d43cca4387b6f44e9236d	X	I don't know if this helps you out but from the autoit part everything works well The answer you get from amazon 'soapenv:Client.badRequest Missing SOAPAction header' means what it actually says something worng with your request. -namely: Missing SOAPAction header What you get was indeed a response but an error response from the server. I suggest trying to rewrite the request I found the most relevant description here: http://docs.aws.amazon.com/AWSSimpleQueueService/2008-01-01/SQSDeveloperGuide/index.html?MakingRequests_MakingSOAPRequestsArticle.html
563d43cca4387b6f44e9236e	X	I had the same problem, your service url needs http or https. Worked for me. The documentation is pretty poor in the sense that its wrong. IMO
563d43cca4387b6f44e9236f	X	I tried that already, if I add http:// to the serviceurl for the config and then ask the listobjects, it asks actually for: bucketnamehttp://s3.amazonaws.com and fails because this is obviously not valid.
563d43cca4387b6f44e92370	X	Also, because it's key based, everything is case sensitive.
563d43cca4387b6f44e92371	X	I am trying to access an external bucket over the Amazon S3 API through .Net / C#. I already tried the login with a 3rd party tool which worked like a charm, now I want to get the items of the bucket inside the framework. I am using this behind a Proxy, that's why I am using the S3config. that's the way I establish the connection itself to amazon. I also already tried placing into the config object initializer because I am in EU and the bucket is located somewhere in US. When I now try to access via : or or I only get Access Denied in the error object that is thrown. The credentials I use are 100% the same as in the 3rd party tool. Am I missing something here ? do I need to use any special way which I just can't find to make it work ? a working python snippet is: this returns correct results, so the actual connection works and also the credentials.
563d43cca4387b6f44e92372	X	This is the code I'm using to return a list of files in a "directory" in my bucket and I know it definitely works. I says directory but actually there isn't such thing. My understanding of S3 is each file/folder is an object. Each object has a key. Key determines where in the tree you will see a folder or file. A key Folder1 I believe will be a Folder called Folder1 at the route. An object with a key Folder1/File1.txt would be a file in Folder1. If other clever people have more to say or corrections, I'm sure they will tell me. But, the code does work.
563d43cca4387b6f44e92373	X	After using the given answers as a new base for research I figured out, that I have to give a serviceurl, a regionendpoint and a communicationprotocol for the S3Config Class on the one side and, because I knew the exact name of the file within the bucket, I needed to use getobject and not an access to the bucket. so the code that got me working is:
563d43cca4387b6f44e92374	X	Sounds promising, will check it out!
563d43cca4387b6f44e92375	X	Is there a way (API call) to know the current time on an Amazon S3 server? Here is a bit of background to explain why I need this: I have an iphone app that sometimes has to download a set of files from a bucket on a Amazon AWS S3 account. Between two such downloads, the server files may be modified by a CMS (Web Content Management System), or not. So, when a second download occurs, The client app tries to be efficient by downloading only the files that have been modified on the server since the previous such download. To achieve this, the app stores the date of the last download and when a new download occurs, it just focuses on the files that have been modified on the server since the date of the last download (using there “modified date” property accessible using the SDK listObjects() function). The problem with this is that the date on the phone and the modified dates on the s3 server may not be compatible. The phone user may have changed his phone date & time settings, etc. To make this work, the saved “last download date” should come from an Amazon S3 API call to make sure all dates used by the app logic are in sync. Is there such thing? Or maybe an alternative or a workaround?
563d43cda4387b6f44e92376	X	You could use a file hash instead of the modified date. An Amazon S3Object has an etag property that is indeed such kind of hash. You retrieve this property the same way as you access date. Have your client device save this hash along with the file. The next time you connect to the server, ask for the etag using the method about and compare the returned value to your local copy. A different etag value will indicate to the client that the file has changed since the last download. This approach would be completely independent of any datetime functionality.
563d43cda4387b6f44e92377	X	Here's the source. Fork it. ?
563d43cda4387b6f44e92378	X	I have a JAR file - jets3t-0.7.4.jar, by which I can access Amazon's S3 storage. I need to modify its source code so that it accesses Ceph object storage instead. I know it can done by modfying the S3 API, but do not know how. Does anyone know how to do this? I googled for information, but didn't really find anything informative. Any help is appreciated. Thanks!
563d43cda4387b6f44e92379	X	
563d43cda4387b6f44e9237a	X	thanks for the link but at my work we use CF 8 .. any suggestions?
563d43cda4387b6f44e9237b	X	Edited answer - lo and behold there is a CFC for that :)
563d43cda4387b6f44e9237c	X	thanks for that link I'll take a look at it :)
563d43cda4387b6f44e9237d	X	I went to that site amazons3.riaforge.org was not really to useful for me, and the second link you provided comes up as "access denied". But thank you for trying to help.
563d43cda4387b6f44e9237e	X	The second links was an example of how you can just directly link to S3 files once you've uploaded them - provided you've set the proper security.
563d43cda4387b6f44e9237f	X	Also, why wasn't the RIAforge.org content useful?
563d43cea4387b6f44e92380	X	ok your right that makes sense in regards to the second link. and I'm just not understanding the information on RIAforge.org, nothing against the site, I'm just having issues trying to find the simplest method to write out a code.
563d43cea4387b6f44e92381	X	I am having a problem trying to figure out what is the proper coldfusion code to upload a simple file into amazon s3 api. Any help is much appreciated!!!
563d43cea4387b6f44e92382	X	There is a good tutorial here. You'll need CF 9.0.1 however. Prior to CF 9 you might be able to use this CFC that Barney Boisvert wrote.
563d43cea4387b6f44e92383	X	Try this CFC: http://amazons3.riaforge.org/ Also, note that you may also access your objects via: http://bucketname.s3.amazonaws.com/name-of-the-object (example)
563d43cea4387b6f44e92384	X	Have you tried something?
563d43cea4387b6f44e92385	X	yes, i am getting results but in it shows in ascending order of lastmodiffied date
563d43cea4387b6f44e92386	X	Then you should add your own code here first.
563d43cea4387b6f44e92387	X	There is not much to add as it is just an api call, I have added above.
563d43cea4387b6f44e92388	X	You can do it manually. Getting no reference in the docs.
563d43cea4387b6f44e92389	X	I need to list objects from Amazon s3 in order such that latest uploaded objects should be listed on top ? How it can be done ? There is not option to sort it above ? Below is my code, Below is my output, If you see LastModified 'LastModified' => string '2010-10-05T23:00:50.000Z' is displayed first and then 'LastModified' => string '2010-10-06T23:00:50.000Z' How do I sort it in descending order of LastModified ?
563d43cea4387b6f44e9238a	X	I'm trying to upload a file to my s3 using the php sdk and for each file I'm setting the ConentDisposition and ContentType just as the documentation says (http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html#_putObject), but after uploading I look at the http header for the file and the only thing set is Content-Type and that's set to the default 'octet-stream':
563d43cfa4387b6f44e9238b	X	Sounds as if the OP is looking to become your competition then, rather than opting in to your service ;S
563d43cfa4387b6f44e9238c	X	I would like to implement a cloud storage service with the same interface of OpenStack Swift or Amazon S3. In other words, my cloud storage service should expose the same API of the above-mentioned services but with a custom implementation. This way, a client will be able to interoperate with my service without changing its implementation. I was wondering if there is an easier approach than manually implementing such interfaces starting from the documentation: http://docs.openstack.org/api/openstack-object-storage/1.0/content/ http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html For instance, it would be nice if there was a "skeleton" of OpenStack Swift or Amazon S3 APIs from which I can start implementing my service. Thanks
563d43cfa4387b6f44e9238d	X	I found exactly what I was looking for: These tools emulate most of Amazon S3 API. They are meant for development and test purposes but in my case I can use them as a starting point for implementing my cloud storage service.
563d43cfa4387b6f44e9238e	X	Someone has done this for you, try jcloud, it supports AWS S3 and swift http://jclouds.apache.org/guides/providers/
563d43cfa4387b6f44e9238f	X	If you are looking for an enterprise / carrier grade object storage software solution, look at Cloudian http://www.cloudian.com. Cloudian's software delivers a fully Amazon S3 compliant API, meaning that it delivers the broadest range of S3 feature coverage and 100% fidelity with the AWS S3 API. The software comes with a Free 10TB license, so pretty much it is free up to 10TB of managed storage, after that it is reasonably priced. You can install the software in any x86 hardware running Linux. Cloudian does not support the Swift API though. [Disclaimer: I work for Cloudian]
563d43cfa4387b6f44e92390	X	I would recommend using Swift (Openstack object store ) which also supports S3 API Take a look at the following link: http://docs.openstack.org/grizzly/openstack-object-storage/admin/content/configuring-openstack-object-storage-with-s3_api.html This way you can work with openstack swift or Amazon S3
563d43cfa4387b6f44e92391	X	Another option is libcloud, it is a python abstraction that supports a number of providers (including S3 and Swift): https://libcloud.readthedocs.org/en/latest/storage/index.html http://libcloud.apache.org/
563d43cfa4387b6f44e92392	X	Is there any way to set the file permission at the time of uploading files through Amazon S3 API. In my current solution i am able to upload the file on my bucket but i can not view the file through the URL which is mentioned in the file properties section. It is opening when i am passing access keys in query string. Is there any settings required in Amazon S3 or i need to set the permissions to all the file at the time of upload. Thanks in Advance. Kamal Kant Pansari
563d43cfa4387b6f44e92393	X	Add a header to your PUT request: x-amz-acl: public-read
563d43d0a4387b6f44e92394	X	You can also use Bucket Policies feature. Here is an example of bucket policy that instructs amazon s3 to make all of the files publicly available, including new files you will upload: Replace your.bucket.name with your actual bucket name You can view and edit Bucket Policies with S3 Browser Freeware. You can find more Bucket Policies examples here.
563d43d0a4387b6f44e92395	X	In C# when you create a response object of >mazon then in response method you will find Addheader. You need to set header as Amazon providing these methods in its web services API kindly refer that.
563d43d0a4387b6f44e92396	X	Do you have code and a publicly accessible URL that could be used for testing?
563d43d0a4387b6f44e92397	X	I found one of similar construction. See if the combination of reading directly from the GET value and using colClasses= improves performance.
563d43d1a4387b6f44e92398	X	You can split on "\\r\\n" instead if it's going to return the Windows line endings as in the example.
563d43d1a4387b6f44e92399	X	Hmm, thanks for that, is there a way to quickly remove \r bit too?...but eitherway the file is quite large >75MB and so data transforms from character to data.frame like that seem to take a long time....so its not the ideal solution at the moment, given that the s3 data has already been uploaded as a csv file...am still hoping for the parameter values to adjust the API request to just download the data.
563d43d1a4387b6f44e9239a	X	Replace strsplit( test, "\\n" ) with strsplit( test, "\\r\\n" ), or just `gsub( "\\r", "", test) before you run any of the other code. I'm not sure what you mean by "just download the data," as it seems to me that what it gave you is the data, in comma-separated form.
563d43d1a4387b6f44e9239b	X	perhaps I should have said "just download the file" rather than getting the data in comma separated form...from the get request...
563d43d1a4387b6f44e9239c	X	So much better than my attempts to reinvent the wheel. Likely faster too.
563d43d1a4387b6f44e9239d	X	that simple solution works out quite well actually...but now the biggest holdup appears to be at the conversion of the response to the character string...using content...the code I am currently using is x <- GET(end.point, add_headers(Date=time.string,Authorization=authorization.string), query=params) y <- read.csv(text=content(x)) any ideas on how to speed that up?
563d43d1a4387b6f44e9239e	X	My guess is that the server response is quite a bit slower than the read.csv step. Have you profiled it with system.time? (Also: Using colClasses is known to speed up all read.* functions.)
563d43d1a4387b6f44e9239f	X	I would like to be able to download a .csv file from my Amazon S3 bucket using R. I have started using the API that is documented here http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectGET.html I am using the package httr to create the GET request, I just need to work out what the correct parameters are to be able to download the relevant file. I have set the response-content-type to text/csv as I know its a .csv file I hope to download...but the response I get is as follows: And no file is downloaded and the data seems to be in the response...I can extract the string of characters that is created in the response, which represents the data, and I guess with some effort it can be converted into a data.frame as originally desired, but is there a better way of downloading the data...straight from the GET command, and then using read.csv to read the data? I think that it is a parameter issues...just not sure what parameters need to be set for the file to be downloaded. If people suggest the conversion of the string...This is the structure of the string I have...what commands would I need to do to convert it into a data.frame? Thanks HLM
563d43d1a4387b6f44e923a0	X	Here's one way: Now convert to a data.frame:
563d43d1a4387b6f44e923a1	X	The answer to your second question: If you want extra speed for the read.csv, try this: Assuming the URL is set up properly (and we have nothing to test this on yet) I'm wondering if you may want to look at the value for GET( ...)$content Perhaps: That was not correct because the data comes across as "raw" format. One needs to convert from raw before it will become encoded as text. I did a quick search of Nabble (it must be good for something after all) to find a csv file that was residing on the Web. This is what finally worked:
563d43d1a4387b6f44e923a2	X	Go for the SDK. I'm using C++ (which doesn't have an SDK) and I go to the extreme of using embedded python and the python SDK rather than using the REST API. There's just no point spending time implementing, debugging and maintaining something which is already done. Also Amazon's REST API is badly documented and often not intuitive.
563d43d2a4387b6f44e923a3	X	I think it's very valuable to be able to use a little bit of the REST API directly, so you understand what's going on. But I would use the SDK for real work.
563d43d2a4387b6f44e923a4	X	Thanks guys for your suggestion. I would be going for SDK only.
563d43d2a4387b6f44e923a5	X	Thanks Sid for your answer!
563d43d2a4387b6f44e923a6	X	I have to use Amazon S3 to upload some static contents programmatically in Java. When I started reading I found that the way to do is either through their SDK (wrapper over REST APIs) or using REST APIs. From Amazon's AWS website, found this: "You can send requests to Amazon S3 using the REST API or the AWS SDK". Wanted to understand that which approach is better. I think using SDK will definitely make programming easier, but what are the pros and cons of using SDK Vs Rest APIs directly. For some reason, I found using REST API directly more difficult than SDK. I was able to do basic things using SDk - create bucket, list objects, get object, invalidate cache etc. But was having some hard time writing the code for REST API - especially generating the signature. May be it will not matter much, if I ultimately use SDK, but I would still like to know how to do it using REST APIs. If anyone has some good code examples in Java on adding objects, get objects, get list etc, it would be very helpful. Thanks!
563d43d2a4387b6f44e923a7	X	If you already have the SDK in your language, go for it; it's a no-brainer from a project perspective. The SDK is additional engineering that they have already done and tested for you at no additional cost. In other words, the SDK is already converting the HTTP REST API into the application domain/language for you. Think of the REST API as a the lowest common denominator that AWS must support and that the SDK (likely) as implementing the REST API below it. In some cases (eg: some Windows Azure SDKs), the SDKs are actually more efficient because they switch to a TCP based communications channel instead of REST (which is TCP plus HTTP), which eliminate some of the HTTP overhead Unless your entire goal is to spend additional time (development + testing) just to learn the underlying REST API, I'd vote SDK.
563d43d2a4387b6f44e923a8	X	What are you passing in for an Authorization header?
563d43d2a4387b6f44e923a9	X	Thanks @Jason. Header is 1. Connection Request Host: s3.amazonaws.com Date: x-amz-content-sha256:e3b855 Authorization: AWS4-HMAC-SHA256 Credential=XXX/20150618/us-east-1/s3/aws4_request, SignedHeaders=date;host;x-amz-content-sha256, Signature=ec7518 Canonical Request: GET 2. For Getting the Contents Host: balas3bucke01.s3-ap-southeast-1.amazonaws.com Date: x-amz-content-sha256:e3b*855 Authorization: AWS4-HMAC-SHA256 Credential=XXX/20150618/ap-southeast-1/balas3bucke01/aws4_request, SignedHeaders=date;host;x-amz-content-sha256, Signature=fd**1429 Please let me know where am i going wrong
563d43d2a4387b6f44e923aa	X	I am using Amazon S3 REST API for listing the contents of my bucket. I am able to establish a connection and get the list of my buckets with request URL being "https://s3.amazonaws.com" and http_request_type = "GET". However when I try to list the contents of the bucket I am getting an error AuthorizationHeaderMalformed The authorization header is malformed; incorrect service "balas3bucke01". This endpoint belongs to "s3". balas3bucke01 is the name of the bucket. My request URL is https://balas3bucke01.s3-ap-southeast-1.amazonaws.com http_request_type = "GET" Why am I getting the above error.
563d43d2a4387b6f44e923ab	X	Hi do you have sample code for file uploading to amazon s3 using REST API in java..Please send me if you have it....
563d43d2a4387b6f44e923ac	X	I'm trying to implement an HTML5 Amazon S3 uploader (by using the REST API), and stumbled upon the following issue: when trying to upload a small, text file, everything works like a charm. When trying to upload a binary file, the file gets bigger on S3, and, obviously, corrupted. Here's what I'm doing: Also, I've tried to create a 10mb file with text (10 million lines of 0123456789) and that one works correctly. If anyone has a solution to this problem, or stumbled upon it, let me know.
563d43d2a4387b6f44e923ad	X	It seems StackOverflow is also good for figuring things out yourself -- I've fixed it just as I finished putting my ideas down. It seems the xhr.send() method can receive the file.slice() blob directly, so no need for FileReader. I hope this helps other people that stumble upon this issue.
563d43d3a4387b6f44e923ae	X	I am facing some problems with the thingiverse api at uploading images to the amazon s3 storage. At step 3 of the file upload guide, amazon always answers with {"error":"Unauthorized"}. Do you have any hints for me, what I might be doing wrong? This is what i have done: Send a POST request to http://api.thingiverse.com/things/629436/copies/ with content: The response is: So I send a request to https://thingiverse-production-new.s3.amazonaws.com/ with body: The response I get is: {"error":"Unauthorized"}. Am I missing any authentication fields? I also tried altering the order of the multipart/formdata parameters. I tried the one i got from the thingiverse api respone as well as the on in the file upload guide. Any help appreciated!
563d43d3a4387b6f44e923af	X	This class is NOT from Amazon. This is a third-party class.
563d43d3a4387b6f44e923b0	X	I have a website hosted on amazon. I want my clients to give access to upload files that are already in their amazon s3 space to my s3 space. Is there any php API that supports this functionality?
563d43d3a4387b6f44e923b1	X	Amazon actually provides one. And there are lots of examples on the web of using it. Google is your friend.
563d43d3a4387b6f44e923b2	X	Amazon providing one PHP API for uploading files to s3 bucket. its a single php file named s3.php You just download that and from your code . for more read this.
563d43d3a4387b6f44e923b3	X	Why does it not look possible? It is just a REST API after all.
563d43d4a4387b6f44e923b4	X	To the best of my knowledge the only Cocoa/Cocoa Touch based toolkit for accessing S3 is ConnectionKit. Otherwise you are stuck with building your own, which could become quite a complex task. May I ask which bits of the API you require? The current release of ConnectionKit does support S3 but is a bit ropey. We're currently in the process of writing the 2.0 version. If we have someone to work with specifically for one protocol, we could focus purely on that for now to our mutual benefit. Please contact me at mikeabdullah.net/other/contact_me.html for further discussion if interested
563d43d4a4387b6f44e923b5	X	The reason why it appears not possible is that each authenticated request to S3 servers needs to have a RFC 2104HMAC-SHA1 signature generated and sent along with the request. To my knowledge, there is currently no way to do this on the iPhone. Am I incorrect or just missing something?
563d43d4a4387b6f44e923b6	X	On the iPhone there is CCHMAC(3) which offers the required functionality
563d43d4a4387b6f44e923b7	X	Thanks Mike. No sooner did I write my last comment I came across that exact library. I hadn't realized it was in there. That's exactly what I am going with. Thank you again!
563d43d4a4387b6f44e923b8	X	I think you're a little confused. NSConnection is for Distributed Objects. Very different to NSURLConnection!
563d43d4a4387b6f44e923b9	X	Yes, you're right, I meant NSURLConnection. Will update answer.
563d43d4a4387b6f44e923ba	X	Does anyone have any suggestions for GETing from or POSTing to Amazon S3 services using their REST API via the iPhone. It does not look like it is possible but I may be reading the docs wrong. Thank you in advance for your help! L.
563d43d4a4387b6f44e923bb	X	In a general case I'd recommend to use ASIHttpRequest, it has a lot of built-in functionality (REST compatible too) and a lot of things, making life easier than with NSURLConnection. It also has S3 support out of box.
563d43d4a4387b6f44e923bc	X	You should be able to use the NSURLRequest stuff to do what you want. This doesn't have any error checking in it and the _data variable should be stored in an instance variable, but the general idea should work for you. You will probably also need to set some request headers to tell the server what encoding the body data is in and so on.
563d43d4a4387b6f44e923bd	X	Thank you vey much for the response, there might be one more approach to communication the keys with the iphone online every time with some encryption, but again when its going to the client there is no way guarantee of its security. As it is said: "The only secure computer is one that's unplugged, locked in a safe, and buried 20 feet under the ground in a secret location... and I'm not even too sure about that one"
563d43d5a4387b6f44e923be	X	I'm able to upload files from iPhone using ASIHTTPRequest wrapper for an application which allows simple storage to my account. The question i'm concerned about is, could distributing the access keys along with the application be a good idea? what is the best way to deal with it in terms of security? are the keys i use sniffable via monitors over https? any suggestions over it will be appreciated.
563d43d5a4387b6f44e923bf	X	I upload files to a server (using ASIHTTPRequest) and then from the server to an AWS account for this very reason. I can control the security on the server much easier than I can on devices. Plus, if I need to change the keys I can do it on the server very quickly. This will add another layer to your application but I think it's well worth it. You can also check out this post Architectural and design question about uploading photos from iPhone app and S3
563d43d5a4387b6f44e923c0	X	Uploading a file directly to S3 is not really a trivial task, especially if you want to support chunking, auto-resume, user metadata, etc, etc. The policy stuff can be quite complex. Consider using a library I maintain: Fine Uploader. It has native support for direct uploads to S3 in all browsers, even IE7. Chunking and auto-resume, among other features, are also supported. Furthermore, I wrote a node.js server-side example myself that, when paired with Fine Uploader S3, will handle all signatures for you.
563d43d5a4387b6f44e923c1	X	can you post this comment as an answer? i may end up using your library. still evaluating how it works, etc.
563d43d5a4387b6f44e923c2	X	I'm not sure that will go over well. It may be considered a poor or link-only answer, quite frankly. My understanding is that the community is looking for details answers that include code, and mine doesn't fit that description, which is why I posted it as a comment. If you do have any questions about Fine Uploader, have a look at the fine-uploader tag on SO though, where we handle support questions for the library.
563d43d5a4387b6f44e923c3	X	THANK YOU! This code helped me out. Some quick comments: To format the date I used moment.js like so : moment.utc(expirationDate).format('YYYY-MM-DD')+'T'+moment.utc(expirationDate).‌​format('HH:mm:ss.SSS')+'Z'. Also for buffers 'utf8' (note: no hyphen) is default encoding so I think "utf-8" is incorrect and extraneous.
563d43d5a4387b6f44e923c4	X	@Zugwalt, you could simplify that quite a bit with moment's built in formatting. moment.utc(expirationDate).toISOString()
563d43d5a4387b6f44e923c5	X	@Jonathan even better! Thanks!
563d43d5a4387b6f44e923c6	X	I'm trying to get an built that allows users to upload a file directly to my Amazon S3 bucket, from a NodeJS powered website. It seems the only tutorials out there, other than the actual amazon docs for this are all very out of date. I've been following this tutorial, for the basic info, but again it's out dated. It doesn't have the method calls to crypto correct, as it tries to pass a raw JavaScript object to the update method, which throws an error because it's not a string or buffer. I've also been looking at the source for the knox npm package. It doesn't have POST support built in - which I totally understand, because it's the browser doing the POST once it has the right fields. Knox does appear to have the right code to sign a policy, and I've tried to get my code working based on this... but again to no avail. Here is what I've come up with, for code. It produces a base64 encoded policy, and it creates a signature... but it's the wrong signature according to Amazon, when I try to do a file upload. I'm obviously doing something wrong, here. But I have no idea what. Can anyone help identify what I'm doing wrong? Where my problem is? Does anyone have a working tutorial for how to generate a proper Amazon S3 Policy, with signature, from NodeJS v0.10.x, for a POST to the s3 REST api?
563d43d6a4387b6f44e923c7	X	Ok, I finally figured it out. After playing the random guessing game for a VERY long time, I thought to myself "maybe i need to sign the base64 encoded policy" - me and BAM that was it. I also re-ordered the conditions to match how the form is posting, though I'm not sure this makes a difference. Hopefully this will help others that run in to the same problem.
563d43d6a4387b6f44e923c8	X	I modified a bit previous example, because it didn't work for me: amazon returned an error about broken signature. Here is how the signature should be created for Browser-Based Uploads Using POST (AWS Signature Version 4) http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-authentication-HTTPPOST.html  Next generated base64Policy and s3Signature i used in the form for uploading. Example is here: http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html Very important is to check that you have the same fields and values in the html form and in your policy.
563d43d6a4387b6f44e923c9	X	why you add fileOwnerId to stringToSign?
563d43d6a4387b6f44e923ca	X	Thanks for your reply @okwap . I have added that because thats the way i have saved files on s3. like bucketname/username/file.txt
563d43d6a4387b6f44e923cb	X	I am trying to delete amazon s3 object using rest API but not getting any success. I have created the URL(signed url) at server side using java and then made XHR request to that URL at client side(i.e. from browser). Java code that i have used to sign the url: And at client side: Using this code for downloading an object from amazon s3 bucket works fine by replacing 'DELETE' request with 'GET'. But delete is not working. I have searched a lot but there is very less help available for rest API.
563d43d6a4387b6f44e923cc	X	Finally, i integrated the aws sdk to delete the object from amazon s3 bucket and it works like lightning. But unable to get help doing it with rest API. So now i have used rest API for uploading and downloading and the sdk for deleting an object.
563d43d6a4387b6f44e923cd	X	moved to Amazon EC2 and the connection speed DRAMATICALLY increased . Though this still alludes me as to why it would be so slow on a non EC2 instance
563d43d6a4387b6f44e923ce	X	Because their operate on the same network (your Ec2 instance and S3)
563d43d6a4387b6f44e923cf	X	Been trying to figure out why uploading to Amazon S3 is amazingly slow using the putObject command (node.js library). The code below reads an entire directory of files and puts them to S3 asynchronously. Tested with a number of different folders with similar results. Uploading the same files using the AWS web interface takes around 3 sec to complete (or less). Why is using the node.js API so slow?? As per Amazon documentation I've even tried spawning multiple children to handle each upload independently. No changes in upload speed.
563d43d6a4387b6f44e923d0	X	Thank you @Lucas Polonio. I heard about this gem, but I wanted to use the aws sdk.
563d43d6a4387b6f44e923d1	X	Im developing a website with AngularJS in frontend that sends requests to a Rails 4 API backend. I have to manage quite images, so I would like to use Amazon S3 (but Im newbie with this and Im a bit lost). Before using S3, I used an angular directive to upload images to Rails. Rails got this image and stored it in a path in the server. Something like this: where photo is the image uploaded to rails: Im trying to do the same but instead of storing the photo in the Rails server, I would like to do it in S3. Im doing something like this (but I recognize, I dont completely understand how it works, so for sure something is wrong). This is my code with S3: Im getting this error: Im confused with the concepts of key and file name. Is the key the path where I would like to store my image in S3?
563d43d6a4387b6f44e923d2	X	It works. I had just need to replace the obj.write(Pathname.new(key)) with obj.write(photo)
563d43d7a4387b6f44e923d3	X	Good that your solution worked. Anyway, you could take a look at the paperclip gem. It handles file uploads with lots of features, including automatically uploading to S3.
563d43d7a4387b6f44e923d4	X	Can I send Large file with it?Not Archive just single file
563d43d7a4387b6f44e923d5	X	Yes, I see no difference between archives and other file types. The only difference comparing to your multipart-upload may be that if something happens (connection lost) during upload - you have to start from scratch. But that's a rare case and was completely fine for us (retry policy handled this).
563d43d7a4387b6f44e923d6	X	Thank you so much
563d43d7a4387b6f44e923d7	X	you're a lifesaver!
563d43d7a4387b6f44e923d8	X	I am using Amazon S3 Low Level API for uploading Large Video File, I am following This link When I am upoading the file, its giving me exception I have checked Inner Exception and its saying this at this line and this is how I am making my S3Client I also tried changing bucketname like bucketname/filename.mp4,but its giving exception I also tried some other file(doc and pdf) it is also giving XML exception. Is there any good alternate approach for uploading Large Video files(Around 200-500MB)?
563d43d7a4387b6f44e923d9	X	I used to send archives to S3 (around 100-300MB). My code looked like this: That's it basically. I had retry-policy and exception handling around that, but this is the core. So just simple PutObject function without any multipart uploads works find for such file-sizes.
563d43d7a4387b6f44e923da	X	I've found a similar problem during a MultiPart upload using the sample code in the doc. I've found that the ETag list is mandatory for the CompleteMultipartUpload part - which is not in the documentation sample. This link has a better explanation of the multi-part upload process: s3 multipart upload
563d43d7a4387b6f44e923db	X	I am looking to upload images to amazon S3 using the rest api that they provided. I got to know how to calculate the signing key for SigV4 from this document. This documentation tells you how the request should be signed. But I find it highly confusing as to what should be signed and where should the cannonical request be placed? Should it be placed in a separate header in the request? Is there a working example/sample to use SigV4 rest api using java?
563d43d8a4387b6f44e923dc	X	If you have a very specific reason for not using the provided SDK, the quickest path to getting this working it to look at how the requests are performed in a library where this is already working. You can look at the Java SDK itself to figure this out, but that's a bit dense. Here is my favorite, although I think it's on sig v3: http://geek.co.il/2014/05/26/script-day-upload-files-to-amazon-s3-using-bash You can find out similar examples for v4: http://geek.co.il/2014/11/19/script-day-amazon-aws-signature-version-4#footnote_0_33255 You can see how everything is compute and what is to be passed in the headers in very few lines of code. EDIT Look at http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-examples-using-sdks.html#sig-v4-examples-using-sdk-java for exactly what you are looking for. It has the bare minimum to get this going in java.
563d43d8a4387b6f44e923dd	X	Are you sure the bucket and key are correct?
563d43d8a4387b6f44e923de	X	Did you find a solution for this? I have the exact same problem
563d43d8a4387b6f44e923df	X	I'm using the s3_direct_upload gem to store images and videos on Amazon s3. When the image or video is changed or deleted, I want to nuke the old image or video on s3 and save everyone money and space. This solution uses the V1 Aws SDK and is no longer valid: http://blog.littleblimp.com/post/53942611764/direct-uploads-to-s3-with-rails-paperclip-and This solution deletes files that were initially uploaded in a batch, but does nothing for the final files post-processing: github - waynehoover/s3_direct_upload Here is the Aws v2 SDK doc, which seems clear enough: http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Client.html#delete_object-instance_method Yet this solution: ...returns only: (And the file is still available on s3 at the original url.) Thoughts? Hasn't everyone had to do this?
563d43d8a4387b6f44e923e0	X	I've written code using the v2 SDK to delete objects from S3. Here is a sample from my codebase: It looks similar to yours, so I don't think that this code is the issue. Have you confirmed your bucket & key names and ensured that your method is actually being called?
563d43d8a4387b6f44e923e1	X	What is s3 galcier? I know s3 and I know glacier but what is s3 glacier?
563d43d8a4387b6f44e923e2	X	Python package boto is offering such an API too: boto.readthedocs.org/en/latest/ref/glacier.html
563d43d8a4387b6f44e923e3	X	I want to list all the object on amazon glacier. So that i can restore require object from glacier. Is there any amazon api to list all object on glacier.
563d43d8a4387b6f44e923e4	X	Check this: http://docs.aws.amazon.com/amazonglacier/latest/dev/using-aws-sdk.html These APIs are only supported via AWS JAVA SDK and .NET SDK. And also check the Glacier API Documentation: http://docs.aws.amazon.com/amazonglacier/latest/dev/amazon-glacier-api.html
563d43d9a4387b6f44e923e5	X	Can you script it with awscli or s3cmd, rather than write it in Java? Using Java seems heavy-handed here.
563d43d9a4387b6f44e923e6	X	The things haven't changed in this regard. People have developed libraries that make use of the s3 apis and parallelize the uploads.
563d43d9a4387b6f44e923e7	X	@TJ- Can you provide an example?
563d43d9a4387b6f44e923e8	X	github.com/tj---/s3-parallel
563d43d9a4387b6f44e923e9	X	does it spawn n upload processes performed in parallel or does it spawn a single upload process for all of the objects (therefore needing only one connection)? I hope it's the latter
563d43d9a4387b6f44e923ea	X	It performs N independent uploads - how many will be executed at a time depends on what kind of ExecutorService you pass to the constructor. S3 does not expose a way to upload multiple objects in a single HTTP request besides manually zipping them up. And even then you'd probably want to do a multi-part upload and split the zip over multiple HTTP requests so if there's a transient failure halfway through you don't have to start the whole upload over from scratch...
563d43d9a4387b6f44e923eb	X	We're looking to begin using S3 for some of our storage needs and I'm looking for a way to perform a batch upload of 'N' files. I've already written code using the Java API to perform single file uploads, but is there a way to provide a list of files to pass to an S3 bucket? I did look at the following question is-it-possible-to-perform-a-batch-upload-to-amazon-s3, but it is from two years ago and I'm curious if the situation has changed at all. I can't seem to find a way to do this in code. What we'd like to do is to be able to set up an internal job (probably using scheduled tasking in Spring) to transition groups of files every night. I'd like to have a way to do this rather than just looping over them and doing a put request for each one, or having to zip batches up to place on S3.
563d43d9a4387b6f44e923ec	X	The easiest way to go if you're using the AWS SDK for Java is the TransferManager. Its uploadFileList method takes a list of files and uploads them to S3 in parallel, or uploadDirectory will upload all the files in a local directory.
563d43d9a4387b6f44e923ed	X	This was giving me a major head-ache, so I thought I'd post the easy solution. My issue was that when using the Java API for Amazon's S3, I could only download 50 objects before it would mysteriously time out. The code looked something like this: It would run and process everything fine for exactly 50 objects, and then time out.
563d43d9a4387b6f44e923ee	X	For whatever reason, the main issue is that I had declared s3 as AmazonS3Client s3. It should have looked like: Just in case anyone else runs into this problem.
563d43d9a4387b6f44e923ef	X	Hopefully while you may callling getObject to download it, you are not closing the InputStream. which is optioned by calling getObject(); you have to close InputStream after dealing with each object. more details read it : http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html#getObject(com.amazonaws.services.s3.model.GetObjectRequest) Thanks
563d43d9a4387b6f44e923f0	X	For Scala developers, here it is recursive function to execute a full scan and map of the contents of an AmazonS3 bucket using the official AWS SDK for Java To invoke the above curried map() function, simply pass the already constructed (and properly initialized) AmazonS3Client object (refer to the official AWS SDK for Java API Reference), the bucket name and the prefix name in the first parameter list. Also pass the function f() you want to apply to map each object summary in the second parameter list. For example will return the full list of (key, owner, size) tuples in that bucket/prefix or will return the total size of its content (note the additional sum() folding function applied at the end of the expression ;-) You can combine map() with many other functions as you would normally approach by Monads in Functional Programming
563d43daa4387b6f44e923f1	X	There's now Fake S3 as well.
563d43daa4387b6f44e923f2	X	+1 for Fake S3 - that was the easiest replacement in my project. Two lines of code changed and voila - we support local fileserver. License also looks good.
563d43daa4387b6f44e923f3	X	S3ninja does not currently provide folders. This is a deal breaker for me.
563d43daa4387b6f44e923f4	X	Is SwiftFS compatible to Amazon S3? My code base works already with S3 and I'm looking for a solution which is compatible to S3 so that I don't need change my code.
563d43daa4387b6f44e923f5	X	No, SwiftFS isn't compatible to Amazon S3, but you could take a look at my other project: RioFS github.com/skoobe/riofs which is a userspace filesystem which operates with Amazon S3 buckets.
563d43daa4387b6f44e923f6	X	We make distributal software that stores some data (attachments) in a) a database or b) Amazon S3. The database is used because it requires no other configuration. Amazon S3 is the better option. What we want now is a solution for customers that don't want to use Amazon S3. We can obviously just use the filesystem but this can be problematic if there are multiple web servers and the files need to be replicated; it also requires us to write extra code to handle the various permuations of problems that can happen. What we would prefere is if there was a piece of server software that essentially replicates Amazon S3's API. That way our clients can install the server on a box; and we don't need to change any code. So ... is there any such software out there?
563d43daa4387b6f44e923f7	X	This is possible via OpenStack Object Storage (code-named Swift), which is open source software for creating redundant, scalable object storage using clusters of standardized servers, specifically its recently added (optional) S3 API layer, which emulates the S3 REST API on top of Object Storage. See Configuring Object Storage with the S3 API for the official documentation - a more insightful and illustrated small tutorial regarding the entire setup is available in S3 APIs on OpenStack Swift (which builds on the more complex Installing an OpenStack Swift cluster on EC2 though). An noteworthy alternative is Ceph, which is a unified, distributed storage system designed for excellent performance, reliability and scalability - interestingly it provides all three common storage models, i.e. Object Storage, Block Storage and a File System and the RADOS Gateway provides Amazon S3 and OpenStack Swift compatible interfaces to the RADOS object store [emphasis mine], see RADOS S3 API for details on currently supported S3 API features.
563d43daa4387b6f44e923f8	X	Have you looked at Cloudian? We use it internally at our company to develop our S3 app. I'm using the Community Edition which is free for up to 10TB of storage. It's got pretty good S3 coverage or at least covers most of the stuff my app uses (I use versioning and multipart uploads so I think my app is advanced). The version-ids and multipart ids etc that it generates are different than those you get from AWS but boto has no complaints so far. It also works with s3fs and other s3 bucket browsers that I have tried. In my opinion it's a good tool for development against the AWS S3 API and should meet your requirements. You can point your app at your local Cloudian server and then when you are ready for production you can point it back at Amazon. Your mileage may vary... Good luck.
563d43dba4387b6f44e923f9	X	We ran into the problem of testing our S3 based code locally and actually implemented a small Java server, which emulates the S3 object API. As it might be useful to others, we setup a github repo along with a small website: http://s3ninja.net - all OpenSource under the MIT license. Being quite simple and minimalistic, this tool is perfect for testing and developement purposes. However, to use in in production, one might want to add some security (altough the AWS hashes are already verified in the API - just the GUI is completely unprotected). Also, it doesn't do any replication or scaling. So this wouldn't be a good choice for large setups.
563d43dba4387b6f44e923fa	X	As it was already mentioned: you could try to use Swift as Amazon S3 alternative. Take a look at SwiftFS filesystem, it let you mount OpenStack container stored in Swift as a local filesystem.
563d43dba4387b6f44e923fb	X	I recently started using Skylable for my S3 needs, it's free (GPL). Their object storage supports replication, HA and deduplication and it's fully S3 compatible. You can run their software on a single server (iron, virtual machine or container) if you don't need redundancy or you can use more nodes if you need HA. The number of replicas can be chosen per bucket, just like with Swift. I started with 2 nodes in replica 2 and added more nodes as our userbase started growing, to cope with the extra network traffic and the space requirements. Adding more nodes is really easy and can be done on a live cluster. In my experience Skylable proved to be faster and more reliable than Swift. It's written in C and OCaml, it's not interpreted. The memory footprint is really low, so I can run a node even on some cheap VPS. Recently they announced to be working on Swift APIs, apparently their goal is to replace Swift.
563d43dba4387b6f44e923fc	X	Hi minjoon, EBS is not something to consider if you want to scale. Consider EBS as a pendrive, you can attach to a single host, EBS don't work as NAS or S3.
563d43dba4387b6f44e923fd	X	Did you found the best solution minjoon? I'm in the Middle of the same decision of what is a better solution. Cheers
563d43dba4387b6f44e923fe	X	I dont think s3fs can be used in production.
563d43dba4387b6f44e923ff	X	I will be launching an application in the very near future which will, in part, require users to upload files (images) to be viewed by other members. I like the idea of S3 as it is relatively cheap and scales automatically. My problem is how I will have users upload their images to S3. It seems there are a few options. 1- Use the php REST API. The only problem is that I can't get it to work for uploading variously scaled versions (ie thumbnails) of the same image simultaneously and uploading them directly to s3 (it works for just one image at a time this way). Overall, it just seems less flexible. http://net.tutsplus.com/tutorials/php/how-to-use-amazon-s3-php-to-dynamically-store-and-manage-files-with-ease/ 2- The other option would be to mount an S3 bucket with s3fs. Then just programmatically move my images into the bucket like I would with NFS. From what I've read, it seems some people are dubious of the reliability of mounting S3. Is this true? http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=fuse+over+amazon Which method would be better for maximum reliability and speed? Would EBS be something to consider? I would really like to have a dedicated box rather than use an EC2 instance, though...
563d43dba4387b6f44e92400	X	For your use case I recommend to use the S3 API directly rather than using s3fs because of performance. Remember that s3fs is just another layer on top of S3's API and it's usage of that API is not always the best one for your application. To handle the creation of thumbnails, I recommend to decouple that from the main upload process by using Amazon Simple Queue Service. That way your users will receive a response as soon as a file is uploaded without having to wait for it to be processed resulting in shorter response times. As for using EBS, that is a different scenario. EBS is just a persistent storage for and Amazon EC2 instance and it's reliability doesn't compare with S3. It's also important to remember that S3 only offers "eventual consistency" as opposed to a physical HDD on your machine or an EBS instance on EC2 so you need to code your app to handle that correctly.
563d43dba4387b6f44e92401	X	Thanks! Do you know how I could make it respond with the URL of the uploaded image with a POST or GET?
563d43dda4387b6f44e92402	X	onprogress: Called with a ProgressEvent whenever a new chunk of data is transferred. (Function)
563d43dda4387b6f44e92403	X	I am developing an App where users can post photos. I have the app working well using Imageshack's API where basically all you have to do is write a HTML form with a post to imageshack and then it redirects the post to a page of your choice where I then use the received information to store a location of the image in my database. My problem is I've heard bad things about Imageshack's reliability/scaleability and I want to move to Amazon's S3. Is it possible to upload a photo to S3, then get a simple response with the location of the image that I can then store in my database via PHP? Thanks, Dan.
563d43dda4387b6f44e92404	X	Yes you can, simple you can use file transfer method of phonegap use your amazone bucket path for url. if you need to upload larger files some times phonegap filetransfer may fail so you can write some native plugin (i tried video upload to amazone s3 and its succes )
563d43dda4387b6f44e92405	X	I am currently using S3 with the Java API to get objects and their content. I've created a Cloudfront distribution using the AWS console and I set my S3 bucket with my objects as the Bucket-origin. But I didn't notice any improvement in the download performance, and I noticed in the console window the url refers to s3: INFO: Sending Request: GET https://mybucket.s3.amazonaws.com /picture.jpg Headers: (Range: bytes=5001-1049479, Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) whereas in the Getting Started guide for Cloudfront, the url should be: http://(domain name)/picture.jpg where (domain name) is specific to the Cloudfront distribution. So the Java API still is getting the file from S3 and not through cloudfront Is there anyway using the Java API for S3 to download files via Cloudfront? If not, what's the best approach I should use to get objects via cloudfront in my java program? I am still kinda new to this stuff, any help greatly appreciated!
563d43dda4387b6f44e92406	X	JAVA API for S3 can not be used for interacting with Cloudfront. If you want to download the content through cloud front distribution, you have to write your own HTTP code (which should be simple). You can also just use http://(cloud front domain name)/picture.jpg in browser and check the download speed first.
563d43dda4387b6f44e92407	X	But, you should know that it can take 24 hours or more for changes in S3 to be active. If you cannot open the stream, the other way is to use getObject(bucketName, key) method.
563d43dda4387b6f44e92408	X	Why not use the sdk if it works?
563d43dda4387b6f44e92409	X	I'm hoping to put it into an SSIS script task
563d43dda4387b6f44e9240a	X	Thanks for your suggestion. I made the modification but unfortunately it still doesn't work.
563d43dda4387b6f44e9240b	X	Is the 403 message returning an error that gives you the string to sign and tells you the signature does not match, or no? What does the error say? (in the response body)
563d43dea4387b6f44e9240c	X	The error message is stating that the signature does not match: <Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message>
563d43dea4387b6f44e9240d	X	I'm trying to get some code working to fetch a file from S3 using the REST API via C#. I've seen other people doing similar things but for some reason I keep getting a 403 error. I've tried to do the same thing with the AWS SDK for .Net and it works so I assume it's the way I'm creating the authorization header. Is anyone able to shed any light on this please?
563d43dea4387b6f44e9240e	X	I don't know if this is the only problem, but it looks like a definite problem: x-amz-date is the header that supercedes the Date: header in the HTTP request itself, but in the string to sign, you just put the date, without "x-amz-date:" or anything in front of it, according to the examples: http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html#RESTAuthenticationRequestCanonicalization There is only one correct signature that can be generated for a request. S3 is going to generate that signature, and compare it to the one you sent, so there's not a single byte of room for error in the string-to-sign.
563d43dea4387b6f44e9240f	X	I tested your code, it works! you just need an extra \n plus change http to https and you're done. Amazon Rest API don't have a good documentation, the lack of examples makes everyone go to the SDK instead.
563d43dea4387b6f44e92410	X	I want to associate a bucket in X account that is created by account Y Account Y has given read and write permissons to X on the bucket via the Email ID This was done using S3Fox - however when I log into X account I see no way to associate the external bucket. I tried entering the bucket name as usual but didnt work So I would like to code my own association via php and rest but cant find the call in the API docs - can someone send me a link or example code on how to create an external bucket in account X Thanks
563d43dea4387b6f44e92411	X	Account X has to configure bucket from Y as "external bucket". but it is worse. the external bucket feature depends on the client software X uses. so the information on wich buckets X listen to is stored at X`s client not at S3. X have to use a s3 client wich supports external buckets. Martin
563d43dea4387b6f44e92412	X	I am using the correct secret and access key..as well as I have the permission for my user to access the s3 bucket, because I am able to upload the file using php and java with same credential..But for resume and pause I need the REST API in php..so i am running the rest api code and still i am getting the signaturedoes not match error..
563d43dea4387b6f44e92413	X	I've been trying to get this to work for 1 week but always I am getting the same error. I also tried to debug the Signature function but I dnt where is the exact problem..I want to upload the file with progress bar as well as want to add the resume and pause functionality in REST-PHP. I am following the below link a :- http://www.anyexample.com/programming/php/uploading_files_to_amazon_s3_with_rest_api.xml Please provide me any proper solution. I am getting this response:- HTTP/1.1 403 Forbidden x-amz-request-id: 3B621260770DE679 x-amz-id-2: vuB+qHCRxq6CdRKIoso82GXO1O0gQNDEs5rLi3my/YiD535nyZQ6Ls64jZ5hB2KW Content-Type: application/xml Transfer-Encoding: chunked Date: Thu, 11 Dec 2014 09:01:52 GMT Connection: close Server: AmazonS3 3ef SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method.AKIAJA6EQQ475TUGTSEQPUT image/jpeg Thu, 11 Dec 2014 09:01:52 +0000 x-amz-acl:public-read /s3.regionname.amazonaws.com/bucket-name/Desss.jpgsdpF9q1WTYzHuLuytn7Dv+3xdIY=50 55 54 0a 0a 69 6d 61 67 65 2f 6a 70 65 67 0a 54 68 75 2c 20 31 31 20 44 65 63 20 32 30 31 34 20 30 39 3a 30 31 3a 35 32 20 2b 30 30 30 30 0a 78 2d 61 6d 7a 2d 61 63 6c 3a 70 75 62 6c 69 63 2d 72 65 61 64 0a 2f 73 33 2e 65 75 2d 77 65 73 74 2d 32 2e 61 6d 61 7a 6f 6e 61 77 73 2e 63 6f 6d 2f 6e 61 6e 6f 68 65 61 6c 2d 69 62 6d 2f 44 65 73 73 73 2e 6a 70 673B621260770DE679vuB+qHCRxq6CdRKIoso82GXO1O0gQNDEs5rLi3my/YiD535nyZQ6Ls64jZ5hB2KW 0
563d43dea4387b6f44e92414	X	I have seen that sort of error message when I'm using an invalid secret key and access key OR my permissions for my user aren't set up for the service I am trying to access. A benefit of using the latest version of the SDK is that you will write less low level code and the error messages will probably be more helpful in identifying your issue.
563d43e0a4387b6f44e92415	X	possible duplicate of Is there an API for Amazon Cloud (Drive and Player)?
563d43e0a4387b6f44e92416	X	My App need to access user Amazon Cloud Drive Details. So that Through My App user can Login, Download, Upload and Delete etc Like There is API For Google Drive And Box. I get one Sample on Link But it is not What I Actually Need. Any response would be much appreciated.I have Also read this Link1 and Link2. And There is one Question is-there-an-api-for-amazon-cloud-drive-and-player. But There is also nothing which My App need And my need is api for Android not other platform.
563d43e0a4387b6f44e92417	X	there is a Cloud Drive API available - https://developer.amazon.com/public/apis/experience/cloud-drive - not sure if it gives you exactly what you need
563d43e1a4387b6f44e92418	X	Within my research I came across many different sources, but somehow I fail to see, which side is generating the private API-key and how is the other side getting hold of it. Many people recommend Amazon S3 Restful API as the role model, hence if I understand that, I could create something similar for my own purposes. Amazon's S3 REST API. e.g. this example here explains the process very nicely, however it fails to explain, which side is generating the API-key? So upon user signup, is it the service side that is generating the private API-key and assigns it to the user id in database? If this is the case though, the client needs to know the API key in order to create the signature for each request, so that the service can actually verify it. So how do both sides get hold of the private API key? In my case I would have a iPhone app and a AngularJS web app as my clients talking to the RESTful API service. Many Thanks,
563d43e1a4387b6f44e92419	X	First, you don't want give out keys to your clients. In general, that's a security nightmare. (Also, key creation can take some hours to propagate. And you'll have to manage the permissions for each key, etc.) So all the signing is done by your server, and your key doesn't leave your server. You want your server to have the S3 key, but only return signed links that will give the client the power to do something (GET a particular file, or PUT a file). It's a bit like the mother-may-I game: The client asks you for a "S3 signed link", then it can talk to S3 to do one thing. Since your server is doing a trivial amount of work (checking request is authorized, then returning a signed URL), you will be able to scale pretty well. For some things, like "list files" or "delete a file", it might be better for your server to call S3 (i.e. making a web request to S3 within the web request from the client) and return the results to the client (instead of messing with signed links). But if you do this, you may run into problems when scaling -- unless you are using the right technologies. (I.e. you want an evented server like node.js) Note that for a PUT request, the signed link must specify a lot of stuff ahead of time (like the file type, etc). You have to read the AWS spec carefully. Be careful of the Confused Deputy problem. Your code will have one key that can see all user's files, so you are responsible for the security between users.
563d43e1a4387b6f44e9241a	X	Thanks for the answer. I was hoping to learn how to do these operations with the REST API though. I only need one of the AWS services (S3), and only a couple of it's functions, so it seems overkill to bring in the entire AWS SDK. Also multiple developers are working on this project and I want to avoid complications caused by plugins and getting everyone's workspaces running identically. Thanks for the tip on IAM. I've noticed the S3 Management Console lets you get into a property page where you can set permissions and bucket policies. Is this essentially the same functionality?
563d43e1a4387b6f44e9241b	X	I'm building a project that will use Amazon S3 to store documents. In particular there are two apps: ...so the public app should only have 'write' permission, and the admin tool 'read' and 'list' permissions. The coding for this project will be done with ASP.NET and C#, and the preference is to use S3's REST API. This use case (uploading, listing and downloading documents) seems pretty basic, but I haven't had much luck finding simple examples. Can some you suggest some links?
563d43e1a4387b6f44e9241c	X	If you install the AWS SDK for .NET from http://aws.amazon.com/sdkfornet it will also put down a sample S3 application at C:\Program Files (x86)\AWS SDK for .NET\Samples\AmazonS3Sample\AmazonS3Sample that will show the basic CRUD operations for S3. As far as permissions go you should take a look at Identity and Access Management with AWS, http://aws.amazon.com/iam/. Using this service you can create different users with profiles that restrict access. So you could create one user for your public application that only has write access and even restrict write to a specific S3 bucket. Then create another user for admin application that has more admin permissions.
563d43e1a4387b6f44e9241d	X	Did you take a look at APIs for S3 Lifecycle Configuration? I have done it using Python boto. Not sure about PHP.
563d43e1a4387b6f44e9241e	X	This question gives the impression that what you are in need of, first, is a more thorough understanding of how S3's Glacier integration actually works at cconceptual level... manually migrating to Glacier is not a thing, and when files are restored to S3 from Glacier, that's temporary; they are also still stored in Glacier, not moved back to S3.
563d43e1a4387b6f44e9241f	X	I am creating a PHP based web application using Amazon's S3 and glacier services. Now I want to give my site users a feature that they can choose any file and make it archive (means move file from S3 to Glacier) and unarchive (means move file from Glacier to S3). I have done some research and didn't find any possible way using Amazon's API. PROBLEM How can I move files between S3 and glacier using API?
563d43e1a4387b6f44e92420	X	You can use the API to define lifecycle rules that archive files from Amazon S3 to Amazon Glacier and you can use the API to retrieve a temporary copy of files archived to Glacier. However, you cannot use the API to tell Amazon S3 to move specific files into Glacier. There are two ways to use Amazon Glacier: Connecting directly via the Glacier API allows you to store archives for long-term storage, often used as a replacement for Tape. Data stored via the Glacier API must also be retrieved via the Glacier API. This is typically done with normal enterprise backup software or even light-weight products such as Cloudberry Backup (Windows) or Arq (Mac). Using Amazon S3 lifecycle rules allows you to store data in Amazon S3, then define rules that determine when data should be archived to Glacier for long-term storage. For example, data could be archived 90 days after creation. The data transfer is governed by the lifecycle rules, which operate on a daily batch basis. The rules can be set via the putBucketLifecycle API call (available in the PHP SDK), but this only defines the rules -- it is not possible to make an API call that tells S3 to archive specific files to Glacier. Amazon S3 has a RestoreObject API call (available in the PHP SDK) to restore a temporary copy of data archived from Glacier back into S3. Please note that restoring data from Glacier takes 3-5 hours.
563d43e2a4387b6f44e92421	X	You could use the Glacier API to upload a file to a Glacier vault, but I don't recommend it. The previous version of our backup app did that. When you upload a file it gets a randomly-assigned name. You can add put your filename in the metadata of the file, but if you want a list of what's in the Glacier vault you have to query and then wait 3-5 hours for the list. Lifecycle policies are the other way to use Glacier. The current version of Arq uses them because each object still looks like an S3 object (no random object names, no delays in getting object lists), but the object contents are in Glacier storage. The only difference is that getting the object contents is a 2-step process: you have to make an API call to request that the object be made downloadable; when it's ready, you can download it. Also there's a "peak hourly request fee" that comes into play if you request objects be made downloadable at too fast a rate. Amazon Glacier pricing is complex. Once an object is "Glacier storage class" there's no way to change it back to "Standard storage class". You have to make a copy of the object that's "Standard storage class" and delete the Glacier object. So maybe a simple solution to your problem is:
563d43e2a4387b6f44e92422	X	I have a lots of documents stored on Amazon S3. My questions are: Does Amazon provide any services/APIs using which I can index the contents of the document and search them (full text indexing and searching)? If it does could someone please point me to any link in the documentation. If it does not then could this be achieved with Lucene and Zend Framework? Have any one of you implemented this? Can I get some pointers? UPDATE: I do not intend to save my index on Amazon S3 rather I am looking forward to indexing the contents of the documents on S3 and serving them based on a search.
563d43e2a4387b6f44e92423	X	You can see this question, or this blog post if you want to do pure lucene, or you can use Solr, which is probably easier. See also this post. Zend has a PHP port of Lucene, which ties in very well. You can look at the Zend documentation for how to use it.
563d43e2a4387b6f44e92424	X	What about subdomains? bucket.domain.tld -> Your S3 Bucket. domain.tld -> Heroku. Or is this against the same origin policy?
563d43e2a4387b6f44e92425	X	Same Origin Policy also applies for subdomains I am afraid. I considering to go with some kind of CORS implementation now I guess.
563d43e2a4387b6f44e92426	X	you can use JSONP for a lot of stuff. I've been able to use it with cross domains in the past.
563d43e2a4387b6f44e92427	X	Thanks, but JSONP is not really a viable option as well. It has some security concerns and only supports GET requests.
563d43e2a4387b6f44e92428	X	Thanks for the reply. As of now my approach already kind of changed and I don't need to apply the above anymore.
563d43e2a4387b6f44e92429	X	I want to build a web app where frontend (static) and backend (API) are, except for sharing the same domain, completely seperated. Usually I would consider this to be no problem, but I have some special requirements: The frontend app will be a single page Javascript application (with a base template, lets call it index.html) and populate the content from the API via AJAX. Since I don't want to implement CORS for the API yet and would like to follow the same-origin policy I want that both, the API and the files on S3 (the bucket), are sharing the same domain in some way. I also don't want to the Django's flatpages app or render the index.html through Django at all. I scanned Google and stackoverflow, but couldn't find a adequate solution so far. As far as I read the naive way (pointing domain to the Heroku app and the S3 bucket somehow) is not possible. Some solutions I have in mind but didn't find sources to: Did anybody tried something like this before and can point me in the right direction? One addition: Later on I want to use something lile PhantomJS to make the single-page app crawlable. This output for crawlers should ideally be hosted in the S3 storage as well.
563d43e3a4387b6f44e9242a	X	That is not possible with your current stack. Your Heroku application and your S3 bucket are actually served through two different domains. The benefit of having two different domains is that you can offload your server from all static assets requests. A convoluted way to achieve what you want would be to appropriately proxy the requests through one unique domain. Luckily for you neither Heroku nor Amazon will let you do that: S3 can host your website and redirect an api folder to your-api.herokuapp.com but only with 301 redirects that don't solve CORS issues. Just tried it if you're curious: At that point the easy solution is to implement a Django middleware for cross-domain sharing.
563d43e3a4387b6f44e9242b	X	I want to enhance my sites loading speed, so I use http://gtmetrix.com/, to check what I could improve. One of the lowest rating I get for "Leverage browser caching". I found, that my files (mainly images), have problem "expiration not specified". Okay, problem is clear, I thought. I start to googling and I found that amazon S3 prefer Cache-Control meta data over Expiry date (I lost this link, now I think maybe I misunderstood something). Anyway, I start looking for how to add cache-control meta to S3 object. I found this page: http://www.bucketexplorer.com/documentation/amazon-s3--how-to-set-cache-control-header-for-s3-object.html I learned, that I must add string to my PUT query. x-amz-meta-Cache-Control : max-age= <value in seconds> //(there is no need space between equal sign and digits(I made a mistake here)). I use construction: Cache-control:max-age=1296000 and it work okay. After that I read https://developers.google.com/speed/docs/best-practices/caching This article told me: 1) "Set Expires to a minimum of one month, and preferably up to one year, in the future." 2) "We prefer Expires over Cache-Control: max-age because it is is more widely supported."(in Recommendations topic). So, I start to look way to set Expiry date to S3 object. I found this: http://www.bucketexplorer.com/documentation/amazon-s3--set-object-expiration-on-amazon-s3-objects-put-get-delete-bucket-lifecycle.html And what I found: "Using Amazon S3 Object Lifecycle Management , you can define the Object Expiration on Amazon S3 Objects . Once the Lifecycle defined for the S3 Object expires, Amazon S3 will delete such Objects. So, when you want to keep your data on S3 for a limited time only and you want it to be deleted automatically by Amazon S3, you can set Object Expiration." I don't want to delete my files from S3. I just want add cache meta for maximum cache time or/and file expiry time. I completely confused with this. Can somebody explain what I must use: object expiration or cache-control?
563d43e3a4387b6f44e9242c	X	Your files won't be deleted, just not cached after the expiration date. The Amazon docs say: After the expiration date and time in the Expires header passes, CloudFront gets the object again from the origin server every time an edge location receives a request for the object. We recommend that you use the Cache-Control max-age directive instead of the Expires header field to control object caching. If you specify values both for Cache-Control max-age and for Expires, CloudFront uses only the value of max-age.
563d43e3a4387b6f44e9242d	X	"Amazon S3 Object Lifecycle Management" flushs some objects from your bucket based on a rule you can define. It's only about storage. What you want to do is set the Expires header of the HTTP request as you set the Cache-Control header. It works the same: you juste have to add this header to your PUT query. Expires doesn't work as Cache-Control: Expires gives a date. For instance: Sat, 31 Jan 2013 23:59:59 GMT You may read this: https://web.archive.org/web/20130531222309/http://www.newvem.com/how-to-add-caching-headers-to-your-objects-using-amazon-s3/
563d43e3a4387b6f44e9242e	X	So when we create lifecycle rules does amazon notifies our server when the file is moved to glacier and returns its ID. How can we get the ID of moved archive in the vault?
563d43e3a4387b6f44e9242f	X	There is no notification when objects are moved between Amazon S3 and Amazon Glacier due to lifecycle rules. The Storage Class of the Amazon S3 object is changed to "Glacier", which indicates that the content has been moved out of S3 and is available from Glacier (eg via the "Initiate Restore" command). The object remains in S3 (except for its contents), so it retains its existing key name (which is its ID). You cannot directly access data moved from S3 to Glacier -- you must restore it to S3 and then access it from S3.
563d43e3a4387b6f44e92430	X	I am developing an application using Amazon S3 and glacier for file storing. The requirement is that I want to move the files from S3 to glacier and when needed from glacier back to S3. My question is that Is it really possible with their PHP API or not?
563d43e3a4387b6f44e92431	X	Never quite looked at it this way, but you do make sense. Thanks a lot!
563d43e4a4387b6f44e92432	X	One more thing that I've quite liked is that some apps have started using DropBox for their storage - this means the user has to provide their DropBox credentials - which might put some users off - but it's a neat solution.
563d43e4a4387b6f44e92433	X	I'm having a hard time consuming the S3 API on Windows phone 7, mainly because of the lack of example for actually putting an object on S3 using the SOAP API? Where do you even put the body of the item? As far as I know, there isn't even a field for it in the putObject method... So, how do you put an object on S3 with windows phone 7.
563d43e4a4387b6f44e92434	X	I do not recommend accessing the S3 API (or the Azure Storage API) direct from your phone. If you try this, then you will need to either have public PUT permissions or you will have your private storage access keys in plain view in the XAP file - it will be easy for a hacker to steal these and you will soon be paying to host PimpMyBreasts, WikiL33ked and SpamThis. Instead, you should host your own storage service where you can at least put some security checks in about what is being uploaded. If you do insist on using S3 directly, then this article covers S3 from C# including PutObject requests - http://www.codeproject.com/KB/cs/s3_ec2studio.aspx Good luck Stuart
563d43e4a4387b6f44e92435	X	I assume that you added a service reference to the Amazon service in your project: http://s3.amazonaws.com/doc/2006-03-01/AmazonS3.wsdl Once added as a service reference, you can invoke AmazonS3Client.PutObjectInlineAsync to upload an object in a S3 bucket. The Data parameter (accepts a byte array) is what you're looking for. Recommended reading: http://timheuer.com/blog/archive/2008/07/05/access-amazon-s3-services-with-silverlight-2.aspx
563d43e4a4387b6f44e92436	X	This has been making me crazy all night. I wrote a DropBox app in PHP/MYSQL that worked perfectly, it pulls files from an Amazon S3 Bucket and sends them to users Dropbox folders. Then I changed the bucket policy on the Amazon S3 bucket to allow files to be pulled from only a handful of referrers, and signed URLS (example: /musicfile.mp3?AWSAccessKeyId=[accesskeyid]&Expires=[expires]&Signature=[signature]). This works great for all purposes, except I learned my Dropbox functionality no longer works, it's because you pass the Dropbox API the URL of the mp3 on Amazon S3, and on Dropbox's side they pull the file in, so now that I have the bucket policy allowing only certain referrers, dropbox gets a permission denied and the API tells me it failed. So I thought easy fix, I would simply add the ?AWSAccessKeyId= blah blah to the end of the file being passed to dropbox and all would work instantly, but, it doesn't because the file then doesn't end in an extension Dropbox recognizes so it again fails to work. Then I thought I'd simply add the referrer from Dropbox to my bucket policy, I still have no idea what it is however and have added every variation of dropbox.com and api.dropbox with and without https, all with no luck. If anyone has any idea or solution you will seriously make my week. The absolute last thing I want to do is be forced to download the file first to my server, then send to dropbox, I really don't want to do that and I know I had this working perfectly already as it was, and it works instantly when I remove my bucket policy entirely, I just want it to work with it.
563d43e5a4387b6f44e92437	X	I assume, because you mention passing a URL to Dropbox, that you're using the Saver? If so, you can tell the Saver what file name to use, so give it the authorized URL and specify a filename so there's a file extension. E.g.: or, in JavaScript: When you say that "because the file then doesn't end in an extension Dropbox recognizes so it again fails to work," what do you mean, exactly? What goes wrong when the file doesn't have an extension?
563d43e5a4387b6f44e92438	X	When all else fails... check the logs. Turn on logging for your bucket, run some tests, wait a few minutes for a log to appear, and then examine the logs to see what the referer is. It seems a safe bet that there won't be a referer because a user agent that isn't a web browser (such as Dropbox's back-end processes) would typically not have a reason to send a referer. If it's any consolation, "securing" a bucket by constraining the referer is pretty much like not securing the bucket at all. It's extremely simple to defeat, and so it's only really effective protection against two classes of people: http://en.wikipedia.org/wiki/Referer_spoofing
563d43e6a4387b6f44e92439	X	To confuse people.
563d43e6a4387b6f44e9243a	X	You can execute commands, like checking something or initializing. But I confess, I would never use it as it is not readable. In for loops it is more common to increase/decrease multiple variables.
563d43e7a4387b6f44e9243b	X	The language allows a lot of things that are known to cause undefined behavior. At least these are harmless.
563d43e7a4387b6f44e9243c	X	It depends on the types of a and b. For user defined types, operator== can be overloaded and have side effects. One such side effect could be to deposit a certain amount of money in your bank account. With that in mind, you could consider it advantageous to invoke that operator many times.
563d43e7a4387b6f44e9243d	X	@juanchopanza: If you are going the operator overloading for obfuscation sake way, they you can also override the operator,() and have extra fun!
563d43e7a4387b6f44e9243e	X	:-) I am clear about for loop , I am specifically asking about if and while statement.
563d43e7a4387b6f44e9243f	X	@AbdulRehman: Well in that case the anser probably is: because it if and while statements require an expression that evaluates to true and (<ex1>,<ex2>) is an expression.
563d43e7a4387b6f44e92440	X	If it is the case why only last condition of an expression matters ?
563d43e7a4387b6f44e92441	X	@Abdul: Well, because you have to pick one and someone decided that it should be the last. You have to understand, that the comma operator doesn't have any special behavior in an if statement compared to using it at other positions in your code.
563d43e7a4387b6f44e92442	X	@AbdulRehman: In principle yes, but I'm not enough of a language laywer to tell you what it is in c/c++-standardese terms.
563d43e8a4387b6f44e92443	X	wellcome on SO! just gave you some reputation ...
563d43e8a4387b6f44e92444	X	Nice example, but I'd rather use a while - loop header for demonstration. for the if case, I don't see a reason, why I wouldn't just write those statements in front of the if statement.
563d43e8a4387b6f44e92445	X	they could simply use assignment expression rather than including expression list in c++ GRAMMAR of If statement.
563d43e8a4387b6f44e92446	X	@AbdulRehman you could write if ( foo(), a == b ) to call a function and then do the test
563d43e9a4387b6f44e92447	X	@AbdulRehman I don't understand: assignment is already an expression...
563d43e9a4387b6f44e92448	X	yes, but it is different than expression in c++ grammar, expression is a list of assignment expressions, while assignment expression is the one without commas in it.
563d43e9a4387b6f44e92449	X	I think I we both agree that the comma expression is unnecessary ;) in case of the for-loop, the grammar for initialise and increment could have been simply a list of statements instead of an expression and we wouldn't have this confusion...
563d43eaa4387b6f44e9244a	X	I dunno; there could be a void operator==(int rhs) { std::cout << rhs; }. This is a code base where people are using , in an if statement, I wouldn't rule it out.
563d43eaa4387b6f44e9244b	X	perhaps you could add some code examples? as it stands, this is more than confusing.
563d43eba4387b6f44e9244c	X	We can write if statement as and only the last condition should be satisfiable to enter if body My question is what is the advantage of commas in if or while statement? Why is it allowed ?
563d43eba4387b6f44e9244d	X	In short: Although it is legal to do so, it usually doesn't make sense to use the comma operator in the condition part of an if or while statement (EDIT: Although the latter might sometimes be helpful as user5534870 explains in his answer). A more elaborate explanation: Aside from its syntactic function (e.g. separating elements in initializer lists, variable declarations or function calls/declarations), in C and C++, the , can also be a normal operator just like e.g. +, and so it can be used everywhere, where an expression is allowed (in C++ you can even overload it). The difference to most other operators is that - although both sides get evaluated - it doesn't combine the outputs of the left and right expressions in any way, but just returns the right one. It was introduced, because someone (probably Dennis Ritchie) decided for some reason that C required a syntax to write two (or more) unrelated expressions at a position, where you ordinarily only could write a single expression. Now, the condition of an if statement is (among others) such a place and consequently, you can also use the , operator there - whether it makes sense to do so or not is an entirely different question! In particular - and different from e.g. function calls or variable declarations - the comma has no special meaning there, so it does, what it always does: It evaluates the expressions to the left and right, but only returns the result of the right one, which is then used by the if statement. The only two points I can think of right now, where using the (non-overloaded) ,-operator makes sense are: If you want to increment multiple iterators in the head of a for loop: If you want to evaluate more than one expression in a c++11 constexpr function. To repeat this once more: Using the comma operator in an if or while statement - in the way you showed it in your example - isn't something sensible to do. It is just another example where the language syntaxes of c and c++ allow you to write code, that doesn't behave the way that one - on first glance - would expect it to. There are many more....
563d43eca4387b6f44e9244e	X	Changing your example slightly, suppose it was this (note the = instead of ==). In this case the commas guarantee a left to right order of evaluation. In constrast, with this you don't know if f(5) is called before or after f(6). More formally, commas allow you to write a sequence of expressions (a,b,c) in the same way you can use ; to write a sequence of statements a; b; c;. And just as the ; creates a sequence point (end of full expression) so too does a comma. Only sequence points govern the order of evaluation, see this post.
563d43eca4387b6f44e9244f	X	There is no advantage: the comma operator is simply an expression with type of the last expression in its expression list and an if statement evaluates a boolean expression. It's a weird operator true, but there's no magic to it - except that it confuses lists of expressions with argument lists in function calls. Note that in the argument list, comma binds stronger to separating arguments.
563d43eda4387b6f44e92450	X	None whatsoever. The comparisons on a in that code are completely redundant.
563d43eda4387b6f44e92451	X	For an if statement, there is no real point in putting something into a comma expression rather than outside. For a while statement, putting a comma expression to the condition executes the first part either when entering the loop, or when looping. That cannot easily be replicated without code duplication. So how about a s do...while statement? There we have only to worry about the looping itself, right? It turns out that not even here a comma expression can be safely replace by moving the first part into the loop. For one thing, destructors for variables in the loop body will not have already been run then which might make a difference. For another, any continue statement inside the loop will reach the first part of the comma expression only when it indeed is in the condition rather than in the loop body.
563d43eda4387b6f44e92452	X	What follows is a bit of a stretch, depending on how devious you might wish to be. Consider the situation where a function returns a value by modifying a parameter passed by reference or via a pointer (maybe from a badly designed library, or to ensure that this value is not ignored by not being assigned after returning, whatever). Then how do you use conditional statements that depend on result? You could declare the variable that will be modified, then check it with an if: This could be shortened to Which is not really worth while. However, for while loops there could be some small advantages. If calculateValue should/can be called until the result is no longer bar'd, we'd have something like: and could be condensed to: This way the code to update result is in only one place, and is near the line where its conditions are checked. maybe unrelated: Another reason why variables could be updated via parameter passing is that the function needs to return the error code in addition to modify/return the calculated value. In this case: then
563e07a42d1761a701f0f46a	X	You can use the API to define lifecycle rules that archive files from Amazon S3 to Amazon Glacier and you can use the API to retrieve a temporary copy of files archived to Glacier. However, you cannot use the API to tell Amazon S3 to move specific files into Glacier. There are two ways to use Amazon Glacier: Connecting directly via the Glacier API allows you to store archives for long-term storage, often used as a replacement for Tape. Data stored via the Glacier API must also be retrieved via the Glacier API. This is typically done with normal enterprise backup software or even light-weight products such as Cloudberry Backup (Windows) or Arq (Mac). Using Amazon S3 lifecycle rules allows you to store data in Amazon S3, then define rules that determine when data should be archived to Glacier for long-term storage. For example, data could be archived 90 days after creation. The data transfer is governed by the lifecycle rules, which operate on a daily batch basis. The rules can be set via the putBucketLifecycle API call (available in the PHP SDK), but this only defines the rules -- it is not possible to make an API call that tells S3 to archive specific files to Glacier. Amazon S3 has a RestoreObject API call (available in the PHP SDK) to restore a temporary copy of data archived from Glacier back into S3. Please note that restoring data from Glacier takes 3-5 hours.
563e07a72d1761a701f0f46b	X	Editing this question to focus on C and Linux; since what Linux and Windows do differs. Otherwise, it's a bit too broad. Also, any higher level language will end up calling either a C API for the system or compiling down to C to execute, so leaving at the level of "C" is putting it at the Least Common Denominator.
563e07a82d1761a701f0f46c	X	Not to mention that not all programming languages have this facility, or it is a facility that is highly dependent on environment. Admittedly rare these days, of course, but to this day file handling is a completely optional part of ANSI Forth, and wasn't even present in some implementations in the past.
563e07a82d1761a701f0f46d	X	It is worth noting that in Unix-like OSes, the in-kernel structure file descriptors are mapped to, is called "open file description". So process FDs are mapped to kernel OFDs. This is important to understand the documentation. For instance, see man dup2 and check the subtlety between a open file descriptor (that is a FD that happens to be open) and a open file description (a OFD).
563e07a82d1761a701f0f46e	X	Yes, permissions are checked at open time. You can go and read the source for the kernel's "open" implementation: lxr.free-electrons.com/source/fs/open.c although it delegates most of the work to the specific file system driver.
563e07a82d1761a701f0f46f	X	(on ext2 systems this will involve reading the directory entries to identify which inode has the metadata in, then loading that inode into the inode cache. Note that there may be pseudofilesystems like "/proc" and "/sys" which may do arbitrary things when you open a file)
563e07a82d1761a701f0f470	X	Note that the checks on file open -- that the file exists, that you have permission -- are, in practice, not sufficient. The file can disappear, or its permissions can change, under your feet. Some file systems attempt to prevent this, but so long as your OS supports network storage it is impossible to prevent (an OS can "panic" if the local file system misbehaves and be reasonable: one that does so when a network share does is not a viable OS). Those checks are also done at file open, but must (effectively) be done at all other file access as well.
563e07a82d1761a701f0f471	X	Not to forget evaluation and/or creation of locks. These can be shared, or exclusive and can affect the whole file, or only a part of it.
563e07a82d1761a701f0f472	X	You voted to close and answered?
563e07a82d1761a701f0f473	X	@BillWoodger: well yes. But it's a fair question (I mean yours). I voted to close as "too broad", and my answer is meant to illustrate how extremely broad the question actually is.
563e07a82d1761a701f0f474	X	I think you are broading the answer a bit too much. ZX Spectrum had an OPEN command, and that was totally different from LOAD. And harder to understand.
563e07a92d1761a701f0f475	X	Although I edited my question to restrict to linux/windows OS in attempt to keep it open, this answer is entirely valid and useful. As stated in my question, I am not looking to implement something or to get other people to do my work, I am looking to learn. To learn you must ask the 'big' questions. If we constantly close questions on SO for being 'too broad', it risks becoming a place to just get people to write your code for your without giving any explanation of what, where or why. I'd rather keep it as a place I can come to learn.
563e07a92d1761a701f0f476	X	This answer seems to prove that your interpretation of the question is too broad, rather than that the question itself is too broad.
563e07a92d1761a701f0f477	X	Very thorough. Does it make sense to stop at "the kernel" because this is the bottommost layer of external access through a high level programming language? One step deeper would be an almost pure hardware layer?
563e07a92d1761a701f0f478	X	Actually what I've written here is part of the kernel. But it's only the top layer - the first bit of kernel code that gets called by user-mode code (such as a high-level programming language). There are layers below this as well, in particular the filesystem (also part of the kernel), the IDE/ATA/SATA interface (also part of the kernel), the disk controller (firmware, i.e. software embedded within the hard drive itself), and then the physical hardware. One could go into a lot of detail. But I thought this top layer was the most relevant part for the question.
563e07a92d1761a701f0f479	X	@Jongware on further consideration I thought this worth an edit. Thanks :-)
563e07a92d1761a701f0f47a	X	For MMIO and IO ports, BIOS (namely, its ACPI part) is only used to setup their locations. Once this is done, all the talk with the hardware concerning data reads and writes doesn't use BIOS: it just uses memory reads/writes for MMIO and in/out/ins/outs instructions for IO ports.
563e07a92d1761a701f0f47b	X	Thanks to you and @Ruslan for your enlightening comments.
563e07a92d1761a701f0f47c	X	What does this have to do with the actual question?
563e07a92d1761a701f0f47d	X	It describes what happens at a low level when you open a file in Linux. I agree the question is rather broad, so this may not have been the answer jramm was looking for.
563e07aa2d1761a701f0f47e	X	So again, no checking for permissions?
563e07aa2d1761a701f0f47f	X	In all programming languages (that I use at least), you must open a file before you can read or write to it. But what does this open operation actually do? Manual pages for typical functions dont actually tell you anything other than it 'opens a file for reading/writing': http://www.cplusplus.com/reference/cstdio/fopen/ https://docs.python.org/2/library/functions.html#open Obviously, through usage of the function you can tell it involves creation of some kind of object which facilitates accessing a file. Another way of putting this would be, if I were to implement an open function, what would it need to do on Linux?
563e07aa2d1761a701f0f480	X	In almost every high-level language, the function that opens a file is a wrapper around the corresponding kernel system call. It may do other fancy stuff as well, but in contemporary operating systems, opening a file must always go through the kernel. This is why the arguments of the fopen library function, or Python's open closely resemble the arguments of the open(2) system call. In addition to opening the file, these functions usually set up a buffer that will be consequently used with the read/write operations. The purpose of this buffer is to ensure that whenever you want to read N bytes, the corresponding library call will return N bytes, regardless of whether the calls to the underlying system calls return less. I am not actually interested in implementing my own function; just in understanding what the hell is going on...'beyond the language' if you like. In Unix-like operating systems, a successful call to open returns a "file descriptor" which is merely an integer in the context of the user process. This descriptor is consequently passed to any call that interacts with the opened file, and after calling close on it, the descriptor becomes invalid. It is important to note that the call to open acts like a validation point at which various checks are made. If not all of the conditions are met, the call fails by returning -1 instead of the descriptor, and the kind of error is indicated in errno. The essential checks are: In the context of the kernel, there has to be some kind of mapping between the process' file descriptors and the physically opened files. The internal data structure that is mapped to the descriptor may contain yet another buffer that deals with block-based devices, or an internal pointer that points to the current read/write position.
563e07aa2d1761a701f0f481	X	Any file system or operating system you want to talk about is fine by me. Nice! On a ZX Spectrum, initializing a LOAD command will put the system into a tight loop, reading the Audio In line. Start-of-data is indicated by a constant tone, and after that a sequence of long/short pulses follow, where a short pulse is for a binary 0 and a longer one for a binary 1 (https://en.wikipedia.org/wiki/ZX_Spectrum_software). The tight load loop gathers bits until it fills a byte (8 bits), stores this into memory, increases the memory pointer, then loops back to scan for more bits. Typically, the first thing a loader would read is a short, fixed format header, indicating at least the number of bytes to expect, and possibly additional information such as file name, file type and loading address. After reading this short header, the program could decide whether to continue loading the main bulk of the data, or exit the loading routine and display an appropriate message for the user. An End-of-file state could be recognized by receiving as many bytes as expected (either a fixed number of bytes, hardwired in the software, or a variable number such as indicated in a header). An error was thrown if the loading loop did not receive a pulse in the expected frequency range for a certain amount of time. A little background on this answer The procedure described loads data from a regular audio tape - hence the need to scan Audio In (it connected with a standard plug to tape recorders). A LOAD command is technically the same as open a file - but it's physically tied to actually loading the file. This is because the tape recorder is not controlled by the computer, and you cannot (successfully) open a file but not load it. The "tight loop" is mentioned because (1) the CPU, a Z80-A (if memory serves), was really slow: 3.5 MHz, and (2) the Spectrum had no internal clock! That means that it had to accurately keep count of the T-states (instruction times) for every. single. instruction. inside that loop, just to maintain the accurate beep timing. Fortunately, that low CPU speed had the distinct advantage that you could calculate the number of cycles on a piece of paper, and thus the real world time that they would take.
563e07aa2d1761a701f0f482	X	I'd suggest you take a look at this guide through a simplified version of the open() system call. It uses the following code snippet, which is representative of what happens behind the scenes when you open a file. Briefly, here's what that code does, line by line: The filp_open function has the implementation which does two things: Store ("install") the returned struct into the process's list of open files. If you're feeling ambitious, you can compare this simplified example to the implementation of the open() system call in the Linux kernel, a function called do_sys_open(). You shouldn't have any trouble finding the similarities. Of course, this is only the "top layer" of what happens when you call open() - or more precisely, it's the highest-level piece of kernel code that gets invoked in the process of opening a file. A high-level programming language might add additional layers on top of this. There's a lot that goes on at lower levels. (Thanks to Ruslan and pjc50 for explaining.) Roughly, from top to bottom: This may also be somewhat incorrect due to caching. :-P Seriously though, there are many details that I've left out - a person (not me) could write multiple books describing how this whole process works. But that should give you an idea.
563e07aa2d1761a701f0f483	X	It depends on the operating system what exactly happens when you open a file. Below I describe what happens in Linux as it gives you an idea what happens when you open a file and you could check the source code if you are interested in more detail. I am not covering permissions as it would make this answer too long. In Linux every file is recognised by a structure called inode. Each structure has an unique number and every file only gets one inode number. This structure stores meta data for a file, for example file-size, file-permissions, time stamps and pointer to disk blocks, however, not the actual file name itself. Each file (and directory) contains a file name entry and the inode number for lookup. When you open a file, assuming you have the relevant permissions, a file descriptor is created using the unique inode number associated with file name. As many processes/applications can point to the same file, inode has a link field that maintains the total count of links to the file. If a file is present in a directory, its link count is one, if it has a hard link its link count will be two and if a file is opened by a process, the link count will be incremented by 1.
563e07aa2d1761a701f0f484	X	Bookkeeping, mostly. This includes various checks like "Does the file exist?" and "Do I have the permissions to open this file for writing?". But that's all kernel stuff - unless you're implementing your own toy OS, there isn't much to delve into (if you are, have fun - it's a great learning experience). Of course, you should still learn all the possible error codes you can receive while opening a file, so that you can handle them properly - but those are usually nice little abstractions. The most important part on the code level is that it gives you a handle to the open file, which you use for all of the other operations you do with a file. Couldn't you use the filename instead of this arbitrary handle? Well, sure - but using a handle gives you some advantages: There's also some other tricks you can do (for example, share handles between processes to have a communication channel without using a physical file; on unix systems, files are also used for devices and various other virtual channels, so this isn't strictly necessary), but they aren't really tied to the open operation itself, so I'm not going to delve into that.
563e07aa2d1761a701f0f485	X	At the core of it when opening for reading nothing fancy actually needs to happen. All it needs to do is check the file exists and the application has enough privileges to read it and create a handle on which you can issue read commands to the file. It's on those commands that actual reading will get dispatched. The OS will often get a head start on reading by starting a read operation to fill the buffer associated with the handle. Then when you actually do the read it can return the contents of the buffer immediately rather then needing to wait on disk IO. For opening a new file for write the OS will need to add a entry in the directory for the new (currently empty) file. And again a handle is created on which you can issue the write commands.
563e07aa2d1761a701f0f486	X	Basically, a call to open needs to find the file, and then record whatever it needs to so that later I/O operations can find it again. That's quite vague, but it will be true on all the operating systems I can immediately think of. The specifics vary from platform to platform. Many answers already on here talk about modern-day desktop operating systems. I've done a little programming on CP/M, so I will offer my knowledge about how it works on CP/M (MS-DOS probably works in the same way, but for security reasons, it is not normally done like this today). On CP/M you have a thing called the FCB (as you mentioned C, you could call it a struct; it really is a 35-byte contiguous area in RAM containing various fields). The FCB has fields to write the file-name and a (4-bit) integer identifying the disk drive. Then, when you call the kernel's Open File, you pass a pointer to this struct by placing it in one of the CPU's registers. Some time later, the operating system returns with the struct slightly changed. Whatever I/O you do to this file, you pass a pointer to this struct to the system call. What does CP/M do with this FCB? It reserves certain fields for its own use, and uses these to keep track of the file, so you had better not ever touch them from inside your program. The Open File operation searches through the table at the start of the disk for a file with the same name as what's in the FCB (the '?' wildcard character matches any character). If it finds a file, it copies some information into the FCB, including the file's physical location(s) on the disk, so that subsequent I/O calls ultimately call the BIOS which may pass these locations to the disk driver. At this level, specifics vary.
563e07aa2d1761a701f0f487	X	Short answer: No you can't.
563e07aa2d1761a701f0f488	X	Medium answer: If you want something like this, then you probably have a design error and you don't need a const static member in the first place.
563e07ab2d1761a701f0f489	X	As soon as main is entered static initialization is done. Hence you can not alter the value. You may provide a function exposing a constant T, but using a mutable T internally.
563e07ab2d1761a701f0f48a	X	@101010, this is not neccessarily a design error. It is very legitimate thing - to have a value which is initialized during application start and than preserved. And you might want to 'enforce' this preservation logic. I, myself, have long longed for one-time-modfiables as class members - so that they are const, but still can be modified in constructor. It can not be done currently, but it does not constitute a design flaw.
563e07ab2d1761a701f0f48b	X	@SergeyA Sorry if things I wrote were misinterpreted, but I didn't mean that the request was absurd, but rather the approach.
563e07ab2d1761a701f0f48c	X	When you say you cannot initialize anything marked const is it only for the class members ? I did initialize a const intin this way within my main(). Thank you.
563e07ab2d1761a701f0f48d	X	One more thing, what is this statement const int& A::T(actualT) ? I didn't know that syntax.
563e07ab2d1761a701f0f48e	X	@user2939212 That's a common syntax to initialize variables. You can write const int& A::T = actualT; instead. This syntax applies to variables of all types, not only to references. For example, you can write int x(5) instead of int x=5.
563e07ab2d1761a701f0f48f	X	"You cannot initialize anything marked const inside a class at runtime" This is obviously not true. You mean static const. Even then it's not really true - ASH's answer seems the most comprehensive, on balance.
563e07ab2d1761a701f0f490	X	@LightnessRacesinOrbit Fixed.
563e07ab2d1761a701f0f491	X	Actually, a dynamically loaded library might have its own globals and they will be initialized when and if the lib is loaded. That concept is not part of the standard (or at least it wasn't).
563e07ab2d1761a701f0f492	X	@JDługosz I dont see any relation to DLLs here. It justs works for me to build normal executables and it always worked through all versions of MSVC including 2015, which is supposed to be fully standard. If this contruction is "not standard", there must be something to prove it in the reference. Please share if you find any.
563e07ab2d1761a701f0f493	X	I mean that static const values can be in a dll which is loaded after main is called.
563e07ab2d1761a701f0f494	X	@JDługosz ah ok, i got what you mean now. Indeed this is a very special case, as you said :)
563e07ac2d1761a701f0f495	X	Thank you for your solution.
563e07ac2d1761a701f0f496	X	Do not use a singleton here.
563e07ac2d1761a701f0f497	X	Hi SergeyA. Could you please let me know why you suggest so?
563e07ac2d1761a701f0f498	X	Because it is not needed. Do not use singleton when a simple global variable will suffice.
563e07ac2d1761a701f0f499	X	How do one make sure that the value is not changed again with just one global variable?
563e07ac2d1761a701f0f49a	X	The solution is provided in the answer. Singleton, on the other hand, will not provide for this at all. Classic singleton, at least.
563e07ac2d1761a701f0f49b	X	Is it possible to initialize a static const member of my class during run-time? This variable is a constant throughout my program but I want to send it as a command-line argument. If this cannot be done, what is the type of variable I should use? I need to initialize it at run-time as well as preserve the constant property.
563e07ac2d1761a701f0f49c	X	You cannot rely on data produced after your main has started for initialization of static variables, because static initialization in the translation unit of main happens before main gets control, and static initialization in other translation units may happen before or after static initialization of main translation unit in unspecified order. However, you can initialize a hidden non-const variable, and provide a const reference to it, like this: Demo.
563e07ac2d1761a701f0f49d	X	I am sorry to disagree with the comments and answers saying that it is not possible for a static const symbol to be initialized at program startup rather than at compile time. Actually this IS possible, and I used it many times, BUT I initialize it from a configuration file. Something like: As you see, these static consts are not necessarily known at compile time. They can be set from the environment, such as a config file. On the other hand, setting them from argv[], seems very difficult, if ever feasible, because when main() starts, static symbols are already initialized.
563e07ac2d1761a701f0f49e	X	No, you cannot do that. If this cannot be done what is the type of variable I should use ? You can use a non-const member. Another option is to make T a private member, make main a friend so only it can modify the value, and then expose the member through a function.
563e07ac2d1761a701f0f49f	X	Not only you can't, you should not try doing this by messing with const_cast. Static const members have a very high chance of ending up in read-only segment, and any attempt to modify them will cause program to crash.
563e07ac2d1761a701f0f4a0	X	Typically you will have more than one configuration value. So put them in a struct, and the normal global access to it is const. You can get fancier and have a global function to return config, so normal code can't even change the pointer, but it is harder to do that by accident. A header file exposes get_config () for all to use, but the way to set it is only known to the code that's meant to do so.
563e07ac2d1761a701f0f4a1	X	No, since you defined the variable as static and const, you cannot change its value. You will have to set its value in the definition itself, or through a constructor called when you create an object of class A.
563e07ac2d1761a701f0f4a2	X	Use a Singleton Pattern here. have a data member which you'd like to initialize at run time in the singleton class. One a single instance is created and the data member is properly initialized, there would be no further risk of overwriting it and altering it. Singleton would preserve the singularity of your data. Hope this helps.
563e0f082d1761a701f0f4a3	X	Hi, yes I think so - we really need to prevent anyone from being able to viewing the files, so only people using our web application shouel be able to access files on S3. many thanks for your help!
563e0f082d1761a701f0f4a4	X	I am developing an application that loads images and video into a Flash player (currently using Flash 8 to develop so this is AS2.0). We are going to host the files on Amazon S3 servers. Can anyone point out the best way to go about loading the files into Flash Player from Amazon S3. I have been using MovieClipLoader to load images from our development server using loadMovie("http://domain/folder/file") and progressive video is loaded in a similar way. I want to be able to load from S3 like I did from our development server. Do I need to go through the signature and authentication process when loading each item into Flash from S3? I dont fully understand how I would generate signature etc in Flash. Can I use the PHP S3 class to do this and send the signature etc as a variable to Flash at the start and use the same signature for loading all images / video? Thanks
563e0f082d1761a701f0f4a5	X	Assets hosted on Amazon S3 are available without any authentication, assuming ACL settings allow read access. So you should not need to supply a signature or authentication to simply download stuff. For example here is an image file I have on S3 that's accessible via a simple url: gromit.jpg (Dead Link) Do you have a requirement that your assets be protected from viewing by anyone w/o credentials?
563e0f092d1761a701f0f4a6	X	Not sure if your use case is what I think it is, but is your signature really set to the string 'signature'. Perhaps have a look at this page: docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html, and this page: docs.aws.amazon.com/AmazonS3/latest/dev/S3_Authentication2.html
563e0f092d1761a701f0f4a7	X	Did you ever get this working?
563e0f092d1761a701f0f4a8	X	I want to delete files and folder from the Amazon s3 bucket. Below is my request, but its not working.
563e0f092d1761a701f0f4a9	X	OK... I have been on this for a while now and I'm not getting anywhere. No matter what I try, I continue to get back the same error message from Amazon. I am using ng-file-upload (https://github.com/danialfarid/ng-file-upload) to upload to S3. I have followed the documentation and have used the demo installation to successfully upload a file to my bucket so I know my AWS Key and Secret Key are correct. My controller is as follows: Now the PHP for generating the policy and signature is as follows: The http headers show "Authorization:Basic U1Q6" which is obviously the issue. Any assistance with this would be greatly appreciated.
563e0f092d1761a701f0f4aa	X	I'm a complete idiot! The header was being created by my own login/authorisation script. I completely forgot! Once I fixed that I had no issues.
563e0f092d1761a701f0f4ab	X	parse.com/questions/…
563e0f092d1761a701f0f4ac	X	I'm uploading image to parse, making some works (like resize and etc) and want to upload after them.
563e0f092d1761a701f0f4ad	X	5000 files - about 1200 users for 100$ a month. Too much for free app.
563e0f092d1761a701f0f4ae	X	The first 5k are free. It would take 20000 files more to ever cost you $100. $.005 per file is a almost as good a price as it will be when u go over your parse 1M calls (7¢ per thousand) trying to build your own form actions.
563e0f092d1761a701f0f4af	X	If you wanted to pay per gb transloadit.com/docs#jquery-plugin
563e0f092d1761a701f0f4b0	X	guys, I want to upload image to S3 storage, but I can't do it. My app is on parse.com and I can't use npm to install aws-sdk Please, help me, I'm newbie in aws and node.js.
563e0f092d1761a701f0f4b1	X	I posted a link as a comment, however I will give it a bit of explanation. I am not sure if it is possible to upload to S3 through Parse (mainly because that would be alot of unnecessary traffic for Parse), however it is possible to upload to S3 directly from your client by using a certificate. This (signed) certificate effectively tells S3 that you are authorizing the device to upload to your bucket as long as the requirements included in the certificate are met. This question on Parse's site give more information about this, as well as Cloud Code that should generate the certificate for you. As always, I would recommend you understand what this code is doing before you use it for any production app/service. You can also probably find some more information about this client-side upload by doing a quick google for something like 'client side upload to S3'.
563e0f092d1761a701f0f4b2	X	Seems like a perfect place to use https://www.inkfilepicker.com Just plug in your own S3 creeds and off you go. If you can't stand not doing something painful use the REST API here for S3 http://docs.aws.amazon.com/AmazonS3/latest/dev/S3_Authentication2.html and build out your cloud code functions with the networking capability available in parse There is a reason inkfilepicker exists tho...
563e0f0a2d1761a701f0f4b3	X	Assume I already have purchased a domain example.com with IP address 203.0.113.2. Using C# and the The Amazon Web Services SDK for .NET 2.0.2.2, I'd like to create a static website using a custom domain using Amazon S3 and Route 53. The manual process is described in the Amazon documentation. When trying to create an alias, I get an exception with the message: First, I created or updated a bucket (e.g. "example.com") in Amazon S3. If it already existed, content is deleted. S3BucketExists returns whether a bucket exist or not, and CreateObject creates a simple page and uploads it to the bucket. Its omitted for brevity sake. I'm able to connect to the S3 hosted site without any problems. Then I use the Route 53 API to update an existing hosted zone or create one for "example.com". All resources, except for the SOA and NS entries are deleted. The hosted zone id ("Z2F56UZL2M1ACD") and DNS names ("s3-website-us-west-1.amazonaws.com.") are public knowledge and documented on Amazon's website. The call to ChangeResourceRecordSets throws the exception. I created an empty ResourceRecords list, with a A record of "203.0.113.2", but have not had any luck creating an alias. That said, I can manually create the alias to the Amazon S3 site afterwards using the "Route 53 Management Console". I'm sure it's something small I'm missing.
563e0f0a2d1761a701f0f4b4	X	After re-reading the documentation, it turns out that one cannot specify the TTL when specifying an alias. The following change works. Replace the code that creates an instance of ChangeResourceRecordSetsRequest to the following: The difference was evident when the output produced by System.Net tracing was compared to the request specified in the Amazon example.
563e0f0a2d1761a701f0f4b5	X	We ran into this recently, and I decided a quick patch to the AWS SDK would be the best answer since Amazon obviously isn't going to fix this any time soon. github.com/skilitix/aws-sdk-net is our fork that fixes the retry on 304 and adds a NotModified boolean to GetObjectResponse - if I spend the time to figure out a more back-compatible API I'll submit a patch to Amazon (in the meantime, we accept PRs :P).
563e0f0a2d1761a701f0f4b6	X	@SimonBuchan Awesome! I will check it out.
563e0f0a2d1761a701f0f4b7	X	Note that we're not keeping up with upstream, we're already ~3 point releases behind, but they seem to be new APIs only and should auto-merge.
563e0f0a2d1761a701f0f4b8	X	Background I am trying to make use of S3 as an 'infinity' large caching layer for some 'fairly' static XML documents. I want to ensure that the client application (which will be running on thousands of machines concurrently and requesting the XML documents many times per hour) only downloads these XML documents if their content has changed since the last time the client application downloaded them. Approach On Amazon S3, we can use HTTP ETAG for this. By default Amazon S3 objects have their ETAG set to the MD5 hash of the object. We can then specify the MD5 hash of the XML document inside the GetObjectRequest.ETagToNotMatch property. This ensures that when we make the AmazonS3.GetObject call (or in my case the async version AmazonS3.BeginGetObject and AmazonS3.EndGetObject), that if the document being requested has the same MD5 hash as is contained in the GetObjectRequest.ETagToNotMatch then S3 automatically returns the HTTP Status code 304 (NotModified) and the actual contents of the XML document is not downloaded. Problem The problem however is that when calling AmazonS3.GetObject (or it's async equivalent) the Amazon .Net API actually sees the HTTP Status code 304 (NotModified) as an error and it retries the get request three times and then finally throws an Amazon.S3.AmazonS3Exception: Maximum number of retry attempts reached : 3. Obviously I could change this implementation to use AmazonS3.GetObjectMetaData and then compare the ETAG and use AmazonS3.GetObject if they do not match, but then there are two requests to S3 instead of one when the file is stale. I'd prefer to have one request regardless of whether the XML document needs downloaded or not. Any ideas? Is this a bug or am I missing something? Is there even some way I can reduce the number of retries to one and 'process' the exception (although I feel 'yuck' about this route). Implementation I'm using the AWS SDK for .NET (version 1.3.14). Here is my implementation (reduced slightly to keep it shorter): I then call this like: This then produces this log file output:
563e0f0a2d1761a701f0f4b9	X	I also posted this question on the Amazon developers forum and got a reply from an official AWS Employee: After investigating this we understand the problem but we are looking for feedback on how best to handle this. First approach is to have this operation return with a property on the GetObjectResponse indicating that the object was not returned or set the output stream to null. This would be cleaner to code against but it does create a slight breaking behavior for anybody relying on an exception being thrown, albeit after the 3 retries. It would also be inconsistent with the CopyObject operation which does throw an exception without all the crazy retrying. The other option is we throw an exception similar to CopyObject which keeps us consistent and no breaking changes but it is more difficult to code against. If anybody has opinions on which way to handle this please respond to this thread. Norm I have already added my thoughts to the thread, if anybody else is interested in participating here is the link: AmazonS3.GetObject sees HTTP 304 (NotModified) as an error. Way to allow it? NOTE: When this has been resolved by Amazon I will update my answer to reflect the outcome. UPDATE: (2012-01-24) Still waiting for further information from Amazon.
563e0f0a2d1761a701f0f4ba	X	This works, but it requires you to stream all the files from S3 through your webserver... this is has a pretty high bandwidth and performance cost for your webserver. As an alternative you might want to look at generating presigned URLs for the files and allowing the webbrowser to pull them directly from S3...
563e0f0a2d1761a701f0f4bb	X	We intend to use local browser caching and Memcached to optimize performance.I would have preferred presigned URls over this but our system design and constraints don't allow it.
563e0f0a2d1761a701f0f4bc	X	I am trying to retrieve some HTML files from Amazon s3 using AWS SDK for .NET. I am able to get the HTML file but the images that are linked to the webpage are not being displayed neither is the relevant style sheet applied. Now, I do understand why this is happening. Because each image and style sheet is a separate object in Amazon s3 and my code is only creating presigned URL for the HTML file: What is the best way to access the images and style sheet related to this HTML file? I can look for all the images and then use the above method to generate presigned URL requests but that is not an efficient method and I can't make the images and style sheet public. Has anyone else encountered a similar issue? Also, is it better if I use Rest API to authenticate user( using authentication Header) so that the browser will have authentication information in header and I will not have to create presigned URL's for each object. A small piece of code for REST API would be very helpful.
563e0f0a2d1761a701f0f4bd	X	The best way to achieve this is by using Generic Handlers(.ASHX). The trick is to change the source of webpage and related objects to your handler: Now, to change the source you either update your old HTML files and create new ones with source pointing to (StreamFile.ashx)Generic Handler or use URL rewrite to write old URL's to new URL. This can be done in IIS or in the web.config.If you do it in IIS it will automatically add code in your web.config. The above code will look for "DevelopmentContent/Course/" in Src string and if found will rewrite the URL to StreamFile.ashx/?file=course{R:1}. R:1 will the rest of the URL-bold part(DevelopmentContent/Course/xyz/xsd/x/sd/ds.htm) which should map to your object key in amazon S3.Now in StreamHandler.ashx will receive the requests from the server with specified URL. You can then get the object key from query string(context.Request.QueryString["file"]) and then create a function to Get the required object. So now all the HTTP requests will be made using your server as proxy.
563e0f0b2d1761a701f0f4be	X	I am trying to do a PUT request via the REST API to establish a lifecycle rule as described here: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html But even when I PUT the exact same XML Data as in Example 1, I get a 'Malformed XML Error'. Also Content-Length and MD5 of the request in this example seem not to be valid as I calculate different values. My put is: and has a Content-Length of 397 and a MD5 of Or7bcOqR6tcsifiqQpq1tw== I get this error: Any help on this is appreciated! Thanks!
563e0f0b2d1761a701f0f4bf	X	I finally figured it out. The endpoint URL was wrong as the parameters got skipped by my code when creating a PUT request. If you get an XML Malformed error make sure your Endpoint URL is correct.
563e0f0b2d1761a701f0f4c0	X	The problem is I have Adobe AIR application and everyone can see its sources. So they can copy the URL where server returns signed URLs and use it.
563e0f0b2d1761a701f0f4c1	X	You don't put your keys in the application. Your application makes a call to your server (which has the keys) and the server responds with a signed upload url for the application to use for file uploading.
563e0f0b2d1761a701f0f4c2	X	It's done on the fly and you set the URL to expire after a time you specify. (maybe 10 minutes to upload a file?) You don't store the URL in the app. For example:
563e0f0b2d1761a701f0f4c3	X	-App - "I need to upload this file" -App -> server "Send me an upload url for this file" -Server -> App "some.S3.url.com/?signed&with&expiry -App uses URL to upload file (man, the formatting in these comments doesn't allow for line breaks, that sucks :-)
563e0f0b2d1761a701f0f4c4	X	Yes, I understood. For example this is a URL where application receives signed URLs: example.com/get_signed_url/USER_ID
563e0f0b2d1761a701f0f4c5	X	transloadit.com - How this works? I think it uploads files first to their server and then to S3.
563e0f0b2d1761a701f0f4c6	X	I have such problem: I have Droplr-like Adobe AIR application, which uploads files to remote location and returns short links for those files. I want to upload all these files to Amazon S3. But, as it is Adobe AIR application and everyone can see its source(and S3 API keys), I cant upload files directly to Amazon S3. As I understood, if someone will get API keys from application's source, he will be able to upload files to my S3 account and I will pay for that. I wanted to solve this by uploading files from application to my server and PHP script will upload them to Amazon S3. Like a proxy. But it will be double traffic and slow operation.
563e0f0b2d1761a701f0f4c7	X	Another option is that you can use signed URLs. Your application can request an upload URL from your server. Your server then generates a signed URL to send back to the application which is then used to upload the file. You can also set the expiration time on signed upload URLs.
563e0f0b2d1761a701f0f4c8	X	This blog article talks about keeping AWS credentials secret. Whilst it covers EC2 mostly, there's some stuff in there about S3 too.
563e0f0b2d1761a701f0f4c9	X	Another approach is the Token Vending Machine, an application you can run to allow mobile and other client apps to obtain temporary credentials without revealing or embedding your account credentials in your application. You can read more about the TVM approach here: http://aws.amazon.com/articles/4611615499399490
563e0f0b2d1761a701f0f4ca	X	thnx, will setting EC2 for CORS work on all browsers?
563e0f0c2d1761a701f0f4cb	X	No, because there are some older browsers that don't follow current standards. I wouldn't be surprised to see IE 6 having trouble, for example. But it will work for any browser that follows the CORS spec. The caniuse web site is very valuable for this. There's a table at caniuse.com/#search=cors showing browser conformance with the spec. Note that older IE versions support CORS, but via non-standard functions. If you use a library like jQuery it will automatically adjust for those browsers for you.
563e0f0c2d1761a701f0f4cc	X	I am hosting my website on S3. On my local host I am using backboneJS above a PHP Rest API that uses mySQL as a database. So i opened an EC2 to host my Rest API but then realized this means cross domain AJAX. How can i use EC2 as a Rest API if my index.html sits on S3? What are my other DB options for S3? many thanks,
563e0f0c2d1761a701f0f4cd	X	Your JavaScript is being executed on web pages served from S3, and it has to access a REST API from a server you run on EC2. Unless the web pages and server are in the same domain (say, example.com), this will be a cross-origin request, prohibited by browsers by default. Solution 1: have your S3 pages and your EC2 server in the same domain. S3 allows static website hosting that makes your S3 objects (web pages) available at the address of your choice. Put them and your EC2 server at addresses on the same domain, and it can work. Solution 2: have your REST API server allow cross-origin requests. Since you control this EC2 server you can modify it to tell web browsers to allow pages from other domains to make such requests to your server. This involves setting several headers on your responses, and usually requires your server to respond to HTTP OPTIONS requests properly, too. See the CORS specification to get started on that. You also ask about other DB options. If you keep your own EC2 server to provide the REST API it can use pretty much any kind of database you like, either running on the same or other EC2 instances, or database-as-a-service offerings like AWS RDB or AWS DynamoDB. But if you want to connect to the database directly from your web pages' JavaScript you would have to use a service that provides an HTTP API directly and that supports CORS. RDB does not provide an HTTP API at all, and DynamoDB does not seem to support CORS at this time, so neither of them would work.
563e0f0c2d1761a701f0f4ce	X	I've a S3 bucket on the Amazon and trying to get the list of all the zip files located within folders under the bucket recursively. For e.g, my zip files are located as shown below : Below is my code : But I get back an empty list. Am I doing anything wrong ? Is there any way to recursively get list of zip files from the bucket ?
563e0f0c2d1761a701f0f4cf	X	Only thing I can see is getting connection to your S3 bucket.Try the following and it may help
563e0f0c2d1761a701f0f4d0	X	I have an S3 Amazon account and currently all the video I upload on my site goes to there. I have added functionality so that user may select to share their video on YouTube as well. For that I am using the YouTube API which does not support a video URL. I was wondering if we could, somehow, provide a direct s3 link so that video gets uploaded to YouTube. here's the flow :- Selected video -> gets converted to mp4 format from encoding.com -> the concerted video gets uploaded to s3 amazon. Note: I have tried downloading my video from s3 to tmp folder and then upload it, this works fine but since we are using load balancing servers it will not work with it. Need solution asap. Thanks
563e0f0c2d1761a701f0f4d1	X	You can specify Youtube as a destination within your Encoding.com job, they have already built the integration with the YouTube API. See http://www.encoding.com/syndicate_your_videos_to_youtube for the API syntax. It can include full meta data. Note that you have to use Google's 2 step authentication process to create a password to use with the Encoding.com API
563e0f0d2d1761a701f0f4d2	X	I am developing an iPhone app to allow user upload photo and share. I want to use S3 to store uploaded images as well as processed images (foe example, thumbnail, reduced size image). I've installed AWS PHP API on my EC2 instance, my questions are: 1) Should photos uploaded from iPhone app go to a EC2 directory first, then copied over to S3 bucket, or it should directly uploaded to S3? 2) How can I create different folders under the S3 bucket through PHP API and how to read the files from the folder? Thanks in advance!
563e0f0d2d1761a701f0f4d3	X	The answer can be found here: Direct upload to s3 without the use of a production server I've never used the PHP SDK, but I was browsing through the AWS SDK for PHP 1.5.14 documentation and came across the following APIs that you will need to utilize: a) create_object : You'll use this to put a object into a bucket. You'll specify the filename. You asked how you can create different folders: you will include the full path into the filename. For instance instead of naming the file "photo1.jpg", you would name it "user1/vacation/photo1.jpg". b) get_object_list : This API will return to you a list of objects given some criteria. If you want to read all the objects from a particular folder, specify the prefix as the path to the folder. For instance, if I want to find all files in the folder "user1/vacation/", I would specify my prefix to be "user1/vacation/".
563e0f0d2d1761a701f0f4d4	X	Yes , it is possible to create new folder using s3 sdk. try bellow code
563e0f0d2d1761a701f0f4d5	X	I have deployed a REST server in an Amazon EC2 instance. I have also configured an Amazon S3 bucket to store all the data generated by users while interacting with the API. The main information stored are images. Users can upload images by doing a PUT HTTP request over certain URL and credentials. The PUT request may be done over the EC2 instance, since the upload needs to be authorized and users cannot access directly to S3 instance. When the EC2 receives a valid PUT petition, I use the AWS PHP SDK to upload the object to the S3 bucket. The method I use is putObject. For this first part, I think that there are not more alternatives. However to allow users to download previous uploads I have two different alternatives: The first one is to provide the user an url with the file that points to the S3 bucket-key, as files are uploaded in a public way. So the user can download the image directly from S3 servers without any interaction with EC2. The second one is to use the REST API running on the EC2 instance to provide the image contents while doing some HTTP GET request. In this case I should use the AWS PHP SDK to "download" the image from S3 servers and return it to the user. The method used would be getObject. Another possible solution that seems dirty to me, is to provide an HTTP Redirect from EC2 instance to S3 bucket url, but then, the user client should achieve two connections to retrieve a simple image (a bad thing if the user is working over mobile devices). I have implemented the second option and seems to work fine. My question is: if accessing the files from the EC2 instance through the REST API, that downloads the contents from S3 instance, would suppose a big overhead over direct accessing files with an url to S3 servers. Both instances are running in the same region (IRELAND). I do not know how the transfer from an S3 to EC2 (or vice-versa) is computed in terms of bandwidth. Would a transfer from S3-EC2-user would compute double than S3-user? Is this transfer done over some kind of local area networks? I prefer the second way as I can control the content access, log who is accessing each file, changing bucket would be transparent for user, and so on. Thanks!
563e0f0d2d1761a701f0f4d6	X	These are actually multiple questions combined into one, but I'll try to answer them. You can set up uploads to go directly to S3, without passing trough your EC2 instance, while still being able to authenticate the upload before it happens. The upload would be performed using a POST request directly to S3. For it to work you need to attach a policy and sign that request (your code on EC2 would generate the policy and signature). For a more detailed guide, see Browser Uploads to S3 using HTML POST. Proxying the S3 content trough your EC2 instance will certainly add some overhead, but the effect really depends on your app's scale. If you proxy a few requests / second and you have small files, the overhead will most likely not be very noticeable. If you have hundreds of requests/second, then proxying them trough a single EC2 instance will not really work (even if your instance could handle your traffic, you might encounter S3 slow down errors). Connections between EC2 and S3 in the same region are fast enough, certainly much faster than any connections between an external host and S3. Data transfers inside a region are not billed, so your S3-EC2-user transfers would cost the same as your S3-user transfers. If you need to handle large traffic, I recommend using Query String Authentication to generate signed URLs to your S3 objects, and just do a redirect to these signed urls from your download code.
563e0f0d2d1761a701f0f4d7	X	I will suggest you to debug it by uploading a file first on your server so will come to know that you need to fix something on amazon side or at your server.
563e0f0d2d1761a701f0f4d8	X	Thanks i am trying
563e0f0d2d1761a701f0f4d9	X	Yes i am not able to upload in normal PHP upload.... I have changed file PHP.ini
563e0f0d2d1761a701f0f4da	X	upload_max_filesize = 900M
563e0f0d2d1761a701f0f4db	X	Try memory_limit = 32M upload_max_filesize = 24M & post_max_size = 32M.
563e0f0d2d1761a701f0f4dc	X	I am new to Amazon S3 , i am developing on PHP program to upload the file. I am using S3.php API which is provided from amazon I can upload file upto 2 MB but if i am trying to upload 4 MB or Bigger than 2MB. It fetch following error Warning: S3::inputFile(): Unable to open input file: in /var/www/api/S3.php on line 224 Note : I increased upload limit in php.ini
563e0f0e2d1761a701f0f4dd	X	You should use the S3 API.
563e0f0e2d1761a701f0f4de	X	with k.open_write() as out: File "/usr/local/lib/python2.7/dist-packages/boto/s3/key.py", line 216, in open_write raise BotoClientError('Not Implemented')
563e0f0e2d1761a701f0f4df	X	I had to replace send_file() with set_contents_from_file(): otherwise I would get an error 400 (Bad Request).
563e0f0e2d1761a701f0f4e0	X	I am using amazon S3 to distribute the dynamically generated files to S3. At a local server, I can use to store generated videos to the location VIDEO_DIR.newvideo.name Is there feasible way to change VIDEO_DIR to S3 endpoint location. So the dynamically generated videos can be written to S3 server directly? Another question is: is there any feasible way to write an object to S3 directly? For example, a chunklet=Chunklet(), how to write this chunklet object to S3 server directly? I can do this first create a local file and use S3 API. For example, But I want to improve the efficiency. Python is used.
563e0f0e2d1761a701f0f4e1	X	Use the boto library to access your S3 storage. You still have to write your data to a (temporary) file first before you can send it though, as the stream writing methods have not yet been implemented. I'd use a context manager to work around that limitation: Use it as a context managed file object:
563e0f0e2d1761a701f0f4e2	X	Martijn's solution is great but it forces you to use the file in a context manager (you can't do out = s3upload(…) and print >> out, "Hello"). The following solution works similarly (in-memory storage up until a certain size), but works both as a context manager and as a regular file (you can do both with S3WriteFile(…) and out = S3WriteFile(…); print >> out, "Hello"; out.close()): (Implementation note: instead of delegating many things to self.temp_file so that the resulting class behaves like a file, inheriting from SpooledTemporaryFile would in principle work. However, this is an old-style class, so __new__() is not called, and, as far as I can see, a non-default in-memory size for the temporary data cannot be set.)
563e0f0e2d1761a701f0f4e3	X	Are you trying to extend the amazon-s3-php-class library? it looks like a stripped down version of the official sdk... You might need to download and use the official SDK to get full functionality.
563e0f0f2d1761a701f0f4e4	X	I am not sure,I thought so,but I couldn't find it on the web and the GIT link does come up each time.
563e0f0f2d1761a701f0f4e5	X	I want to store user specific images,like our FB profile does,so that I can populate the gallery on his dashboard and show it.If I store them randomly I won't be able to idemtify them on per user basis,or rather it would be more tedious!!
563e0f0f2d1761a701f0f4e6	X	You mean to say I won't be able to do what I am trying to?? :P
563e0f0f2d1761a701f0f4e7	X	@KillABug You should still be able to pull that off, you should still be able to use a directory-like naming convention too, it just won't actually have folders. My use case I had files that had to be processed and pushed into a database according to what "folder" they were in. Any files in my buckets still adhered to a directory-like naming convention: s3://myBucket/pretendFolder/fileName.gz, I would just look for the characters between the first / and the second /. Since the S3 web interface behaves like there are folders, I kept using the slashes instead of another separator character
563e0f0f2d1761a701f0f4e8	X	@KillABug I'm starting to tangent away from your actual question. For your use here, if you have a completely empty bucket s3://myBucket/ and you want to push a file into s3://myBucket/myDir/MySubDir/picture.jpg, you can just call putObject() and pass your file into the key of s3://myBucket/myDir/MySubDir/picture.jpg without checking for existance of "folders".
563e0f0f2d1761a701f0f4e9	X	I think his biggest issue is using a partially implemented library. If you go to aws.amazon.com/sdkforphp and downlod the sdk it comes as a .phar file that you can include or you can install the sdk via composer if you are so inclined. then check out this page: docs.aws.amazon.com/aws-sdk-php-2/guide/latest/quick-start.html and look at the Creating a client and Iterators sections
563e0f0f2d1761a701f0f4ea	X	I am working on the AWS api and having an issue with the check for existing objects(FOLDERS). I went through this question and it does not help me because I am using the latest updated SDK. I searched the SDK and found this which should work i.e. doesObjectExist,but I am not able to find the function definition anywhere.My s3.php file doesn't have this function.Here is my S3.php class. Also I read that S3 does not support folder structures but just visually makes it look like its stored in a folder due to the flat file system. Now,if I have to search a folder 1024x768 on S3,do I have just check the root of the bucket? I mean like this I need to check if the folder exists and if not create it on the fly dynamically using the API functions.If it exists store the files over there. I need help in achieving this functionality.Please can anyone suggest solution with their experience. Thank you for your attention.
563e0f0f2d1761a701f0f4eb	X	You may want to give a little more thought to the whole "flat file system". When I first started writing PHP to interface with the S3 API I was expecting to be able to iterate over a directory, and the files in each folder. That's not how it works though! This is from Ryan Parman, one of the responses in a question you referenced: "S3 is a flat file system. There are no folders. There are simply filenames with slashes in them. Some S3 browsing tools choose to display the concept of "folders" in their software, but they're just pretend. "/albums/Carcassonne-France/" returns false because there is not a singular object with that name." If you have objects with the following paths: s3://myBucket/myFolder/taco.jpg s3://myBucket/myFolder/jambox.jpg If you check for existence of the object s3://myBucket/myFolder/ it will return false since there is no object with that specific name. It might be helpful to know more about why you want to create the folder if it doesn't exist, but if you want to place an object into S3 in a specific "folder", you can just use the putObject() method.
563e0f0f2d1761a701f0f4ec	X	@Dan is correct. However, it might be worthwhile to check out the S3 StreamWrapper, which enables you to use PHP's built-in file system functions with S3.
563e0f0f2d1761a701f0f4ed	X	hmm valid points, but is there no way to do this using bucket policies?
563e0f102d1761a701f0f4ee	X	Well, you can't base action on specific AWS accounts, because Facebook doesn't have one (or none that we know, anyway). You can't restrict it to particular IP addresses, because there's no way to know what IP addresses Facebook might use. If Facebook does just put your URL in their pages, you can use a policy to restrict access to requests that are referred from Facebook URLs. That would do what you want, but not be very secure. It's trivial for anybody to fake the referral header.
563e0f102d1761a701f0f4ef	X	I have an amazon s3 account which I use to save images. I save images as private so that only my website can fetch them using s3 apis. However, my website has one function that posts updates to facebook. For this I need the PICTUREURL. So my question is how do I make s3 bucket available only to facebook?
563e0f102d1761a701f0f4f0	X	I doubt that you can make it available only to Facebook. And even if you could, I don't think that would make the picture visible to visitors. But here goes, anyway. To make the picture visible to Facebook, use an expiring, pre-signed URL. Instead of sending the regular S3 URL for the PICTUREURL parameter, create a URL that expires in a few minutes (long enough for Facebook to grab the picture) and use that. I'm not sure what language or S3 library you are using, but they all support creation of such a pre-signed URL. The bad news is that this might not help. That's because Facebook might just use the URL you give it to display the picture, instead of copying the picture and displaying its copy. If so, then the eventual end users will be using the URL you gave Facebook, and it's going to expire before they see it. Of course, you could use a URL that expires far in the future, but if you do that, you might as well make the S3 objects public in the first place.
563e0f102d1761a701f0f4f1	X	Can you give us more information? What does the timeout look like? Are you actually making the service call, or is it timing out trying to check out a connection? Does ThreeSharp call the REST or SOAP version of the API?
563e0f102d1761a701f0f4f2	X	Also, if you're writing a brand new piece of software, you should consider using the new AWS SDK for .NET (aws.amazon.com/sdkfornet ), as ThreeSharp is no longer being maintained.
563e0f102d1761a701f0f4f3	X	I tried to use the new SDK. Nothing odd happened. No timeout.
563e0f102d1761a701f0f4f4	X	The 2006-03-01 version of the S3 API (docs.amazonwebservices.com/AmazonS3/latest/dev/…) defines a copy operation, so there is definitely now a copy operation
563e0f102d1761a701f0f4f5	X	We're uploading files to a temporary folder in a bucket. After that, we're trying to copy the uploaded files to its actual folder then delete the files in the temporary folder. It doesn't timeout when working with a single file. We're using the ThreeSharp API. Stack Trace: [WebException: The operation has timed out] System.Net.HttpWebRequest.GetRequestStream() +5322142 Affirma.ThreeSharp.Query.ThreeSharpQuery.GenerateAndSendHttpWebRequest(Request request) in C:\Consulting\Amazon\Amazon S3\Affirma.ThreeSharp\Affirma.ThreeSharp\Query\ThreeSharpQuery.cs:386 Affirma.ThreeSharp.Query.ThreeSharpQuery.Invoke(Request request) in C:\Consulting\Amazon\Amazon S3\Affirma.ThreeSharp\Affirma.ThreeSharp\Query\ThreeSharpQuery.cs:479
563e0f102d1761a701f0f4f6	X	I believe there's no COPY function in Amazon APIs today. When you want to create a copy of an object in Amazon S3, today you must re-upload your existing object to the new name. If you do not have a copy of the object, you must first download the object and then re-uploaded to Amazon S3, incurring data transfer charges for both the download and the upload as well as a GET and PUT request charge. (from http://doc.s3.amazonaws.com/proposals/copy.html) So, program library you use is doing all this job for you - it's downloading your file to your machine first, and then uploads it back to Amazon. I suggest you to upload your file straight to it's actual folder.
563e0f102d1761a701f0f4f7	X	I've read the S3 documentation several times and I'm adding metadata to an S3 object with this code... When reading the object from the S3 bucket I'm using this code.... But I'm getting an empty string every time even though the outputFolder variable definitely has a value. Am I doing something really silly wrong here? As far as I can tell this is consistent with the documentation
563e0f102d1761a701f0f4f8	X	use this instead of reading the meta from putobject response hope this may help
563e0f102d1761a701f0f4f9	X	this doesn't really answer your question, but get_file_and_metadata returns two things. If you just want the image file, and not that metadata, i think you want to do "@db_image, metadata = @client.get_file_and_metadata('/IMG_1575.jpg')"
563e0f112d1761a701f0f4fa	X	I tried that as well. It's just a different way to get the same problem. I can get the data, but it's all just text. thanks for the reply though. i edited the post to reflect that.
563e0f112d1761a701f0f4fb	X	I have an ipad app that uses dropbox to sync images to the cloud so that i can access them with a webapp and process them etc etc. the part i'm having trouble with is getting the file from dropbox to s3 via carrierwave. i have a photo model and i can create a new photo and upload and an image successfully. I can also put a file on dropbox. However when i try to get a file off of dropbox and put it on s3, the contents of the file is just text. Are there some sort of mime types i need to set or something? I am using dropbox_sdk and the get_file_and_metadata method. It returns me the file object successfully, but the contents are all just text. this is me hard coding the image file so i can be sure it exists.. the part i don't know how to do is say take this image @db_image and use that file when creating a new photo and store it on S3. I'm thinking it might be a 'mime type' issue, but i've read that that is only based on the file ext. any insight you guys could share would really help me get past this hurdle. thanks!
563e0f112d1761a701f0f4fc	X	Figured this out. Instead I used the direct_url.url method that is part of the dropbox-api gem used with the carrierwave gem. the direct_url.url method returns a secure full url path to that file that you can use as the remote_url value for carrierwave. Now, i'm pretty new at ruby, so i'll be posting a better way to step through the results, as that seems pretty slow and clunky.
563e0f112d1761a701f0f4fd	X	The S3 API doesn't let you delete by folder directly, because folders don't 'exist' in S3 (they're a logical presentation to the user). Instead you have to specify multiple object keys to DeleteObjectsRequest. You can search for all keys under a 'folder' by specifying the Prefix property of ListObjectsRequest, so you'd combine that with delete to do what you want.
563e0f112d1761a701f0f4fe	X	I am trying to delete all the files inside a folder which is basically the date. Suppose, if there are 100 files under folder "08-10-2015", instead of sending all those 100 file names, i want to send the folder name. I am trying below code and it is not working for me. I am using the above code and it is not working.
563e0f112d1761a701f0f4ff	X	means i can't use knox or is there any way to stick up with it. If not then any other solution to implement the same. Because i have gone through link you provided but no solution.
563e0f112d1761a701f0f500	X	Well, you can give it a try. Just download/fork knox and put the code from the pull request in it. Then make sure it works for what you need. It looks like it does a lot of heavy moving for you, so should at least make it easier for you.
563e0f112d1761a701f0f501	X	ok thanks i will go for it.
563e0f112d1761a701f0f502	X	i am not able to get the expresso for the knox can you help me?
563e0f112d1761a701f0f503	X	expresso? That's a testing framework.. not quite sure what the issue is. You are probably better off posting in the pull request so that the original author can guide you along.
563e0f112d1761a701f0f504	X	I'm trying to find some example code that utilizes node.JavaScript, Express. I have gone through the some examples with knox but I did not found the multipart upload. Can someone point me to a gist or other example that contains this kind of information? Please help I will look forward for your suggestions. Thank you.
563e0f112d1761a701f0f505	X	The AwsSum library can do all of S3's Multipart operations. It's been a part of the library since inception. The following operations have been implemented, so you should be able to use them to complete your uploads in a number of operations: So that's about everything you need. :)
563e0f112d1761a701f0f506	X	Here is a pull request to the knox repository that implements such functionality. It has not been merged, so it may not work properly. But they have the link to S3's doc about how to implement the multipart upload, if that would help you. https://github.com/LearnBoost/knox/pull/17
563e0f122d1761a701f0f507	X	Yes, I have read about this but couldn't find a way to implement all of the CURL functionality with just URL Fetch.
563e0f122d1761a701f0f508	X	I'm working on a PHP application hosted on Google App Engine which requires access to objects stored in an S3 Bucket. I've looked at the APIs available for Amazon S3 and all of them make use of CURL. But CURL is not allowed in Google App Engine. Is there a way to access Bucket Contents and user other functions available in the S3 API without using CURL in PHP?
563e0f122d1761a701f0f509	X	Amazon recently released a wrapper for file_get_contents which is what urlfetch uses - so take a look at http://blogs.aws.amazon.com/php/blog/tag/stream - looks like you can do $contents = file_get_contents("s3://{$bucket}/{$key}"); You should look at the URL Fetch docs - it's the google recommended way to do calls https://developers.google.com/appengine/docs/php/urlfetch/
563e0f122d1761a701f0f50a	X	Use the URL Fetch API instead of CURL https://developers.google.com/appengine/docs/php/urlfetch/
563e0f122d1761a701f0f50b	X	Thanks geoff. You have totally put my mind at rest. I thought that I was being a bit verbose and redundant going the database route, bloating my system for the sake of ease. Incidentally, do you upload directly to S3? I'm uploading to my server then to S3. I'm not sure how I should handle my batch uploads to S3. It seems like running it under the same PHP process as the page request would be insane. How would you recommend doing it?
563e0f122d1761a701f0f50c	X	@Laykes - Firstly my site is asp.net on windows, so you'll have to adapt your process appropriately. I also upload to S3 via the server. I have a Windows service running that does the uploading - as soon as a browser upload occurs, I tell the service about it and leave it to do the uploading to S3. Our server is on EC2 so uploads to S3 are super fast. I guess you could also just have a process that runs every few minutes that looks for new files to upload. I certainly wouldn't do it in the PHP process though.
563e0f122d1761a701f0f50d	X	@Laykes - Part 2 - We also have a need to upload large(2GB) files to S3. We tried uploading directly to S3 using various 3rd party software or our own but found reliability to be a problem(at least on our internet connection). Uploading to S3 doesn't have chunked or resume support, so if an upload failed 90% in we'd have to start again. So those also go up via a server. We FTP to the server on EC2 and then move them up to S3 from there. Again, EC2 to S3 is really fast and reliable.
563e0f122d1761a701f0f50e	X	Thats reassuring. I'm currently in pre-pre-pre-alpha stages of development. I don't think I will be able to consider moving to EC2 until about February. I'm planning on being marketable around Christmas but EC2 was always my final platform to go into production on. It doesn't seem that cheap though unfortuantely. Would you agree? Money isn't so much an issue, budget would be around $500-1000 per month, but I'm not sure if I can get something massively scalable for that price.
563e0f122d1761a701f0f50f	X	I have my own CMS which has a file manager. A lot of the files and formats which people can create are stored locally in a database. These are trivial examples like CSS files, basic content etc. The file manager can do all the things thats docs.google.com does. I actually based the entire methodolgy and design around the google docs browser. Now, I am adding Amazon S3, so that my file manager will also display files uploaded to Amazon S3. I have a few logistical questions. All of my files and the heirarchical structure is stored in my assets and folders table in my mysql database. If I add Amazon S3, files will be uploaded to Amazon and I want to know how I should integrate them. I can do one of two things. Either: Whenever the user browsers any particular folder my script can also go off to Amazon and do something like: Then I can merge the results of my database query with the results. I could even cache to prevent some issues with performance. Alternatively, since I am following this structure for uploads: Client -> Server -> Amazon I need to process the files. This means that I can store a lot of the details in my database. There would be very little need to goto Amazon to list the structure because I can look locally. What do you think is the best option? I think the second option. This has a few benefits. Database Benefits Database Cons
563e0f122d1761a701f0f510	X	I have a fair bit of experience using Amazon S3 for file storage for a website and you'll definitely want to go the database route. S3 is way to slow to query all the time and as you mentioned you'll have the additional costs(albeit small). The speed becomes even more and more of an issue, the more files you have stored in a bucket as listObjects() only returns 1000 at a time. The performance issues are easy to see simply by using any of the S3 tools(eg Bucket Explorer, Cloudberry, or even Amazons own tools) to browse a bucket with lots of files. The extra effort required to ensure your database stays in sync with S3 is well worth it.
563e0f122d1761a701f0f511	X	I'm using Amazon S3 to uploads some files. But I need to grant that either all of them get uploaded or none. This is what I'm doing: This is my currently working code. However, since the document can have a lot of pages, and a fragile internet connection, then I'm thinking about deleting manually every file if an error is thrown and the pageNumber is greater than zero. But, since the error is probably due to no-connection, then I'll need to enqueue that task for when there's connection. A lot of work. Is there any way to grant via transactions that either all of the files get uploaded or none of them? I've been looking the API docs for this but can't find a way to do this.
563e0f122d1761a701f0f512	X	what exactly do you want to do? check how many keys (files) are in the bucket? if so you can use S3 LIST API, you just get keys headers without downloading entire keys
563e0f132d1761a701f0f513	X	I am on Windows server and thus won't be able to use S3CMD. There is another windows tool but it is commercial. Unfortunately I won't be able to buy it. Is there any way to calculate the bucket size on Amazon S3 using Amazon S3 API and without having to download all files on the client because I have 500K files in one of the bucket. Any help is appreciated.
563e0f132d1761a701f0f514	X	Recommendations are out of scope for Stack Overflow… but Dropbox is really easy I find.
563e0f132d1761a701f0f515	X	Exactly what I was looking for. It looks like for what I need this should be simple to implement. Thanks for the info.
563e0f132d1761a701f0f516	X	I have a TCL/TK Windows application that creates a small executable that I distribute to my customers. Because it is an exe file I can not email the file. Instead I upload it to an Amazon S3 bucket then create a URL link and email the link to them. They download the file from the link and run the exe. What I would like to do is add the ability upload to an Amazon bucket within the application that will enable me to upload the file and create a URL that I can copy and email to the customer. I have seen Amazon S3 API's written for other languages, python, java, but not TCL. Has anyone done this? How hard is it? Can you point me to a tutorial? Actually I do not have to use a S3 bucket. If there is another suggestion for how to distribute small files to customers from within TCL programs I am open to suggestions. Besides what has been laid out above the only other requirement is that multiple people must be able to upload to the same location, the TCL program runs on Windows and I would like to not use a 3rd party program. Security is not a major concern, nor is privacy, these things are handled other ways.
563e0f142d1761a701f0f517	X	Actually, Tcl does provide an S3 package, but since I don't have Amazon S3 account, I cannot test it out.
563e0f142d1761a701f0f518	X	I wanna get a file's properties stored in amazon s3, is it easy to do that using api or php?
563e0f142d1761a701f0f519	X	The easiest way for PHP is the AWS SDK for PHP - Google it. Here are PHP examples of all of the properties you can receive: http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html some include getBucket, getObjectACL, list files, list buckets, etc You can also use the command line awscli here: http://aws.amazon.com/cli/
563e0f142d1761a701f0f51a	X	How to upload file to Amazon S3 in Qt 4 using their REST API? Maybe there are ready libraries already? Thanks!
563e0f142d1761a701f0f51b	X	Uploading a file using a REST API is just using HTTP POST method. You can already find answer on this question on Stackoverflow - see Upload file with POST method on Qt4
563e0f142d1761a701f0f51c	X	Was messing around that issue yesterday, and found QCloud repo ( on 7 page oF Google ), that is a realy good point to star with... repo author's summary with some examples and fundamentals
563e0f142d1761a701f0f51d	X	why not use the AWS Elastic Transcoder for this
563e0f142d1761a701f0f51e	X	after a little resarch i think this is what you need to look at encoding.com/api#OutputDestinations
563e0f142d1761a701f0f51f	X	Exactly the same as encoding.com's Output Destinations pointed out by Dragon above. I definitely appreciate the response though. Thanks.
563e0f152d1761a701f0f520	X	does S3 service is a requirement?
563e0f152d1761a701f0f521	X	I am creating a video website and I have codeigniter libraries that are successfully uploading the files to amazon S3 and encoding the files with encoding.com to MP4's. My problem is that if I upload the file to S3 first, and then send to encoding.com it works perfectly but it doesn't send back to amazon S3. Would it be proper to just somehow download the encoding.com URL onto my server and then reupload it to S3 again? The url is like: http://encoding.com.result.s3.amazonaws.com/684981684684-681684384384_53169775.mp4 I don't see anything in the encoding.com api about reuploading the finished file back to S3 or onto the host server. Is it standard practice to just use the encoding.com generated URL to show the client side files? Like using encoding.com as a CDN? I'm just confused on the best order to do what I'm trying to accomplish. Anyone have any ideas?
563e0f152d1761a701f0f522	X	Probably not exactly the answer you're looking for, but this is super straight forward with zencoder. S3 to s3 is fully supported and one of the nice, easy features about the platform. An s3 URL looks something like: "url": "s3://mybucket/path/file.mp4" Works for both inputs and outputs. Zencoder is straightforward to implement, faster, and has better features so save yourself some time and energy.
563e0f152d1761a701f0f523	X	It looks like you are using only the AWS SDK for PHP 2.x adapter, so you do not need to include "amazonwebservices/aws-sdk-for-php" in your composer.json.
563e0f152d1761a701f0f524	X	im using symfony2 as framework form my application, i want to connect to my ceph client as it is said in API (same as amazon s3). composer.json: im using gaufrette bundle to manage my filesystem what i got so far is: so following the api's this is the way i'm connecting to amazon s3 serwers but im not using amazon s3 serwers im using my own ceph serwers. and my question is how i change the host in amazon s3 client ? now it's like this: [curl] 56: Problem (2) in the Chunked-Encoded data [url] https://mybucket.s3.amazonaws.com and i want this https://mybucket.s3.amazonaws.com to be https://mybucket.s3.custom.com here is the api i followed: https://github.com/KnpLabs/KnpGaufretteBundle#awss3
563e0f152d1761a701f0f525	X	Try setting the base_url parameter. Please check the AWS SDK for PHP User Guide to see all of the client configuration options.
563e0f152d1761a701f0f526	X	i'm pretty sure it's not the case but maybe the storage is FAT32 formated just comes in my mind when you say "files less than 4GB work" --> ntfs.com/ntfs_vs_fat.htm
563e0f152d1761a701f0f527	X	Found the cause. It is in the Java SDK 1.1.7.1 implementation. Please see this post on Amazon forum: [RepeatableFileInputStream skip() causes problem for large files >10GB: bug?] (forums.aws.amazon.com/thread.jspa?threadID=62975&tstart=0)
563e0f152d1761a701f0f528	X	I implemented S3 multi-part upload in Java, both high level and low level version, based on the sample code from http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?HLuploadFileJava.html and http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?llJavaUploadFile.html When I uploaded files of size less than 4 GB, the upload processes completed without any problem. When I uploaded a file of size 13 GB, the code started to show IO exception, broken pipes. After multiple retries, it still failed. Here is the way to repeat the scenario. Take 1.1.7.1 release, So far the problem shows up consistently. I did a tcpdump. It appeared the HTTP server (S3 side) kept resetting the TCP stream, which caused the client side to throw IO exception - broken pipe, after the uploaded byte counts exceeding 8GB . Anyone has similar experiences when upload large files to S3 using multi-part upload?
563e0f152d1761a701f0f529	X	thanks allot :)
563e0f152d1761a701f0f52a	X	i'm trying to use AFAmazonS3Manager to upload some files to amazon s3, but i have 403 forbidden error, i try to list all objects in a bucket, same error. the bucket is read/write for all. code : the error : i use transmit (mac) and the samples of the Amazon s3 iOS API and it work fine, i want to use NSURLSession to upload files (with AFNetworking). PS: in AFAmazonS3Manager, some competition block have'long long' as parameters but AFNetwoking use NSIteger, so i modify them, i don't think that's the problem, but i'm really stack on this so ..
563e0f162d1761a701f0f52b	X	The reason why this is failing is because there is a bug on how the signature is being calculated in the AFAmazonS3RequestSerializer. It's trying to create the signature using: And at that point, self.bucket is null and request.URL.path contains the bucket. You can workaround that issue either by doing this: or this:
563e0f162d1761a701f0f52c	X	I had the same issue when using the putObjectWithFile method. I was not setting a bucket but passing the bucket name on destinationPath which AFAmazonS3Manager appears to intend to support. Either bucket property is set or pass it on path. I submitted a pull request here with the fix that worked for me https://github.com/AFNetworking/AFAmazonS3Client/pull/38 You are also correct on having to change long long to NSInteger.
563e0f162d1761a701f0f52d	X	great question! just out curiosity can you tell us why, what you are trying to do , what are these files?
563e0f162d1761a701f0f52e	X	is it ok for me to ask these questions?
563e0f162d1761a701f0f52f	X	I am wondering why such requirement appears. If you need to replace all files at once, maybe there's some way to upload them to temporary bucket in a regular way and then change bucket names?
563e0f162d1761a701f0f530	X	You could have a look to JetS3t, which is quite fully featured in regard to S3 syncing with multithreading.
563e0f162d1761a701f0f531	X	This method still uses individual put operations and is not inherently faster than anything else. The answer was accepted but it seems that all you've done is point to a tool that does the same thing he could do in code.
563e0f162d1761a701f0f532	X	as with the previous answer, the implication here seems to be that these tools are somehow doing something that can't otherwise be accomplished with the API and I don't believe that is the case
563e0f162d1761a701f0f533	X	Does amazon s3 support batch uploads? I have a job that needs to upload each night ~100K of files that can be up to 1G but is strongly skewed towards small files (90% are less than 100 bytes and 99% are less than 1000 bytes long). Does the s3 API support uploading multiple objects in a single HTTP call? All the objects must be available in S3 as individual objects. I cannot host them anywhere else (FTP, etc) or in another format (Database, EC2 local drive, etc). That is an external requirement that I cannot change.
563e0f162d1761a701f0f534	X	Does the s3 API support uploading multiple objects in a single HTTP call? No, the S3 PUT operation only supports uploading one object per HTTP request. You could install S3 Tools on your machine that you want to synchronize with the remote bucket, and run the following command: Then you could place this command in a script and create a scheduled job to run this command each night. This should do what you want. The tool performs the file synchronization based on MD5 hashes and filesize, so collision should be rare (if you really want you could just use the "s3cmd put" command to force blind overwriting of objects in your target bucket). EDIT: Also make sure that you read the documentation on the site I linked for S3 Tools - there are different flags needed for whether you want files deleted locally to be deleted from the bucket or ignored etc.
563e0f172d1761a701f0f535	X	Alternatively, you can upload S3 via AWS CLI tool using the sync command. aws s3 sync local_folder s3://bucket-name You can use this method to batch upload files to S3 very fast.
563e0f172d1761a701f0f536	X	seph, thanks for the suggestions...
563e0f172d1761a701f0f537	X	I am looking for a sample vb.net project that will upload a file to Amazon S3 storage. I am happy to use any official Amazon API, but do not want to use a 3rd party product. Regards, Leigh
563e0f172d1761a701f0f538	X	Last time I checked, I found a lot of useful information over at CodeProject: Beginning with Amazon S3 by StormSpirit Team. It's in C#, but you can easily convert it to VB.NET with online-converter, i.e. Telerik's Code Converter
563e0f172d1761a701f0f539	X	Okay I have understood now how it works. Basically I was setting the life cycle for the entire folder instead of a specific prefix. But 100 lifecycles per bucket are pretty much nothing. This is what I am trying to achieve: People will upload files on an S3, and those files will be deleted after X days (Bascially I file uploaded site with a trial option of 7 days). What is the best way of doing this?
563e0f172d1761a701f0f53a	X	Once you set a lifetime configuration to your bucket, it will be valid for all of its files. This means that, if you have a files/ folder (or, technically, prefix), and you apply a lifetime policy of 7 days expiration for the files prefix on your bucket, each and every file uploaded with this prefix will get deleted 7 days after it was uploaded. This seems to be exactly what you are looking for. You do not need a policy per file. A single one will fit your needs.
563e0f172d1761a701f0f53b	X	It doesn't seem to work that way. If I apply an expiration to the folder, all the files within the folder will get expiry the same day.
563e0f172d1761a701f0f53c	X	The prefix I set is *.png for 7 days.
563e0f172d1761a701f0f53d	X	@jQuerybeast Regarding your first comment, you said exactly what I said :) Regarding your second comment, *.png is not really a prefix, but a suffix. It won't work.
563e0f172d1761a701f0f53e	X	Amazon S3 API has added Object Expiration which deleted all the files uploaded within a folder after few days. Is it possible to make the same for each file from the day it was uploaded? For example when I upload foo.png, after X days, delete that file not all the files within the folder.
563e0f172d1761a701f0f53f	X	Your file path is not more than a prefix in S3. So, if you have a structure as follows: And you want your rule to apply only to foo.png, set it to "folder1/folder3/foo.png" (there will be only one file matching the "entire-name" prefix in your bucket). But be aware of the limits regarding number of rules. From Object Expiration docs: To set an object’s expiration, you add a lifecycle configuration to your bucket, which describes the lifetime of various objects in your bucket. A lifecycle configuration can have up to 100 rules. Each rule identifies an object prefix and a lifetime for objects that begin with this prefix. The lifetime is the number of days since creation when you want the object removed.
563e0f182d1761a701f0f540	X	Let me explain my requirement.. Is there a workaround? Can I do away with temporary file store on my server.. Please let me know if I am not clear.. EDIT - Is it OK to get byte array from FileItem object and store it rather than the file itself..?
563e0f182d1761a701f0f541	X	Your whole idea is to avoid I/O right ? you don't need to save the file before doing the upload, you could simple send the array of bytes to amazon REST API. Here is my sample VB.NET code that do both upload and download:
563e0f182d1761a701f0f542	X	ok thanks thats what i thought I just wanted to make sure lol.
563e0f182d1761a701f0f543	X	Is there a way I can use s3's object lifecycle to prune through current versioned objects and delete those objects premantely that have a deleted marker on them in lets say a month or a week? If there isn't, how is house cleaning performed on versioned buckets?
563e0f182d1761a701f0f544	X	This page describes object lifecycle management generically. This page describes lifecycle configuration more specifically. You cannot do precisely what you want with S3's built-in lifecycle. With the versioning feature, there are two types of objects: the current version, and old non-current versions. The current version is the latest, most recently uploaded version of your object. For now, if the latest thing you've done to an object is cover it with a delete-marker, treat the delete-marker as the current version. With S3's lifecycle, you can set up a rule to permanently delete non-current versions after X days. (You can also set up rules to move to glacier after X days.) But it sounds like what you're asking is for a rule where you can permanently delete non-current-but-only-if-the-current-is-a-delete-marker. This isn't natively supported. If you want to only delete objects whose current version is a delete marker, you'll have to write your own listing agent to walk your bucket, enumerate these objects, and delete them yourself. There might be existing tools for this already; I haven't checked.
563e0f182d1761a701f0f545	X	Yes lifecycle configuration can be set on a bucket programmaticallysimply by using Amazon S3 API or in console Amazon S3. Normally there are some lag before an updated or may be new lifecycle configuration ispropagated to Amazon S3 systems. There may be few minutes delay beforeit fully takes effect. and on disabling or deleting a lifecycle rule, after a small delay Amazon S3 stops scheduling new objects for deletion or transition. Any objects that were already scheduled will be unscheduled and will not be deleted or transitioned. Deletion Two out comes are possible: If the version ID maps to a specific object version, then Amazon S3 deletes the specific version of the object. If the version ID maps to delete marker of that specific object, Amazon S3 deletes the delete marker.means that object appear again in bucket. check it http://docs.aws.amazon.com/AmazonS3/latest/dev/DeletingObjects.html
563e0f182d1761a701f0f546	X	I tried to display the image from my amazon s3 buckets to web browser. But the result gave me an error "The specified key doesnt exist". The file is exist inside buckets. Here is my codes I've tried to do like: What I want to do is I want to download images from my bucket on AWS. Please advise. Thank you
563e0f182d1761a701f0f547	X	This either means the key does not exist... or at least does not exist where you are looking. Have you tried with a key that has a simpler name just to verify? Are you sure the object in S3 has the .jpg extension? Are you using versioning? What if you don't set the response header override? What if you set the region explicitly to the region the object is in?
563e0f182d1761a701f0f548	X	I wish videojs would offer some rtmp support, support for youtube videos, and maybe a bit more HTTP Live Streaming support and/or documentation. I love VideoJS :) and don't want to ever have to use JWPlayer :(
563e0f192d1761a701f0f549	X	what does it mean "Our service is actually used for transcoding the video itself" ?
563e0f192d1761a701f0f54a	X	This looks nice, but is there any way to have these performance benefits without flash? Moving from html5 video TO flash doesn't seem like the best idea to do nowadays...
563e0f192d1761a701f0f54b	X	@c089 very good question. I wonder the same thing. Did you find any info on doing the same with html5?
563e0f192d1761a701f0f54c	X	nope, the project I was thinking about at the time never got anywhere and after that the requirement never popped up... but look at brandons answer above mentioning videojs.com :)
563e0f192d1761a701f0f54d	X	If I make an Amazon s3 MP4 resource publically availible and then throw the Html5 Video tag around the resource's URL will it stream? Is it really that simple. There are a lot of "encoding" api's out there such as pandastream and zencoder and I'm not sure exactly what these companies do. Do they just manage bandwidth allocation(upgrading/downgrading stream quality and delivery rate/cross-platform optimization?) Or do encoding services do more then that.
563e0f192d1761a701f0f54e	X	This is Brandon from Zencoder. What you're looking for is probably something like Video JS (videojs.com) for video playback. You can just upload an MP4 to S3 and reference it in a player (or the video tag directly, but that has additional issues). Our service is actually used for transcoding the video itself, not delivery. We actually created Video JS to help our customers (and the web at large) with easy, compatible HTML5 playback. If you have any other questions just ask. Thanks.
563e0f192d1761a701f0f54f	X	Answer to first part of your question is, YES it is really that simple. There is a howto about it and a working demo at the end of the article that you can see as a proof of concept. Hope this helps.
563e0f192d1761a701f0f550	X	Amazon S3 is a really good choice for serving up video content. We've been using it for a couple of years with no issues and the cost has been unbeatable. You should also look at using Amazon CloudFront and configuring your media to use their "streaming distributions". It basically uses your S3 files but copies them to edge locations around the internet and uses RTMP to provide a better playback experience for users and to save you money on bandwidth. http://aws.amazon.com/cloudfront/
563e0f192d1761a701f0f551	X	Amazon S3 in combination with Amazon CloudFront as scalable CDN is pretty streight forward and good to build great video solutions, even Netflix-like systems using adaptive bitrate (ABR) video in HTML5 using the Media Source Extentions with MPEG-DASH or HLS, like done by Netflix or Youtube. Here you can find a pretty good tutorial on that: http://www.bitcodin.com/blog/2015/02/create-mpeg-dash-hls-content-for-amazon-s3-and-cloudfront/
563e0f192d1761a701f0f552	X	I realize this question has been asked here, however I'm still unable to get this to work. This works (forwarding) as expected. Now I'm trying to create the record in #3 using the SDK. This does create the record, however it is not forwarding. I notice when I examine the record in the web control panel, the Alias Hosted Zone ID is not appearing. Upon further examination I noticed that the Alias Target name has a . at end. s3-website-us-east-1.amazonaws.com. If I manually remove that extra "." the Hosted Zone ID appears. I have no idea how that extra dot is getting there (the code I'm using below doesnt have this).
563e0f192d1761a701f0f553	X	What about the TripsCpnstants and How to get return server url to view the image. Please help
563e0f1a2d1761a701f0f554	X	Edited have you got it, its just the expiry date of your image
563e0f1a2d1761a701f0f555	X	No i didn't get any edits. What if i dont want any expiry date....is it mendatory?
563e0f1a2d1761a701f0f556	X	If you dont set the expiry date it will affect you when you reads the image. Better to set expiry date but not mandatory.
563e0f1a2d1761a701f0f557	X	Ok fine. I will set it but can you please tell me how to receive server path where image is uploaded by this code.
563e0f1a2d1761a701f0f558	X	I am trying to implement Amazon S3 SDK to upload image from Android application but always getting this 400 error, malformed xml bad request. I've taken source code from this reference link I've correct access_key, secret_key and bucket_key. There is no error related to this. If anybody have a working sample in order to access this S3 API, please share. I am not able to find SDK error and stuck with the same.
563e0f1a2d1761a701f0f559	X	I think this would help you to upload image EDIT Use this for generating URL
563e0f1a2d1761a701f0f55a	X	Is there a way to programmatically access an Amazon S3 account's usage data? I'm building an application that will charge end-users for their use of my Amazon S3 bucket. Because I'll be the middleman between AWS and the end user for PUT/DELETE operations, I'll be able to keep track of uploads and storage usage, but I'm allowing users to directly access their files with public access links so I won't be able to directly monitor downstream usage. As such, my plan is to check download usage regularly. Is there anywhere in the AWS API where I can access usage statistics?
563e0f1a2d1761a701f0f55b	X	There is no way to get usage statistics with the API. This issue is actively discussed on the AWS forum over the years with no feedback from the AWS team. https://forums.aws.amazon.com/thread.jspa?messageID=277024 The alternatives would be to turn on Amazon S3 server log and to analyze it yourself. Another option would be to take advantage of Amazon DevPay service. Thanks Andy EDIT: Here's the official Amazon S3 documentation related to Amazon S3 access logs: http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?ServerLogs.html
563e0f1a2d1761a701f0f55c	X	Since December 2011, if you have premium support you can monitor your estimated charges (either aggregated or per-server) using CLoudWatch. There's no official documentation right now but I wrote up an overview: http://blog.bitnami.org/2011/12/monitor-your-estimated-aws-charges-with.html
563e0f1a2d1761a701f0f55d	X	Has anyone had any experience using Amazon S3 file uploads on Xamarin? It should be simple, but I am having trouble getting it to work. I'm trying to use https://github.com/xamarin/amazon to do my upload of a file like so: But I'm getting this exception: System.ObjectDisposedException: The object was used after being disposed. at System.Net.WebConnection.BeginWrite (System.Net.HttpWebRequest request, System.Byte[] buffer, Int32 offset, Int32 size, System.AsyncCallback cb, System.Object state) [0x0001f] in /Developer/MonoTouch/Source/mono/mcs/class/System/System.Net/WebConnection.cs:1033 at System.Net.WebConnectionStream.BeginWrite (System.Byte[] buffer, Int32 offset, Int32 size, System.AsyncCallback cb, System.Object state) [0x0026c] in /Developer/MonoTouch/Source/mono/mcs/class/System/System.Net/WebConnectionStream.cs:541 The github repo hasn't been updated in a year so maybe it's just broken? All I want to do is PUT and DELETE files so my next step is to just hit the REST API with RestSharp rather than use a wrapper but surely this is something others have done, can anyone shed some light?
563e0f1a2d1761a701f0f55e	X	One alternative which may be of interest, is to create a new AWS account (thus benefiting from the free tier) and add it alongside an existing account, via consolidated billing. Then all stats/logs etc are tied solely to that account. Naturally if you already have a large setup and whatnot then this may not be simple - thought I would put it out there anyway :)
563e0f1a2d1761a701f0f55f	X	That was my initial thought but i'm going to have dozens of clients to handle simultaneously. So this solution is not the right one for my specific issue.Thanks for the help though :)
563e0f1a2d1761a701f0f560	X	Thanks, i'll try that, this seems to be the right way to go !
563e0f1a2d1761a701f0f561	X	I'd like to know if there's a way for me to have bucket-level stats in amazon s3. Basically i want to charge customers for storage and GET requests on my system (which is hosted on s3). So i created a specific bucket for each client, but i can't seem to get the stats just for a specific bucket. I see the API lets me or But i just can't find how to get the number of requests issued to said bucket and the total size of the bucket. Thanks for help ! Regards
563e0f1a2d1761a701f0f562	X	I don't think that what you are trying to achieve is possible using Amazon API. The GET Bucket request does not contain usage statistics (requests, etc) other than the timestamp of the latest modification (LastModified). My suggestion would be that you enable logging in your buckets and perform the analysis that you want from there. S3 starting page gives you an overview on it: Amazon S3 also supports logging of requests made against your Amazon S3 resources. You can configure your Amazon S3 bucket to create access log records for the requests made against it. These server access logs capture all requests made against a bucket or the objects in it and can be used for auditing purposes. And I am sure there is plenty of documentation on that matter. HTH.
563e0f1b2d1761a701f0f563	X	Thanks for instant reply but I'm using paperclip and want to generate multi sized images, In case If I saved file localy then I've to make local file for each size of image and then upload it on amazon which will become headache when I've to maintain url of amazon not changes and upload image to same url.
563e0f1b2d1761a701f0f564	X	I'm using Paperclip, ImagMagick, Rmagick, Amazon-s3 I'm getting this error when getting an image from url and after custom resizing image replacing the changed image to amazon. Magick::ImageMagickError (no encode delegate for this image format //s3.amazonaws.com/beu-dev/temp_images/final_images/000/000/377/original/template_37720121205-5921-99989h.png' @ error/constitute.c/WriteImage/1153): app/models/temp_image.rb:38:inwrite' line#38 is last line before end of this method Note: One more thing This code works perfectly when using system file system, but when started using amazon s3 Error happening
563e0f1b2d1761a701f0f565	X	I think you need to use the local file name instead of URL. Imagemagick can't just write the file to URL via http. To replace the source file you need to use the Amazon S3 API.
563e0f1b2d1761a701f0f566	X	Issue solved. 1. Check "identify -list format" If you don't see the .jpeg format in the list. Then you need to add the .jpeg library. curl -O http://www.ijg.org/files/jpegsrc.v8c.tar.gz Sudo make install Now reinstall imagemagick then everything works fine now.
563e0f1b2d1761a701f0f567	X	Hi! Welcome to StackOverflow! StackOverflow is for programming questions, and this is not a question. If your question is "how do I do this?", please explain what you have already tried.
563e0f1b2d1761a701f0f568	X	Ok. So I got response after successfully uploaded the files into amazon s3. In that response having location,date,success code etc..So What I need to pass to get the file with get request.
563e0f1b2d1761a701f0f569	X	Sounds good @Michael but using api means sdk right
563e0f1b2d1761a701f0f56a	X	Yes, as far as I know you can't set up an expiring url using the AWS Console, so you have to use the api in some fashion. That could be via the command line tools though, if that helps.
563e0f1b2d1761a701f0f56b	X	My requirement is download a files from Amazon s3 with out using Aws sdk for java and using rest api calls.
563e0f1b2d1761a701f0f56c	X	If you make the file publicly accessible, you can use a simple url to download it. If you need it to be private you will need to use an api or a tool which handles the authorization for you. You can make the file public as part of the upload api call or after the fact using the AWS Console. You can also create expiring urls that are only usable for a set period of time using the api.
563e0f1b2d1761a701f0f56d	X	I further clarified this in my own answer.
563e0f1b2d1761a701f0f56e	X	The question is how do you avoid distributing your secret key with your app when you provide S3 space for users? ASIHTTPRequest expects a secret key.
563e0f1c2d1761a701f0f56f	X	let's start off with the problem statement: My iOS application has a login form. When the user logs in, a call is made to my API and access granted or denied. If access was granted, I want the user to be able to upload pictures to his account and/or manage them. As storage I've picked Amazon S3, and I figured it'd be a good idea to have one bucket called "myappphotos" for instance, which contains lots of folders. The folder names are hashes of a user's email and a secret key. So, every user has his own, unique folder in my Amazon S3 bucket. Since I've just recently started working with AWS, here's my question: What are the best practices for setting up a system like this? I want the user to be able to upload pictures directly to Amazon S3, but of course I cannot hard-code the access key. So I need my API to somehow talk to Amazon and request an access token of sorts - only for the particular folder that belongs to the user I'm making the request for. Can anyone help me out and/or guide me to some sources where a similar problem was addressed? Don't think I'm the first one and the amazon documentation is so extensive that I don't really know where to start looking. Thanks a lot!
563e0f1c2d1761a701f0f570	X	Have you looked at the Amazon AWS SDK for iOS? From the docs: The AWSiOSDemoTVM and AWSiOSDemoTVMIdentity samples demonstrate a more secure mechanism for transferring AWS security credentials to a mobile client. These samples require a server application, in this case the token vending machine (TVM), which is provided as a separate download. The sample applications register with TVM, either anonymously or with a user-supplied user name and password. The TVM uses the AWS Security Token Service to get temporary security credentials and pass them to the mobile application. The TVM is available in two forms, one that supports anonymous registration and one that requires a user name and password to register a device and receive security tokens. To download and install the TVM for Anonymous Registration, go to http://aws.amazon.com/code/8872061742402990. To download and install the TVM for Identity Registration, go to http://aws.amazon.com/code/7351543942956566. From Authenticating Users of AWS Mobile Applications with a Token Vending Machine: This article discusses an architecture that enables applications running on a mobile device to more securely interact with Amazon Web Services such as Amazon Simple Storage Service (S3), Amazon SimpleDB, Amazon Simple Notification Service (SNS), and Amazon Simple Queue Service (SQS). The architecture discussed uses a "Token Vending Machine" to distribute temporary security credentials to the mobile application. Your token can limit access to a specific bucket on S3, so it appears to be the best option.
563e0f1c2d1761a701f0f571	X	ASIHTTPRequest has direct support for Amazon S3. http://allseeing-i.com/ASIHTTPRequest/S3
563e0f1c2d1761a701f0f572	X	To further clarify Terry Wilcox's answer... You need to generate temporary security credentials on your server using AWS STS. STS is AWS' "Security Token Service". It allows you to create access keys programmatically and set specific permissions and expiration dates. Since you already have an API/backend for your app that authenticates your users, you can make an API call that will generate temporary AWS credentials that only have access to that user's folder. If you do not have a backend for your app, Amazon provides a Java app call TVM (Token Vending Machine) that you can easily deploy your own instance of to Elastic Beanstalk. Relevant AWS articles: http://aws.amazon.com/articles/4611615499399490 http://docs.aws.amazon.com/STS/latest/UsingSTS/STSUseCases.html#MobileApplication
563e0f1c2d1761a701f0f573	X	You can restrict the user access to folder level. Refer this sample Credential Management.
563e0f1c2d1761a701f0f574	X	Start by getting rid of the @, so you'll see any errors being displayed
563e0f1c2d1761a701f0f575	X	But readfile() is probably a better function to use if all you're doing with the file is spooling it direct to the browser
563e0f1c2d1761a701f0f576	X	yes, I know. However, I will have to write the buffer to somewhere else as well. that's why I am using reading it manually rather than using 'readfile'
563e0f1c2d1761a701f0f577	X	Just to check, I tried with readfile and interestingly, it is not working either. However, all three ways works just fine if I give a path for local version of the file. But when its trying to retrieving from s3, only 'file_get_contents' is working, other two ways isn't. Just wondering, is this anything to do with the fact that, I am trying from my local system. It might work if I try on a ec2 server instance? Do you think so?
563e0f1c2d1761a701f0f578	X	If you're seeing the binary data with the headers removed, then it suggests that the problem is with your headers, not with the file access from S3
563e0f1c2d1761a701f0f579	X	This article on the AWS PHP Development Blog might be helpful as well: Streaming Amazon S3 Objects From a Web Server
563e0f1c2d1761a701f0f57a	X	I am using amazon s3 API and setting the client to read as stream. It is working fine for me to use file_get_contents("s3://{bucket}/{key}"), which read the full data for the file(I am using video file & testing on my local system). However, I am trying to optimize the memory used by the script and thus trying to read and return data by chunk as below: This is not working on my local system. I am just wondering what might be the issue using this technique. by searching, I found that this is also a very widely used technique. So, if anybody can please give any suggestion about what might be wrong here or any other approach, I should try with, it will be very helpful. Thanks.
563e0f1c2d1761a701f0f57b	X	OK, finally got the solution. Somehow, some other output are being added to buffer. I had to put: In this way to clean anything if were added. Now its working fine. Thanks Mark Baker for your valuable support through the debugging process.
563e0f1d2d1761a701f0f57c	X	It's probably not a good idea unless you either want to make all those files available for deletion by anyone, or have your private secret key in your public flash file (easily findable with a decompiler or even a hex editor).
563e0f1d2d1761a701f0f57d	X	In addition to what @JonatanHedborg said, you may also encounter problems when including Authorization in your header. More info here: helpx.adobe.com/flash-player/kb/… The best approach is probably to communicate with an intermediate server-side service.
563e0f1d2d1761a701f0f57e	X	according to Amazon S3 REST API deleting an object requires a DELETE request, like But the URLRequest.method field in Flash can be set only to GET or POST, so I cannot create such a request. Any idea?
563e0f1d2d1761a701f0f57f	X	Could it be possible to make a POST request to your server and then make a DELETE request from let's say php to amazon?
563e0f1d2d1761a701f0f580	X	Check out the as3httpclient lib, it makes DELETE requests available
563e0f1d2d1761a701f0f581	X	Thanks Jamie, that did the trick, bounty coming in 16 hours
563e0f1d2d1761a701f0f582	X	Thanks for the lead Rocky, but I was unable to find a way to store the file as rrs.
563e0f1d2d1761a701f0f583	X	The zend framework provides a php wrapper for the amazon S3 api that simplifies the lower level REST functionality. http://framework.zend.com/manual/en/zend.service.amazon.s3.html For example to store a file in s3 all you need to do is By default, objects are stored in buckets as regular storage. Is there any functionality in the zend framework that will allow me specify objects to be stored as Reduced Redundancy Storage (RRS) in S3? If not is there any way I can set the default storage of all objects in a bucket as RRS?
563e0f1d2d1761a701f0f584	X	http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectPOST.html Have you tried the following after RockyFords advice? The code below should upload it in the correct mode.
563e0f1d2d1761a701f0f585	X	It looks like you can pass a third argument to Zend_Services_S3::putFile and putObject that is $meta and it accepts a scalar or an array. and S3 just wants a header change to set an object or file as RRS: Q: How do I specify that I want to store my data using RRS? All objects in Amazon S3 have a storage class setting. The default setting is STANDARD. You can use an optional header on a PUT request to specify the setting REDUCED_REDUNDANCY. you may have to dig in the API a bit to find exactly what to pass, but this should point you in the right direction.
563e0f1e2d1761a701f0f586	X	Are the files already on S3? If not, can't you concatenate (or zip) before uploading?
563e0f1e2d1761a701f0f587	X	Is there a way to concatenate small files which are less than 5MBs on Amazon S3. Multi-Part Upload is not ok because of small files. It's not a efficient solution to pull down all these files and do the concatenation. So, can anybody tell me some APIs to do these?
563e0f1e2d1761a701f0f588	X	Amazon S3 does not provide a concatenate function. It is primarily an object storage service. You will need some process that downloads the objects, combines them, then uploads them again. The most efficient way to do this would be to download the objects in parallel, to take full advantage of available bandwidth. However, that is more complex to code. I would recommend doing the processing on "in the cloud" to avoid having to download the objects across the Internet. Doing it on Amazon EC2 or AWS Lambda would be more efficient and less costly.
563e0f1e2d1761a701f0f589	X	Thank you, sound good!
563e0f1e2d1761a701f0f58a	X	I am developing an API using Codeigniter and I want to let users upload images to my Amazon S3 account. I am using Phils Restserver and Donovan Schönknecht S3 library (for Ci). It works perfectly to upload a local file to Amazon but how can I get the image file sent via normal external form? Using the built in Ci upload library it works fine but then I have to store the files locally on my own server and I want them on S3. Can the two be combined? I guess what I am asking is how can I "get" the image file that is sent to the controller and resize it and then upload it to S3? Do I perhaps need to temporary save it on the local server, upload it to S3 and then remove it from the local server? This is my "upload" modal: Thankful for all input!
563e0f1e2d1761a701f0f58b	X	If I understand you correctly, you want a user to upload an image via a form, resize that image, then transfer that to Amazon S3. You'll have to store the file locally (at least for a few seconds) to resize it with CI. After you resize it, then you can transfer it to Amazon S3. In your success callback from the transfer, you can delete the image from your server.
563e0f1e2d1761a701f0f58c	X	You should definitely check out the CI S3 library. The "spark" is available here - http://getsparks.org/packages/amazon-s3/versions/HEAD/show
563e0f1e2d1761a701f0f58d	X	This does not provide an answer to the question. Please expand on what exactly solved the problem.
563e0f1e2d1761a701f0f58e	X	This does not provide an answer to the question. To critique or request clarification from an author, leave a comment below their post - you can always comment on your own posts, and once you have sufficient reputation you will be able to comment on any post.
563e0f1e2d1761a701f0f58f	X	I have an issue with "submitted-image-url" option when post a share using LinkedIn API. All my images stored on Amazon S3. For example "https://s3.amazonaws.com/news-img/client_619/619_1424690228983-DarthVaderSEOToaster2.jpg". When I try to use different source for image, from whatever another website, it works well. Could you please help me? Why images from Amazon S3 cannot be fetched by LinkedIn? Do I need add some exceptions in my S3 bucket? One more, with other social networks like Twitter and Facebook everything works fine. Thank you, Oleg
563e0f1e2d1761a701f0f590	X	Tricky http headers... Need to set correct 'content-type' header when upload images to S3 bucket. E.g. 'Content-Type': image/' + imageExtension. Solved!
563e0f1f2d1761a701f0f591	X	I am using high level api for amazon s3 upload and getting a call back for progress but the method upload.getprogress().getPercentageTransferred() at times gives percentage above 100..Why does it shows above 100? Are there any changes needed
563e0f1f2d1761a701f0f592	X	haha, thanks for the reply, I could be mistaken...but I'm so confused that I'm just not sure... $.getJSON("api.github.com/users/githubuser/events") Works fine on a test server on say my desktop (or even from my other websites if I make it through the Chrome developer console) but when I try to make the same call after I've uploaded that website to Amazon S3 then it does not work because I get the XMLHttpRequests error. Logically this suggests to me that it has something to do with my configuration on the Amazon S3 bucket since it did work in a test environment and certain websites?
563e0f1f2d1761a701f0f593	X	I'm hosting a static website via Amazon S3 and I have some Javascript on there that will make a call to another domain and parse the XML, JSON, or whatever that it gets. I followed the many posts on stackoverflow and various blog posts it linked to that claimed to get it working but even after following very closely I could never replicate the results. I even tried adding with and without the following to the rule, The following link allows you to test if CORS is enabled by sending XMLHttpRequests and it says it is not valid so CORS is not set up or recognized properly. http://client.cors-api.appspot.com/client/ A possible lead is what is suggested in Amazon S3 documentation here, http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETcors.html that says we need to set the "s3:GetCORSConfiguration" permission, which I did via a line like... "Action": ["s3:GetObject", "s3:GetCORSConfiguration"], in the "edit bucket policy" section from the AWS control panel but it gives an error and cannot save because it doesn't recognize this action? A potentially similar post on stackexchange here, HTTP GET to amazon aws from jquery or XMLHttpRequest fails with Origin is not allowed by Access-Control-Allow-Origin seems to suggest that if I have a website hosted on S3 that it can not configure it to make XMLHttpRequests that are GET to a 3rd party resource? I feel like I'm going in circles...anyone out there have any leads/advice? Thanks.
563e0f1f2d1761a701f0f594	X	I think you are confused or I misread your question. You enable CORS on your site so other sites can make requests to your page. Enabling CORS on your S3 site will allow example.com to talk to your S3 page. It does not allow your site to talk to example.com. In order for you to make requests to other domains, they have to enable the privileges. You can not magically turn it on for their domains. It is like saying that you give permission to yourself to walk into the whitehouse and use the president's bathroom. When you hop the fence, the secret service will deny that request with force.
563e0f1f2d1761a701f0f595	X	There might be some relation here to the example we just worked through on our apps which was related to the https which was a hardcoded configuration on some of our clients machines. I don't know exactly what that means but it might be a place to check. instead of http:// try https:// XMLHttpRequest to Amazon S3 fails only on some computers
563e0f1f2d1761a701f0f596	X	but it failed, Its just that too many bugs will occur due to file naming, server timeout issues, etc.... This isn't too helpful, as it is unclear what you want your end result to be. Do you want each video file to be a separate S3 object in the same bucket? Do you want a zipped file of your whole 1 TB directory? Do you want the buckets to mirror your directory structure? Please clarify what your desired output is.
563e0f1f2d1761a701f0f597	X	You want to try something like s3cmd to handle the upload.
563e0f1f2d1761a701f0f598	X	I know how to upload one single file to Amazon S3, I use this: I have a large 1TB directory of videos I want to upload them on Amazon S3. I tried to loop recursively through each directory and upload each file alone, but it failed. Its just that too many bugs will occur due to file naming, server timeout issues, etc... I want the bucket to mirror my exact directory structure. The folder I want to copy is held on a dedicated server serving Apache. Is there a way I could just upload the folder through the API? It is also 1TB, so what's the best solution?
563e0f1f2d1761a701f0f599	X	Even better, use the official SDKs or CLI tools. Also, if you're using PHP in 2014, but not using Composer, you're doing it wrong.
563e0f1f2d1761a701f0f59a	X	Does the virus scanner on your machine check outgoing files?
563e0f202d1761a701f0f59b	X	just for google: one of the best command line free anti-viruses is clamAV multi platform so you can work with any OS, and use Any programming lang that can execute system applications, php, java, c# most high level programming langs
563e0f202d1761a701f0f59c	X	How can I check files before uploading to Amazon S3,any tips? Updated: It need for my web-site when users upload a file i want to check whether it's not infected. Is there any API which allows for check files programmatically ? Thanks in advance!!!
563e0f202d1761a701f0f59d	X	I think the best solution is @Joe's answer to a similar question for c#: I would probably just make a system call to run an independent process to do the scan. There are a number of command-line AV engines out there from various vendors.
563e0f202d1761a701f0f59e	X	If you mean manually: try the online virusscanner at http://www.virustotal.com/ - It checks against a lot of antivirus programs for you.
563e0f202d1761a701f0f59f	X	Our webserver can serve but we need some more complexity in php code, image uploaded status will be kept in db. It seems the only solution for the time being, though.
563e0f202d1761a701f0f5a0	X	Well, how about you process the image on your server, add it to the queue database, display it to the user and then run a background task uploading it to S3?
563e0f202d1761a701f0f5a1	X	Yes, a field in images table should be added to check if it's uploaded yet and set its address accordingly.
563e0f202d1761a701f0f5a2	X	can anyone explain this part of the process please "add it to the queue database"
563e0f202d1761a701f0f5a3	X	I am looking for a best practice while uploading images to amazon s3 server and serving from there. We need four different sizes of an image. So just after image upload we convert the image and scale in 4 different widths and heights. And then we send them to the amazon s3 using official php api. But for a 1M image the client sometimes wait up to 30 seconds which is a very long time. Instead of sending images immediately to S3, it may be better to add them to a job queue. But the user should see the uploaded image immediately.
563e0f202d1761a701f0f5a4	X	You could simply achieve that by queuing the files, in for example: a database, and then running a cron job, or having a constantly running php script. If you say you want the users to see the images instantly, should they see them instantly on S3?
563e0f202d1761a701f0f5a5	X	Yeah, ideally you'd sign the request just as you're sending them instead of all-at-once at the beginning.
563e0f212d1761a701f0f5a6	X	@RyanParman yes, that's right, but my library is in JS and I need to generate signatures server-side, and I'd rather make less requests to the server.
563e0f212d1761a701f0f5a7	X	Ah, gotcha. Yeah, that makes a difference. :)
563e0f212d1761a701f0f5a8	X	check out my edit
563e0f212d1761a701f0f5a9	X	I'm trying to upload a large file (1.5GB) to Amazon S3 by using the REST Api and HTML5 file slicing. Here's how the upload code looks like (code stripped down for readability): chunk_size is 6MB. After a chunk finishes uploading, the next one follows, and so on. But sometimes (every 80 chunks or so), the PUT request fails, with e.type == "error", e.target.status == 0 (which surprises me), and e.target.responseText == "". After a chunk fails, the code re-attempts to upload it, and gets the exact same error. When I refresh the page and continue the upload (the same chunk!), it works like a charm (for 80 chunks or so, when it gets stuck again). Here's how the request looks in chrome dev tools:  Any ideas why this might happen, or how to debug something like this? EDIT: Here is the OPTIONS response: 
563e0f212d1761a701f0f5aa	X	I finally found the issue by sniffing packets: there are two issues: for PUT requests that get a 4xx (didn't test for other non-2xx responses), the xhr request returns as aborted (status = 0); still haven't found an explanation for that, check out Why does a PUT 403 show up as Aborted? Amazon S3 responded with a 403 that said RequestTimeTooSkewed, because my signatures are generated when the upload starts, and after 15 minutes (the timeout that triggers the RequestTimeTooSkewed error), it fails, and the signatures have to be regenerated. That 403 error is never seen in the dev tools console or by the js code, because of the first problem.. After regenerating the signatures, everything works like a charm.
563e0f212d1761a701f0f5ab	X	Did you verify whether browser is making any 'OPTIONS' request. If yes what is the response headers.
563e0f212d1761a701f0f5ac	X	Did you ever make any progress on this? I am working on a similar issue.
563e0f212d1761a701f0f5ad	X	I'm developing a web application wrapped in Phonegap to upload recorded videos to Amazon S3. I've done some research, and have concluded there are two ways to do this: use the Amazon AWS JavaScript SDK, or using the Phonegap FileTransfer API (along with the Amazon S3 REST API). When you capture a video in a PhoneGap application, it's only possible to request the URI to the video file (not the file contents itself). You can use this URI in a FileTransfer to post to S3 like so: (From http://coenraets.org/blog/2013/09/how-to-upload-pictures-from-a-phonegap-app-to-amazon-s3/) When you use the AWS SDK, you can upload files like so: (From http://www.cheynewallace.com/uploading-to-s3-with-angularjs/) I have two questions:
563e0f212d1761a701f0f5ae	X	In my web application amazon s3 is using image uploading following code is used for uploading images. But documents are not uploading using this earlier I was using same code for uploading both. Now I changed the API parameter SourceFile to Body then document is working fine but image is not in other case images working fine. I gave Body for upload documents and SourceFile for upload images. How can I use same api for both?
563e0f212d1761a701f0f5af	X	May be specifying is creating problem for image. Also for image you will need to provide SourceFile. Please check aws php sdk doc
563e0f212d1761a701f0f5b0	X	The Body parameter represents the actual content of the file you are uploading. This can be a string of data or a resource created with fopen(). The SourceFile parameter allows you to specify the path on disk to a file, and the SDK will take care of opening the file to retrieve and send its contents. Which one you choose to use has nothing to do with the type of file it is. For ContentType, the SDK will attempt to guess the content-type of the data/file you provide, and you only need to specify it, if the content-type cannot be determined.
563e0f222d1761a701f0f5b1	X	This worked like a charm! Thank you SO much for your clear and straightforward answer! You're a lifesaver!
563e0f222d1761a701f0f5b2	X	I am trying to upload an image file from my Trigger.io mobile app directly to Amazon S3 (see here: http://aws.amazon.com/articles/1434). I'm able to do this on the web without any problems using jQuery and the FormData API as follows: However, I'm unable to get this working with the Forge request API. This is what I have tried: But, I receive the following error from Amazon: I would circumvent this forge.request entirely in favor of $.ajax, but my file was retrieved using the Forge File API and just shows up on S3 as [object Object] (I assume because it's a Forge file, not a true file object from an HTML <input />). So, how can I upload a file in Trigger.io to Amazon S3 using FormData and the Forge API? Any help is greatly appreciated! Thanks!
563e0f222d1761a701f0f5b3	X	You are able to use the jQuery ajax $.ajax function and the FormData JavaScript API to upload an image file from your Trigger.io mobile app directly to Amazon S3 as you have done on the web. You will need to perform the following steps: As an example, once you have retrieved your image file (Step 1), you could use the following uploadImage function to upload your image to Amazon S3:
563e0f222d1761a701f0f5b4	X	A Bucket policy would do the trick: stackoverflow.com/questions/13169108/…
563e0f222d1761a701f0f5b5	X	I'd like to investigate possibility to filter uploads on Amazon S3 via filetype (extension). For example, I'd like to provide access to upload only *.jpg and *.png files into specific bucket. Is it possible to restrict via standard S3 web-interface or via some API calls (I'll write my own little script for that).
563e0f222d1761a701f0f5b6	X	When posting to s3 i expect a response from them but that never happens. The images post, and i get a request from my api, but i do not get one from S3. I get a 204 No Content error, failure to load response. but if i scroll down it actually shows the url in the location part also the images actually get posted to s3. the code looks like this: form.html main.js Amazon controller Amazon model
563e0f222d1761a701f0f5b7	X	dup? stackoverflow.com/questions/16485629/…
563e0f222d1761a701f0f5b8	X	also refer this netpalantir.it/news/index/…
563e0f222d1761a701f0f5b9	X	I'm writing an app that allows users to browse and upload large files to Amazon S3 via a web application. I'm using the Amazon AWS library for .Net. However because the files are large I want to provide a 'Cancel' option on the website. I can't see anything in the API as to how to cancel a file that's uploading using S3. Does anyone know how to cancel file uploads? Thanks.
563e0f232d1761a701f0f5ba	X	You can do it just by using Thread.Abort()
563e0f232d1761a701f0f5bb	X	I don't see in the second code example where the union between old and new metadata is taking place. When I execute the code, the metadata for the object gets overwritten with the latest metadata.
563e0f232d1761a701f0f5bc	X	Seems to work, but the new key loses the Content-Type, which is really bad when using S3 to serve web images.
563e0f232d1761a701f0f5bd	X	For those of you using the AWS SDK you need to use AmazonS3Client.copyObject method
563e0f232d1761a701f0f5be	X	Does it copying the content of the key or only the metadata? Copying the whole key takes time, and decrease the efficiency, right?
563e0f232d1761a701f0f5bf	X	The question asked for Python, but this was the only answer using the Java SDK that I've been able to find on SO. Thanks!
563e0f232d1761a701f0f5c0	X	Been a while since I've used it, but I'm pretty sure that's among the things passing the original k.metadata field on the copy accomplishes.
563e0f232d1761a701f0f5c1	X	If you have already uploaded an object to an Amazon S3 bucket, how do you change the metadata using the API? It is possible to do this in the AWS Management Console, but it is not clear how it could be done programmatically. Specifically, I'm using the boto API in Python and from reading the source it is clear that using key.set_metadata only works before the object is created as it just effects a local dictionary.
563e0f232d1761a701f0f5c2	X	It appears you need to overwrite the object with itself, using a "PUT Object (Copy)" with an x-amz-metadata-directive: REPLACE header in addition to the metadata. In boto, this can be done like this: Note that any metadata you do not include in the old dictionary will be dropped. So to preserve old attributes you'll need to do something like: I almost missed this solution, which is hinted at in the intro to an incorrectly-titled question that's actually about a different problem than this question: Change Content-Disposition of existing S3 object
563e0f232d1761a701f0f5c3	X	In order to set metadata on S3 files,just don't provide target location as only source information is enough to set metadata.
563e0f242d1761a701f0f5c4	X	You can change the metadata without re-uloading the object by using the copy command. See this question: Is it possible to change headers on an S3 object without downloading the entire object
563e0f242d1761a701f0f5c5	X	For the first answer it's a good idea to include the original content type in the metadata, for example:
563e0f242d1761a701f0f5c6	X	If you want your metadata stored remotely use set_remote_metadata Example: key.set_remote_metadata({'to_be': 'added'}, ['key', 'to', 'delete'], {True/False}) Implementation is here: https://github.com/boto/boto/blob/66b360449812d857b4ec6a9834a752825e1e7603/boto/s3/key.py#L1875
563e0f242d1761a701f0f5c7	X	In Java, You can copy object to the same location. Here metadata will not copy while copying an Object. You have to get metadata of original and set to copy request. This method is more recommended to insert or update metadata of an Amazon S3 object
563e0f242d1761a701f0f5c8	X	here is the code that worked for me. I'm using aws-java-sdk-s3 version 1.10.15
563e0f242d1761a701f0f5c9	X	I receive the binary file on Node.js, which means that I don't have an image path like I'd have if I had my image in a directory on my computer. Does this mean that I can't use the fs library because that deals with filesystem?
563e0f242d1761a701f0f5ca	X	Are you using express or connect (which both use formidable to parse file uploads) to handle 'localhost:3031/upload/image';?
563e0f242d1761a701f0f5cb	X	I'm using express to handle 'localhost:3031/upload/image'. I can then access the uploaded file by looking at the req.body (request's body). Of course, it's a jumble. Not sure how to turn that into a Buffer that Amazon S3's pushObject wants.
563e0f242d1761a701f0f5cc	X	so then you don't need to convert it to a Buffer at all, you just use fs.stat(req.body) to get the file size and then fs.createReadStream(req.body) to create a stream to then pass that to S3 as the Body param
563e0f242d1761a701f0f5cd	X	fs.stat(req.body) complains that it needs a string (req.body is binary data, which is not a string). If I convert the binary .jpg to a string, I won't be able to access it later using a web browser though.
563e0f242d1761a701f0f5ce	X	I'm trying to take an image and upload it to an Amazon S3 bucket using Node.js. In the end, I want to be able to push the image up to S3, and then be able to access that S3 URL and see the image in a browser. I'm using a Curl query to do an HTTP POST request with the image as the body. curl -kvX POST --data-binary "@test.jpg" 'http://localhost:3031/upload/image' Then on the Node.js side, I do this: My file is 0 bytes, as shown on Amazon S3. How do I make it so that I can use Node.js to push the binary file up to S3? What am I doing wrong with binary data and buffers? UPDATE: I found out what I needed to do. The curl query is the first thing that should be changed. This is the working one: curl -kvX POST -F foobar=@my_image_name.jpg 'http://localhost:3031/upload/image' Then, I added a line to convert to a Stream. This is the working code: So, in order to upload a file to an API endpoint (using Node.js and Express) and have the API push that file to Amazon S3, first you need to perform a POST request with the "files" field populated. The file ends up on the API side, where it resides probably in some tmp directory. Amazon's S3 putObject method requires a Stream, so you need to create a read stream by giving the 'fs' module the path where the uploaded file exists. I don't know if this is the proper way to upload data, but it works. Does anyone know if there is a way to POST binary data inside the request body and have the API send that to S3? I don't quite know what the difference is between a multi-part upload vs a standard POST to body.
563e0f242d1761a701f0f5cf	X	I believe you need to pass the content-length in the header as documented on the S3 docs: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html After spending quite a bit of time working on pushing assets to S3, I ended up using the AwsSum library with excellent results in production: https://github.com/awssum/awssum-amazon-s3/ (See the documentation on setting your AWS credentials) Example: UPDATE So, if you are using something like Express or Connect which are built on Formidable, then you don't have access to the file stream as Formidable writes files to disk. So depending on how you upload it on the client side the image will either be in req.body or req.files. In my case, I use Express and on the client side, I post other data as well so the image has it's own parameter and is accessed as req.files.img_data. However you access it, that param is what you pass in as img_path in the above example. If you need to / want to Stream the file that is trickier, though certainly possible and if you aren't manipulating the image you may want to look at taking a CORS approach and uploading directly to S3 as discussed here: Stream that user uploads directly to Amazon s3
563e0f252d1761a701f0f5d0	X	I just answered this on stackoverflow.com/questions/12186993/…
563e0f252d1761a701f0f5d1	X	EDIT: Emerson Farrugia answered this at: What is the algorithm to compute the Amazon-S3 Etag for a file larger than 5GB? As you may already know, when one uploads large files using the amazon multipart upload API: "Complete Multipart Upload" will return an ETag with the following format: where the number is the number of parts uploaded with "Upload Part". Now my question is how this multiparthash is calculated. I find something in a blog stating that: where Ln is the index and Hn is the ETAg of part n. But this, for me at least, is not working. Is there any official documentation on this or any know-how?
563e0f252d1761a701f0f5d2	X	I'm using Amazon's provided High-Level API to upload files to Amazon S3. I use a lightly-modified version of the example provided: Meanwhile, from another thread, I'm measuring its progress: The progress reporting itself seems to be working well, as I'm getting progress that makes sense. However, once it reaches 100%, the upload stalls. It looks a lot like the call to isDone() is blocking, as it simply will not update. The percentage, once it gets to 100%, will not update again. It appears to update twice and then hang. If I check for the existence of the file externally, using Cyberduck, it appears to have uploaded the file successfully.
563e0f252d1761a701f0f5d3	X	From your code snippet, you don't get any more updates because you have fallen through your loop, since upload.isDone() is true. If you add: after the end of your loop, you will see the Completed message. You probably see multiple 100% messages, because the TransferManager is waiting for the upload to complete.
563e0f252d1761a701f0f5d4	X	Added the edgecast tag for you. Where are your images now? What language would you like to do the upload with (you have to call curl from somewhere, even if it's a bash script).
563e0f252d1761a701f0f5d5	X	I'd like to upload in PHP
563e0f252d1761a701f0f5d6	X	Don't you have to expose your secret key for this?
563e0f252d1761a701f0f5d7	X	no of cause not! then it would not be secret. You just have to use your secret key to generate a HASH, then send the HASH to S3 which will use your secret key to authenticate your HASH
563e0f262d1761a701f0f5d8	X	Here is a tutorial for php on howto do it: ioncannon.net/programming/1539/… if you want to search it then search "amazon s3 CORS" and there should be allot that pops up.
563e0f262d1761a701f0f5d9	X	We have a web application which needs to store uploaded images with EdgeCast or Amazon S3. To do so we need to re-upload those images to EdgeCast / S3 first. There are a number of options I can think of: Which is the best solution, and are there any other ones? I suspect it is either 1 or 2. Edit: My application is in PHP
563e0f262d1761a701f0f5da	X	Not sure about EdgeCast but I know with Amazon S3 the best way to do this was to POST the file directly to the file server. See http://doc.s3.amazonaws.com/proposals/post.html Doing it this way you would provide a HTML FORM with some field like folder id, file name, public key, timestamp etc to ensure it was secure and only you could upload to the server. The server would redirect their browser once the upload was complete from that page they redirected to you could inspect the query string to find out if the upload was successful or not then record the FileID into your DB. Works great for reducing load on your server and gives the user a faster experience, however it can lead to orphan files if the upload succeeds but the DB insert fails.
563e0f262d1761a701f0f5db	X	EdgeCast storage supports Rsync/sFTP configuration to automatically sync content from a storage server. EdgeCast also supports "reverse-proxy" or "Customer Origin" configuration to pull content from another web-server into cache automatically. You can use S3 as this Customer-origin location or use EdgeCast's own Cloud Storage service or any other Web-server outside the EdgeCast network.
563e0f262d1761a701f0f5dc	X	What I do is upload the files to a temp directory then have a cron script run that PUT's the files onto AWS, so as not to cause the upload process to take any longer for the end-user.
563e0f262d1761a701f0f5dd	X	AWS provides an SDK for PHP to do this. It even has support for multi-part uploads, which is great news for developers. It should be noted that the AWS SDK for PHP will also do retry logic and (if you're using Multipart, which you should) can resume failed uploads.
563e0f262d1761a701f0f5de	X	I have developed an iOS application which can upload larger videos to an Amazon S3 server In order to do that, I have used : I have integrated a MD5 checksum to my put request to in-order to validate after file uploaded to Amazon server. Now I want to validate the checksum after the file has been uploaded to server. I need to know what is the Amazon API that I can get the checksum after upload to server. I've read below article too and it says we can measure the checksum for the data integrity. Which object gives the checksum after file uploaded? Update S3PutObjectResponse eTag function will give you md5 hash of the uploaded file , but now it gives me an error when i try to print value "ErrorCode:BadDigest, Message:The Content-MD5 you specified did not match what we received."
563e0f262d1761a701f0f5df	X	Thanks for the reply. I tried this approach but now, its not resuming the upload (java.awsblog.com/post/Tx2Y9J2CUDVJ5LZ/…). Following are the steps which are not working: 1. Once the information from persistable Upload is serialized in a file, when the data is retrieved and deserialized, I call the method transferManager.resumeUpload(persistableUpload); This gives no error but doesn't do anything.
563e0f262d1761a701f0f5e0	X	I'm trying to upload a file on Amazon S3 using their APIs. I tried using their sample code and it creates various parts of files. Now, the problem is, how do I pause the upload and then resume it ? See the following code as given on their documentation: I have also tried the TransferManager example which takes an Upload object and calls a tryPause(forceCancel) method. But the problem here is, it gets cancelled everytime I try and pause it. My question is, how do I use the above code with pause and resume functionalities ? Also, just to note that I would also like to upload multiple files with same functionalities.... Help would be much appreciated.
563e0f262d1761a701f0f5e1	X	I think you should use the Transfer Manager sample if you can. If it's being canceled, it's likely that it just isn't possible to pause it(with the given configuration of the TransferManager you are using). This might be because you paused it too early to make "pausing" mean anything besides canceling, you are trying to use encryption, or the file isn't big enough. I believe the default minimum file size is 16MB. However, you can change the configuration of the TransferManager to allow you to pause depending on tryPause is failing, except in the case of encryption where I don't think there's anything you can do. If you want to enable pause/resume for a file smaller than that size, you can call the setMultipartUploadThreshold(long) method in TransferManagerConfiguration. If you want to be able to pause earlier, you can use setMinimumUploadPartSize to set it to use smaller chunks. In any case, I would advise you to use the TransferManager if possible, since it's made to do this kind of thing for you. It might be helpful to see why the transfer is not being paused when you use tryPause.
563e0f262d1761a701f0f5e2	X	TransferManager performs the upload and download asynchronously and doesn't block the current thread. When you call the resumeUpload, TransferManager returns immediately with a reference to Upload. You can use this reference to enquire on the status of the upload.
563e0f262d1761a701f0f5e3	X	Thanks so much for the thorough answer. I'll take a look into this info.
563e0f272d1761a701f0f5e4	X	I have a huge number of objects on Amazon S3, of which only a small subset are accessed regularly. Thus, I'd really like to make use of a distributed caching system, like Ehcache. (Note, I'd use Cloudfront, but the data needs to be accessed from an API server, not from an enduser, and the Cloudfront does not support authentication last time I checked.) Can anyone tell me whether or not this is feasible, practical, or whether or not there exists a library or example of using Ehcache to cache objects from Amazon S3? Naturally, my app server is implemented in Java and is running on a Linux environment. Thanks much.
563e0f272d1761a701f0f5e5	X	Interesting idea - However, before diving into it eventually, I'd like to stress that authentication is available in Amazon CloudFront since September 2009, albeit likely not as you envision it, i.e. you can use a Signed URL to Serve Private Content: You can distribute private content with a static signed URL or a dynamic signed URL. You use a static signed URL when distributing private content to a known end user [...]. In this case, you create a signed URL and make the URL available to your end users as needed. You use a dynamic signed URL to distribute content on-the-fly to an end user for a limited purpose [...] In this case, your application generates the signed URL. This is further detailed in the Overview of Private Content: A CloudFront private distribution is based on a policy statement that specifies any or all of the following constraints: [emphasis mine] Whether this approach is feasible for your use case depends on the architecture of your solution, insofar you'll need to generate these signed URLs by some means and use those from the API server in turn; given your 'end user' is the API server, you might pre generate static URLs as suggested, on the other hand the most obvious approach might be to perform the signed URL generation process dynamically in the API server itself and cache the generated URL<->Signed URL map for reuse eventually (i.e. via Memcached or Ehcache indeed). This authentication scheme is obviously more cumbersome than simple HTTP authentication for example, on the other hand it provides more flexibility as well, see e.g. the tutorial Restricting Access to Files in a CloudFront Distribution Based on Geographic Location (Geoblocking) for an advanced use case, which is summarized in a Guest Post: Geo-Blocking Content With Amazon CloudFront on the AWS blog as well.
563e0f272d1761a701f0f5e6	X	I ended up using JetS3t to read files from S3 and store them in a distributed ehcache cluster. The approach works quite well so far, although I find that JetS3t creates a large number of temporary files that have to be dealt with.
563e0f272d1761a701f0f5e7	X	did you find a solution ?
563e0f272d1761a701f0f5e8	X	Any update here?
563e0f272d1761a701f0f5e9	X	I'm using Simpl3r, a simple high level Android API for robust and resumable multipart file uploads using the Amazon S3 service, to upload media files to my bucket. On some uploads, I'm getting a SSLException error. Here's the code where the exception is thrown: (My class is a subclass of an IntentService, as per the Simpl3r example) Here's the stack trace: The exception is not caught by my Exception clause. Meaning that the app is stuck in a "uploading" state that never ends. Any ideas?
563e0f272d1761a701f0f5ea	X	How long do these uploads take? Could you be getting reset?
563e0f272d1761a701f0f5eb	X	Very simple: do not store any secrets client-side. You will need to involve a server to sign the request.
563e0f272d1761a701f0f5ec	X	You'll also find that signing and base-64 encoding these requests is much easier server-side. It doesn't seem unreasonable to involve a server here at all. I can understand not wanting to send all of the file bytes to a server and then up to S3, but there's very little benefit to signing the requests client-side, especially since that will be a bit challenging and potentially slow to do client-side (in javascript).
563e0f272d1761a701f0f5ed	X	please note that this uses Signature v2 which will soon be replaced by v4: docs.aws.amazon.com/AmazonS3/latest/API/…
563e0f272d1761a701f0f5ee	X	Make very sure to add ${filename} to the key name, so for the above example, user/eric/${filename} instead of just user/eric. If user/eric is an already existing folder, the upload will silently fail (you will even be redirected to the success_action_redirect) and the uploaded content will not be there. Just spent hours debugging this thinking it was a permission issue.
563e0f272d1761a701f0f5ef	X	I'm implementing a direct file upload from client machine to Amazon S3 via REST API using only JavaScript, without any server-side code. All works fine but one thing is worrying me... When I send a request to Amazon S3 REST API, I need to sign the request and put a signature into Authentication header. To create a signature, I must use my secret key. But all things happens on a client side, so, the secret key can be easily revealed from page source (even if I obfuscate/encrypt my sources). How can I handle this? And is it a problem at all? Maybe I can limit specific private key usage only to REST API calls from a specific CORS Origin and to only PUT and POST methods or maybe link key to only S3 and specific bucket? May be there are another authentication methods? "Serverless" solution is ideal, but I can consider involving some serverside processing, excluding uploading a file to my server and then send in to S3.
563e0f282d1761a701f0f5f0	X	I think what you want is Browser-Based Uploads Using POST. Basically, you do need server-side code, but all it does is generate signed policies. Once the client-side code has the signed policy, it can upload using POST directly to S3 without the data going through your server. Here's the official doc links: Diagram: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html Example code: http://docs.aws.amazon.com/AmazonS3/latest/dev/HTTPPOSTExamples.html The signed policy would go in your html in a form like this: Notice the FORM action is sending the file directly to S3 - not via your server. Every time one of your users wants to upload a file, you would create the POLICY and SIGNATURE on your server. You return the page to the user's browser. The user can then upload a file directly to S3 without going through your server. When you sign the policy, you typically make the policy expire after a few minutes. This forces your users to talk to your server before uploading. This lets you monitor and limit uploads if you desire. The only data going to or from your server is the signed URLs. Your secret keys stay secret on the server.
563e0f282d1761a701f0f5f1	X	You're saying you want a "serverless" solution. But that means you have no ability to put any of "your" code in the loop. (NOTE: Once you give your code to a client, it's "their" code now.) Locking down CORS is not going to help: People can easily write a non-web-based tool (or a web-based proxy) that adds the correct CORS header to abuse your system. The big problem is that you can't differentiate between the different users. You can't allow one user to list/access his files, but prevent others from doing so. If you detect abuse, there is nothing you can do about it except change the key. (Which the attacker can presumably just get again.) Your best bet is to create an "IAM user" with a key for your javascript client. Only give it write access to just one bucket. (but ideally, do not enable the ListBucket operation, that will make it more attractive to attackers.) If you had a server (even a simple micro instance at $20/month), you could sign the keys on your server while monitoring/preventing abuse in realtime. Without a server, the best you can do is periodically monitor for abuse after-the-fact. Here's what I would do: 1) periodically rotate the keys for that IAM user: Every night, generate a new key for that IAM user, and replace the oldest key. Since there are 2 keys, each key will be valid for 2 days. 2) enable S3 logging, and download the logs every hour. Set alerts on "too many uploads" and "too many downloads". You will want to check both total file size and number of files uploaded. And you will want to monitor both the global totals, and also the per-IP address totals (with a lower threshold). These checks can be done "serverless" because you can run them on your desktop. (i.e. S3 does all the work, these processes just there to alert you to abuse of your S3 bucket so you don't get a giant AWS bill at the end of the month.)
563e0f282d1761a701f0f5f2	X	If you don't have any server side code, you security depends on the security of the access to your JavaScript code on the client side (ie everybody who has the code could upload something). So I would recommend, to simply create a special S3 bucket which is public writeable (but not readable), so you don't need any signed components on the client side. The bucket name (a GUID eg) will be your only defense against malicious uploads (but a potential attacker could not use your bucket to transfer data, because it is write only to him)
563e0f282d1761a701f0f5f3	X	You can do this by AWS S3 Cognito try this link here : http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-examples.html#Amazon_S3 Also try this code Just change Region, IdentityPoolId and Your bucket name  
563e0f282d1761a701f0f5f4	X	To create a signature, I must use my secret key. But all things happens on a client side, so, the secret key can be easily revealed from page source (even if I obfuscate/encrypt my sources). This is where you have misunderstood. The very reason digital signatures are used is so that you can verify something as correct without revealing your secret key. In this case the digital signature is used to prevent the user from modifying the policy you set for the form post. Digital signatures such as the one here are used for security all around the web. If someone (NSA?) really were able to break them, they would have much bigger targets than your S3 bucket :)
563e0f282d1761a701f0f5f5	X	I'm working on a javascript code that records images from webcam and sends them directly to amazon storage via REST API. I've checked some JS webcam plugins like JQuery webcam plugin (http://www.xarg.org/project/jquery-webcam-plugin/) and JPEGCam (http://code.google.com/p/jpegcam/), but I couldn't make them send compressed data as custom-formatted REST query. Does anyone know how to: I know it is possible to get raw image data and compress it manually in javascript, but I hoped for a native method that would be both more efficient and easier to use...
563e0f292d1761a701f0f5f6	X	"Resource id #20" means that you're trying to echo the variable that contains some sort of resource (in your case perhaps it is handler to curl).
563e0f292d1761a701f0f5f7	X	It's very, very unlikely that a shared server can handle uploading a 1.8GB file using PHP.
563e0f292d1761a701f0f5f8	X	overriding the max execution time in your local file, or even in a local php.ini doesn't necessarily have any effect in a shared hosting environment, as your host may have those disabled.
563e0f292d1761a701f0f5f9	X	@Mihir, could you please share the php code that you tried.
563e0f292d1761a701f0f5fa	X	I know about that but file is less than 2 GB. I get fopen error if file is bigger than 2 GB. So it's not the issue in this case.
563e0f292d1761a701f0f5fb	X	I am trying to upload a big file (around 1.8GB) to amazon s3 using their php api. Somehow, upload fails. I have added lines to increase timeout, so I don't think it should be an issue. What I get is "Resource id #20" everytime. I tried uploading small files and that worked fine. I am executing this script from a shared server, could it be due to resource limitation? Please help me solve this.
563e0f292d1761a701f0f5fc	X	Have you looked at using the multipart/large file upload support. See the rest api or its easier if you use the php sdk
563e0f292d1761a701f0f5fd	X	PHP has a signed integer Problem by uploading files larger than 2GB - see here: http://bugs.php.net/bug.php?id=27792
563e0f292d1761a701f0f5fe	X	I am using LibS3, a C library that talks to my amazon S3 server and I have noticed that for checking the existence of a bucket libs3 sends a GET request with a query in the URL "?location", and the amazon S3 server responds with an appropriate HTTP response if the bucket exists or not. The problem is that in the documentation of the Amazon S3 server for checking the existence of a bucket it's said that you must make a HTTP HEAD request, http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html, so I'm confused on what method it's used in the server... If the both of them are supported and how is list objects different from check existence using GET.
563e0f292d1761a701f0f5ff	X	You can send both GET and HEAD requests to check existence of a bucket. Amazon will return 404 HTTP code if it doesn't exist. The main difference is that HEAD does not contain request body (thus takes less bandwidth).
563e0f292d1761a701f0f600	X	Let the server do all the work and just send back the URLs. It keeps the client lightweight and you won't have to reimplement functionality on every supported platform.
563e0f2a2d1761a701f0f601	X	For my app I store/receive images from amazon s3. In the desktop client my API/server handles the file upload or download. But now I wonder if I should do the same when it comes to my iOS app? Should I download the S3 sdk for the iOS and make my phone scale/upload the images to amazon s3 / making api calls to S3 in order to get the image urls? Or should I first send the images from the phone to the server. Then let the server/backend scale and upload the images to S3 / let my server send the images urls to the iOS app?
563e0f2a2d1761a701f0f602	X	As a general rule of thumb, keep your client as dumb as possible. If you already have this functionality on your server, you don't want to re-implement it.
563e0f2a2d1761a701f0f603	X	I am very much new to java NIO. Since I only send the request and socket connections are handled by amazon API itself, can we still use Selectors here?
563e0f2a2d1761a701f0f604	X	If the entire connection is handled by a (Java) API then probably not. As far as I understood your question I thought you were doing the calls on your own and use a REST-API.
563e0f2a2d1761a701f0f605	X	I am trying to list the objects from s3 bucket which has very large data. I am using "listObjects" method in multiple threads (50 to 100 threads). For each thread, I am giving commonPrefix which the API will list all the objects under that commonprefix. I have to use many threads in pull the large data in reasonable time. So I defined 100 threads but I am facing the following exceptions: I want to know the efficient use of multithreads for amazon s3 requests
563e0f2a2d1761a701f0f606	X	It sounds like Amazon doesn't want you to do 100 queries at a time. In addition 100 connections will put a huge strain on the user's network connection and probably slow down the whole thing more than you think, at least on some systems. It would be smarter to use a NIO Selector instead of 100 threads.
563e0f2a2d1761a701f0f607	X	Connect (docs.aws.amazon.com/redshift/latest/mgmt/…) and submit your CREATE TABLE and COPY commands
563e0f2a2d1761a701f0f608	X	Did you manage to get this working ? do you have any blog post or anything related to how this is done ? tx
563e0f2a2d1761a701f0f609	X	The correct way is using a jdbc driver and treating redshift as a psql database. Here is an example I posted for a ruby programmer. stackoverflow.com/questions/24438238/…
563e0f2a2d1761a701f0f60a	X	for my own learning, why did you decide to go from: S3 -> Redshift instead of S3 -> Kinesis -> Redshift?
563e0f2a2d1761a701f0f60b	X	Kinesis is not a bridge between S3 and Redshift. Kinesis is an endpoint you would stream data to... process it.. and place that process data into S3 and/or Redshift
563e0f2a2d1761a701f0f60c	X	I currently have a file in S3. I would like to issue commands using the Java AWS SDK, to take this data and place it into a RedShift table. If the table does not exist I would like to also create the table. I have been unable to find any clear examples on how to do this so I am wondering if I am going about it the wrong way? Should I be using standard postgres java connectors instead of the AWS SDK?
563e0f2b2d1761a701f0f60d	X	Connect (http://docs.aws.amazon.com/redshift/latest/mgmt/connecting-in-code.html#connecting-in-code-java) and submit your CREATE TABLE and COPY commands
563e0f2b2d1761a701f0f60e	X	I am wondering the best way to achieve de-duplicated (single instance storage) file storage within Amazon S3. For example, if I have 3 identical files, I would like to only store the file once. Is there a library, api, or program out there to help implement this? Is this functionality present in S3 natively? Perhaps something that checks the file hash, etc. I'm wondering what approaches people have use to accomplish this. Thanks!
563e0f2b2d1761a701f0f60f	X	You could probably roll your own solution to do this. Something along the lines of: To upload a file: To upload subsequent files: To read a file: You could also make this technique more efficient by uploading files in fixed-size blocks - and de-duplicating, as above, at the block level rather than the full-file level. Each file in the virtual file system would then contain one or more hashes, representing the block chain for that file. That would also have the advantage that uploading a large file which is only slightly different from another previously uploaded file would involve a lot less storage and data transfer.
563e0f2b2d1761a701f0f610	X	That's because "folders" in S3 are not really folders. Why don't you just include the client-notification as part of the upload?
563e0f2b2d1761a701f0f611	X	yes, this is also possible. Nevertheless, I'm still curious how to do this
563e0f2b2d1761a701f0f612	X	I'm working on an auto-update solution, and I'm using Amazon S3 for distribution. I would like for this to work like follows: To do this, I somehow need to list all files in an amazon bucket's folder, and find the one which has been added last. I've tried $s3->list_objects("mybucket");, but it returns the list of all objects inside the bucket, and I don't see an option to list only files inside the specified folder. What is the best way to do this using Amazon S3 PHP api?
563e0f2b2d1761a701f0f613	X	To do this, I somehow need to list all files in an amazon bucket's folder, and find the one which has been added last. S3's API isn't really optimized for sort-by-modified-date, so you'd need to call list_buckets() and check each timestamp, always keeping track of the newest one until you get to the end of the list. An automatic PHP script detects that a new file has been added and notifies clients You'd need to write a long-running PHP CLI script that starts with: Maybe throw an occasional sleep(1) in there so that your CPU doesn't spike so badly, but you essentially need to sleep-and-poll, looping over all of the timestamps each time. I've tried $s3->list_objects("mybucket");, but it returns the list of all objects inside the bucket, and I don't see an option to list only files inside the specified folder. You'll want to set the prefix parameter in your list_objects() call.
563e0f2b2d1761a701f0f614	X	S3 launched versioning functionality of files in bucket http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html. You could get latest n files by calling s3client.listVersions(request) and specifying n if you want.See http://docs.aws.amazon.com/AmazonS3/latest/dev/list-obj-version-enabled-bucket.html Example is in java. Not sure if the API for PHP is added for versioning.
563e0f2b2d1761a701f0f615	X	I am using Amazon S3 as images server. I use php API to put and delete objects to bucket with versions. Now i want to rename folder at S3, i understand that physically i will have to copy paste those files but with another path in folder-name place, and after copy i want all pics from old folder to be erased(in order not to occupy memory). However, my bucket has versions, so that means my erase iteration will just set delete markers and do nothing physically. Is there a way to get all versions of i-th file as a list, and iterate over them?
563e0f2b2d1761a701f0f616	X	Yes it is possible but you wil have to write some code. There is an API that let you retrieve all versions of an object in a bucket. http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html Based on this (and higher level SDK or AWS CLI equivalent) developing the behaviour you describe is relatively easy.
563e0f2c2d1761a701f0f617	X	I am using a bucket policy that denies any non-SSL communications and UnEncryptedObjectUploads. } This policy works for applications that support SSL and SSE settings but only for the objects being uploaded. I ran into these issues: CloudBerry Explorer was able to RENAME objects with the full SSL/SSE bucket policy only after I enabled in Options – Amazon S3 Copy/Move through the local computer (slower and costs money). All copy/move inside Amazon S3 failed due to that restrictive policy. That means that we cannot control copy/move process that is not originated from the application that manipulates local objects. At least above mentioned CloudBerry Options proved that. But I might be wrong, that is why I am posting this question. Is there something wrong with my bucket policy? I do not know those Amazon S3 mechanisms that used for objects manipulating. Does Amazon S3 treat external requests (API/http headers) and internal requests differently? Is it possible to apply this policy only to the uploads and not to internal Amazon S3 GET/PUT etc..? I have tried http referer with the bucket URL to no avail. The bucket policy with SSL/SSE requirements is a mandatory for my implementation. Any ideas would be appreciated. Thank you in advance.
563e0f2c2d1761a701f0f618	X	IMHO There is no way to automatically tell Amazon S3 to turn on SSE for every PUT requests. So, what I would investigate is the following : write a script that list your bucket for each object, get the meta data if SSE is not enabled, use the PUT COPY API (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) to add SSE "(...) When copying an object, you can preserve most of the metadata (default) or specify new metadata (...)" If the PUT operation succeeded, use the DELETE object API to delete the original object Then run that script on an hourly or daily basis, depending on your business requirements. You can use S3 API in Python (http://boto.readthedocs.org/en/latest/ref/s3.html) to make it easier to write the script. If this "change-after-write" solution is not valid for you business wise, you can work at different level use a proxy between your API client and S3 API (like a reverse proxy on your site), and configure it to add the SSE HTTP header for every PUT / POST requests. Developer must go through the proxy and not be authorised to issue requests against S3 API endpoints write a wrapper library to add the SSE meta data automatically and oblige developer to use your library on top of the SDK. The later today are a matter of discipline in the organisation, as it is not easy to enforce them at a technical level. Seb
563e0f2c2d1761a701f0f619	X	This question applies to both Desktop browsers as well as mobile apps using Phonegap / Cordova. For uploads, the signature has to have the amazon secret encoded and hence created on the server for safety purposes. The alternative would be to generate the signature in the browser but that would mean exposing the Amazon Secret key. Is there an alternative? I found apisigning.com, which forwards requests sent to them to the Amazon Advertising API. Is there something similar for Amazon S3?
563e0f2c2d1761a701f0f61a	X	RE your presigning link which links to the S3GetPreSignedURLRequest method, how are you supposed to fill in the SecretKey info w/o having it stored on the client? Update: I re-looked at the reference and it says AccessKey, not SecretKey. Is the idea here that you just store one of the keys and it's used like a public key of sorts?
563e0f2c2d1761a701f0f61b	X	I want to make calls to the Amazon S3 rest API through an iPhone app. It means that I will have to write in my iPhone app the secretAccessKey and the accessKey of the Amazon S3 service. If my app goes on the appstore, is it going to be dangerous for me? Maybe some people will extract my secretKey and my key to use it for other purposes? Is there a way to protect my app from this kind of attacks? Thanks! Martin
563e0f2c2d1761a701f0f61c	X	If possible you shouldn't store your keys in your app. You can see a lengthy discussion of the topic here: Architectural and design question about uploading photos from iPhone app and S3 (check out Adrian Petrescu's answer). There are a couple of options here. First, upload your data to a central server and then onto S3. Your keys stay private on your server. Or you can look at presigning your URLs.
563e0f2c2d1761a701f0f61d	X	I know the time cannot be wrong because the script I am running to connect to Walrus is run from the same virtual machine that is hosting Walrus. As you might have noticed the server_url points to localhost. I did try localtime() instead of gmtime() anyways just to be sure, and to no avail. There was nothing helpful in cloud-error.log unfortunately. Thanks for checking my signature though.
563e0f2c2d1761a701f0f61e	X	Also do you have any evidence that Walrus does not support SOAP?
563e0f2c2d1761a701f0f61f	X	From the way you talk, I am assuming that you have worked on the project. If that is so, thanks for giving evidence. In the future though where would I look for a changelog for Eucalyptus?
563e0f2c2d1761a701f0f620	X	@KevinTindall Yes, I'm the lead on Eucalyptus storage. I should have made that more clear. If you have more questions you can also find us on freenode IRC in #eucalyptus-devel
563e0f2c2d1761a701f0f621	X	@KevinTindall, with regard to the changelog, it is here. It looks like the S3 SOAP support removal was not included, that is a problem and I will work to get that fixed in the doc.
563e0f2c2d1761a701f0f622	X	Thanks, for the update :)
563e0f2d2d1761a701f0f623	X	I have been learning how to use Amazon S3 API by using the open source Eucalyptus. So far I have been able to successfully use REST, but now I would also like to use SOAP. I seem to be having trouble generating the correct signature for my request. The service is giving me a 403 Forbidden error: My code is in Python 2 and uses the SUDS-Jurko library for sending SOAP requests: I changed the server endpoint, because otherwise the WSDL points to the regular S3 servers at Amazon. I also have two different ways of creating the signature in my create_signature function. I was swapping between one and the other by simply commenting out the second one. Neither of the two seem to work. My question is what am I doing wrong? SOAP Authentication: http://docs.aws.amazon.com/AmazonS3/latest/dev/SOAPAuthentication.html SUDS-Jurko Documentation: https://bitbucket.org/jurko/suds/overview Edit: I realized I forgot to include an example of what timestamp and signature is printed for debugging purposes. Edit 2: Also, I know that the download_file function does not download a file :) I am still in testing/debug phase Edit 3: I am aware that REST is better to use, at least according to Amazon. (Personally I think REST is better also.) I am also already aware that SOAP is deprecated by Amazon. However I would like to go down this path anyways, so please do me a favor and do not waste my time with links to the deprecation. I assure you that while writing this SOAP code, I was already well aware of the deprecation. In fact one of the links I posted has the deprecation notice printed at the top of its page. However, if you have evidence showing that Walrus completely ditches SOAP or that they stopped working on the SOAP portion, I would like to see something like that. But please do not tell me Amazon has deprecated SOAP.
563e0f2d2d1761a701f0f624	X	The S3 SOAP API does not support "new" features so the REST API should be used where possible: http://docs.aws.amazon.com/AmazonS3/latest/dev/SOAPAPI3.html https://forums.aws.amazon.com/message.jspa?messageID=77821 IIRC recent versions of Eucalyptus do not support SOAP with S3. That said, the signature looks good to me so I would check if the client/service hosts have the correct time, if there is a difference of more than 15 minutes authentication would fail. You could also check the cloud-error.log on the Walrus service host as there may be more details on the failure there.
563e0f2d2d1761a701f0f625	X	Eucalyptus does not support SOAP for S3 as of Eucalyptus version 4.0.0. If you are using an older version of Eucalyptus (pre 4.0), then SOAP should work. Note, however, that the wsdl provided by S3 is not necessarily current or accurate for even their own service. S3 is notorious for changing the API without version or wsdl bumps, particularly since they stopped updating the SOAP API. So, there are likely responses from Walrus that do not conform to the published WSDL because our XML was updated based on the responses we see from S3 (via REST) and the SOAP and REST responses diverged. The signatures should be compatible, however, so worst case you would see 500 or 400 errors, but not 403 responses. My recommendation is if you really want to learn the S3 SOAP API, you'll have to use S3 proper. The S3 SOAP support in Eucalyptus is there pre-4.0 but may not be compliant with the current state of S3 SOAP--we stopped testing against the S3 SOAP API directly when the AWS SDKs stopped using SOAP in favor of better REST support since that was the API moving forward.
563e0f2d2d1761a701f0f626	X	Eucalyptus does support SOAP (see ZachH's comment about deprecation): https://www.eucalyptus.com/docs/eucalyptus/4.0.2/schemas/aws-apis/s3/index.html I decided to scrap using python, and I produced some working code with C# instead. Turns out C# has better SOAP libraries.
563e0f2d2d1761a701f0f627	X	Check out the S3 documentation for the ListBucket operation: docs.amazonwebservices.com/AmazonS3/2006-03-01/…. To obtain a1-a5000 specify prefix="/l1/" delimeter="/". To obtain /l1/a123/* specify prefix="/l1/a123/", delimeter="/". Is that what you had in mind?
563e0f2d2d1761a701f0f628	X	Oren, You are right it is working now. Thanks a lot. Maybe the test bucket structure I created was wrong.
563e0f2d2d1761a701f0f629	X	it is amazing that this wasn't built in still, but this saved me a lot of time. thanks.
563e0f2d2d1761a701f0f62a	X	I improved it somewhat here: stackoverflow.com/questions/4849939/… (it also lists individual files now). Still can't believe that this isn't built into one of the many Ruby S3 gems.
563e0f2d2d1761a701f0f62b	X	I am storing two million files in an amazon S3 bucket. There is a given root (l1) below, a list of directories under l1 and then each directory contains files. So my bucket will look something like the following l1/a1/file1-1.jpg l1/a1/file1-2.jpg l1/a1/... another 500 files l1/a2/file2-1.jpg l1/a2/file2-2.jpg l1/a2/... another 500 files .... l1/a5000/file5000-1.jpg I would like to list as fast as possible the second level entries, so I would like to get a1, a2, a5000. I do not want to list all the keys, this will take a lot longer. I am open to using directly the AWS api, however I have played so far with the right_aws gem in ruby http://rdoc.info/projects/rightscale/right_aws There are at least two APIs in that gem, I tried using bucket.keys() in the S3 module and incrementally_list_bucket() in the S3Interface module. I can set the prefix and delimiter to list all of l1/a1/*, for example, but I cannot figure out how to list just the first level in l1. There is a :common_prefixes entry in the hash returned by incrementally_list_bucket() but in my test sample it is not filled in. Is this operation possible with the S3 API? Thanks!
563e0f2e2d1761a701f0f62c	X	right_aws allows to do this as part of their underlying S3Interface class, but you can create your own method for an easier (and nicer) use. Put this at the top of your code: This adds the common_prefixes method to the RightAws::S3::Bucket class. Now, instead of calling mybucket.keys to fetch the list of keys in your bucket, you can use mybucket.common_prefixes to get an array of common prefixes. In your case: I must say I tested it only with a small number of common prefixes; you should check that this works with more than 1000 common prefixes.
563e0f2e2d1761a701f0f62d	X	I am starting to develop an app to access the Amazon S3 storage using the SOAP API. I have read the documents that says the the method PutObject must be used if the file size is greater than 1 MB. Now PutObject uses DIME attachment. Is there a sample code or example or a fragment of code that someone can show me on how to do DIME attachement with GSOAP for the PutObject method of Amazon S3. I want to use GSOAP because of portability and to make it generic. I do not want to use the .NET API provided by Amazon for the same reason. I want in GSOAP particularly as I have worked in GSOAP earlier. Thanks, david
563e0f2e2d1761a701f0f62e	X	I put together something that uploads files larger than 1MB using PutObject, it should also work for smaller files. I share it for others who might find it useful. Also see my previous post on using GSOAP to access S3 AMAZON AWS S3 using GSOAP C C++ The link also contains the method to generate the signature. Here is the code for PutObject. It uses the latest GSOAP from sourceforge. After wsdl2h to generate the header and soapcpp2 to generate the gsoap client code the following will be the code to access the service PutObject...... Requirements : OpenSSL GSOAP Build with the compiler preprocessor directive WITH_OPENSSL. Include the library files libeay32 and ssleay32. Take the methods to generate signature from the link above. Hope it helps. Thanks, david
563e0f2e2d1761a701f0f62f	X	Say I have the following directories and files in an Amazon S3 bucket (files are in bold): How can I list all objects and immediate subdirectories of a given directory with the .NET AWS S3 API, without recursively getting everything below that directory? In other words, how can I "browse" the contents of a directory at a single level? For example, imagine I want to browse the contents of bucketname/folder1/. What I would like to see is the following: ...and nothing else. I don't want to list the files and directories in subdirectories, I just want to list the files and subdirectories at the folder1 level. Is there a way to apply filters to a single AWS API call so that it doesn't return everything and force me to manually parse only what I need? I've found that this code let's me get just the immediate subdirectories (as intended), but I can't figure out how to include the immediate files too:
563e0f2e2d1761a701f0f630	X	I had the opposite problem (I knew how to get the files in the specified folder, but not the subdirectories). The answer is that Amazon lists files differently than it does sub-folders. Sub-folders are listed, as your example shows, in the ListObjectsResponse.CommonPrefixes collection. Files are listed in the ListObjectsResponse.S3Objects collection. So your code should look like this: my google search turned up this post on the burningmonk blog with this in the comment section: When you make the Lis­tO­b­jects request, to list the top level fold­ers, don’t set the pre­fix but set the delim­iter to ‘/’, then inspect the ‘Com­mon­Pre­fixes’ prop­erty on the response for the fold­ers that are in the top folder. To list the con­tents of a ‘root­folder’, make the request with pre­fix set to the name of the folder plus the back­slash, e.g. ‘rootfolder/’ and set the delim­iter to ‘/’. In the response you’ll always have the folder itself as an ele­ment with the same key as the pre­fix you used in the request, plus any sub­fold­ers in the ‘Com­mon­Pre­fixes’ property.
563e0f2e2d1761a701f0f631	X	amazon have sdk for it aws.amazon.com/articles/3051?_encoding=UTF8&jiveRedirect=1
563e0f2e2d1761a701f0f632	X	I've seen the SDK, I was clarifying if there was a built in sync command, as I can't find any information on it.
563e0f2e2d1761a701f0f633	X	i doubt it there is no sync command. but you can create one for your own. check this link codeproject.com/Articles/131678/Amazon-S3-Sync
563e0f2e2d1761a701f0f634	X	Cheers for this, I actually wrote my own using the SDK yesterday. And it's working quite well.
563e0f2e2d1761a701f0f635	X	I have an Amazon s3 bucket, and wish to have the app data folder for my .net application sync itself to reflect changes in the bucket. So, for instance if a user uploads a photo through my web application. When they launch the desktop application I want the app data folder to sync with the bucket state, so it would automatically download the uploaded photo etc. Is there any api or sdk made for this? I've seen examples of an application updating a bucket to reflect changes to local storage but I wish to do it the other way around.
563e0f2e2d1761a701f0f636	X	So the question is simple, what is the biggest number of CommongPrefixes displayed when you list s3 objects, and what is the biggest MaxKeys. Default is 1000. Amazon s3 api / bucket get
563e0f2e2d1761a701f0f637	X	To answer my own question, maximum number of CommonPrefixes and MaxKeys is 1000. Caution, TOGETHER 1000. This means that you can have 0 Keys displayed, and maximum 1000 CommonPrefixes or 990 Keys displayed, and maximum 10 CommonPrefixes
563e0f2f2d1761a701f0f638	X	HmacSHA1 generates different signatures on different systems using same secret, maybe?
563e0f2f2d1761a701f0f639	X	According to C# in Depth - Strings in C# and .NET, linked from Determine a string's encoding in C#, .NET strings are UTF-16.
563e0f2f2d1761a701f0f63a	X	I do not see a response from Amazon confirming the issue.
563e0f2f2d1761a701f0f63b	X	I am following the official Amazon S3 REST API documentation here and am having problems computing the same authorization values they show in their examples. The base64 HMAC-SHA1 hash they show for the first example is: But I keep coming up with: I am tearing my hair out here. What can I possibly be doing wrong? From their very first example:
563e0f2f2d1761a701f0f63c	X	Okay I found the problem. The keys used in the examples are wrong.
563e0f2f2d1761a701f0f63d	X	do you want to make the images publicly readable?
563e0f2f2d1761a701f0f63e	X	@Sebastian I dont know almost nothing about security. So, any help is really welcome. My guess to your question is actually not. I guess I just want to make the images public to request coming from my domain. Does it make sense? I have read there is something more you can add to the policy in order to make this. But anyway, Im not even able to make them public to everyone.
563e0f2f2d1761a701f0f63f	X	@Sebastian something came to my mind. Is it possible to check who is the user or owner of an object? I have two AWS accounts, and its possible that I uploaded the files with the credentials of the other user. In my policy I granted access to put to everyone, right? Does it make sense or Im getting crazy...
563e0f2f2d1761a701f0f640	X	Its working!! I really apologize for my mistake. I will explain it below.
563e0f302d1761a701f0f641	X	Sorry, I have replaced my policy with yours, but still the same. I cannot open any object created with the aws sdk api from the browser :(
563e0f302d1761a701f0f642	X	Im developing an AngularJS (frontend) + Rails API (backend) website. Im using Amazon S3 to store images. Im able to upload an image from Rails to S3 using the aws sdk. I see the image in my S3 bucked. However when I try to get it later from an AngularJS view, my GET request gets a Forbidden: Access denied error. I guess the problem is related to the bucket or image object permissions, but I cant figure out what. This is a summary of how I upload the image: NOTE: I dont use any permissions in the upload request (when I check the image in S3, I see in properties there isnt any permissions set). And this is my bucket policy: This is my GET request: Just in case it helps, when I create an object in a bucket manually, I can also set it as public (with make public menu). However, when I upload the image from the aws sdk API, I cant do it (I get the following objects were not make public due to error).
563e0f302d1761a701f0f643	X	Make sure you set this policy and try to access your image from a browser: When this works set this CORS config and try from Angular:
563e0f302d1761a701f0f644	X	I finally fixed it. The problem was that I have two different AWS accounts. As defined in my bucket policy (above), so far, I grant read and write access to everyone. So, when I upload the images I was using the key credentials for the first user. I was able to upload. However, I couldnt open the files because I was using the S3 AWS account of second user. I just changed the key credentials and everything is working perfectly.
563e0f302d1761a701f0f645	X	i have never used image upload before, don't really understand the mechanism . Trying to upload image from backend, to amazon s3 buckets. does the image have to be converted to binary in this case? the following code is what i got from amazon tutorial (can't find a good document&tutorial of how to use their api..) what i need to add on the code to upload image ? var params = {Bucket:bucketName, Key: keyName, Body: "?"}; s3.putObject(params, function(err,data){ if(err) console.log(err) else console.log("Successfully uploaded data to " + bucketName + "/" + keyName); })
563e0f302d1761a701f0f646	X	there's alternative such as readasurl, to convert the image into a string, when the image is small. so.. no need to even use Amazon S3, which is so complicated...
563e0f302d1761a701f0f647	X	I have an application that allows users to upload contents to Amazon S3, and returns the link of the uploaded content. I have been wondering how to allow only users that own the content to access it, and i got into http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using-authorization-header.html the authorization header. A way to use it i thought is: generating a link to my application host for each content (e.g: from bucket.s3.amazonaws.com/29347524.jpg to -> myapp.com/image/154155.jpg) and serve it to user. When i receive a request i'll be checking if the user is authenticated in my application or not, and in successful match i'll allegate the authorization header to the request and forward it to amazon. I would like not to download the content from amazon's server from my application's server and serve the content to the client. I think this is a useless waste of band. Is there maybe any way to forward the request after adding some headers? So that the client is answered by Amazon when he requests the content but the request is made to my server and modified in some parts. Do you know any other way to perform an authentication like this on Amazon's S3 content ? Any suggestion will be appreciated
563e0f302d1761a701f0f648	X	I would look at the pre-signing facility in S3: http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html Your application server can generate a time-bound URL as described there, and redirect the user using HTTP 302 with the corresponding Location header. If you Google for "amazon s3 presign url", you'll find a few more resources: both blog posts and official Amazon docs.
563e0f302d1761a701f0f649	X	I am trying to figure out the best way to download a file from Dropbox and upload it to Amazon s3 using carrierwave. Can use the Dropbox Chooser? Or do I have to use the Core API from dropbox to get the file? If I can do it through just the Chooser app, then how do I pass that file into Amazon s3? Do I even need carrierwave?
563e0f312d1761a701f0f64a	X	To confuse people.
563e0f312d1761a701f0f64b	X	You can execute commands, like checking something or initializing. But I confess, I would never use it as it is not readable. In for loops it is more common to increase/decrease multiple variables.
563e0f312d1761a701f0f64c	X	The language allows a lot of things that are known to cause undefined behavior. At least these are harmless.
563e0f312d1761a701f0f64d	X	It depends on the types of a and b. For user defined types, operator== can be overloaded and have side effects. One such side effect could be to deposit a certain amount of money in your bank account. With that in mind, you could consider it advantageous to invoke that operator many times.
563e0f312d1761a701f0f64e	X	@juanchopanza: If you are going the operator overloading for obfuscation sake way, they you can also override the operator,() and have extra fun!
563e0f312d1761a701f0f64f	X	wellcome on SO! just gave you some reputation ...
563e0f312d1761a701f0f650	X	Nice example, but I'd rather use a while - loop header for demonstration. for the if case, I don't see a reason, why I wouldn't just write those statements in front of the if statement.
563e0f322d1761a701f0f651	X	Agreed. Added some words to make that clear.
563e0f322d1761a701f0f652	X	:-) I am clear about for loop , I am specifically asking about if and while statement.
563e0f322d1761a701f0f653	X	@AbdulRehman: Well in that case the anser probably is: because it if and while statements require an expression that evaluates to true and (<ex1>,<ex2>) is an expression.
563e0f322d1761a701f0f654	X	If it is the case why only last condition of an expression matters ?
563e0f322d1761a701f0f655	X	@Abdul: Well, because you have to pick one and someone decided that it should be the last. You have to understand, that the comma operator doesn't have any special behavior in an if statement compared to using it at other positions in your code.
563e0f322d1761a701f0f656	X	@MikeMB in decltype you can use it as a simple SFINAE. decltype( test_expression, void(), result_we_want) will test the first expression (determine if it is valid), and if it is, will return result_we_want type. Parameter pack expansion tricks -- using discard=int[]; (void)discard{ 0, ( (std::cout << ts << '\n'), void(), 0 )... }; will expand the statement std::cout << ts << '\n' for each element in the ts... pack (in order) and discard the result. (not all , are comma operator in this case, but some are)
563e0f322d1761a701f0f657	X	they could simply use assignment expression rather than including expression list in c++ GRAMMAR of If statement.
563e0f322d1761a701f0f658	X	@AbdulRehman you could write if ( foo(), a == b ) to call a function and then do the test
563e0f322d1761a701f0f659	X	@AbdulRehman I don't understand: assignment is already an expression...
563e0f322d1761a701f0f65a	X	yes, but it is different than expression in c++ grammar, expression is a list of assignment expressions, while assignment expression is the one without commas in it.
563e0f322d1761a701f0f65b	X	I think I we both agree that the comma expression is unnecessary ;) in case of the for-loop, the grammar for initialise and increment could have been simply a list of statements instead of an expression and we wouldn't have this confusion...
563e0f322d1761a701f0f65c	X	perhaps you could add some code examples? as it stands, this is more than confusing.
563e0f322d1761a701f0f65d	X	superb while examples. So is it better than duplication of code ?
563e0f322d1761a701f0f65e	X	@AbdulRehman: Yes, duplication of code is bad. Especially the code way down at the end of the while loop is disconnected from it purpose, and you could very well forget to update it when for some reason the first instance is updated. Note that the "disconnected" argument is the sole reason why the third (increment) component of a for(..;..;..) clause exists, and I've never heard anybody complain that that is useless.
563e0f322d1761a701f0f65f	X	no need for comma: while(Success == fallibleCalculation(result) && result.isBared()) { ... } and for void functions (in this case bad api in the first place) use a for to counter the disconnect argument for while
563e0f322d1761a701f0f660	X	I dunno; there could be a void operator==(int rhs) { std::cout << rhs; }. This is a code base where people are using , in an if statement, I wouldn't rule it out.
563e0f332d1761a701f0f661	X	We can write an if statement as and only the last condition should be satisfiable to enter the if body. What is the advantage of commas in an if or while statement, and why is it allowed?
563e0f332d1761a701f0f662	X	Changing your example slightly, suppose it was this (note the = instead of ==). In this case the commas guarantee a left to right order of evaluation. In constrast, with this you don't know if f(5) is called before or after f(6). More formally, commas allow you to write a sequence of expressions (a,b,c) in the same way you can use ; to write a sequence of statements a; b; c;. And just as a ; creates a sequence point (end of full expression) so too does a comma. Only sequence points govern the order of evaluation, see this post. But of course, in this case, you'd actually write this So when is a comma separated sequence of expressions preferrable to a ; separated sequence of statements? Almost never I would say. Perhaps in a macro when you want the right-hand side replacement to be a single expression.
563e0f332d1761a701f0f663	X	In short: Although it is legal to do so, it usually doesn't make sense to use the comma operator in the condition part of an if or while statement (EDIT: Although the latter might sometimes be helpful as user5534870 explains in his answer). A more elaborate explanation: Aside from its syntactic function (e.g. separating elements in initializer lists, variable declarations or function calls/declarations), in C and C++, the , can also be a normal operator just like e.g. +, and so it can be used everywhere, where an expression is allowed (in C++ you can even overload it). The difference to most other operators is that - although both sides get evaluated - it doesn't combine the outputs of the left and right expressions in any way, but just returns the right one. It was introduced, because someone (probably Dennis Ritchie) decided for some reason that C required a syntax to write two (or more) unrelated expressions at a position, where you ordinarily only could write a single expression. Now, the condition of an if statement is (among others) such a place and consequently, you can also use the , operator there - whether it makes sense to do so or not is an entirely different question! In particular - and different from e.g. function calls or variable declarations - the comma has no special meaning there, so it does, what it always does: It evaluates the expressions to the left and right, but only returns the result of the right one, which is then used by the if statement. The only two points I can think of right now, where using the (non-overloaded) ,-operator makes sense are: If you want to increment multiple iterators in the head of a for loop: If you want to evaluate more than one expression in a c++11 constexpr function. To repeat this once more: Using the comma operator in an if or while statement - in the way you showed it in your example - isn't something sensible to do. It is just another example where the language syntaxes of c and c++ allow you to write code, that doesn't behave the way that one - on first glance - would expect it to. There are many more....
563e0f332d1761a701f0f664	X	There is no advantage: the comma operator is simply an expression with type of the last expression in its expression list and an if statement evaluates a boolean expression. It's a weird operator true, but there's no magic to it - except that it confuses lists of expressions with argument lists in function calls. Note that in the argument list, comma binds stronger to separating arguments.
563e0f332d1761a701f0f665	X	For an if statement, there is no real point in putting something into a comma expression rather than outside. For a while statement, putting a comma expression to the condition executes the first part either when entering the loop, or when looping. That cannot easily be replicated without code duplication. So how about a s do...while statement? There we have only to worry about the looping itself, right? It turns out that not even here a comma expression can be safely replace by moving the first part into the loop. For one thing, destructors for variables in the loop body will not have already been run then which might make a difference. For another, any continue statement inside the loop will reach the first part of the comma expression only when it indeed is in the condition rather than in the loop body.
563e0f332d1761a701f0f666	X	What follows is a bit of a stretch, depending on how devious you might wish to be. Consider the situation where a function returns a value by modifying a parameter passed by reference or via a pointer (maybe from a badly designed library, or to ensure that this value is not ignored by not being assigned after returning, whatever). Then how do you use conditional statements that depend on result? You could declare the variable that will be modified, then check it with an if: This could be shortened to Which is not really worth while. However, for while loops there could be some small advantages. If calculateValue should/can be called until the result is no longer bar'd, we'd have something like: and could be condensed to: This way the code to update result is in only one place, and is near the line where its conditions are checked. maybe unrelated: Another reason why variables could be updated via parameter passing is that the function needs to return the error code in addition to modify/return the calculated value. In this case: then
563e0f332d1761a701f0f667	X	None whatsoever. The comparisons on a in that code are completely redundant.
563e0f332d1761a701f0f668	X	My question is what is the advantage of commas in if or while statement? Why is it allowed ? It exists because statements and expressions are different things in C. A compound expression is a construct that is understood from theory (and some other languages) and would be missing without having added it in the form of the comma. Its use in the for statement was the original justification of why they needed it. But, by making the language more complete from a sound theoretical point of view, it later finds uses that nobody planned. The early C++ was a translator that generated C as its output, and having a sequential expression was absolutely essential in allowing inline functions to really generate "in line" logic in the C code. That includes any place the expression appears, including the condition of an if statement. Similarly, it has been used in "interesting" macros. And as much as C++ did away with macros by providing inline functions, as late as up-to-x11 compilers found the Boost FOREACH range loop (eventually, an emulation of the feature added to the language in x11) very handy, and that was a devilishly clever set of macros that involved the comma operator. (Hmm, the current version expands into multiple statements using chained if/else, rather than cramming it all into a single while.) Now, there is another way to put any statement into an expression (lambdas), so future crazy business of macros that emulate even newer language features or domain-specific embedded languages might not need to use that anymore. So, don't write code like that. Unless it's clear and indeed simpler than writing helper functions or splitting into multiple statements. But it may be just the thing for a macro that you want to easily use in one place and that place is inside the parens of an if or while. That could be justified in a domain-specific language hosted inside the C++ source code, or a language emulation feature like (perhaps) an alternative to exception handling used in an embedded real-time system. In short, it doesn't have a normal good usage. But it's there for completeness and you never know when someone will find it useful.
563e0f342d1761a701f0f669	X	Could you post the exact wording of the error message?
563e0f342d1761a701f0f66a	X	The specified bucket does not exist
563e0f342d1761a701f0f66b	X	Have you tried specifying region in the client factory?
563e0f342d1761a701f0f66c	X	Hi Antony thanks for your help but its not that i can copy a file to a different bucket but not the same one to rename.
563e0f342d1761a701f0f66d	X	Looking around, this kind of problem can be caused by the naming rules of the bucket. But the name looks right in your example.
563e0f342d1761a701f0f66e	X	Really struggling to rename a file within the same bucket with Amazon S3 SDK can anyone help, i am referring to copy object here in the api docs. http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html#_copyObject So here is my call but it keeps returning specifed bucket does not exsist? Before someone points out the obvious and says does your bucket exist yes it definitely exists i can run a call with the same keys and get all my files from that bucket? I really want to be able to rename a file via the api you can do it within the Amazon S3 Browser. Please help Thanks UPDATE WORKING VERSION you have to include the bucket in copy source???
563e0f342d1761a701f0f66f	X	I'd like to load an image directly from a URL but without saving it on the server, I want to upload it directly from memory to Amazon S3 server. This is my code: The Amazon API gives me the error "Could not determine content length". The stream fileStream ends up as "System.Net.ConnectStream" which I'm not sure if it's correct. The exact same code works with files from the HttpPostedFile but I need to use it in this way now. Any ideas how I can convert the stream to become what Amazon API is expecting (with the length intact)?
563e0f342d1761a701f0f670	X	Have you updated your bucket policy?
563e0f342d1761a701f0f671	X	No, I new with amazon, what is need to be added there?
563e0f342d1761a701f0f672	X	Still does not upload , maybe something else?
563e0f352d1761a701f0f673	X	Try printing the error
563e0f352d1761a701f0f674	X	there is no errors. The uploadRequest.key = "filename" is ok? I have just opened a bucket without any internal folders. It is ok?
563e0f352d1761a701f0f675	X	The code looks like it should be working.
563e0f352d1761a701f0f676	X	i find an error AWSRegion is not valid for bucket , thank you a lot!
563e0f352d1761a701f0f677	X	Hi i try to upload a file to my bucket at AWS s3 without successes. I just need to upload a file from some application from some phones. i use AWS SDK version2 on ios 8. this is my function for Upload a DB: UPLOAD AppDelege.m ** ** Thanks
563e0f352d1761a701f0f678	X	Try changing your bucket policy to this (replace bucketName): This allows you to Add/Read objects from your bucket, you should read AWS S3 Documentation in order to know what exactly you want your policy to be, but for now, this should do the trick.
563e0f352d1761a701f0f679	X	I give up on using AWS SDK. I have tried with AFAmazonS3Manager and It works well. AFAmazonS3Manager: AFNetworking Client for the Amazon S3 API.
563e0f352d1761a701f0f67a	X	ok i see, so a follow up question, how can I use the link I got to download that file to my server? the server is not authnticated with the user credentials, so how can he access the file to download it?
563e0f352d1761a701f0f67b	X	you dont. you download it on the client side using Javascript, then post to s3. bo server involved.
563e0f352d1761a701f0f67c	X	I have a working upload form in my webapp that lets user upload files from their computer. The file is uploaded directly to amazon s3 without going through my server first this is how my form looks like, and Im using jquery file upload plugin. I want to give users the option for users to choose files from their google drive, using the drive api I added the google picker which lets user pick files from there. after they pick the file, I get in the callback the file metadata (including a file url). How can I use that to upload the file directly to s3? I dont want to upload the file to my server and then copy it to s3, I want to upload the file dircetly to s3. Is that poosible without having to download the file on client side and then upload it? Thx
563e0f352d1761a701f0f67d	X	The only way to upload a file to S3 is to send the file to S3. S3 does not have the capability to fetch an object from an external URL. Download to your server directly from Google and then upload to S3, or you could use AWS Lambda invoked through AWS API Gateway to handle the actual download/upload transfer in the background (sync or async).
563e0f352d1761a701f0f67e	X	JS library in backbone, require.js web application to render PDF page. The PDF page exist on Amazon S3 server, for fetching that I am calling REST API which will return that https://s3.amazonaws.com/bucket/asjkasjkasj.pdf. But I am getting XMLhttp request can not load   No Access-Control-Allow-Origin header is present on requested source.
563e0f362d1761a701f0f67f	X	Based on PDF.js documentation, you can load a PDF from another server by: Enable CORS (Cross Origin Resource Sharing) To enable CORS in AWS S3, you can add this in Edit CORS Configuration inside bucket Permission Properties: Test your CORS using: Make sure you see Access-Control-Allow-Origin: *. And then test using your PDF.js. Create a proxy inside your server to download the PDF file from S3
563e0f362d1761a701f0f680	X	i am using amazon s3 javascript browser api, when i'm trying to download one file it works fine, when i'm trying to download multiple files some of the files downloaded as expected, but some of the files doesn't downloaded and i got 403 The request signature we calculated does not match the signature you provided. Check your key and signing method on some of the files. all the files are in the same bucket , and all the files are good. for example: in the bucket i have 3 files test1.jpg , test2.jpg and test3.jpg if i try to download all the 3 files sometimes test1 and test2 downloaded and for test3 i got 403 and somtimes test2 and test 3 was download and i got 403 for test1. I send several requests in parallel through this code(to amazon s3). Thanks.
563e0f362d1761a701f0f681	X	Yes I have the values beforehand but maybe S3 changes something on upload (not sure if that ever happens...) The $response is actually an array with a lot of values returned from the api.
563e0f362d1761a701f0f682	X	None of the values you've mentioned will ever be changed on upload.
563e0f362d1761a701f0f683	X	I'm uploading images to amazon s3 using PHP. When I upload a file I want to store information about it in my database so I'll make less API requests when I need to get info about the file (file name, size, type) Creating new objects: After a successful upload, is it possible to get the file name, size, and file type from the $response variable without making new GET requests?
563e0f362d1761a701f0f684	X	Those values aren't in $response, but you don't need them. You know everything you want to know already:
563e0f362d1761a701f0f685	X	I can access the returned array from $response using header For example to get the file size
563e0f362d1761a701f0f686	X	I'm currently adding the "Range" header with a value of "bytes = 0" so it will only download the response headers and 0 bytes from the response body, is this valid/optimal?
563e0f362d1761a701f0f687	X	Can you be more specific why you think that it doesn't work for using the "HEAD" option - as per my answer below, I'm not aware of any special restrictions for a HEAD request in comparison to a GET request.
563e0f372d1761a701f0f688	X	Well, I changed the "GET" for a "HEAD" on my string to encode and it doesn't work, that's my problem. But if I do GET and get 0 bytes from the response body I think that work as well.
563e0f372d1761a701f0f689	X	By the way, I can't use the Amazon S3 SDK for .NET because I'm using MONO and Unity3D
563e0f372d1761a701f0f68a	X	I've been using this method https://coderwall.com/p/kmodkq but I think that it doesn't work for using the "HEAD" option (which is supposed to get the file metadata but not the file body). Any help would be appreciated.
563e0f372d1761a701f0f68b	X	Using a HEAD request for an Amazon S3 object is fully supported and the method of choice for retrieving the information you are looking for: The HEAD operation retrieves metadata from an object without returning the object itself. This operation is useful if you are interested only in an object's metadata. To use HEAD, you must have READ access to the object. A HEAD request has the same options as a GET operation on an object. The response is identical to the GET response except that there is no response body. [emphasis mine] Section Examples in the referenced documentation features a Sample Response which surfaces the desired Last-Modified HTTP header:
563e0f372d1761a701f0f68c	X	really good question +1
563e0f372d1761a701f0f68d	X	Good Q, any luck figuring it out?
563e0f372d1761a701f0f68e	X	I am trying to upload files to Amazon S3 storage using Amazon’s Java API for it. The code is When I run the code after commenting the first two lines and uncommenting the third one, ie stream is a FileoutputStream, the file is uploaded correctly. But when data is a base64 encoded String, which is image data, the file is uploaded but image is corrupted. Amazon documentation says I need to create and attach a POST policy and signature for this to work. How I can do that in java? I am not using an html form for uploading.
563e0f372d1761a701f0f68f	X	First you should remove data:image/png;base64, from beginning of the string: Sample Code Block:
563e0f372d1761a701f0f690	X	Did a little more digging and found that my original assumption of the transfer manager was pretty much correct, it does remain open until you shut it down via tx.shutDownNow() but this should only be called when everything is done as it will leave a partial file if shutDownNow() is called during a transfer. Funny how I can't find the answer to something until AFTER I ask others about it...
563e0f372d1761a701f0f691	X	I'm having trouble with the Amazon S3 download hanging after the last file completes its download, "locking" the file from being deleted as it is "in use" still by the java app, otherwise they work fine. Additionally, the progress doesn't appear to be updating correctly as the largest file simply says 100 once for its progress then proceeds to download without any further updates until it is completed, at which point it says "State: Completed" before the script hangs. My code is below: I pretty much hacked apart the sample code for the Amazon S3 Transfer Progress Sample that comes with the SDK to create a download version of the method without a GUI, so I'm surprised it even works. I'm not that great with Java and even worse with the AWS API, so any pointers are welcome.
563e0f372d1761a701f0f692	X	I found the solution to the issue of the hang. Apparently tx = new TransferManager(credentials); should be tx = new AmazonS3Client(credentials);. I'm not sure how the transfer manager works, but I'm assuming the reason it's hanging is that it's not closing the connection when it's done, but that's probably a whole other topic...
563e0f382d1761a701f0f693	X	This doesn't look like it is meant to scale in a web application. My app can have potentially 100's if not 1000's of users and groups.
563e0f382d1761a701f0f694	X	You didn't mention that this was for an app. Added a reference to Security Token Service in my answer.
563e0f382d1761a701f0f695	X	Yes it is a web application. 'GetFederationToken' matches what i had in mind. docs.aws.amazon.com/STS/latest/UsingSTS/CreatingFedTokens.html
563e0f382d1761a701f0f696	X	I have a requirement where users are part of groups..user1 is part of group1, user2 is part of group2, user3 is part of group1 and group2..etc.. each group has static content which i want to host in Amazon S3. So far in my research, i concluded that i will need to create a bucket for each group. My question is: How do i control access to users ? user1 should only be able to access resources of bucket belonging to group1.(upload, download etc.) users should not be able to access resources of groups they are not part of. I imagine this is a typical scenario, but so far my google-fu is not yeilding any fruitful results. I have a NodeJS/Express REST API as my middle tier. Please advise me on how to engineer for this requirement. Thanks
563e0f382d1761a701f0f697	X	You do not need a bucket per group. You could create one bucket, then in each bucket create a directory for each group and grant access only to the corresponding group. You can use IAM to create the groups and users. Refer to this AWS doc for a walkthrough on this scenario. If you need to scale to many users and groups, you should look at the AWS Security Token Service which scales to millions of users and doesn't require IAM credentials at all. Your application is responsible for authenticating users and managing their accounts and group membership, but the token service can allow access to AWS resources including S3.
563e0f382d1761a701f0f698	X	Note that S3 is a dynamic system, so doing a 'quick benchmark' will give you terrible numbers. Here is an article about a similar test run on ELB: rightscale.com/blog/cloud-management-best-practices/…
563e0f382d1761a701f0f699	X	I have developed a cloud storage system that uses the same API structure as Amazon S3. Now I want to run some performance tests on getting object data and object metadata. In such a way that I can compare my system with Amazon S3, OpenStack storage and other systems. I have looked at some common file system benchmark tools, there is too much work to convert them for Cloud Storage systems. I am looking for some benchmark tools similar to SIEGE, that not only can performance http requests, but also have some workload simulation features. For example, one simulation can be storing an entire static HTML website in the Cloud Storage then performance some workload stress test etc. Can someone help and suggest some existing framework or tools that can be relatively easy to be fit for such cloud storage system benchmark scenario?
563e0f382d1761a701f0f69a	X	As you are the provider of the cloud system. There is many aspects you should want to benchmark. as provider For all thoses things there is specifics tools/api/controls. Sometime it is closely related to your hardware, sometime less. But the connexion between hardware to software result to specifics measure and integration problems. Defining what is a benchmark or routing a 'end to end' query from the 'objet storage api' to the disks can be just crazy hard. If your goal is to get a benchmark(in higther level of api) that could end in improving your system then you only solution is to have a total control(and understanding) of your cloud system; Nagios like tools, aren't fitted for this kind of tests. You need CMDB and some fetching tools to an big data oriented storage. You need to understand that all solutions of benchmark are primary data, and as cloud can be very complex, there is a lot of data. What you will learn from your data isn't just some graphicals data, but also some how to ask your questions. Even getting the rights questions will ask you work. As I said in my first short answer we use VMware VMmark to conduct this kind of test, but that just a small part. There is a so greate numbers of tool (juste to do some real time monitoring - benchmarking that) that one person can't know them all. A work, I'm doing some AI progs (bayesian network for failure detection, evolutionnary algorithms for repartion ...) to enable better management of those things. Just to tease you : Do you expect to conduct benchmark when you install a new client, swap storage of two others and running the emergency plan of a last one, all in same time ? A correct benchmark should cover so many cases. Today cloud must manage the complexity of the world, every chaotic event; nothing should distrub the service. So just to say what is a benchmark is pretty hard. (feeding the cmdb is itself a challenge) as client yep :-) I'm also client of cloud providers like every human will do in near future. Just a little background. Openstack as been initialy release by organizations with very specific needs(Just to think that, in the 'Compute' part of the 'openstack' api there is nothing related to share/cluster processing that look like what lhc consume). So what is a normal website ? Youtube ? Amazon ? Even if it is just for an example a "entire static HTML website" could hardly be use to compare cloud solution. This week I have work as well on the translation of vCloud api into openstack (loose loose game), vCloud is mush well defined, with mush more objects that openstack, but even with this we just cover so little needs of applications management. So how the client could compare two cloud solutions ? In fact, before trying hes own solution he can't. That why clients, come to visit us, ask what we are using and how, our process ... In the end the commercials to the job, generaly few months free of charges just to install the client and find what we should do to reconfigure our cloud to his applications. Very few clients know how many cpu/ram/disk/iops they use; some of them buys dedicated resources(as it is dedicted we can't share to other client) they will never use. Then any benchmark tool for normal web site should do the job. If you want to play you can openstack 'inner' tools like swiftstack and tempest to get some sort of feedback, but you have to define what a normal use of a web site should look. If you look for openstack products related you should also look at the wiki. But if you want just more than A is faster than B is the condition you set, it will be near to impossible as a client. I believe to have explain why not any 'client' have answser to your question until now, while your question is vital in many commercials/industrials/ecologicals aspects.
563e0f382d1761a701f0f69b	X	You can probably look into COSBench, which is a tool to benchmark object storage cloud services.
563e0f382d1761a701f0f69c	X	Can you show all your code if possible trying to figure out how to grab the info sent back from the response of the signature of Amazon S3
563e0f382d1761a701f0f69d	X	Hey do you have any idea how I can go about getting the signed request from retrofit? This is my question on SO with my attempt stackoverflow.com/questions/30411672/…
563e0f382d1761a701f0f69e	X	I'm attempting to convert all my asynctasks and HttpPost code to use Retrofit, so far so good, but I'm having trouble uploading user files to an amazon s3 bucket. The file upload has two parts: Here's an example list of parameters provided to me from the first call. According to the documentation, these values can change or not be included, and the 2nd api call must call use these params in the exact order they were provided. In order to deal with the dynamic content. I created a custom converter to have retrofit return me a LinkedHashMap in my first API call. Then in the second api call, Once I have these values I create a FormUrlEncodedTypedOutput by iterating over the HashMap and adding each item. Everything up to here seems to be working. I'm getting the necessary upload params and the ordering seems to be consistent. I'm a bit less sure about how I have my multipart retrofit call setup. I then use that inside a synchronous retrofit call inside an intentservice. This results in a Amazon error. I've been googling and it seems like Amazon prefers the "key" value to be first? However, if i put the "key" in front of "AWSAccessKeyId" I get a 403 unauthorized error. Do i have my retrofit call setup correctly? If someone could help me figure this out, I'd appreciate it. It's taken a few days to convert most of my uploading code over to retrofit and if I've been stuck on this for a while. Thanks!
563e0f392d1761a701f0f69f	X	Solution was to use a @PartMap instead of the FormUrlEncodedTypedOutput.
563e0f392d1761a701f0f6a0	X	I am currently using an HTTP handler in an attempt to serve up a file from my S3 bucket. I am not getting an error, but the first run through always fails. The file gets "served" to the webpage and the typical "Open or Save" message appears. If I click "Save", everything works as it should. If I click "Open", however, it simply says that the file could not be downloaded. "Retry" is an option at this point. If I click "Retry" the file opens normally. This happens -EVERY- time. I'm not sure what I'm doing wrong. If anyone has any insight after reviewing my code below, PLEASE help me out. A bit of background. Here's the standard message that comes up when I click the file link. Here's the message that comes up when I click "Open" in the previous image. If I click "Retry" from here, it works just fine.
563e0f392d1761a701f0f6a1	X	I am having trouble uploading images from my ios app to amazon s3 using their newest sdk. How can I upload images without using their cognito service? For example, I have an api in my website that returns the following information Now my question is, even without setting up those Cognito credentials that is in the sample app of amazon sdk, how do i use the above information to upload to the bucket (assuming I know the bucket name)? Thank you
563e0f392d1761a701f0f6a2	X	This question already has an answer here: Hello :) I'm looking at the feasibility of having my node application stream HTTP POST file uploads directly through to an Amazon S3 Bucket. I'm looking at using Formidable and Knox for this. The part I'm unsure about is that S3 requires that you know the total number of bytes in the file before transmission. Would I be right in thinking that the only way to accomplish this then would be to use the HTML5 File API (And possibly an ActiveX control on Internet Explorer) to check the file size on the client and send this as part of the HTTP POST request ?
563e0f392d1761a701f0f6a3	X	With the recent CORS support, you can easily send files directly to s3, without your server having to handle anything. I recently wrote a short tutorial, with rails, but again the server is just used to compute some keys, so adapting it to express shouldn't be hard at all. But with such a solution, you'll need to use the jQuery File Upload plugin, and you probably won't need knox http://pjambet.github.com/blog/direct-upload-to-s3/ Hope it'll help you.
563e0f392d1761a701f0f6a4	X	Maybe this can help - I made this to use the JQuery File Upload plugin upload directly to S3. I did not need to check the size of the file. https://gist.github.com/3995819
563e0f392d1761a701f0f6a5	X	Nice. Thanks!!! It's interesting that I can get the MD5 using eTag even if it is a folder but it gives me errors when I try to do Files.getDigest() on a folder. This works well though! Thanks again!
563e0f3a2d1761a701f0f6a6	X	There is no concept of folder on amazon s3. simpli filenames that contain /. So, i dont really understand what do you mean by "even it is a folder"
563e0f3a2d1761a701f0f6a7	X	Well, suppose you have a file in S3 called file1.txt and there will be a "folder" called /folder1. If there are no concepts of folders, that means that this file contains a others files which in my mind is a folder
563e0f3a2d1761a701f0f6a8	X	@Yko, not really, your file1.txt is not actually called file1.txt but folder/file1.txt. The proof of that is that you can not list only *** the files in ***"current folder", but you can list files by prefix, and if your prefix is folder/, then you will get folder/a.txt and folder/dir/b.txt
563e0f3a2d1761a701f0f6a9	X	I am building a java application as a learning exercise. For now, I want to make an app that will take the contents in a folder and replicate it to my Amazon S3 bucket. By searching around, I found out that the best way to tell if 2 files are identical is to take the MD5 value. How do I iteratively take the MD5 of each file in my bucket? According to this link: http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?ListingObjectKeysUsingJava.html The ObjectListing class can list the object keys and (metadata?) of your files in the bucket. But, it doesn't really specify how I can interact with the files. For example, for my local files, I use guava and does something like this: Looking at the AWS S3 API, you can use S3Object.getObjectContent() which will return an InputStream. Is that the best way to work with the objects directly on S3? Finally, what is the best way to sync from my local folder to a bucket in S3? Any tips is welcomed. Thanks!!!!
563e0f3a2d1761a701f0f6aa	X	S3ObjectSummary has memeber called eTag which is md5 hash of the object
563e0f3a2d1761a701f0f6ab	X	Are you really sure you want to do this over a browser? Besides the complexity, uploading a 10GB+ file means having the page open for some time. Is that realistic (what if the page is closed)? Aren't your better of making a small app dat can access the S3 from the desktop (or even buy one)?
563e0f3a2d1761a701f0f6ac	X	@Rogier you are correct it's probably not the best solution (if even a feasible solution). I was trying to avoid having to install anything on the client side but that doesn't look very realistic for extremely large files. The route you mentioned is pretty much the path I was leaning towards.
563e0f3a2d1761a701f0f6ad	X	Yeah. Depending on the platform, you could use for example Transmit (im on Mac) and make a droplet (drag an drop file for uploading). Cost a bit of money, but in terms of hours you can't beat that ;-)
563e0f3a2d1761a701f0f6ae	X	I'm uploading files via AJAX to Amazon S3 (using the browser File API and storing the actual upload script on Amazon S3 as an iframe to get around the Amazon S3 cross-site issues, courtesy to jquery-file-upload for the idea). I have that working and it works great for small files (< 50 MB or so). However, I'm looking to store extremely large files on Amazon S3. I'd like to store things like a configured virtual machine, which could be 10+ GB in size. From my understanding of the HTML5 file API, large files can be chunked up into small bits on the client and uploaded. It is then the responsibility of the server code to join the files together and move the file to S3. I understand the concept but am not sure of the best implementation. I'm using Heroku for the app server and I normally upload files directly to Amazon S3, skipping Heroku's servers completely. However, if I chunk the upload into small bits, I would have to have some code that joins the parts before actually putting it in S3. But Heroku has some limitations on how much data can be used with them, and I don't think that joining a 10 GB file would work out effectively on their servers (not 100% sure but doubtful). So my current thought is that I have to have a web service app setup on an Amazon EC2 server where my app posts the upload parts to. The EC2 app is then responsible for joining the upload parts and putting the final joined file into S3. Once the file is loaded into S3, S3 sends a response to the original app hosted on Heroku, which then creates a resource that points to the stored file in S3. Is there any realistic way of getting around having a separate EC2 server to join the files? There's no cost for sending files between EC2 and S3, but I don't want to have to maintain 2 apps to accomplish what I want (main app on Heroku and the file-joining app on EC2).
563e0f3a2d1761a701f0f6af	X	Amazon S3 API supports a multipart upload. File is automatically merged on the S3 side. I don't know how flexible the new html5 file API is.. if you managed to upload a file directly talking to S3 (wow) you might also be able to use the multipart feature. May I ask for a sample of your current implementation? Makes me curious How multipart uploads to S3 are working http://aws.typepad.com/aws/2010/11/amazon-s3-multipart-upload.html REST API for multipart uploads http://docs.amazonwebservices.com/AmazonS3/latest/dev/UsingRESTAPImpUpload.html The trickiest thing (if possible) probably will be to split a (large) file in the browser Before you start developing something for the sake of coolness make sure there isn't a more practical/pragmatic solution for your original problem EDIT: File slicing is possible - indeed: html5 rocks! If you implement this well you can probably go for unlimited filesizes without exploding the users memory https://developer.mozilla.org/en/docs/DOM/Blob http://www.html5rocks.com/en/tutorials/file/dndfiles/#toc-slicing-files
563e0f3a2d1761a701f0f6b0	X	I have tried to upload a file to Amazon S3 using a presigned url, which works. Now I would like to use the Amazon API to upload the file. For convenience I added the library using the maven repository like this: I also wrote some colde that is supposed to upload the file to the server: However, I am never reaching the point where I can debug the code. The compiler in Android Studio gives me this error, which I do not understand how to solve: Error:(36, 53) error: cannot access AmazonServiceException class file for com.amazonaws.AmazonServiceException not found I hope you can help :)
563e0f3b2d1761a701f0f6b1	X	Found the problem. I Also had to include the aws-android-sdk-core library.
563e0f3b2d1761a701f0f6b2	X	I would take a look at the AWS Java SDK source for the same thing. Should be easy to find at github.com/aws/aws-sdk-java.
563e0f3b2d1761a701f0f6b3	X	why don't you want to use the SDK ? It contains many boiler plate code to handle signature etc that will avoid you to handle these low level details
563e0f3b2d1761a701f0f6b4	X	Do you have a solution for this? I'm trying to upload to S3 without the SDK.
563e0f3b2d1761a701f0f6b5	X	@sebsto: The Amazon SDK for Android has 20K methods which would likely put you over the Dex limit.
563e0f3b2d1761a701f0f6b6	X	@user2744821, hi. Did you made upload to S3 without SDK? Please share some more code.
563e0f3b2d1761a701f0f6b7	X	I've been working on using the REST API of Amazon's S3 to upload a file from my Android device to a bucket I have. I have the KEY and SECRET_KEY, but am not sure how to properly generate the signatureValue they are looking for in their requests. I'm using a HttpPut to their servers, but am not sure how to properly generate the signatureValue. So far here's what I have: Then here are the methods I use to generate the signature value: I used the Amazon S3 Signature tester, and my string was correct, but I never got the right encoded value. Thanks for any help or a push in the right direction.
563e0f3b2d1761a701f0f6b8	X	[Ref: https://aws.amazon.com/articles/1434] The link above also describes the input parameteres in the HTTP request.
563e0f3b2d1761a701f0f6b9	X	I would double check that the date matches what is expected and sent in the http headers (are you setting the "x-amz-date" header?), it gave me some headache when signing requests "manually". Also, adding the error message from S3 might help us to understand what is wrong and help you.
563e0f3b2d1761a701f0f6ba	X	So I can! Thanks :-) do you know if this is a supported format?
563e0f3c2d1761a701f0f6bb	X	According to their doco it is. (docs.amazonwebservices.com/AmazonS3/latest/API/…)
563e0f3c2d1761a701f0f6bc	X	Disadvantage is that https won't work then
563e0f3c2d1761a701f0f6bd	X	I have created the file in an amazon S3 bucket. I know that the url format is: However I want to be able to work out what the 's3-eu-west-1' bit is without having to explicitly know this in my application. I have seen that in the API there is a call which I can make to get the location... But this only returns 'eu' so im wondering how to get the other parts. The less the user has to configure on the application the better :-)
563e0f3c2d1761a701f0f6be	X	You can just use {bucket}.s3.amazonaws.com. i.e. http://ptedotnet.s3.amazonaws.com/UserContent/Uploads/54/26.jpg
563e0f3c2d1761a701f0f6bf	X	Use S3SignURL to make a signed URL
563e0f3c2d1761a701f0f6c0	X	I am building a Rails 4 app with admin uploads of large files to Amazon S3. To validate the transfer of the large files, I would like to include the Content-MD5 field in the request header per the Amazon docs: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html I started with Paperclip + S3 and the MD5 verification is working: With Paperclip, the large file is first uploaded to my server, then transferred to Amazon S3. This blocks the Rails process and consumes redundant bandwidth, so I am trying to use the S3UploadDirect gem to upload the file directly the S3 bucket: https://github.com/waynehoover/s3_direct_upload This gem is a wrapper around the code from Railscasts episode 383 and uses jquery-fileupload-rails for the actual upload: I can upload the file, but I cannot figure out how to pass the Content-MD5 information into the upload request header.
563e0f3c2d1761a701f0f6c1	X	It's open sourced now
563e0f3c2d1761a701f0f6c2	X	Is there an open-source equivalent to the Amazon S3 storage service running under Linux? For example a bucket-based file system like: Thanks.
563e0f3c2d1761a701f0f6c3	X	Riak CS is a new Amazon S3 API-compatible product for your own cloud. It's not open-source, but it may be a viable alternative for your consideration.
563e0f3c2d1761a701f0f6c4	X	Eucalyptus is an open source attempt to provide EC2 and S3 type clouds, up to cloning the API of Amazon, as far as possible (i.e. you don't just get the functionality you mention, you get it in the same format as you'd make the calls to the real S3). I believe Walrus is what you're after. OpenStack also does object storage.
563e0f3c2d1761a701f0f6c5	X	Thank you for your answers and sorry for not being clear about my problem. Currently i have about 4TB of image files spread over 3 servers. My application spreads these files randomly. On which server the file is located, is stored at the mysql database (replicated to each of these servers). The image files are served by nginx and php directly from these servers (no proxy). When one of these servers crashes, i have no failover and no redundancy. Of course i can recover anything from backups, but 1 to 1.5TB of data on each server need much time to recover. After a bit of research, i found MogileFS as the optimal solution.
563e0f3d2d1761a701f0f6c6	X	Im more concerned with am I sending the data right at the moment such as the use of "bucketString" in my little example. Also I notice in your example you have "data:" twice, doesn't one over write the other?
563e0f3d2d1761a701f0f6c7	X	Sorry your right, I will update.
563e0f3d2d1761a701f0f6c8	X	Since you appear new to Javascript, I would definitely recommend reading and listening to any content from douglas crockford, I't will enlighten you to the fundamentals (and nature) of Javascript - just don't take him as too seriously tho.
563e0f3d2d1761a701f0f6c9	X	Also have a good look at the parameters in $.ajax http://api.jquery.com/jQuery.ajax/, you man not want to full REST due to browser compatibility and the parameter URL is just that - A url. Other HTTP headers are set though the other properties.
563e0f3d2d1761a701f0f6ca	X	I am wanting to try and do a handful of things with the use of jQuery and Amazons S3 API via REST. My key issue is not being familiar with REST well enough (or not as well as I thought I knew) to know if this approach would even remotely work right. I have tried searching endlessly for some tidbit of an example and came up fruitless, maybe I am searching for the wrong things I don't know, but as a last ditch effort I figured I'd hit up my new favorite place, here.. What I need to do is send. PUT a request to the API to create the bucket. Based on the S3 API docs I came up with though concept isn't complete with the above I am just starting it out, and I started questioning if this idea of approach was even going to work.. And if it is to work with the above or in any means provided here for help how would I also work with the response to know if it was successful or not? I know if I can nail this one piece down I can handle for the most part the rest of my issues to come. Its just tackling the first hump and figuring out if I am going about it the right way. Its also worth mentioning that I have been tasked with doing this purely javascript style with or without the help of a lib like jquery. I can't use PHP, or the like in this concept. So if anyone can throw me a bone i'd be greatly appreciative. On a side note, does anyone know if theres a means of actually testing something like this stuff out without actually having a S3 account, cause I can't afford to pay for an account just for the sake of testing let alone any other reason.
563e0f3d2d1761a701f0f6cb	X	Firstly, I am getting the feeling that you are quite new to consuming web-services client side. It is often best to start with something simple. If I have a resource that returns a string... say test.html -> "Hello World!" And the URL for this web-service is some-realy-long-id.s3.amazonaws.com then we have the following: You must remember that requests from the browser follow the same-origin policy, so unless you are planning to use JSOP, or some other cross-domain hack you will run into trouble. p.s. another little piece of advice is to use right hand braces in Javascript as it performs semi-colon insertion (which will bite you if you return a object literal). Oh yes and a lot of old browsers do not support 'PUT' which you may need to consider.
563e0f3d2d1761a701f0f6cc	X	Are you using an EC2 instance on Amazon? If so, is it in the same region as your S3 bucket? Which instance type? I handle image resizing in a similar way and it doesn't take that 23 secs, even for large images.
563e0f3d2d1761a701f0f6cd	X	@user1091949 They are in the same region us-east.May be bandwidth can be an issue!For me its variable between 1mbps-2mbps.Your say?
563e0f3d2d1761a701f0f6ce	X	How big are the images? Also, is the 23secs the length of time to transfer from your computer, plus the time it takes to resize?
563e0f3d2d1761a701f0f6cf	X	The size may vary...The numbers I have put are for images upto 2 mb and around.But the application has no restriction for size yet(may be we have it later to around 10-12 mb) as it is in development right now.
563e0f3d2d1761a701f0f6d0	X	I will study both the approaches rineez.The first looks more apt but will need a POC to check the use.Actually I ask the user to select a size from a set of defined sizes like original,1024x768,800x600,100x100 and then I show the image.So I will have to keep them ready before the user selects the options.That's what I do,I create the versions before the user can select and when he selects they come up easily.
563e0f3e2d1761a701f0f6d1	X	This question says something about image resizing and in memory representation,but I did not get how can I implement it in my scenario.
563e0f3e2d1761a701f0f6d2	X	I have another issue with amazon and its related to file uploads.I am using jqueryFileUpload and amazon API's to uplaod files to amazon S3.I have succeeded in uploading it,but it involves a trick. I had to store the image on my server and then move it to S3 from there using putObjectFile method of S3.Now the plugin comes with great functions to crop/resize images and I have been using them since long.Now when I integrate the plugin with AWS,i am facing performance issues with upload.The time taken for uploads is longer than normal and this raises questions of us using AWS S3 over traditional way. I had to make changes to my UploadHandler.php file to make it work.These are the changes made.i added a part of AWS upload code to the file from line 735 to 750 Here is a link to s3 class on git. The normal upload to my current server(not amazon),same image uploads in 15 secs,but on amazon S3 it takes around 23 secs and I am not able to figure out a better solution.I have to store the image on my sever before uploading to S3 as I am not sure if I can process them on the fly and upload directly to S3. Can anyone suggest the right way to approach the problem?Is it possible to resize the images to different sizes in memory and upload directly to S3 avoiding the overhead of saving it to our server?If yes can anyone guide me in the right direction? Thank you for the attention.
563e0f3e2d1761a701f0f6d3	X	I believe the approximate 8secs is the overhead here for creating versions of image in different sizes. You may take different approaches to get rid of the resizing overhead at time of upload. The basic idea will be to allow the uploading script to finish execution and return the response, and do the resizing process as a separate script. I like to suggest following approaches: Approach 1. Don't resize during the upload! Create resized versions on-the-fly only when it is being requested for the first time and cache the generated images to serve directly for later requests. I saw a few mentions of Amazon CloudFront as a solution in some other threads in Stackoverflow. Approach 2. Invoke the code for creating resized versions as a separate asynchronous request after the upload of original image. There will be a delay in scaled versions being available. So write necessary code to show some place holder images in the website until the scaled versions become available. You will have to figure out some way to identify whether scaled version is available yet or not(For example check file is existing, or set some flag in database). Some ways for making asynchronous cURL requests are suggested here if you would like to try it out. I think both approaches will have equal level of complexity. Some other approaches are suggested as answers for this other question.
563e0f3e2d1761a701f0f6d4	X	Be careful, as stated here docs.google.com/support/bin/answer.py?answer=50092. "Important notes: Your storage quota includes plenty of bandwidth for ordinary use. If there's excessive bandwidth use, we may limit your access for a period of time.". So Google Docs (G-Drive) might not be the right solution in this case
563e0f3e2d1761a701f0f6d5	X	really useful info! thx a lot
563e0f3e2d1761a701f0f6d6	X	If you use S3 or G or any other external provider, make sure you have them on your own servers as well, and have an automatic process to switch over in case the external provider stop working, catches fire, goes out of business, or just mucks up your account. This requires that your image index has the local and external URLs on hand
563e0f3e2d1761a701f0f6d7	X	thank you again
563e0f3e2d1761a701f0f6d8	X	where did you find the =s parameter option? possible values I've found are 512-800. Is there any doc for the supported parameters? thx
563e0f3e2d1761a701f0f6d9	X	Just look at the web pages that Google generates. Either in Google docs browser or Picasa (which uses same storage system)
563e0f3e2d1761a701f0f6da	X	I'd like to save some of my site monthly bandwidth allocation and I'm wondering if I can use Flickr PRO or I should rely on Amazon S3 as an hosting service for my web site images. (My Web Application allows users to upload their own pictures and at the moment it's managing around 40GB of data) I've never used Amazon's services and I like the idea of using Flickr REST Api do dynamically upload images from my webApp. I like also the idea of having virtually unlimited space to store images on Flickr for only 25$/year but I'm not sure if I can use their service on my web site. I think that my account can be banned if I use Flickr services to store images (uploaded by users of my website) that are not only for 'personal use'. What's your experience and would you suggest other services rather than Amazon's S3 or is this the only available option at the moment? Thanks edit: Flickr explicitly says 'Don’t use Flickr for commercial purpose', you could always contact them to ask to evaluate your request but it sounds to me like I can't use their services to achieve what I want. S3 looks like the way to go then... Even though a rough estimate of what I'm going to spend every month is still scaring is there any cheaper place to host my images?
563e0f3f2d1761a701f0f6db	X	400 images per user seems high? Is that figure from actual stats? Amazon S3 is great and it just works! A possible cheaper option is Google. Google docs now supports all file types, so you can load the images up to a Google docs folder, and share the folder for public access. The URL's are kind of long e.g. http://lh6.ggpht.com/VMLEHAa3kSHEoRr7AchhQ6HEzHVTn1b7Mf-whpxmPlpdrRfPW216UhYdQy3pzIe4f8Q7PKXN79AD4eRqu1obC7I Add the =s paramter to scale the image, cool! e.g. for 200 pixels wide http://lh6.ggpht.com/VMLEHAa3kSHEoRr7AchhQ6HEzHVTn1b7Mf-whpxmPlpdrRfPW216UhYdQy3pzIe4f8Q7PKXN79AD4eRqu1obC7I=s200 Google only charge USD5/year for 20GB. There is a full API for uploading docs etc
563e0f3f2d1761a701f0f6dc	X	I love amazon S3. There are so many great code libraries (LitS3) and browser plugins (S3Fox) and upload widgets (Flajaxian) that make it really easy to use. And you only pay for what you use. I use it a lot and have only ever experienced down time on one occasion. Nivanix is an s3 competitor. I haven't used them, but they have a bit more functionality (image resizing) etc. Edit:The link about Nivanix is dead now(2015/07/21), because Nivanix was dead.
563e0f3f2d1761a701f0f6dd	X	Have you already gone through this solution: stackoverflow.com/questions/3871430/… ?
563e0f3f2d1761a701f0f6de	X	It wasnt that particular post; but similar. I ended up using the TransferUtility + the ConfigOptions to split the parts into the correct size
563e0f3f2d1761a701f0f6df	X	I am trying to upload a large file to Amazon S3. I first used the PutObject and it worked fine but took about 5 hours to upload a 2GB file. So I read some online suggestions and tried it with the TransferUtility. I have increased the timeout but this TransferUtility API always give me "The request was aborted. The request was canceled." error. code sample:
563e0f3f2d1761a701f0f6e0	X	Try This
563e0f3f2d1761a701f0f6e1	X	I tried this and it fixes the problem.
563e0f402d1761a701f0f6e2	X	This isn't work. The only way to set new cache-control headers is by "moving" the file, as the updated question says.
563e0f402d1761a701f0f6e3	X	My Django project uses django_compressor to store JavaScript and CSS files in an S3 bucket via boto via the django-storages package. The django-storages-related config includes This works except that when I visit the objects in the S3 management console I see the equals sign in the Cache-Control header has been changed to %3D, as in max-age%3D100000, and this stops caching from working. I wrote a little script to try to fix this along these lines: but this does not change the metadata as displayed in Amazon S3 management console. (Update. The documentation for S3 metadata says After you upload the object, you cannot modify object metadata. The only way to modify object metadata is to make copy of the object and set the metadata. For more information, go to PUT Object - Copy in the Amazon Simple Storage Service API Reference. You can use the Amazon S3 management console to update the object metadata but internally it makes an object copy replacing the existing object to set the metadata. so perhaps it is not so surprising that I can’t set the metadata. I assume get_metadata is only used when creating the data in the first place. end update) So my questions are, first, can I configure django-storages so that it creates the cache-control header correctly in the first place, and second, is the metadata set with set_metadata the same as the metadata viewed with S3 management console and if not what is the latter and how do I set it programatically?
563e0f402d1761a701f0f6e4	X	Use ASCII string as values solves this for me.
563e0f402d1761a701f0f6e5	X	cache_control is a property of key, not part of metadata. So to set cache-control for all the objects in a bucket, you can do this:
563e0f402d1761a701f0f6e6	X	I am working on file sharing for objects stored on amazon S3.Now the path for the object stored on S3 is default like this https://s3.amazonaws.com/bucket_name/path_to_file/file_name.jpg/docx etc.Now I want to share these file URLs via email through my app. Currently when I share I see the entire URL as is in the email.I want it to be sent in an encoded form so that its hard to guess the exact location of the file. I am using PHP and I was planning to use base_64_encode/decode functions or md5 the URLs but not sure if thats the right way to go. So,I am looking for some tool or API (by amazon ot 3rd party) that can do it for me. I would also like to shorten the URLs while sharing. Would like to seek advice and guidance from someone implemented something similar. Not sure if it comes under URL-REWRITING but tagging it under it. Thank you
563e0f402d1761a701f0f6e7	X	option 1: You can map your url and s3's url in your server, and give your url to user. When user make request to your url, redirect it to s3's. You can ref http://en.wikipedia.org/wiki/URL_redirection option 2: You can call API provided by url redirect service provider, e.g., http://tiny.cc/api-docs
563e0f412d1761a701f0f6e8	X	yes - should work.
563e0f412d1761a701f0f6e9	X	It seems that amazon S3 .NET SDK is not supported in windows 8 store apps because of some restrictions in WinRT.Is there any other work around? In case I use REST api, can i use the WinJS.xhr function to call and process the web service?
563e0f412d1761a701f0f6ea	X	This will work with the REST api. Check out: Uploading Image to Amazon s3 with HTML, javascript & jQuery with Ajax Request (No PHP) http://aws.amazon.com/articles/1434 Note that in WinJS you DO NOT have to deal with CORS here as HTML/JS Windows Store apps do not have the cross domain restriction that you have in the browser. http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html You can also test this out using Fiddler's composer (http://fiddler2.com/) to ensure you have the proper request and then recreate in your JavaScript
563e0f412d1761a701f0f6eb	X	would you mind sharing the upload script (sans API docs). It would be helpful to see how you are sending the temporary file up to S3
563e0f412d1761a701f0f6ec	X	I wrote that for my previous employer, I don't have access to the code anymore. It was pretty basic usage of the API though, nothing too fancy!
563e0f412d1761a701f0f6ed	X	I'm generating a PDF file using the PdfLib PHP extension. I start a new file, load an image into it, and call end_document to finish the file. All is working fine, the file is generated correctly, but I'm trying to upload the file to the Amazon S3 cloud immediately (in the same function that generates the file) after generating it. The file is uploaded but it is not complete, seems like an empty pdf is uploaded. The location where I keep the file locally contains the file as it should be: the image is loaded into it and the file is about 600KB in size. The uploaded file is like an empty skeleton, the file that was generated when I called PdfLib->begin_document but before anything is actually placed inside it, I can't open it and it is 104 Bytes in size. I tried sleeping for several seconds after end_document and before the upload, because I thought maybe PdfLib needed some time to finish things up, but this has no effect, the results are exactly the same. Does anyone have any ideas on how to fix this, what can cause this, where to look for a solution? UPDATE The problem seems to be the S3 api class I'm using. I was using this one. Using the official SDK the upload succeeds as expected. I will continue using the official SDK. The original question, why it failed using the other API class, remains.
563e0f422d1761a701f0f6ee	X	I'd like to securely display a grid of thumbnail images to an authenticated user on our site. All the images will be stored in Amazon S3. One way, I suppose, is to implement "security by obscurity" by uploading these images with public read access, and making the keys long and random. I also could set up ACLs, but then I'd have to disclose the access key in the url (I think), or pull the image into my application via the API and display it securely through the web server. Is there a preferred way to do this? And to be able to display the images quickly without requiring tremendous requests to S3 from the server every time a page is generated? Thanks in advance
563e0f422d1761a701f0f6ef	X	You can generate urls to s3 with an expiry date. Generating such a URL does not require a request to S3 and does not result in the disclosure of your secret key: you use your secret key to generate a signature that is appended to the URL (the access key id is in that URL but that's ok) See the docs on query string authorization
563e0f422d1761a701f0f6f0	X	is there has any methods to limit incoming buffer size ?
563e0f422d1761a701f0f6f1	X	You can manage the buffers sizes and bandwidth by using response.pause() and response.resume(). This will cause the TCP algorithms to back off appropriately. (see nodejs.org/api/http.html#http_response_pause )
563e0f422d1761a701f0f6f2	X	thanks for help, I will try it :)
563e0f432d1761a701f0f6f3	X	I want provide a rest service api by restify ( or express ) solution1: using request stream pipe to put files to amazon by knox solution2: using formdiable and fs to pre store file on server local temp folder and then put files to amazon by knox which way is better between solution 1 and solution 2 or other better way ?? and does here has any performance issues need to know?
563e0f432d1761a701f0f6f4	X	Depending on the server load and your available bandwidth, your diskio is your biggest potential bottleneck. I would say go with solution 1 that streams the file directly and doesn't store it to physical media. Make sure you don't attempt to store the entire object to memory either or you could run into a system resources problem.
563e0f432d1761a701f0f6f5	X	is there a way to use this with django-storages?
563e0f432d1761a701f0f6f6	X	@DataGreed I am not sure, I understand what you mean, but I can tell you that boto works with django-storages and I use the latter in my application to redirect default file storage to Amazon S3
563e0f432d1761a701f0f6f7	X	In my Django application (RESTful API with DjangoRestFramwework), I have to store images on Amazon S3. Due to some technical constraints, we have to use direct upload by a client application using signed URLs. Everything works fine but after the upload is performed by the client, the content type is always set to binary/octet-stream, while I naturally want it to be, say, image/jpeg. As it is mentioned in the Amazon S3 documentation here, response-content-type header has to be added to the signed URL in order to ensure the Content-Type of the response by S3 will be correct. Here is my code for signing the URL (written according to boto documentation): This code works almost fine: it generates a URL which I then successfully use for upload. The curl request is: Unfortunately, when I access the file, the content type is still wrong (binary/octet-stream). Am I doing anything wrong?
563e0f432d1761a701f0f6f8	X	Today, after a lot of experiments I found an answer. I was inspired by... official Amazon S3 documentation :) Here is the URL signing code: The principal change here is the headers section, where I put 'Content-Type': 'image/jpeg' ('x-amz-acl' : 'public-read' is also useful since it makes your file publicly available for reading). Then you have to place Content-Type header to your HTTP request. In my case, I used curl as follows: Enjoy!
563e0f432d1761a701f0f6f9	X	I am not sure if HEAD OBJECT will get it but I found a solution to it. Please find my answer below.
563e0f432d1761a701f0f6fa	X	It is actually very easy to get the MIME type of objects stored in S3. All you have to do is response = _s3.GetObject(request); string MIMEtype = response.ContentType; response.ContentType will then get the MIME type from s3. We are using Cloud berry explorer pro to upload objects and to maintain our s3 account. It has a lot of default MIME types and it assigns that to objects as you upload them. You can also add your own MIME types. You can specify the extension and the MIME type, so the next time you upload something it automatically assigns a MIME type to that object.
563e0f432d1761a701f0f6fb	X	@Abhi.Net: While GetObject() works fine in principle, @Roman is correct to suggest HEAD resp. GetObjectMetadata() here, which is significantly more efficient for your use case - see my update to his answer for details.
563e0f442d1761a701f0f6fc	X	Thank you guys for your solution. Now, I know when to choose GetObject() and GetobjectMetaData(). Although in my case I have to retrieve the objects(Diff types- css, html,jpeg, gif, flash) so I will have to go with GetObject() but 'll keep GetGetobjectMetaData() in my mind in case I just need the meta data.
563e0f442d1761a701f0f6fd	X	Is there a way to retrieve the MIME types of the objects in S3. I am trying to implement a solution in which I will be getting multiple objects from S3. Instead of using there key and then getting a sub string to calculate the MIME type, can I get the MIME type from Amazon S3 in some way? I am using cloud berry explorer pro and I know it let's you set the MIME type, but how do we retrieve this information using the AWS SDK for .NET or the REST API?
563e0f442d1761a701f0f6fe	X	The REST API offers the HEAD Object operation for this purpose and the AWS SDK for .NET conveniently wraps the very same functionality via the GetObjectMetadata() method: The GetObjectMetadata operation is used to retrieve information about a specific object or object size, without actually fetching the object itself. This is useful if you're only interested in the object metadata, and don't want to waste bandwidth on the object data. The response is identical to the GetObject response, except that there is no response body. [emphasis mine]
563e0f442d1761a701f0f6ff	X	To get the file and mimeType of the file in the same request...
563e0f442d1761a701f0f700	X	Use this: blueimp.github.com/jQuery-File-Upload
563e0f442d1761a701f0f701	X	@apneadiving that worked exactly how I wanted it to. The plugin page had some very helpful examples that pointed me in the right direction and I've implemented exactly what I want. If you make an answer to the question instead of a comment, I will gladly accept your answer. Thank you very much, you helped save me a lot of effort.
563e0f442d1761a701f0f702	X	Nice to read :)
563e0f442d1761a701f0f703	X	Can you share how you did this with multiple files and S3? I can upload single files, but can't quite figure out how to upload more than one at a time.
563e0f452d1761a701f0f704	X	If you want to write code to make all of the different S3 API calls client side, generate the policies, handle the specific S3 errors, etc, yourself then use jquery file upload. If you want something that just works for simple to extremely complex upload to S3 workflows, use Fine Uploader.
563e0f452d1761a701f0f705	X	Whilst this may theoretically answer the question, it would be preferable to include the essential parts of the answer here, and provide the link for reference.
563e0f452d1761a701f0f706	X	@IlmariKaronen I posted upon asker request. But feel free to downvote if you like
563e0f452d1761a701f0f707	X	Someone flagged your answer as "not an answer". I don't quite agree, but it is pretty borderline. That said, the question isn't exactly the clearest one ever, either.
563e0f452d1761a701f0f708	X	This issue has been bothering me for many hours and I can't seem to find a solution to it. I have a rails 3.2 app that allows users to upload files to an Amazon S3 account using carrierwave_direct, fog, and carrierwave (dependency for carrierwave_direct). Using carrierwave_direct allows the user to skip uploading the file to the server by POSTing it directly to Amazon S3 (saves server processing and timeouts like Heroku for large files). It works fine if all you do is select 1 file, upload it to Amazon, and want a redirect_to a URL you provide Amazon. It does this by POSTing the form to Amazon S3, and Amazon responds to a provided URL (you specify this URL in your form) with some params in the URL, which are then stored as a pointer to the file on Amazon in your model. So the lifecycle is: select 1 file, POST to Amazon, Amazon responds with a URL that sends you to another page, and you can then save a record with a pointer to the Amazon file. What I've been trying to figure out is how do I allow multiple files to be selected and uploaded and update the upload progress? I'm trying to do this with pure javascript (using the file API provided by modern browsers) so I don't want any 3rd party tools. Also, in the pursuit of learning this in-depth, I'm avoiding any plugins and am trying to write the code myself. The functionality I'm trying to obtain is: At this point, I could even do without an individual progress bar; I'd be happy just to get multiple files POSTed to Amazon S3 without page refreshes. I am not partial to any of the gems. I'm actually afraid I'm going to have to write what I want to do from scratch if I really want it done in a specific way. The goal is multiple file uploads to an Amazon S3 account via AJAX. I would be ecstatic with even general concepts of how to approach the problem. I've spent many hours googling this and I just haven't found any solutions that do what I want. Any help at all would be greatly appreciated. EDIT 2014-03-02 Raj asked how I implemented my multiple upload. It's been so long I don't recall all the "why" behind what I did (probably bad code anyway as it was my first time), but here is what I had going on. The model I was uploading was a Testimonial, which has an associated image being stored in Amazon S3. It allowed a user to select multiple images (I think they were actually PDF files I converted to images) and drag/drop them onto the screen. While uploading, I displayed a modal that gave the user feedback about how long it would take. I don't pretend to remember what I was doing on a lot of this, but if it helps feel free to use it. Here's the view: And the JS: The Testimonial model:
563e0f452d1761a701f0f709	X	As advised in comment, use jQuery Upload: http://blueimp.github.com/jQuery-File-Upload/
563e0f452d1761a701f0f70a	X	I have started writing a basic library for this functionality. I have a working version on github, and am writing a series of blog posts to detail how to achieve this. 'Working' code can be found at : https://github.com/joeandrews/s3multipartupload. I am in the process of writing a series of blog posts, which when complete will be posted on http://blog.fuuzik.com. I hope this helps in the mean time though.
563e0f452d1761a701f0f70b	X	Thanks! Now, as I am trying to create this policy and later sign this using my amazon access key. How do I ensure that my indentation and the indentation which will be generated at Amazon's end will be same? So that, ultimately, my signature matches with that of Amazon's.
563e0f452d1761a701f0f70c	X	@TriptiR Just use the formatting what you see in Amazon's text (e.g. 2 spaces for indenting, arrays in one line, space after ':' and ',' etc.).
563e0f452d1761a701f0f70d	X	I found one issue with in the JSON policy itself(1 parameter was different). I tried something else after your suggestion, I decoded the original base64 string from amazon tutorial by using [link](base64encode.org) and re-encoded that using my program. Though I cant see any difference in indentation now, the base64 encoded strings are still different.
563e0f462d1761a701f0f70e	X	@TriptiR Edited the answer, there are 2 differences besides indentation.
563e0f462d1761a701f0f70f	X	Thanks, as it was unsuitable to change the question itself. Still, I couldn't get it working even after correcting this. In steps for creating a POST policy, there is a requirement - "Create a policy using UTF-8 encoding". I believe, I am missing something at this point. How to ensure that my JSON-policy is created using UTF-8 encoding?
563e0f462d1761a701f0f710	X	I am trying to upload a image file to amazon s3. The setup is as follows: Web server: golang Front-end: simple html form for testing In reference with this document : http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html I referred to the example provided in the above document and tried this: http://play.golang.org/p/3zn5fSDasK The result I got with this The base 64 encoded policy which is on link of amazon tutorial: Why my base64 encoded policy doesn't match with that of Amazon's?
563e0f462d1761a701f0f711	X	Simply because the 2 JSON source texts which produce those base64 strings have different indentation and different content as seen below (they are not equal char-by-char). Decode both of the base64 strings, and you will see the differences. You can do that either with a program (not necessarily Go), or simply use an online service like this one. You shouldn't worry about indentation, it doesn't matter in JSON. But when you encode a text using Base64, that encodes all characters of the source including spaces and tabs used to indent, so different indentation will result in different Base64 encoded forms. But comparing the 2 decoded JSON, there are also other differences: First one: 2nd one: The complete decoded JSON texts: The first one: And the 2nd one:
563e0f462d1761a701f0f712	X	Seems like you are having same issue : stackoverflow.com/questions/30778579/…
563e0f462d1761a701f0f713	X	possible duplicate of Transport Security has Blocked a cleartext HTTP
563e0f462d1761a701f0f714	X	You should take a look at a post on AWS Mobile Development Blog.
563e0f462d1761a701f0f715	X	Thanks it worked! But is that enough? Looking at: mobile.awsblog.com/post/Tx2QM69ZE6BGTYX/… they suggest to add some more stuff
563e0f462d1761a701f0f716	X	I am trying to load images from amazon s3 (async). But I get these errors in my log: NSURLSession/NSURLConnection HTTP load failed (kCFStreamErrorDomainSSL,-9802) I am making all my api calls with "https", it worked in ios 8.3 / xcode6 So
563e0f472d1761a701f0f717	X	You are running into the HTTPS errors of App Transport Security read here: Transport Security has Blocked a cleartext HTTP But the basic jist is add the following to your info.plist You can do that via a text editor or in XCODE as such: 
563e0f472d1761a701f0f718	X	Add the following to your info.plist exactly as typed. NSAppTransportSecurity Once you create that make its value a dictionary called NSAllowsArbitraryLoads Set the value to true or yes
563e0f472d1761a701f0f719	X	You are running into the HTTPS errors of App Transport Security read here: Transport Security has Blocked a cleartext HTTP But the basic jist is add the following to your info.plist
563e0f472d1761a701f0f71a	X	Correct, There is no limitation over number of object stored on Amazon S3 Bucket. For large file you can upload your data in parts and then merge when all parts get uploaded. I am one of the Developer of Bucket Explorer, Supporting Multipart operation for Amazon S3 Object using Amazon Multipart API for Upload as well as Exclusive Multipart Download Operation.
563e0f472d1761a701f0f71b	X	What is the size of data that can be sent using the GET PUT methods to store and retrieve data from amazon s3 cloud and I would also like to know where I can learn more about the APIs available for storage in Amazon S3 other than the documentation that is already provided.
563e0f472d1761a701f0f71c	X	The PUT method is addressed in the respective Amazon S3 FAQ How much data can I store?: The total volume of data and number of objects you can store are unlimited. Individual Amazon S3 objects can range in size from 1 byte to 5 terabytes. The largest object that can be uploaded in a single PUT is 5 gigabytes. For objects larger than 100 megabytes, customers should consider using the Multipart Upload capability. [emphasis mine] As mentioned, Uploading Objects Using Multipart Upload API is recommended for objects larger than 100MB already, and required for objects larger than 5GB. The GET method is essentially unlimited. Please note that S3 supports the BitTorrent protocol out of the box, which (depending on your use case) might ease working with large files considerably, see Using BitTorrent with Amazon S3: Amazon S3 supports the BitTorrent protocol so that developers can save costs when distributing content at high scale. [...]
563e0f472d1761a701f0f71d	X	Possible related: stackoverflow.com/questions/23098223/…
563e0f472d1761a701f0f71e	X	I'm building an app on the MEAN (MongoDB, Express, AngularJS, node.js) stack that requires uploading image files to Amazon S3. I'm doing it in the following way: First, an http get is sent to my API, which specifies the 'policy document' for the interaction and returns it to the AngularJS frontend. That backend code looks like this (with the variables filled in): The frontend (AngularJS) code for this process looks like this: However, as it is currently, if I post an image I get the following error: If I change the http POST call to the AngularJS shorthand and remove the "content-type": "multipart/form-data" specification, like such: I get the following error instead: I have tried any combination of conditions and specifications that I can think of. What am I missing here?
563e0f472d1761a701f0f71f	X	I believe the problem was with the format of the "file" I was attempting to upload. I was using this AngularJS directive, and the format it returns resized images in isn't as an actual file. I'm still using that same directive, but no longer using the image resizing. I changed the structure of my upload process to do most of the work on the node.js backend. The AngularJS function looks like this: And the node.js/express endpoint to post looks like this: This is now using Amazon's AWS sdk for node.js.
563e0f482d1761a701f0f720	X	Recently i also faced to the same problem. You need to do convert a dataURI to a Blob. And upload. Upload the file returned from this function. Its uploading fine with S3 upload. }
563e0f482d1761a701f0f721	X	I got that much, however it doesn't solve the real problem of SL4 not accessing the clientaccesspolicy.xml file. I'll add an edit, that if I force the end point to point to http instead of https required by amazon, fiddler does show that it access the file, so it appears to only be an https issue.
563e0f482d1761a701f0f722	X	Did you check out the answers here? stackoverflow.com/questions/2449737/…
563e0f482d1761a701f0f723	X	This is an odd one. I'm using Amazon S3 for storage of files in my Silverlight 4 application. Because of the file restrictions associated with the REST API and S3 (files have to be < 1mb for REST), I'm trying to get the SOAP calls to work. I followed the tutorial written by Tim here http://timheuer.com/blog/archive/2008/07/05/access-amazon-s3-services-with-silverlight-2.aspx minus the parts about CNAME's since he updated and said it was bad to do that for security, but kept having issues connecting till it just magically started working this morning and I was able to get a list of all my buckets! So I thought it was fixed, until a few minutes ago when I restarted Chrome and then tried the application again, and it no longer connected to the SOAP endpoint and VS gave me the cross-domain error. However, I thought about all the stuff I had done earlier to get it working, and the only thing I could think of was that I had a tab open with the clientaccesspolicy.xml file open via bucket.s3.amazonaws.com/clientaccesspolicy.xml. So I tried opening it up again in a new tab, opened my application in another, and then the SOAP calls started working! It only works when the file is open in a tab!!! I've tried it in Firefox and IE as well, same thing! I have Fiddler, and it doesn't seem to actually ever make a call to the clientaccesspolicy.xml, unless it's hidden inside one of the SSL calls which then there's no way to tell, but there's no straight out calls to .s3.amazonaws.com/clientaccesspolicy.xml going through Fiddler like some other questions on here said there would be. Would really appreciate some help here guys, thanks. Edit: Since someone will probably ask for it, this is the clientaccesspolicy.xml file I'm currently using. I know it's not the most secure, just trying to get this to work before I take the wildcards out Edit 2: This appears to be an issue HTTPS. If I force my endpoint to be http, instead of the https required by Amazon, Fiddle does show SL accessing the clientaccesspolicy.xml file.
563e0f482d1761a701f0f724	X	When you open the clientaccesspolicy.xml file in another tab I'm guessing you are passing some credentials which allows you to access it. This sets a cookie which Silverlight can then use to access the clientaccesspolicy.xml file as well. When you close the browser you lose the cookie and thus the access to the file.
563e0f482d1761a701f0f725	X	So I figured it out. The first problem, about why it would work if I opened it, was not because of a cookie being set (per say), but that accessing it that way over https made me accept the SSL security policy for amazon. The second problem, I shouldn't have had to accept it. The SSL wildcard amazon uses, *.s3.amazonaws.com, doesn't match buckets that contain periods in them. So as I was following Tim's tutorial I made all my buckets like this bucketname.domain.com and when I tried to access it that way through SOAP (and subsequently https), it wasn't working because the wildcard wasn't being matched. Changed all my buckets to contain no buckets, and it worked. Should also note that Tim's tutorial no longer works as he's using http and in June of this year Amazon forced SOAP calls over https, so http calls no longer work.
563e0f482d1761a701f0f726	X	What does $e->getMessage() say?
563e0f482d1761a701f0f727	X	Does the user own the bucket? Also, are the credentials from an IAM user? If so, does their policy allow the putObject operation?
563e0f482d1761a701f0f728	X	Do not use file_get_contents(), that will load your whole file into memory unnecessarily. Do 'Body' => fopen(drupal_realpath($file_path), 'r') or 'SourceFile' => drupal_realpath($file_path)
563e0f482d1761a701f0f729	X	I am trying to integrate PHP Amazon SDK - s3 api. I am giving the correct credentials. But I am not sure about putObject parameters. This is the code that I tried: After make the request I am getting this Response:
563e0f492d1761a701f0f72a	X	If you are looking to code it, why are you talking about clones? I don't see anything special in your requirements. What's the question?
563e0f492d1761a701f0f72b	X	I really don't know why I mentioned the clone, but my question pretty much is what would you recommend for modules and or programs for my requirements.
563e0f492d1761a701f0f72c	X	Are you asking for advice about setting up the service or advice about programming an interface to such a service? What is your question?
563e0f492d1761a701f0f72d	X	Advice about programming such a service to do what I want. For example for the USERDB, I could use PostgreSQL and DBIx::PgLink from cpan but id like advice of what would be better to use than PostgreSQL and the module I mentioned if there is even something better to use.
563e0f492d1761a701f0f72e	X	Thanks, But I was already planning on using MogileFS for the storage.
563e0f492d1761a701f0f72f	X	Good stuff. Two great mind think a like ;-) Or fools never differ :(
563e0f492d1761a701f0f730	X	As MogileFS isn't mentioned anywhere here uptil now then this answer is very useful as a reference for future seekers.
563e0f492d1761a701f0f731	X	I am rethinking about using mogilefs for this, because it seems to be for a few servers with not a whole lot of storage. I am planing to be running 400+TB per rack. But now I am not sure what to use for storage.
563e0f492d1761a701f0f732	X	I am looking to code a file storage application in perl similar to amazon s3. I already have a amazon s3 clone that I found online called parkplace but its in ruby and is old also isn't built for high loads. I am not really sure what modules and programs I should use so id like some help picking them out. My requirements are listed below (yes I know there are lots but I could start simple then add more once I get it going): Thanks in advance
563e0f492d1761a701f0f733	X	Perhaps MogileFS may help? Also there was a recent discussion about MogileFS performance on the Google Groups / Mailing list which maybe of interest to you. /I3az/
563e0f492d1761a701f0f734	X	here I found a ruby impl https://github.com/jubos/fake-s3 hope that helps mike
563e0f492d1761a701f0f735	X	I have created a super simple server, see the put routine in Photo::Librarian::Server.pm it supports the s3cmd put of a file, nothing more for now. https://github.com/h4ck3rm1k3/photo-librarian-server https://github.com/h4ck3rm1k3/photo-librarian-server/commit/837706542e57fbbed21549cd9e59257669d0220c
563e0f4a2d1761a701f0f736	X	The output of Hive on EMR is a file named 000000_0 (perhaps a different number if there is more than 1 reducer). How do I get this file to be named differently? I see two options: 1) Get Hive to write it differently 2) Rename the file(s) in S3 after it is written. This is could be a problem: from what I've read S3 doesn't really have a "rename". You have to copy it, and delete the original. When dealing with a file that is 1TB in size, for example, this could cause performance problems or increase usage cost?
563e0f4a2d1761a701f0f737	X	The AWS Command Line Interface (CLI) has a convenient mv command that you could add to a script: Or, you could do it programmatically via the Amazon S3 COPY API call.
563e0f4a2d1761a701f0f738	X	I have my bucket on Amazon S3 filled with lot of images. I want to develop a API that would hotlink all the images to my website. For this i want to write a code that would fetch the URL`s for all the images from bucket into a PHP - array. I could`nt find code that would dynamically fetch the URL of all the files in bucket without passing the file name. Waiting for help !!
563e0f4a2d1761a701f0f739	X	The best place to get the info is here: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/service-s3.html Basically you create an iterator off of the bucket and then for each one you get the object URL.
563e0f4a2d1761a701f0f73a	X	How can i transfer file amazon s3 to youtube using php api, also is there available to set bitrate when uploading.
563e0f4b2d1761a701f0f73b	X	Relevant link on Stack Overflow: stackoverflow.com/questions/2576923/dropbox-com-api-for-net and external link dkdevelopment.net/2010/05/18/…
563e0f4b2d1761a701f0f73c	X	Is there an online backup service (like Dropbox), which supports a C# or JAVA API? I want to use this as part of a build process to host my code offsite. Thanks
563e0f4b2d1761a701f0f73d	X	You can take a look at Amazon S3 API, it can be accessed with WebServices. If you just want to store code and keep different versions you should use versionning system such as Subversion, git or mercurial. Look at the links below for more informations. Resources : On the same topic :
563e0f4b2d1761a701f0f73e	X	If it is for code, why not use a hosted SVN like Assembla or Codesion?
563e0f4b2d1761a701f0f73f	X	If you want to just store your data so please look at this below link. Login to this site and store your data. It's look like windows desktop with my computer and drives. http://www.theskypc.com
563e0f4b2d1761a701f0f740	X	I have a scenario in which multiple lines of text are to be appended to an existing text file... Is it possible to do this using Jclouds? (That would be ideal for me as jclouds supports a lot of cloud providers)... Even if this is not doable using jclouds, does the native API of Amazon S3/Rackspace Cloudfiles/Azure storage support appending content to existing blobs? If this is doable, then kindly point me to good working examples which show the same...
563e0f4b2d1761a701f0f741	X	This is not possible in the underlying blob stores I know of.
563e0f4c2d1761a701f0f742	X	This typically happens if the remote server doesn't speak SSL.
563e0f4c2d1761a701f0f743	X	It is working fine at another server.
563e0f4c2d1761a701f0f744	X	Your cURL wasn't compiled with support to perform SSL requests. Recompile using proper binaries.
563e0f4c2d1761a701f0f745	X	Should I need to reinstall cURL ?
563e0f4d2d1761a701f0f746	X	Could you add to your question the configure line? E.g. ./configure --with-curl ... etc from the php info page.
563e0f4d2d1761a701f0f747	X	it actually needs to save other libraries. I have found this solutions a year later :)
563e0f4d2d1761a701f0f748	X	Issue is generally occurred due to upgrade PHP 5.2 to 5.5
563e0f4d2d1761a701f0f749	X	I'm trying to test in PHP Amazon S3 on my localhost on Ubuntu system but keep getting the same error: S3::listBuckets(): [35] error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol It is the function to display bucket list. Here is the Amazon API function that has been called by this function.
563e0f4d2d1761a701f0f74a	X	It seems that you have changed your PHP version as this bug is occurred several time in PHP 5.4 but it works perfectly in previous versions. You can re-install cURL with Open SSL again.
563e0f4d2d1761a701f0f74b	X	Is there some reason the CMS deployment can't bundle the files? Validating/Downloading one bundle of 3MB will be far more efficient than verifying 100 files and downloading each individually.
563e0f4d2d1761a701f0f74c	X	I already do that to some extent. saving the date of the last download and, the next time, downloading only those files which have been modified on the server since.
563e0f4e2d1761a701f0f74d	X	Well, an approach of that sort is probably all you can really do.
563e0f4e2d1761a701f0f74e	X	I have an iphone app that sometimes has to download a set of files from a bucket on a Amazon AWS S3 account. A typical such download will involve maybe 100 files. Most of these files are very small though and all combined, we are still under 3MB. At the moment, I use the listObjectsInBucket function and then loop on all files and use the API/SDK function getObject to get them one by one. The problem is that it takes a very long time to do it that way so I would like to have some advice regarding a faster strategy that would work in my scenario (many small files that have to stay available individually so that they can be modified by a CMS). Thanks in advance.
563e0f4e2d1761a701f0f74f	X	If you want the files to remain resident in the application's local storage, it might make sense to store all the files locally along with metadata on the files (i.e. checksum, last modified timestamp, etc.). You could then compare this metadata against metadata you store in S3 metadata fields, syncing up only those files where the metadata differs.
563e0f4e2d1761a701f0f750	X	Great answer - thank you for that. Max.
563e0f4e2d1761a701f0f751	X	I can set up my EC2 instances so that certain users other than myself are allowed to SSH in. Is there anyway of achieving a similar situation with S3 in giving certain users access to buckets without revealing the Access ID and Secret keys? Thanks for any help. Max.
563e0f4e2d1761a701f0f752	X	Yes, the method to do this is described in the Access Control Lists documentation. Note that when using ACLs, the other users to whom you grant permissions must have their own Amazon S3 account. If your users don't have their own account, see:
563e0f4e2d1761a701f0f753	X	Why is $bucket = 'BUCKET'? Shouldn't it be 'gf-redshift-upload-dev'?
563e0f4f2d1761a701f0f754	X	Apologies simply missed putting the arn as BUCKET not to display what it actually was.
563e0f4f2d1761a701f0f755	X	Add "Version":"2012-10-17" to your policy, and change "arn:aws:s3:::BUCKET*" to "arn:aws:s3:::BUCKET/*" then retry. You may also want to use the IAM policy simulator or install the awscli and test basic S3 commands while you debug your policy.
563e0f4f2d1761a701f0f756	X	First off Im not php so please be kind ! due to failings within SSIS and losing patience I thought of using php as a go between to help move and retrieve files from the AMAZON S3 bucket. Theres an access issue that we cant seem to get round Steps so far WE use IIS The Amazon PHP SDK was downloaded user C:\inetpub\wwwroot\phpaws a index.php file was created the code Then when the API is ran we get on both post and get error msg Error executing "GetObject" AWS HTTP error: Client error: 403 AccessDenied (client): Access Denied - AccessDeniedAccess the permissions on account are as follows: Which means we can do all commands any suggestions on how to get around this error would be great? Many thanks Robert TO be clear This is the full permissions listed { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": "s3:", "Resource": "arn:aws:s3:::BUCKET/" } { "Effect": "Allow", "Action": "s3:ListAllMyBuckets", "Resource": "arn:aws:s3:::*" } , { "Effect": "Allow", "Action": [ "s3:ListBucket", "s3:GetBucketLocation" ] , "Resource": "arn:aws:s3:::BUCKET" } ] }
563e0f4f2d1761a701f0f757	X	You can use mc tool and use mc share command to give presignedURL access to your private s3 objects.
563e0f4f2d1761a701f0f758	X	Requirement is to share a file, stored in Amazon's S3 to any user, who may or may not have amazon access, using an expiring temporary link preferably. We could use the Presigned URLs generated using the access key and secret key of an IAM User with restricted roles. Generating Expiring Presigned URLs was pretty straight forward and I was able to achieve it using the API from Amazon's Java SDK. This URL contains the Access Key ID, visible to anybody who has access to the URL. Though it is not of much security concern, as I understand from this post,are there any other ways of achieving this scenario, where the access key id is not shared? What about the temporary security credentials concept? I would also like to know about other ways to share files in S3 to any external user.
563e0f4f2d1761a701f0f759	X	Thanks. I will try out your script. I had the same issues with trying to add the parameters.
563e0f4f2d1761a701f0f75a	X	I've pushed a commit to the repo that should resolve this bug: github.com/aws/aws-sdk-ruby/commit/…
563e0f4f2d1761a701f0f75b	X	Awesome. Thanks for updating the gem so quickly. I really appreciate it.
563e0f4f2d1761a701f0f75c	X	Is there any chance that S3 could be changed to take the key etc. in a query string for a signed url? That would make it significantly easier to use these signed urls from a browser. Or is there a recommended technique for how to deliver these encrypted files directly to an end user in a browser - ajax w/ CORS or ...?
563e0f502d1761a701f0f75d	X	The presigned url does accept the key, just not much else. For cors, you will need to configure your bucket to accept these headers using #put_bucket_cors. I agree, it would be better if these values could be sent as part of the querystring though.
563e0f502d1761a701f0f75e	X	I am currently using the Ruby aws-sdk, version 2 gem with server-side customer-provided encryption key(SSE-C). I am able to upload the object from a rails form to Amazon S3 with no issues. But I'm having some issues with retrieving the object and generating a signed url link for the user to download it. The link is generated, but when I clicked on it, it directs me to an amazon page saying. The error message is not helping as I'm unsure what parameters I need to add. I think I might be missing some permissions parameters. Get Method http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Object.html#get-instance_method Presigned_Url Method http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Object.html#presigned_url-instance_method
563e0f502d1761a701f0f75f	X	When you generate a pre-signed GET object URL, you need to provide all of the same params that you would pass to Aws::S3::Object#get. This means you need to pass the same sse_customer_* options to #presigned_url: This will ensure that the SDK correctly signs the headers that Amazon S3 expects when you make the final GET request. The next problem is that you are now responsible for sending those values along with the GET request as headers. Amazon S3 will not accept the algorithm and key in the query string. Please note - while testing this, I found a bug in the presigned URL implementation of the current v2.0.33 version of the aws-sdk gem. This has been fixed now and should be part of v2.0.34 once it releases. See the following gitst for a full example that patches the bug and demonstrates: You can view the sample script here: https://gist.github.com/trevorrowe/49bfb9d59f83ad450a9e Just replace the bucket_name and object_key variables at the top of the script.
563e0f502d1761a701f0f760	X	"direct upload" vs "normal upload" isn't a well-defined concept. Those aren't terms I would use, since they are too vague... but I would assume they are referring to user uploads, and allowing the browser to upload directly to S3, instead of uploading to your server and then your server uploading the file to S3. Does that fit the context of whatever you are reading?
563e0f502d1761a701f0f761	X	@Michael-sqlbot yes I am talkin about that exactly (user uploads), but I am just not sure what's the best approach !! if browser direct upload to S3 will reduce things like bandwidth and cpu like John says in his answer, so why should I upload to server ? I am just confusing ..
563e0f502d1761a701f0f762	X	So should I always upload directly to Amazon S3 because I will get those benefits ? also I would like to know what most people use for their application !
563e0f502d1761a701f0f763	X	I've seen many posts talking about direct uploading to amazon S3 but none of them tell when we should really do that ! Is "direct upload to S3" always better than uploading to S3 via Rails app using gems like paperclip or carrierwave... ? And when I should use direct upload vs normal upload to S3 ?
563e0f502d1761a701f0f764	X	Uploading directly to Amazon S3 has the benefit of reducing load on your back-end web servers (for both CPU and bandwidth). It is great for mobile apps that can call the Amazon S3 API directly, but is also applicable to web pages that can upload via a POST to Amazon S3.
563e0f502d1761a701f0f765	X	Amazon features a "Edit Redirection Rules" in the S3 management console:  But I wanted to automate that and update it via command line, which I can't find. Even on Amazon's documentation page, it shows screenshots from the S3 interface, and there's no mention to any web API.
563e0f502d1761a701f0f766	X	The various AWS SDKs call this putBucketWebsite:
563e0f512d1761a701f0f767	X	I am using Amazon S3 to host images for a public REST API and serve them. Currently, my bucket allows anyone to enter in the URL to the image, without any signature included in the params, and the image will be accessible. However, I'd like to require an expiring signature in each image request, so users will have to go through the API to fetch images. How do I do this? Is this a bucket policy?
563e0f512d1761a701f0f768	X	You simply set all of the files to private. If you want to be able to give out pre-signed URLs, you'll need to generate them as you hand them out. AWS’ official SDKs make this trivial.
563e0f512d1761a701f0f769	X	Indeed it's looking like the way we are going to go. It just feels expensive and unnecessary somehow. Chat thread ID's are unique, generated from an md5 hash of the particpants userids (the userid's are ordered and its a little more complex than this) This way when a user begins talking to someone they have an existing history with we can perform the threadID algorithm and get the correct ID everytime, which could be a file. I'm actually leaning towards using DynamoDB for everything and keeping a cached copy of the most recent 20 items in a memcached elasticache instance to keep costs down.
563e0f522d1761a701f0f76a	X	I suppose going the DB route would also allow us to potentially add multi-user chat if needed sometime in the future. I guess we could use our thread_id algorithm to generate the hash key in dynamodb, using a timestamp as the range key and also using the thread_id as the key for the cached items. Does this sound like a reasonable solution to you? While using an RDB is attractive cost wise, we have fallen in love with the no hassle scaling that dynamodb offers us.
563e0f522d1761a701f0f76b	X	Seems reasonable. My going in position is always that you will ALWAYS find uses for data once you have it accumulated, and its better to have it in a way that makes it easy to use/implement that new stuff when the opportunity arises.
563e0f522d1761a701f0f76c	X	We are using a cluster of node.js servers to power our chat system. We would like users to be able to see their previous conversations with any other given user and as such need to store chat logs. Since these never need to be edited or queried in any way it does not make sense to store them in a database and as such we would like to simply keep them as simple json files stored on Amazon S3. Using Amazons PHP api it is now possible to use standard fopen/fwrite commands making streaming to S3 possible. But in this case we need to do it with Node.js. Essentially we would like to be able to open a stream when the users begin chatting, and append to the end of a chatlog file on S3, live while the users are chatting. If this is not possible, what are our other options? We have considered creating local writeStreams on the node.js servers, using them to essentially buffer the data, detect the end of a conversation and then upload/replace the file to S3. It just seems overly verbose and given the the node.js servers are within Amazons network it feels like a poor mans solution when clearly streaming to S3 is now a possibility in PHP. Thanks in advance
563e0f522d1761a701f0f76d	X	I don't agree with your logic here: Just because you only want to write once and read-many, doesn't mean a DB is not an appropriate solution. You could probably use S3, but given the brief description of your problem I would think a classic DB solution is going to serve you better in the long run. If you go the DB route, you have lots of well supported options within AWS (RDS-SQL Server, Postgres, MYSQL), DynamocDB and SimpleDB. Even if you end up storing the chats on S3, its not unreasonable to think you may end up storing the metadata about each chat in a DB such as DynamoDB. How many chats are you going to store? If you end up with 100's of thousands or millions of seperate chat files, how will you navigate a user to the right one when they want it? This is a where a DB will come in handy, and then it begs the question, if you are going to use a DB to store the metadata, why not just store the content as well?
563e0f522d1761a701f0f76e	X	There are several design decisions that could avoid the usage of a full scan, such as indexes, or a simple prefix tree, for example.
563e0f522d1761a701f0f76f	X	you'r right Viccari, the index part is the most challenging one. Note it's not just a prefix issue, many records with a common prefix but differ in after-delimiter-part have to be skipped in order to list a "folder". This skip process could introduce additional overhead.
563e0f522d1761a701f0f770	X	Thanks Viccari. Sure, it's kv fashion with s3 file's namespace, i'm curious of the s3 architecture part, how did s3 did this: you could use any string as the delimiter and the total file number is unlimited. Take this example dataset: folder1/[level2]/[level3], we have one million level2 each with one million level3, and do a ls with prefix=folder1/ and delimiter=/, all [level3] would be folded up. This is a real challenging work to implement.
563e0f522d1761a701f0f771	X	You can have unlimited objects in an Amazon s3 bucket, and use getBucket api to list your objects. The funnies part is you can use any character as a delimiter(which likes "/" in linux file systems). With large quantities of objects, how could the list api response in real-time? you cannot expect a full scan, right? what are the technologies behind s3 storage architecture? Here is some search work results, anyone knows more detail?
563e0f522d1761a701f0f772	X	I am adding as an answer because there is not enough room in the comments section: There are several design decisions that could avoid the usage of a full scan, such as indexes, or a simple prefix tree, for example. Even though several S3 client apps will list prefixes as folders, there is not such concept in S3. Within a bucket, all files are hierarchically in the same level. The organization of the files is in a key/value fashion, rather than in a tree fashion (like one would expect in a "folder"-like system). Please see this related question for more information. So, if you want to list your "folders", yes, it is likely that you will need to list your prefixes, i.e. get objects based on prefix and skip the ones having additional information.
563e0f522d1761a701f0f773	X	To no mess up with the files names, you could create another file with the MD5 content, like image_file_1.tif.md5 .
563e0f522d1761a701f0f774	X	We put hundreds of image files on Amazon S3 that our users need to synchronize to their local directories. In order to save storage space and bandwidth, we zip the files stored on S3. On the user's end they have a python script that runs every 5 min to get a current list of files, and download new/updated files. My question is what's the best way determine what is new or changed to download? Currently we add an additional header that we put with the compressed file which contains the MD5 value of the uncompressed file... We start with a file like this: We compress it (with 7zip) and put it to S3 (with Python/Boto): The problems is we can't get a large list of files from S3 that include the x-amz-meta-uncompressedmd5 header without an additional API for EACH one (SLOW for hundreds/thousands of files). Our most practical solution is have users get a full list of files (without the extra headers), download the files that do not exist locally. If it does exist locally, then do and additional API call to get the full headers to compare local MD5 checksum against x-amz-meta-uncompressedmd5. I'm thinking there must be a better way.
563e0f522d1761a701f0f775	X	You could include the MD5 hash of the uncompressed image into the compressed filename. So image_file_1.tif could become image_file_1.xxxx1234.tif.z Your user python file which does the synchronising would therefore have the information needed to determine if it needed to go get the file again from S3, and could either strip out the MD5 part of the filename, or maintain it, depending on what you wanted to do. Or, you could also maintain, on S3, a single file containing the full file list including the MD5 metadata. So the python script just need to fetch that single file, parse that, and then decide what to do.
563e0f522d1761a701f0f776	X	I have an existing salesforce implementation that uses web services to upload attachments into a Amazon S3 bucket. This process has been working properly for the last 2 years or so. Now, quite suddenly, we are getting intermittent failures in this system (the system is only used 4 times a year, so it may have been broken for some time). I've looked into it quite a bit, and I'm at a total loss. The system generates PutObjectInline requests through the S3 Soap API. It may generate several hundred requests over the period that it runs (usually 10-15 minutes). Of the requests made, about 50% fail (more on this below). Each failure is given a HTTP 400 status from the server with the message "Invalid URI". The body of the response is blank. Successful transmissions use the same URI as the failures. The entire transmission set is being uploaded into the same bucket. The failures form an odd pattern, from the look of it, every other (pass, fail, pass, fail) transmission is failing, with occasional chains of 3-4 transmissions succeeding. I looked into the idea that we might be transmitting the data too quickly, but AWS has a very specific code for that: 503. Also the error itself seems to point to a connection issue of some kind. Does anyone know what would cause this kind of issue? This is an example of one of the failed requests (I've sripped out some of the information to save space and protect privacy):
563e0f532d1761a701f0f777	X	Seems like you are not the only one having the same problem. This reported yesterday, also getting 400 errors with no body in the response. s3 upload, 400 response, no body
563e0f532d1761a701f0f778	X	I'm trying to upload a file using J2ME code to Amazon S3 through the Amazon REST API(POST object). This is my code: After executing it. I'm getting this below response: But the file is not uploaded to Amazon S3. Please help me to fix it.
563e0f532d1761a701f0f779	X	It looks like your file line is commented out, your are not even sending a file. Having said that I strongly suggest you used the AWS Java SDK: http://aws.amazon.com/sdkforjava/
563e0f532d1761a701f0f77a	X	there isn't any fixed set of files, user may select random files to be download every time...
563e0f532d1761a701f0f77b	X	@kami998 That will be fine. You pass the file set into the job parameters.
563e0f532d1761a701f0f77c	X	I am using Amazon S3 storage for my files and i need to provide download functionality in which user can select multiple files and download them at once (as dropbox)... I tried to implement this functionality by downloading each file in memory stream and create a zip file and returned to user, but its too much time consuming, I need to know that is there any way that this process can be implemented asynchronously that user don't have to be wait longer and downloading starts immediately as dropbox do... I am using MVC Web API... Thanks in advance...
563e0f532d1761a701f0f77d	X	From a high level, I would probably do something like this. Trigger a job with the file set the user wants to download. Have a worker that downloads the files from s3, compresses them into a zip file and then uploads it back to a temporary location in s3. Once the job is completed, send the user a signed URL to the zip file itself. Clean up the zip file after a certain amount of time. Maybe 24 hours?
563e0f532d1761a701f0f77e	X	Good answer and thanks for pointing out that this is not documented. I have created an issue to track this: github.com/boto/boto/issues/729
563e0f532d1761a701f0f77f	X	This is now documented! boto.readthedocs.org/en/latest/ref/… And should probably note that the last byte in the last chunk should be the file size - 1, to head off any off-by-one errors.
563e0f532d1761a701f0f780	X	Amazon S3 REST API documentation says there's a size limit of 5gb for upload in a PUT operation. Files bigger than that have to be uploaded using multipart. Fine. However, what I need in essence is to rename files that might be bigger than that. As far as I know there's no rename or move operation, therefore I have to copy the file to the new location and delete the old one. How exactly that is done with files bigger than 5gb? I have to do a multipart upload from the bucket to itself? In that case, how splitting the file in parts work? From reading boto's source it doesn't seem like it does anything like this automatically for files bigger than 5gb. Is there any built-in support that I missed?
563e0f532d1761a701f0f781	X	As far as I know there's no rename or move operation, therefore I have to copy the file to the new location and delete the old one. That's correct, it's pretty easy to do for objects/files smaller than 5 GB by means of a PUT Object - Copy operation, followed by a DELETE Object operation (both of which are supported in boto of course, see copy_key() and delete_key()): This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3. A PUT copy operation is the same as performing a GET and then a PUT. Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket. However, that's indeed not possible for objects/files greater than 5 GB: Note [...] You create a copy of your object up to 5 GB in size in a single atomic operation using this API. However, for copying an object greater than 5 GB, you must use the multipart upload API. For conceptual information [...], go to Uploading Objects Using Multipart Upload [...] [emphasis mine] Boto meanwhile supports this as well by means of the copy_part_from_key() method; unfortunately the required approach isn't documented outside of the respective pull request #425 (allow for multi-part copy commands) (I haven't tried this myself yet though): You might want to study the respective samples on how to achieve this in Java or .NET eventually, which might provide more insight into the general approach, see Copying Objects Using the Multipart Upload API. Good luck! Please be aware of the following peculiarity regarding copying in general, which is easily overlooked: When copying an object, you can preserve most of the metadata (default) or specify new metadata. However, the ACL is not preserved and is set to private for the user making the request. To override the default ACL setting, use the x-amz-acl header to specify a new ACL when generating a copy request. For more information, see Amazon S3 ACLs. [emphasis mine]
563e0f542d1761a701f0f782	X	The above was very close to working, unfortunately should have ended with mp.complete_upload() instead of the typo "upload_complete()"! I've added a working boto s3 multipart copy script here, based of the AWS Java example and tested with files over 5 GiB: https://gist.github.com/joshuadfranklin/5130355
563e0f542d1761a701f0f783	X	Were you hoping to use an existing package or write your own? Since you're willing to use a subdomain, this is very do-able if you use the proper CORS headers, but I am not aware of any packages which can do this for you
563e0f542d1761a701f0f784	X	I can write my own. So I can host the entry point on S3, a back end on a subdomain and I just need to use proper CORS headers. I need to do some research about CORS then, since I barely know what it is, but I don't know how to implement it. Do you have any recommended reading?
563e0f542d1761a701f0f785	X	It is possible to set the root domain on S3? I.e. mysite.com instead of www.mysite.com? This is for a completely new site.
563e0f542d1761a701f0f786	X	Yes. Simply follow Amazon's documentation. I recommended the www only from the fact that this is fairly standard.
563e0f542d1761a701f0f787	X	Basically I want to make a blog. Since the actual blog content is static, it seems fitting to upload it to S3 - cheap & basically infinitely scalable in case Hacker News or Reddit ever hit it (unlikely, but you never know). However, I want to have some dynamic parts to it, like search. My current thinking is that the blog would be a HTML uploaded to a domain on S3 while the actual server side component that would be called from Javascript and would return the search results would be hosted somewhere else (probably in a subdomain). Basically the first hit is always to S3. If the user wants to get more "interactive" only then does he actually query a server. But as long as the access is read only, no extra interaction (most likely scenario in case of a traffic spike), S3 can handle things gracefully, unlike a puny VPS. According to this question/answer: Serving Django API on Heroku and single-page app on Amazon S3 on the same domain it is not possible. Is this still the case now? Thank you.
563e0f542d1761a701f0f788	X	I would recommend that you configure www.yoursite.com to point to your S3 bucket and run the dynamic content on a subdomain (dynamic.yoursite.com). I don;t know how you plan to do the dynamic portion, but you have several options: Keep them as separate sites which link to each other. This may work well, for example, if you have your pages on S3 and your search index on your VPS. When the user does a search, the search request is POSTed to a page on dynamic.yoursite.com, which returns the results list. This results list can reference CSS, scripts, and images from www.yoursite.com as needed, and each result can link back to the static content. For more complex sharing, both S3 or your VPS can be configured to allow CORS. This will allow AJAX requests to be sent as well. (Note that while browser support is very wide, there are still a few that do not support CORS yet)
563e0f542d1761a701f0f789	X	Are you trying to do this programatically? Is it a one-off transfer or will you have to do repeatedly?
563e0f542d1761a701f0f78a	X	Yes I want to do it programatically and I want to do it repeatedly. Whenever user request for file I have to upload that file to his server.
563e0f542d1761a701f0f78b	X	unfortunately I don't think this is possible. The only way to get a file from S3 is via the API or a Direct Url. The only suggestions I have are to get the user to pull the file from a url or use EC2 as the 'proxy' server. Transfers from S3 to EC2 are very quick and there are no extra bandwidth charges.
563e0f542d1761a701f0f78c	X	Did you find a solution for this? I'd love to hear it, since I have a very similar (if not the same) requirement. This is my question here: stackoverflow.com/q/8403401/291915
563e0f542d1761a701f0f78d	X	+1 to hear whether you found a solution
563e0f552d1761a701f0f78e	X	I have some files that are stored on S3. On users request, I want to transfer them to FTP server of a third party site. Amazon S3 does not support FTP/SFTP. Currently I am downloading the file from S3 to my local server using S3 APIs and then transferring it to third party FTP server. S3 --API--> Local --FTP--> Third party FTP Now, instead I want to transfer the files directly to third party FTP server directly from S3 without downloading it to my local server. S3 ---CloudFront or Other Service---> Third Party FTP How can I do it using cloudfront or any other services? Any Help will be appreciated. Thanks in advance.
563e0f552d1761a701f0f78f	X	S3 only has APIs to get data to it and from it. It also has an API function to copy data between two buckets, but that's about it. If you require to transfer data from S3 to other places and want to save the download from S3 to your local machine I suggest you start a t1.micro instance and put a script on it to download the files to it (you won't pay the bandwidth because between S3 and EC2 instance on the same region you don't pay anything and its significantly faster) and then upload from that instance to the remote 3rd party FTP site.
563e0f552d1761a701f0f790	X	To eliminate the possibility that the client code is somehow making valid credentials invalid, can you try using your key and secret in a gsutil config file, and see if you can perform gsutil operations with those credentials? Also, did you create your HMAC credentials this way? cloud.google.com/storage/docs/migrating#keys
563e0f552d1761a701f0f791	X	Perhaps signature => 'v2' in the constructor? Google doesn't seem to have implemented v4 and you might be defaulting to that. The wording of the error suggests an invalid format to the auth header, rather than a mismatched signature.
563e0f552d1761a701f0f792	X	@MikeSchwartz Yes, I created my credentials through the Interoperability. Yes, they work when I use gsutil. I can run "gsutil ls gs://devtest" and it properly lists the files in the bucket. I added some additional info to my post. Any help would be greatly appreciated.
563e0f552d1761a701f0f793	X	@Michael-sqlbot I tried that but I don't think that config actually gets used anymore. I updated my post with more info about the Auth header.
563e0f552d1761a701f0f794	X	I suspect this is something about the way that PHP SDK is producing the credentials on the wire. That error message happens when the credentials GCS receives are neither AWS nor GOOG1 credentials. I suggest running gsutil -D ls and looking at the Authorization header it sends; then capture the Authorization header the PHP SDK is sending, and compare them. I see gsutil sending credentials like: Authorization: GOOG1 GOOG<...>
563e0f552d1761a701f0f795	X	I'm in the process of migrating from Amazon S3 to Google Storage and I can't seem to get my credentials to work. Here's some sample code that I put together to test my credentials: Here's what I get back: InvalidSecurity Error executing "PutObject" on "https://storage.googleapis.com/devtest/test"; AWS HTTP error: Client error response [url] https://storage.googleapis.com/devtest/test [status code] 403 [reason phrase] Forbidden InvalidSecurity (client): The provided security credentials are not valid. - InvalidSecurityThe provided security credentials are not valid. Incorrect Authorization header I've tried googling 100 different combinations of this issue and can't find anything. I have Interoperability enabled, at least I think I do since I don't think I can get the key/secret without it being enabled first. And I have the Google Storage API enabled. Any help would be greatly appreciated. Edit: here's the Authentication Header in case that helps: AWS4-HMAC-SHA256 Credential=GOOGGUxxxxxxxxxxx/20150611/US/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=9c7de4xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx I noticed it stays "aws4_request" even when I specify 'signature' => 'v2'. Not sure if that matters. I took a look at the S3Client code and it doesn't use the 'signature' config key as far as I can tell. The only thing I found was 'signature_version' which when set to v2, I get this error: Unable to resolve a signature for v2/s3/US. Valid signature versions include v4 and anonymous. I'm using Laravel 5.1 with composer package aws/aws-sdk-php version 3.0.3 Any ideas?
563e0f552d1761a701f0f796	X	S3 only supports v4 signatures, and this requirement is enforced by the PHP SDK. It seems that Google Cloud Storage only supports v2 signing, so you wouldn't be able to use the same library to talk to both. Google does provide their own PHP SDK, which might make talking to Cloud Storage a bit easier.
563e0f552d1761a701f0f797	X	One problem is that you're using request['file_count']. Should be @request['file_count'].
563e0f552d1761a701f0f798	X	That was something I missed as well. Thanks Mischa!
563e0f562d1761a701f0f799	X	I did find that a little while after I posted the question and it solved my initial problem and then more came up. I figured everything out though after a bit more debugging so thanks! I'm so used to Javascript automatically interpreting Integers correctly when building strings that I had a pretty major brain fail.
563e0f562d1761a701f0f79a	X	I am working on a website that requires a document tree for file management and have been running into trouble getting files into my Amazon S3 bucket. My approach is to have the user upload files from the client side into a jQuery plugin I have written. The plugin takes all of the files and packages them up into a Ajax call that gets sent to my internal API that then handles the files. From here, the files make their way to my S3 uploading logic, but here is where I start having problems. Once the object has come into my server, it's in a split apart form and I am not sure which parts are necessary or not. Ajax did it's magic and broke apart my files into a JSON readable array, but now that its in a new form, I'm not sure how to process it. Whenever I go to upload the file(s) to the S3 server it says the following. Given I am just throwing the new formatted array of the file into the S3 logic, but I don't know how else to do this. Every other part of my solution so far works in that I can save strings into my S3 bucket no problem. So my question is, how can I take a file object passed into Rails and then pass it into S3? What format do I need? Below is my code for my API method to upload the files. Here is a screen shot of the data that is coming through to the server for the files. 
563e0f562d1761a701f0f79b	X	The error occurs in this line: To fix it, do this: Or:
563e0f562d1761a701f0f79c	X	Thanks, that will do nicely :)
563e0f562d1761a701f0f79d	X	I am hoping to access the Amazon S3 API to determine how much data has been transferred on a single S3 Object. Is this possible? I have looked through the documentation yet can not find anything.
563e0f562d1761a701f0f79e	X	This is not possible via the API as such, however, you should be able to calculate this yourself by facilitating Server Access Logging: [...] An Amazon S3 bucket can be configured to create access log records for the requests made against it. An access log record contains details about the request such as the request type, the resource with which the request worked, and the time and date that the request was processed. [...] This is can be configured via the AWS Management Console as well, see Managing Bucket Logging for details. Any decent analytics package should be able to provide respective aggregates for the generated Server Access Log Format, ideally including the transferred data already - otherwise you would need to do the math yourself from the number of requests (still inconvenient, but likely automatable one way or another). Good luck!
563e0f562d1761a701f0f79f	X	Amazon S3 API for Ruby only streams objects when passing a block to the read method. I'm developing a driver for em-ftpd, and It needs an IOish object to stream the S3 data to the client. If simply pass myS3Object.read, whole file is loaded into the memory, as stated by S3 API. Is there any way of encapsulating that into something like a custom IO class, so I can pass the stream to em-ftpd? Here is my code: Here is the code inside em-ftpd that gets the result of my code, preferably as an IOish object, using a block to get its data chunks: Thank you.
563e0f562d1761a701f0f7a0	X	Turns out that I needed to create a class with a buffer of the S3 data and the methods read and eof?.
563e0f562d1761a701f0f7a1	X	That's great I'll take a look.
563e0f562d1761a701f0f7a2	X	This is perfect - thanks.
563e0f562d1761a701f0f7a3	X	@John. No problem
563e0f562d1761a701f0f7a4	X	We're moving from Nirvanix to Amazon S3. What I need is to simulate Nirvanix style child accounts for S3 storage. (In a nutshell these provide isolated storage with predefined limit, and a separate authentication for each sub-user, still managed by the same master account). We'll have more than 100 users so the bucket-per user won't work (that's still limited at 100 right?). The storage is used directly from a desktop application (and not, for example, via our servers, though there is a central server if that helps). We want a single S3 billing account that pays for everything, but we want our customers objects safely segmented from each other. Nirvanix provides this out of the box (http://developer.nirvanix.com/sitefiles/1000/API.html#_TocCreatingChildAccounts) - this is essentially what I'm trying to replicate with S3. I understand how to segment objects for each sub-user, e.g. using the "prefix" notation of Objects (E.g. "USER1/object1", "USER2/something_else). What I can't work out: 1) How can I set permissions so that each customer can only access his files? If I give "the app" access to the S3 storage, then that obviously means that every user of the app could access anyones files. It seems like you can set rich ACLs, but what I can't understand is "who" you can set permissions against. Is it only AWS users? Does that mean the only way to do this is to have my customers each have an AWS account? If so, can I create accounts on their behalf? E.g. through an API call? What we certainly cannot allow is having every user create an account through the AWS website (yuck!). 2) Any ideas about the best way to manage quotas for each customer? This concerns me because from what I can tell, we'd have to limit this from the desktop application. This is obviously ripe for abuse because S3 will just keep allowing more data. I guess we could probably live with having a script we run daily which sanity checks the storage limits for "abuse", but just wondered if there was a better way. Thanks all! John
563e0f562d1761a701f0f7a5	X	Amazon has a new beta service called AWS Identity and Access Management (IAM) that will allow you to segment your buckets. In the using with S3 section of the documentation, there are examples describing your use case: Example 1: Allow each User to have a home directory in Amazon S3 In this example, we create a policy that we'll attach to the User named Bob. The policy gives Bob access to the following home directory in Amazon S3: my_corporate_bucket/home/bob. Bob is allowed to access only the specific Amazon S3 actions shown in the policy, and only with the objects in his home directory. Unfortunately I don't think you can currently enforce quotas using IAM. Also, depending on your platform, you probably want to use one of the SDK's available to simplify your interactions with these services. You absolutely don't want to distribute your standard secret key in a desktop application without taking some serious precautions. Any user with your secret key could have full access to all your AWS services.
563e0f562d1761a701f0f7a6	X	Yes, definitely. Here are samples : docs.aws.amazon.com/AmazonS3/latest/dev/UploadInSingleOp.html
563e0f572d1761a701f0f7a7	X	@TJ- I want users of my product to upload to my bucket, like this - docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html
563e0f572d1761a701f0f7a8	X	http://aws.amazon.com/articles/1434 "If you have an AWS account, you can interact with the S3 service using specialized tools to upload and manage your files. It is very convenient to have access to this online storage resource for yourself, but there may be situations where you would like to allow others to upload files into your account. For this purpose, S3 accepts uploads via specially-crafted and pre-authorized HTML POST forms. You can include these forms in any web page to allow your web site visitors to send you files using nothing more than a standard web browser." This excerpt shows that we can use web pages to allow users of a product to upload directly to s3. However, is there a way we can do this without using a browser based approach. As in, can i develop a desktop application to let my users upload to s3 directly?
563e0f572d1761a701f0f7a9	X	Uploading via a browser is just one way of interacting with Amazon S3. It is also possible to call Amazon S3 APIs directly from your application by taking advantage of a Software Development Kit (SDK) for your preferred language. See: AWS Tools and SDKs You also interact via the AWS Command-Line Interface (CLI), available for Windows, Linux and Mac.
563e0f572d1761a701f0f7aa	X	I'm using these codes for getting metadata from key of buckets. But I'm looking for bucket not the keys
563e0f572d1761a701f0f7ab	X	I see. I'm not too sure about the bucket itself. I have updated the answer with something else that you may be able to try.
563e0f572d1761a701f0f7ac	X	I'd like to get the list of metadata/headers of an amazon s3 bucket? I can get the metadata in OpenStack Swift Storage like following -> the curl command that request the metadata: response for above curl command: In openstack swift I can get the above metadata of a swift container(bucket) by using the python-swiftclient API. I want to do same thing for AmazonS3 bucket. so, how can I get the metadata of an AmazonS3 bucket with Python ?(I am using boto library) Is it possible or am I "beating the air"? UPDATE: I'm trying to get metadata of a bucket not the keys of bucket.
563e0f572d1761a701f0f7ad	X	You can try this boto snippet code: You can read more information for boto on : https://boto.readthedocs.org/en/latest/s3_tut.html Hope that helps. UPDATE: Something else that you can try: Hope you are seeing some metadata that you wanted in the response headers.
563e0f572d1761a701f0f7ae	X	I was using the services from Parse a while back, and they had implemented an amazing feature for uploading data, with a method something like this: Which let me keep track of the file-upload. Since Parse only let me upload max 10mb files, I chose to move to the cloud-area to explore a bit. I've been testing with Amazon's S3-service now, but the only way I can find how to upload data is by calling [s3 putObject:request];. This will occupy the main thread until it's done, unless I run it on another thread. Either way, I have no idea of letting my users know how far the upload has come. Is there seriously no way of doing this? I read that some browser-API-version of S3's service had to use Flash, or set all uploads to go through another server, and keep track on that server, but I won't do either of those. Anyone? Thanks. My users are supposed to be uploading video with sizes up to 15mb, do I have to let them stare at a spinning wheel for an unknown amount of time? With a bad connection, they might have to wait for 15 minutes, but they would stare at the screen in hope the entire time.
563e0f572d1761a701f0f7af	X	Seems like I didn't quite do my homework before posting this question in the first place. I found this great tutorial doing exactly what I was asking for. I would delete my question, but I'll let it stay just in case it might help other helpless people like myself. Basically, it had a delegate-method for this. Do something like this: Then use this appropriately named delegate-method: It will be called for every packet it uploads or something. Just be sure to set the delegates. (Not sure if you need both delegates to be set though)
563e0f582d1761a701f0f7b0	X	Test with two small ~1kB files.
563e0f582d1761a701f0f7b1	X	@istruble: well, multipart only works for 5BM+ chunks. But still, I can only test on a few files and hope that the amazon part is free of any bug and that my tests were exhaustive.
563e0f582d1761a701f0f7b2	X	Thank you. I learned something new today.
563e0f582d1761a701f0f7b3	X	I doubt this will work remotely with amazon S3...
563e0f582d1761a701f0f7b4	X	You're just downloading data to calculate a hash, but not storing the data. As far as any given program is concerned, S3 is just another data source, accessed via web requests.
563e0f582d1761a701f0f7b5	X	I see, it's time to try EC2 :)
563e0f582d1761a701f0f7b6	X	I need to move large files (>5GB) on amazon S3 with boto, from and to the same bucket. For this I need to use the multipart API, which does not use md5 sums for etags. While I think (well only 98% sure) that my code is correct, I would like to verify that the new copy is not corrupted before deleting the original. However I could not find any method except downloading both objects and comparing them locally, which for 5GB+ files is quite a long process. For the record, below is my code to copy a large file with boto, maybe this can help someone. If there is no good solution to my problem maybe someone will find a bug and prevent me from corrupting data. This code only works for original key sizes >= 5368709121 bytes.
563e0f582d1761a701f0f7b7	X	You should be able to compute a SHA-1 hash on a data stream (see this SO thread for C++ code, which could give hints for a python approach). By redirecting your hashed data stream to the equivalent of /dev/null, you should be able to compare SHA-1 hashes of two files without first downloading them locally.
563e0f582d1761a701f0f7b8	X	There is no way to do what you want without knowing how AWS calculates the etag on multipart uploads. If you have a local copy of the object, you can calculate the md5 of each part that you are copying on the local object and compare it to the etag in the key that each mp.copy_part_from_key() returns. Sounds like you have no local object though. You also have a small non-obvious problem hiding in boto that may or may not cause you to lose data in a very rare case. If you look at the boto source code, you'll notice that mp.complete_upload() function actually doesn't use any of the etags for any of the parts returned by AWS when uploading. When you use multipart_complete, it actually does a totally new multipart list itself and gets a new list of parts and etags from S3. This is risky because of eventual consistency and the list may or may not be complete. The multipart_complete() should ideally use the etags and part info that was returned by each remote copy to be completely safe. This is what Amazon recommends in its documentation (See the Note under Multipart Upload Listings). That said, it's less likely a problem if you confirm the file size of both objects to be the same. The worst case I believe is that a part is not listed in the multipart upload listing. A listed part should never be incorrect itself.
563e0f582d1761a701f0f7b9	X	I'm also interested in this question. I found uploads.im website which seems free. Interesting of someone has experience with it?
563e0f592d1761a701f0f7ba	X	hmm amazon seems way too hard to use .. and imgur api isnt free for commercial use
563e0f592d1761a701f0f7bb	X	Does Flickr not work?
563e0f592d1761a701f0f7bc	X	I am building an Image Sharing feature for my website, where users can upload images and show it off on my website, through their own profile gallery. Except I dont want to physically host the uploaded images. Are there API's that can allow me to descretly save the uploaded images (from the users) to another Host, such as Flickr, Google etc.. I know Flickr has a good API, but could I use it for this? And does the Flickr Upload API only work with authorised users ? becuase I dont want them to upload to their Flickr account, but just host them with 1 big account so to speak thanks.
563e0f592d1761a701f0f7bd	X	You could use Amazon S3, also Imgur has an API
563e0f592d1761a701f0f7be	X	I hoped for a more detailed answer with a short snippet. I don't want to risk my data by making some newbie mistake, since I never programmed for S3 Api. But this at least point to the minimum information so I'll award you the bounty. But if you have some more help/suggestions please let me know.
563e0f592d1761a701f0f7bf	X	I find it strange that after looking everywhere I don't find any tool to delete all the versions of a file older than X days (not the actual file) of a S3 bucket that has versioning enabled. I would believe this is a very common issue because without it the buckets with time would become huge. Is there any existing solution (even commercial)? If there is no ready made way, could you point me to some info or give me suggestions on how to code this myself in C#? I guess I have to use recursion for this kind of problem. Thanks
563e0f592d1761a701f0f7c0	X	If you use the amazon s3 API, you can do that. I use AmazonS3Client + DeleteObjectRequest method on amazon SDK: http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/DeleteObjectRequest.html Should really be straightforward. Hope it helps
563e0f592d1761a701f0f7c1	X	I am using Amazon S3 in my android app for uploading files to the cloud storage. Every user can upload and download files from the cloud storage i.e. S3 I want to keep track of every user's upload and download... for eg User abc uploaded 26MB and downloaded 94MB One solution to this is to implement this on my mobile app, store/track size while uploading and downloading. Does AWS lets us know via any analytics or is there any other 3rd party api which gives transfer details.
563e0f592d1761a701f0f7c2	X	AWS Console does not provide any useful method to track user's upload and download, One suggestion is that you should post these information using locally if you have a local database, but if you have a remote database to save your records you may send these informations to your server using web service.
563e0f592d1761a701f0f7c3	X	OP didn't get around to mentioning it, but the aurguable/theoretical benefit in uploading through CloudFront is to get the traffic into Amazon's high-bandwidth, low-latency network as close to the end user as possible, potentially improving connection quality, stability, performance. With the CloudFront edge operating as a TCP proxy, retries related to the end user's Internet connection quality would be handled more quickly.
563e0f592d1761a701f0f7c4	X	Thanks for your comment
563e0f592d1761a701f0f7c5	X	I'd like to upload image to S3 via CloudFront. If you see the document about CloudFront, you can find that cloud front offers put method for uploading to cloudFront There could be someone to ask me why i use the cloud front for uploading to S3 If you search out about that, you can find the solution What i wanna ask is whether there is method in SDK for uploading to cloud front or not As you know , there is method "putObejct" for uploading directly to S3 but i can't find for uploading cloud front ... please help me..
563e0f5a2d1761a701f0f7c6	X	Data can be sent through Amazon CloudFront to the back-end "origin". This is used for using a POST on web forms, to send information back to web servers. It can also be used to POST data to Amazon S3. If you would rather use an SDK to upload data to Amazon S3, there is no benefit in sending it "via CloudFront". Instead, use the Amazon S3 APIs to upload the data directly to S3. So, bottom line:
563e0f5a2d1761a701f0f7c7	X	Is there any way I can ask the CloudFront API for the name of the bucket it uses on Amazon S3?
563e0f5a2d1761a701f0f7c8	X	This is possible via the GET Distribution action: To get the information about a distribution, you do a GET on the 2012-03-15/distribution/ resource. Have a look at the sample syntax in the Responses section, which specifically includes fragments for either S3Origin or CustomOrigin, e.g. abbreviated: Please note that The S3Origin element is returned only if you use an Amazon S3 origin for your distribution, whereas The CustomOrigin element is returned only if you use a custom origin for your distribution. Furthermore, for more information about the CustomOrigin element and the S3Origin element, see DistributionConfig Complex Type.
563e0f5a2d1761a701f0f7c9	X	You are correct in your belief. The AWS SDK for .NET does not currently support creating presigned URLs for multipart uploads.
563e0f5a2d1761a701f0f7ca	X	Thank you. Can you post the same as answer so that I can accept it. In case anybody's reading this thread and interested, I wrote a blog post about consuming Amazon S3 REST API for multi part upload using C#: gauravmantri.com/2014/01/06/….
563e0f5a2d1761a701f0f7cb	X	Hi, and how can I create presigned URLs for multipart uploads use java sdk? thank you.
563e0f5a2d1761a701f0f7cc	X	Our requirement is to upload objects in Amazon S3 using a browser based interface. For this we're utilizing Query String Authentication mechanism (we don't have end-user's credentials during the upload process and we're using ASP.Net to write code to do so. I'm running into issues when trying to do multipart uploads. I'm using AWS .Net SDK (version 2.0.2.5) to create a query string using the following code below: This works great if I don't do multi-part upload. However I'm not able to figure out how to do a multipart upload using Query String. Problems that I'm running into are: I am inclined to believe that .Net SDK does not support this scenario and I have to resort to native REST API to create query string. Is my understanding correct? Any insights into this would be highly appreciated. Happy New Year in advance.
563e0f5a2d1761a701f0f7cd	X	You are correct in your belief. The AWS SDK for .NET does not currently support creating presigned URLs for multipart uploads
563e0f5a2d1761a701f0f7ce	X	What happens if you try accessing it from the AWS console at console.aws.amazon.com/s3 ?
563e0f5a2d1761a701f0f7cf	X	The AWS console interface says "an error occurred" if I try to view the folder, or try to delete the folder.
563e0f5a2d1761a701f0f7d0	X	Very interesting...
563e0f5a2d1761a701f0f7d1	X	I just began to use S3 recently. I accidentally made a key that contains a bad character, and now I can't list the contents of that folder, nor delete that bad key. (I've since added checks to make sure I don't do this again). I was using an old "S3" python module from 2008 originally. Now I've switched to boto-2.0, and I still cannot delete it. I did quite a bit of research online, and it seems the problem is I have an invalid XML character, so it seems a problem at the lowest level, and no API has helped so far. I finally contacted Amazon, and they said to use "s3-curl.pl" from http://aws.amazon.com/code/128. I downloaded it, and here's my key: I think I was doing a quick bash for loop over some files at the time, and I have "lscolors" set up, and so this happened. I tried ./s3curl.pl --id <myID> --key <myKEY> -- -X DELETE https://mybucket.s3.amazonaws.com/info/&#x1b;[01 (and also tried putting the URL in single/double quotes, and also tried to escape the '['). Without quotes on the URL, it hangs. With quotes, I get "curl: (3) [globbing] error: bad range specification after pos 50". I edited the s3-curl.pl to do curl --globoff and still get this error. I would appreciate any help.
563e0f5a2d1761a701f0f7d2	X	You can use the s3cmd tool from here. You first need to run You can then delete the file using
563e0f5b2d1761a701f0f7d3	X	Using Amazon REST API, is there way to download multiple files perhaps in one Bucket with a single operation? For example to zip all the objects and download them as one file? Note that I am looking for operation efficiency and than custom solutions. For example of I was to download each individual file from S3 and Create the ZIP file myself the operation could take up to 40 seconds for 100 files of 1.5MB size , I am looking for a solution that allow me to do this in less 10 seconds.
563e0f5b2d1761a701f0f7d4	X	There is no REST call that can do this in S3. As you no doubt realize, the transfer bandwidth doesn't likely account for much of the wall-clock time your process is taking. The most likely solution would be to use either multiple threads or asynchronous I/O (depending on your language and environment) to send the requests to S3 in parallel groups, combining the results when all of the desired objects have been successfully fetched. Of course, if what you are doing, now, does not reuse the user agent's connection so that you can take advantage of HTTP keep-alives, you should see some level of performance improvement if you can enable that functionality, by avoiding unnecessarily repeated connection setups (and potentially, SSL negotiations).
563e0f5b2d1761a701f0f7d5	X	Amazon S3 is a storage service AFAIK so don't look into that. I would look into web services.
563e0f5b2d1761a701f0f7d6	X	I need to be able to pass a book title into some amazon api function and pull out the proper image, isbn ect to then store in my own DB. Any particular web service you'd recommend for that?
563e0f5b2d1761a701f0f7d7	X	Yes, I think anything else is overkill. Thanks!
563e0f5b2d1761a701f0f7d8	X	I don't think I need a relational database. I just need to be able to pass a book title into some amazon api function and pull out the proper image, isbn ect to then store in my own DB. Any particular web service you recommend for that?
563e0f5b2d1761a701f0f7d9	X	those services are for your own sake. they are totally independent from Amazon stores. I really don't have any clue on how to access Amazon's store via API.
563e0f5b2d1761a701f0f7da	X	Thanks for your help!
563e0f5b2d1761a701f0f7db	X	I am working on a site (ASP.NET MVC) that will ultimately display millions of book. I already have the titles and authors of these books in my MySQL database. The goal is when a user searches for book, the top 20 matches (title and author) will appear on the page. I then plan to use the Amazon API to get more information (isbn, image, description etc) for these 20 books and flesh out these items via Ajax. I would then also add this info to MySQL so next time these specific books are requested, I already have the data. My question is what Amazon Web Service should I use? There are so many like Amazon S3, Amazon SimpleDB etc. I just don't know which would be best for my needs. Cost is also a factor. Any guidance would be greatly appreciated.
563e0f5b2d1761a701f0f7dc	X	The API you're looking for is Amazon's Product Advertising API: https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html
563e0f5b2d1761a701f0f7dd	X	in short, Amazon S3 is a technology oriented on large data storage whilst SimpleDB is a non relational database (as mongoDB and raven could be). We use the first for storing the static files (javascript, css and pictures). The first is cheaper but you can only retrieve a "file" at once. The second gives you some degree of support to queries. If you need a relational database, you could use Amazon RDS which is a MySql database ready for replicas.
563e0f5c2d1761a701f0f7de	X	"Eucalyptus Walrus" would be a good name for a band. (Sorry I can't help with the question, though!)
563e0f5c2d1761a701f0f7df	X	It does not look like this would be the problem. I was able to go one step farther by removing the http:// at the beginning of the ServiceURL. But now the problem is that I cant connect ... .LoginException: Login Failure ! My guess would be that AWS SDK is not using the :8773/services/Walrus into the signing part ! So I am still searching.
563e0f5c2d1761a701f0f7e0	X	I have downloaded the Amazon AWS SDK for C#, I have no problem accessing the EC2 part of our private cloud running Eucalyptus, I can list, Images, Instances, Zones ... This is working fine : But I need to access the Walrus (S3) part of our Cloud. This is how I try to access the Walrus, the code is almost identical, but with this call I will get an exception. This is not working: I will receive this exception : From Eucalyptus site : Eucalyptus implements an IaaS (Infrastructure as a Service) private cloud that is accessible via an API compatible with Amazon EC2 and Amazon S3 What am I missing ? Note: The same code work flawlessly with Amazon S3, the problem is to access Eucalyptus Walrus.
563e0f5c2d1761a701f0f7e1	X	You can access Walrus using the current Amazon AWS SDK for C#. It just doesn't work quite the way you would expect if your Walrus URL contains a path component like... http://eucalyptus.test.local:8773/services/Walrus Here's how to setup the AmazonS3Client and a request. Notice that the path portion of the service url gets put into the bucketname. I've tested this setup with presigned urls and with the DeleteObjectRequest. I'm using version 1.5.19.0 of the SDK from https://github.com/aws/aws-sdk-net
563e0f5c2d1761a701f0f7e2	X	AWSSDK for .NET doesn't seem to understand port numbers in ServiceUrls... I just wrote a patch for this in the SimpleDB client code the other day, but I haven't actually looked at it in the S3 client... You could try temporarily hosting you Walrus service on port 80 and retesting to verify if this is the problem.
563e0f5c2d1761a701f0f7e3	X	beware CORS not supported in all browsers
563e0f5c2d1761a701f0f7e4	X	thx, i'm ok with enable-cors.org/client.html
563e0f5c2d1761a701f0f7e5	X	I have an angularjs app that I want to host on amazon s3, is there a way to get it to play nice with a sinatra api hosted on heroku? I need to use ng-resource to make put requests to the heroku app, e.g. at a subdomain rest.site.com where site.com is s3.. anticipate same origin policy issues.. Edit: The following looks like a start; http://beckyl11.blogspot.co.uk/2012/07/allowing-cross-origin-requests-cors-in.html
563e0f5c2d1761a701f0f7e6	X	You talking about downloading from your logging bucket?
563e0f5c2d1761a701f0f7e7	X	See the AWS docs on Server Access Logging?
563e0f5c2d1761a701f0f7e8	X	I guess this is not exaclty what i am looking for, what it really give an global idea of working with ASW. Thanks gray, your comment was the most constructive of all :)
563e0f5d2d1761a701f0f7e9	X	I am looking for a way to track downloads of files that are stored in the amazon S3 service. I downloaded the API, and then I get pretty lost. I just want to know what objects and methods I have to use from the API in order to get the information of the files stored there. I want to track downloads including the addresses of those that make the requests. Thanks for advance.
563e0f5d2d1761a701f0f7ea	X	If you are talking about downloading the files in a logging bucket, we use something like the following: Here's the sample code: If this isn't what you want, please edit your post so we can help you better.
563e0f5d2d1761a701f0f7eb	X	I have tried using the SDK as it has a signature creater, and it is where I originally started with this process ... I have even asked Amazon to help directly and they say "here, read our web page" .. so much for a paid support plan.
563e0f5d2d1761a701f0f7ec	X	I'm confused, here. Are you trying to make the S3 requests through Flex, or through PHP?
563e0f5d2d1761a701f0f7ed	X	My original attempt and current succesful file upload is through Flex, but I then realized that if two people upload the same file name, it will overwrite the current file on S3. I decided to try and use a REST interface with PHP to "rename" the file but extensive attempts have failed. I was hoping to head back to the original working Flex application I had written to rename the file during upload so I could avoid the REST (PHP) solution entirely. I have not found a Flex solution, so am still persuing all avenues. REST still is failing for me. I can provide a URL to the PHP REST I am using
563e0f5d2d1761a701f0f7ee	X	a Copy worked with the SDK... and to tell you the truth ... I was using the SDK that was available when i started this project (1.2.6) .. they must had made some changes since then, when i grabbed 1.3.1 and tried the script .. viola. You sir earn points. THANK YOU
563e0f5d2d1761a701f0f7ef	X	tl;dr, but +1 to @Charles for all the hard work!
563e0f5d2d1761a701f0f7f0	X	I have been trying for weeks to properly format a REST request to the Amazon AWS S3 API using the available examples on the web but have been unable to even successfully connect. I have found the code to generate a signature, found the proper method for formatting the "string to encode", and the http headers. I have worked my way through the signatureDoesNotMatch errors just to get a Anonymous users can not perform copy functions, Please authenticate message. I have a working copy of an Adobe Flex application that successfully uploads files, but with their "original" file name. The point of using the REST with the Amazon API is to perform a PUT (copy) of the file, just so I can rename it to something my back end system can use. If I could find a way to get this REST submission to work, or perhaps a way to specify a "new" filename within Flex while uploading I could avoid this whole REST situation all together. If anyone has successfully performed a PUT/Copy command on the Amazon API via REST I would be very interested in how this was accomplished - OR - if anyone has been able to change the destination file name using the Flex fileReference.browse() method I would also be eternally grateful for any pointers. When I submit a malformed or incorrect header I get the corresponding error message as expected: Query: PUT /bucket/1-132-1301047200-1.jpg HTTP/1.1 Host: s3.amazonaws.com x-amz-acl: public-read Connection: keep-alive Content-Length: 34102 Date: Sat, 26 Mar 2011 00:43:36 +0000 Authorization: AWS -removed for security-:GmgRObHEFuirWPwaqRgdKiQK/EQ= HTTP/1.1 403 Forbidden x-amz-request-id: A7CB0311812CD721 x-amz-id-2: ZUY0mH4Q20Izgt/9BNhpJl9OoOCp59DKxlH2JJ6K+sksyxI8lFtmJrJOk1imxM/A Content-Type: application/xml Transfer-Encoding: chunked Date: Sat, 26 Mar 2011 00:43:36 GMT Connection: close Server: AmazonS3 397 SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method.50 55 54 0a 0a 0a 53 61 74 2c 20 32 36 20 4d 61 72 20 32 30 31 31 20 30 30 3a 34 33 3a 33 36 20 2b 30 30 30 30 0a 78 2d 61 6d 7a 2d 61 63 6c 3a 70 75 62 6c 69 63 2d 72 65 61 64 0a 2f 6d 6c 68 2d 70 72 6f 64 75 63 74 69 6f 6e 2f 31 2d 31 33 32 2d 31 33 30 31 30 34 37 32 30 30 2d 31 2e 6a 70 67A7CB0311812CD721ZUY0mH4Q20Izgt/9BNhpJl9OoOCp59DKxlH2JJ6K+sksyxI8lFtmJrJOk1imxM/AGmgRObHEFuirWPwaqRgdKiQK/EQ=PUT Sat, 26 Mar 2011 00:43:36 +0000 x-amz-acl:public-read /bucket/1-132-1301047200-1.jpg-removed for security- 0 but when sending properly formatted requests, it says I'm not authenticated: Query being used: PUT /1-132-1301047200-1.jpg HTTP/1.1 Host: bucket.s3.amazonaws.com Date: Sat, 26 Mar 2011 00:41:50 +0000 x-amz-copy-source: /bucket/clock.jpg x-amz-acl: public-read Authorization: AWS -removed for security-:BMiGhgbFnVAJyiderKjn1cT7cj4= HTTP/1.1 403 Forbidden x-amz-request-id: ABE45FD4DFD19927 x-amz-id-2: CnkMmoF550H1zBlrwwKfN8zoOSt7r/zud8mRuLqzzBrdGguotcvrpZ3aU4HR4RoO Content-Type: application/xml Transfer-Encoding: chunked Date: Sat, 26 Mar 2011 00:41:50 GMT Server: AmazonS3 AccessDenied Anonymous users cannot copy objects. Please authenticate ABE45FD4DFD19927CnkMmoF550H1zBlrwwKfN8zoOSt7r/zud8mRuLqzzBrdGguotcvrpZ3aU4HR4RoO 0 Date: Sat, 26 Mar 2011 00:41:50 GMT Connection: close Server: AmazonS3
563e0f5d2d1761a701f0f7f1	X	I have been trying for weeks to properly format a REST request to the Amazon AWS S3 API using the available examples on the web Have you tried the Amazon AWS SDK for PHP? It's comprehensive, complete, and most importantly, written by Amazon. If their own code isn't working for you, something's gonna be really wrong. Here is example code using the linked SDK to upload example.txt in the current directory to a bucket named 'my_very_first_bucket'. Appropriate API docs: You can navigate the rest of the methods in the left menu. It's pretty comprehensive, including new bucket creation, management, deletion, same for objects, etc. You should be able to basically drop this in to your code and have it work properly. PHP 5.2-safe. Edit by Silver Tiger: Charles -      The method you provide is using the API SDK functions to upload a file from the local file system to a bucket of my choosing. I have that part working already via Flex and uploads work like a charm. The problem in question is being able to submit a REST request to AWS S3 to change the file name from it's current "uploaded" name, to a new name more suited name that will work with my back end (database, tracking etc, which I handle and display seperately in PHP with MyySQL).      AWS S3 does not truly support a "copy" function, so they provided a method to re-"PUT" a file by reading the source from your own bucket and placing a new copy with a different name in the same bucket. The difficulty I have been having is processing the REST request, hence the HMAC encryption.      I do appreciate your time and understand the example you have provided as i also have a working copy of the PHP upload that was functioning before I designed the Flex application. The reason for the Flex was to enable status updates and a dynamically updated progress bar, which is also working like a charm :). I will continue to persue a REST solution as from the perspective of Amason zupport, it will be the only way i can rename a file already existing in my bucket per thier support team. As always, if you have input or suggestions regarding the REST submission I would be greatful for any feedback. Thanks, Silver Tiger Proof copy/delete works: Edit by Silver Tiger: Charles -      No REST needed, no bothers ... SDK 1.3.1 and your help solved the issue. the code I used to test looks a lot like yours : Now I will implement the delete after the copy, and we're golden. Thank you sir for your insight and help. Silver Tiger
563e0f5d2d1761a701f0f7f2	X	I´ve got JWPlayer installed and I´ve got an Amazon S3 bucket with some videos. Those videos are meant to be private, allowing only my site´s visitors to watch them. Now, I´ve got this code, that´s working: What I don´t know how to manage is the Amazon´s video itself. I mean, I have a public and a private key, where should I put them? Because if I just post the video´s links (https://...) it won´t play, because it salys "forbidden" (naturally). Any ideas on what should I do? I´ve look into jwplayer´s API and didn´t found anything, not at the setup wizard... Any help will be very much appreciated! Rosamunda
563e0f5d2d1761a701f0f7f3	X	I think you should provide a proxy for auth, setting a server side cookie for user's session in order to let him watch your video but avoid embedding it or linking it on different domains. There's no other way to do it without letting videos to be public. Sorry if this comes a little late (I just saw your question).
563e0f5e2d1761a701f0f7f4	X	I think this is the reason!
563e0f5e2d1761a701f0f7f5	X	sorry folks, it doesn't work. same problem, same error message too. I know that the file is being read as I get a file not found error if I change it's name. I also know that the ENV variables are being read as I get a exception if I change the name of the key in S3.rb. Anyway, it's not the end of hte world and I think I will just move on from here and leave it alone. It just means that I will have to be very careful when putting stuff on github but I can live with that. Thanks for your help. If I get it working I'll let you all know.
563e0f5e2d1761a701f0f7f6	X	also - puts ENV['S3_KEY'] from rails console works fine - so I reckon it might just be a paperclip issue - thanks anyway
563e0f5e2d1761a701f0f7f7	X	I appologise in advance for the length of this post.... I'm developing a rais app that uses paperclip to store stuff on Amazon S3. The app is hosted on Heroku. I'm developing on Ubuntu Karmic. The problem that I am about to describe occurs in development (on my localhost) and production (on Heroku). The standard way of passing S3 creds to paperclip is by putting them in config/s3.yml like so: When I do this, everything works just fine. But this makes it difficult to share my code with others so Heroku suggest an alternative method - http://docs.heroku.com/config-vars. They advise that you should put your S3_KEY and S3_SECRET into your .bashrc like so: They then suggest that you create config/initializers/s3.yml (note the slightly different path) and put the following into that file: BUT, When I do this, paperclip throws a wobbler and spits out the following error message: So clearly it's all kicking off inside the storage.rb module. Stepping through the stack trace: The parse_credentials method on Line 176 is flagged - here's the call as it appears in the code: The parse_credentials method attempts to call another method, find_credentials, and this is where I believe the problem lies. Heres the code for find_credentials: I can't see how the find_credentials method is equipped to read values from my .bashrc file. It's got two cases where it can read from YAML and one where it's looking for a hash. My model references the credentials like so: If I remove the :s3_credentials hash from the model, the stringify_keys error goes away and the rails console throws the error message that appears at the end of the find_credentials method: i.e. "Credentials are not a path, file, or hash". So I'm stumped. I realise that this is possibly a question for the guys at Heroku (who I am actually going to email this link to in the hope that they can answer it) and it's also possibly a question for the doods at thoughtbot, but I thought that StackOverflow might be the best place to ask this as it's proven to be quite a reliable forum for me in the past. As I said at the beginning, my app works fine when I take the standard approach of sticking my key and secret into config/s3.yml, but I would prefer to use the method that Heroku suggest because it makes things WAY easier for me and it means I can store my repo on my public github page for others to use without having to write any customer merge drivers in Git to keep my api keys out of the public domain. Once again, sorry for the lengthy post, and if you've made it this far, I applaud you. Anyone got any ideas? I've tried sticking the ENV variables in etc/bash.bashrc as well as ~/.bashrc and after rebooting, I still have the same problem. The problems occur on development machine as well as on Heroku. I've made sure to push my config-vars to Heroku as well. I've been at it for 8 hours straight!! I'm gonna go watch the football now.
563e0f5e2d1761a701f0f7f8	X	After much searching I found the answer here - http://tammersaleh.com/posts/managing-heroku-environment-variables-for-local-development The trick is to remove the S3.rb file altogether and just refer to the ENV variables in the model like so: Anyway, David, thanks for your suggestion. I don't know if you want to update the Heroku docs to say that some users have had to do it this way. Thanks again though.
563e0f5e2d1761a701f0f7f9	X	Rename the file config/initializers/s3.yml to config/initializers/s3.rb and give it a try.
563e0f5e2d1761a701f0f7fa	X	Here's your problem: needs to be i.e. the assignments are not being interpreted.
563e0f5e2d1761a701f0f7fb	X	I want to upload multiple files in to amazon using S3 API. This is my code uploadcontentintoamazone.php API integration is working perfectly. But temp file is uploaded instead of my uploaded file. Thanks in advance
563e0f5e2d1761a701f0f7fc	X	try this way:
563e0f5e2d1761a701f0f7fd	X	You are iterating over the wrong array, you need to do it like this: (you also didn't set the destination filename properly)
563e0f5f2d1761a701f0f7fe	X	If the bucket has an ACL that permits anonymous access then you can retrieve a file by the following pattern: //s3.amazonaws.com/<bucket name>/<full key name with slashes>. Is that what you as looking for?
563e0f5f2d1761a701f0f7ff	X	@JasonSperske It is in a private bucket.
563e0f5f2d1761a701f0f800	X	Have you tried this? I did something similar and got a 403 signature doesn't match. Same was described in that forum post. "Unfortunately it's not quite correct to say you can do it both ways. It's fine if you're using the URL generated by the Java SDK in a generic fashion. Unfortunately if you hand that URL off to a .NET app and it uses it's standard WebRequest class to consume the URL, .NET decodes the %2F to a / and the request then fails with a 403 - Signature does not match."
563e0f5f2d1761a701f0f801	X	As I understood your post it works if you manually use the slahes, but you don't want to write the code to do this, no? Using url.getQuery should do exactly this.
563e0f5f2d1761a701f0f802	X	You can't mix and match. When generatePresignedUrl is called it takes the entire key and escapes it, and then creates a signature using the escaped key. You cannot use the signature from the escaped key and combine it with the unescaped key in the URL. Hypothetically, if you could perform the signature code without escaping, you could use the key without escaping. I was just wondering if someone has an easy way of doing this.
563e0f5f2d1761a701f0f803	X	Hey, sorry for this. I responded too fast. Doing some more research and looking at the code, I came to the same conclusion. See my recent edit. Long story short I think the easiest way would be to override the erronous method or alternatively implement your own version by checking out how Amazon does it internally.
563e0f5f2d1761a701f0f804	X	Yes, I came to the same conclusion and walked through the code and its at a frustratingly inconvenient place. That's why I came here :) Figured someone probably did it already.
563e0f5f2d1761a701f0f805	X	I am using the Java Amazon SDK to work with S3 for storing uploaded files. I would like to retain the original filename, and I am putting it at the end of the key, but I am also using virtual directory structure - something like <dirname>/<uuid>/<originalFilename>. The problem is that when I want to generate a presigned URL for downloading using the api like: The sdk url escapes the entire key, including the slashes. While it still works, it means that the name of the file downloaded includes the entire key instead of just the original filename bit at the end. I know that it should be possible to do this without escaping the slashes, but I'm trying to avoid rewriting a lot of the code already in the SDK. Is there a common solution to this? I know I've used web apps that follow the same pattern and do not have the slash escape problem.
563e0f5f2d1761a701f0f806	X	This is a bug in the current Java SDK: If you look at https://github.com/aws/aws-sdk-java/blob/master/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L2820 The method presignRequest which is called internally has the following code: The key is URL encoded here before signing which I think is the error. You might be able to inherit from the AmazonS3Client and override the funcion to fix this. In some places it is suggested to use url.getQuery() and prefix this with your original awsURL (https://forums.aws.amazon.com/thread.jspa?messageID=356271). However as you said yourself this will produce an error, because the resource key will not match the signature. The following problem might also be related, I did not check out the proposed workarround: How to generate pre-signed Amazon S3 url for a vanity domain, using amazon sdk? Amazon recognized and fixed a similar bug before: https://forums.aws.amazon.com/thread.jspa?messageID=418537 So I hope it will be fixed in the next version.
563e0f5f2d1761a701f0f807	X	I'm still hoping for a better solution than this, but seeing as @aKzenT has confirmed my conclusion that there is not an existing solution for this I wrote one. Its just a simple subclass of AmazonS3Client. I worry it's brittle because I had to copy a lot of code from the method I overrode, but it seems like the most minimal solution. I can confirm that it works fine in my own code base. I posted the code in a gist, but for the sake of a complete answer: UPDATE I updated the gist and this code to fix an issue I was having with ~'s. It was occurring even using the standard client, but unescaping the ~ fixed it. See gist for more details/track any further changes I might make.
563e0f5f2d1761a701f0f808	X	version 1.4.3 of the Java SDK seems to have fixed this problem. Perhaps, it was fixed earlier, but I can confirm that it is working correctly in 1.4.3.
563e0f5f2d1761a701f0f809	X	We have a large number of people (10k+) who return to my clients' sites on a regular basis to use a web app we built, improve, and host for them. We have been making fairly frequent backward-incompatible updates to the web app's javascript as our app has improved and evolved. During deployments, the javascript is minified and concatenated into one file, loaded in the browser by require.js, and is uploaded to and hosted on Amazon S3. The file name & url currently doesn't change at all during updates. This last week we deployed a major refactor to the web app and got a few (but not a lot) of reports back that the app stopped working for some people, particularly in firefox. It seemed like a caching issue. We were able to see it initially in a few browsers in testing but it seemed to go away after a refresh or two. It dawned on me that I really don't know what browser-caching ramifications deploying a new version of a javascript file (with the same name) on S3 will have and whether this situation warrants cache-busting or manipulating S3's headers or anything. Can someone help me get a handle on this? Are there actions I should be taking during deployments to ensure that browsers will immediately get the new version of a javascript file? If not, we run the risk of the javascript and the server API being out of sync and failing, which I think happened here. Not sure if it matters, but the site's server runs Django and the app and DB are deployed to Heroku. Static files are deployed to S3 using S3Boto via Django's collectstatic command.
563e0f5f2d1761a701f0f80a	X	This depends a lot on the behaviour of S3 and the headers it sends when requesting files on S3. As you experienced, browsers will show different caching behaviour - so the best option is to use unique filenames. I would suggest to use cachebuster hashes - in this way you can be sure that the new file always gets requested by browsers and you can use long cache-lifetime headers if you host the files on your own server. You can for example create a MD5 hash of your minified file and append it (like mycss-322242fadcd23.css). Or you could use the revision number of your source control system. You have to use the cache buster in all links to this file, but you can normally easily do this in your templates where you embed your static resources. Depending on your application, you could probably use this Django plugin that should do this work for you.
563e0f602d1761a701f0f80b	X	Say we want a REST API to support file uploads, and we want uploads to be done directly on S3. According to this solution Amazon S3 direct file upload from client browser - private key disclosure, we have to create POLICY and SIGNATURE for user to be allowed to upload to S3. However, we want a single entry point for the API, including uploads. Can we: 1. in our API, catch POST https://www.example.org/users/1234/objects 2. calculate POLICY and SIGNATURE to allow direct upload to S3 3. return a 307 "Temporary Redirect" to https://s3-bucket.s3.amazonaws.com How to pass POLICY and SIGNATURE in the redirect? What is best practice here?
563e0f602d1761a701f0f80c	X	You dont redirect, instead your API should return the policy and signature in the response (say in JSON). Then the browser can use these values to directly upload to S3 as in the document. This is a two step process.
563e0f602d1761a701f0f80d	X	Is there an error ?
563e0f602d1761a701f0f80e	X	Its not an error, but i guess this is not correct way of authenticating endpoint...
563e0f602d1761a701f0f80f	X	I've amazon aws s3 endpoint, access key and secret key, i want to validate my endpoint using these key in java, can someone help how to test my endpoint if you can share some sample code will be very helpful. Thanks I tried the below code but its not authenticating:
563e0f602d1761a701f0f810	X	An endpoint is an Amazon supplied value. http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html E.g. s3.amazonaws.com for US Standard. Or s3-<region>.amazonaws.com, E.g. s3-eu-west-1.amazonaws.com You are using an incorrect endpoint. Plus in most situations you want to set the Amazon Region, not the endpoint specifically. See http://docs.aws.amazon.com/AmazonS3/latest/dev/create-bucket-get-location-example.html for an example. With your Account credentials, Region and Bucket name, Amazon S3 API has all it needs to find your files.
563e0f602d1761a701f0f811	X	Thanlks for the reply. I put my libraries which come with jets#t in Applibs folder and my jars in classes folder. I am still getting the above error. Am I placing these files correctly? I am a total noob to Glassfish btw.
563e0f602d1761a701f0f812	X	What is applib? Why you put your jar in classes? Have you actually read my answer?
563e0f602d1761a701f0f813	X	Applibs and Classes are two of the directories I found in the domains/domain1/lib folder ... but anyway , I followed your method and dumped everything related to jets3t in it and it worked. Thanks for the effort.
563e0f602d1761a701f0f814	X	Where do I put the Amazon JetS3t and the related jar files in Glassfish 3? I have a sample Restful Web application which currently uses a Arraylist to maintain objects at back-end, I would like them to be stored as text files on Amazon S3. So I am using jets@t API for that. When I deploy the application onto Glassfish, it throes an error like this Thanks in advaance for your help.
563e0f602d1761a701f0f815	X	GlassFish v3 has a well defined Class Loader hierarchy which identifies the common class loader as the proper way to deal with shared libraries. So to make a long story short, putting you libraries and other framework JARs in domains/domain1/lib is all you need to do. Good luck!
563e0f602d1761a701f0f816	X	Can I use redis for caching images as I have that installed. Memcache is there too as part of Mysql 5.6 distro.
563e0f612d1761a701f0f817	X	Go ahead with Redis then, it is cool.
563e0f612d1761a701f0f818	X	I am using Amazon S3 for storing images and YII CListView is used to list each page item. Inside partial view that represents each item of CListView, there are 2 calls to Amazon S3 api , once for checking if file exists and second to actually fetch image and show. Since page size is 20 , and average latency is 200ms per image and therefore for 20 images, total of 20 * 200ms * ( 2 requests per item ) = 8 seconds are taken Since in javascript , I hide the view till all page elements are loaded, so it keeps on loading for 8 to 10 seconds when page size is 20 and it is quite slow.. Do we have some YII extension that has solved this problem with CListView and S3 download integration which does it faster ?
563e0f612d1761a701f0f819	X	Considering that when you cold launch your app, you don't know nothing about your images, the images might or not exists on S3, I don't see much options you can take here, just one: cache. Having cached the retrieved items would make the difference. Memcache would suit if your item size is not bigger than 1M. On the other hand, I would reduce drastically the calls to S3 (as they cost), maybe keep locally a flag to know if the image exists on S3 or not. And would directly link to CDN rather than fetch it. You can even preload the next pages, based on where the user is. You can use a Message Queue as Beanstalkd to manage background jobs.
563e0f612d1761a701f0f81a	X	thanks for your reply. There is no example for multipart upload in given samples of SDK that's why I'm asking this. But I have found some code from somewhere else which is given below, but its giving exceptions which are given below. Please check it if you can help me in that.
563e0f612d1761a701f0f81b	X	thanks for your reply. Please check Edit part of my question if you can help me in solving my problem.
563e0f612d1761a701f0f81c	X	in response you must be getting string that need tobe signed.. just double check the string you are signing is correct.
563e0f612d1761a701f0f81d	X	I have changed base64 conversion method, now its giving response "HTTP/1.0 411 Length Required". Any idea what I'm doing wrong??
563e0f612d1761a701f0f81e	X	are u setting "Content-Length" in header?
563e0f612d1761a701f0f81f	X	No I'm not setting Content-Length in header.
563e0f612d1761a701f0f820	X	I'm searching on google from last two days regarding using amazon multipart upload api in my cocoa project for mac. I have downloaded AWS sdk for ios. But didn't find how to use that sdk in cocoa project. Can anyone provide me some example code to achieve multipart uploading using amazon S3 multipart upload??? Edit: As AWS SDK for IOS is not compatible with Cocoa applications, I'm using Rest api to upload file using libcurl. I'm using following code (by taking reference from http://dextercoder.blogspot.in/2012/02/multipart-upload-to-amazon-s3-in-three.html): But its giving response "< HTTP/1.0 403 Forbidden SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method." 
563e0f612d1761a701f0f821	X	You have a basic examples for AWS services like S3, SDB etc provided with the SDK itself: http://docs.amazonwebservices.com/mobile/sdkforios/gsg/Welcome.html?r=1498
563e0f612d1761a701f0f822	X	you can refer java code from http://dextercoder.blogspot.in/2012/02/multipart-upload-to-amazon-s3-in-three.html. tells how to upload file in multiparts without AWS.
563e0f642d1761a701f0f823	X	Some off-topic things: Another way to get that dict: __import__('collections').Counter(len(str(__import__('random').random())) for i in range(0, 1000000)).
563e0f642d1761a701f0f824	X	if you look at Math.random().toString(2).length you'll see a different pattern, perhaps more to what you'd expect
563e0f642d1761a701f0f825	X	@KevinGuan And the JavaScript ES6 counterpart (sortof): Array.apply(null, Array(1e5)).map(() => Math.random()).reduce((records, n) => records[String(n).length] ? (records[String(n).length] += 1, records) : (records[String(n).length] = 1, records), {})
563e0f642d1761a701f0f826	X	A JS version shorter than the previous: Array.apply(null, Array(1e5)).map(() => Math.random()).reduce((records, n) => (records[String(n).length] = records[String(n).length] + 1 || 1, records), {})
563e0f642d1761a701f0f827	X	@Jaromanda X, binary representation will show a similar effect. It has to do with the difference between number of significant digits and the number of digits needed to represent tiny numbers without using exponents.
563e0f642d1761a701f0f828	X	It should be noted that Javascript has a Number.toFixed() method when one want to make a mathematical use of the string representation of the number.
563e0f642d1761a701f0f829	X	I encounter this curious phenomenon trying to implement a UUID generator in JavaScript. Basically, in JavaScript, if I generate a large list of random numbers with the built-in Math.random() on Node 4.2.2: The numbers of digits have a strange pattern: I thought this is a quirk of the random number generator of V8, but similar pattern appears in Python 3.4.3: And the Python code is as follows: The pattern from 18 to below is expected: say if random number should have 20 digits, then if the last digit of a number is 0, it effectively has only 19 digits. If the random number generator is good, the probability of that happening is roughly 1/10. But why the pattern is reversed for 19 and beyond? I guess this is related to float numbers' binary representation, but I can't figure out exactly why.
563e0f642d1761a701f0f82a	X	The reason is indeed related to floating point representation. A floating point number representation has a maximum number of (binary) digits it can represent, and a limited exponent value range. Now when you print this out without using scientific notation, you might in some cases need to have some zeroes after the decimal point before the significant digits start to follow. You can visualize this effect by printing those random numbers which have the longest length when converted to string: This prints only the 23-long strings, and you will get numbers like these: Notice the zeroes before the first non-zero digit. These are actually not stored in the number part of a floating point representation, but implied by its exponent part. If you were to take out the leading zeroes, and then make a count: ... you'll get results which are less strange. However, you will see some irregularity that is due to how javascript converts tiny numbers to string: when they get too small, the scientific notation is used in the string representation. You can see this with the following script (not sure if every browser has the same breaking point, so maybe you need to play a bit with the number): This gives me the following output: So very small numbers will get a more fixed string length as a result, quite often 22 characters, while in the non-scientific notation a length of 23 is common. This influences also the second script I provided and length 22 will get more hits than 23. It should be noted that javascript does not switch to scientific notation when converting to string in binary representation: The above will print a string of over 450 binary digits!
563e0f642d1761a701f0f82b	X	It's because some of the values are like this: And thus they're longer.
563e0f652d1761a701f0f82c	X	I am developing an iOS app that uses a large amount of images that are needed for animations for short videos. I want to save my application assets as static files in cloud and once they are needed download them using secure API call (either JSON, XML or any other alternative for that matter). What is the best option for that. I have checked Parse, Dropbox, iCloud, Google Drive, but I am puzzled since I see only instructions for dynamic data that lets users access content they have created and not static assets. What would be best option for that?
563e0f652d1761a701f0f82d	X	If you just want an easy way to serve static files I would take a look at Amazon S3. You can just upload files through the online console and then get the public URL to those files to use in your app. You can also use the S3 API to upload files through your web service or iOS app. Hope this helps!
563e0f652d1761a701f0f82e	X	I'd go for Parse (basically because it is fast to learn and develop), you can create a table with the images and change the writing permissions if you are afraid somebody could modify the table. Another option that you can check it's the special Config table so you can upload custom files (zip files i.e.) and download them in demand.
563e0f652d1761a701f0f82f	X	Choose an answer ?
563e0f652d1761a701f0f830	X	doesnt seem to work. both.get empty page, despite all files public and not running as website bucket.
563e0f652d1761a701f0f831	X	@devdude, can you turn on Javascript debugging to see if the script is failing anywhere?
563e0f652d1761a701f0f832	X	Firebug: response is null handleList()list.html (line 104) [Break On This Error] filex = response.getElementsByTagName('Contents');
563e0f652d1761a701f0f833	X	Didn't worked for me neither, Rufus Pollock's solution worked well
563e0f652d1761a701f0f834	X	Great job, works perfectly and very well documented
563e0f662d1761a701f0f835	X	Are these APIs callable from Javascript? Any samples? otherwise I'm not sure how I'd use them from within a browser. I was hoping for something I could serve up from within the S3 space.
563e0f662d1761a701f0f836	X	I have an Amazon S3 account where I would like to store several directories of files. I would like a visitor to this site to be able to see and download the files and folders I have placed there. These files and folders will change regularly and I would prefer not to have to rewrite any html each time I added or removed files. How can I arrange for the viewers of my site to be presented with a simple list of files/ folders?
563e0f662d1761a701f0f837	X	You can use Javascript to list the files. Here is the solution provided by Amazon: http://aws.amazon.com/code/Amazon-S3/1713 You place list.html in every directory you want to list. I have made my own listing file that provides a collapsible tree view: https://github.com/phatmann/jS3Tree/blob/master/index.html Neither of these files will work if you are using the S3 website feature.
563e0f662d1761a701f0f838	X	I've created a simple bit of JS that creates a directory index in HTML style that would fit what you are looking for: https://github.com/rgrp/s3-bucket-listing You can install this either directly into your s3 bucket or into a separate website (thanks to the fact that the S3 REST API supports CORS!). The README has full instructions on this: https://github.com/rgrp/s3-bucket-listing
563e0f662d1761a701f0f839	X	you should use the Amazon S3 API to list the buckets and files within them a bucket can represent a folder (will be easier than using prefix's on the file name) after creating your buckets and uploading the files to them you can present the buckets on a page with a List All My Buckets request once a user clicks on a given bucket you can get the files in it using a List Bucket request another last click on the file to generate a url for the object so the user can download it without wasting your bandwidth you can find many implementations to the amazon s3 api here
563e0f662d1761a701f0f83a	X	in addition to what jnoller bellow suggests, boto also supports S3 and CloudFiles as well as Google Cloud Storage. stackoverflow.com/questions/7624900/…
563e0f662d1761a701f0f83b	X	I got to know that Rackspace Cloud Files is based on OpenStack Object Storage service (Swift). As OpenStack allows configuring/manipulating object storage using S3 APIs through swift3 http://docs.openstack.org/trunk/openstack-object-storage/admin/content/configuring-openstack-object-storage-with-s3_api.html I am thinking whether Rackspace Cloud Files provides S3 API support as well. I have a client written for Amazon Web Services using S3 RESTful APIs so was thinking to reuse it for Rackspace Cloud Files as well.
563e0f662d1761a701f0f83c	X	The S3 plugin for Swift is not deployed as part of Rackspace Cloud Files (most production deploys of openstack don't deploy it by default). However, if you want better flexibility in the app, you can use a cross cloud toolkit such as libcloud (python), fog (ruby), jclouds (java), pkgcloud (node/js). This means you can use a simpler abstraction and support multiple providers within your application.
563e0f662d1761a701f0f83d	X	For your information: Google App Engine only supports JDK 5 & 6. developers.google.com/appengine/docs/java/gettingstarted/…
563e0f672d1761a701f0f83e	X	but in this link they told that problem fixed in app engine 1.7.3 but still i have problem so do you know about that thing may be some where my mistake in my code or configuration.thanks for your response
563e0f672d1761a701f0f83f	X	It seems that the bug-report differs to your problem. You are getting a NullPointerException at VersionInfo line 124. I think System.getProperty throws the NullPointerException (os.name or os.version). But in their documentation they write that System.getProperty is allowed. Try to set the properties on your own as explained HERE.
563e0f672d1761a701f0f840	X	ya you are right, i also try to put <system-properties> <property name="os.name" value="..." /></system-properties> this property but i don't know what i have write in value field????? i put different value than upload on app engine still its give me same error as above.
563e0f672d1761a701f0f841	X	Debug your application on a local server and look which property throws the exception. Then try different strings that match the property. I think most of the values are optional (but not all of them). If that is not working try to LOG the properties (load them on startup and write them into your log file).
563e0f672d1761a701f0f842	X	Hey da_re thanks above error gone when i write following code in appengine-web.xml <system-properties> <property name="java.util.logging.config.file" value="WEB-INF/logging.properties"/> <property name="os.name" value="Linux" /> <property name="os.version" value="6.1" /> <property name="java.vm.name" value="Java HotSpot(TM) Client VM" /> <property name="java.vm.version" value="23.5-b02" /> <property name="user.language" value="en" /> <property name="user.region" value="us" /> </system-properties>
563e0f672d1761a701f0f843	X	i am working on gwt2.4, jre7 and GAE 1.7.3.in development mode my code working properly but when i uploaded my apps on app engine than i am getting following error i am also find out solution on following link but still it didn't works (http://code.google.com/p/googleappengine/issues/detail?id=8166)
563e0f672d1761a701f0f844	X	//EDIT: He added the following XML to his appengine-web.xml to get it work: You can't use the Amazon Libs because of restrictions in the Google App Engine (no Threads, ...). But there are few solutions that should help you: [1] http://socialappdev.com/using-amazon-s3-with-google-app-engine-02-2011 [2] https://github.com/handstandtech/s3-simple-appengine Why don't you use Google Cloud Storage with the GAE Java SDK? The GCS API is almost the same as the S3 API.
563e0f672d1761a701f0f845	X	some time app engine didn't get value of os.name,os.version,java.vm.name etc so we have to specify manually in appengine-web.xml file 
563e0f682d1761a701f0f846	X	What platform(s)
563e0f682d1761a701f0f847	X	App in iOS and Android backend using rails
563e0f682d1761a701f0f848	X	On iOS you might want to look at using AFNetworking for uploading your image to a rails RESTful API.
563e0f682d1761a701f0f849	X	I have a web app in Rails that has a profile image upload, I`m using Refile gem to upload the photo direct to Amazon S3. But now I need to do this upload from a mobile app too. Witch is the best way to do that? I think about a Rest API in Rails that receives the binary data and then uses Async Jobs (Sidekiq) to upload to Amazon S3, but and not sure if the approach is best way. Can anyone help me? Thanks!
563e0f692d1761a701f0f84a	X	first sorry for my english I am new to this topic as well as mentioned in the title, I have to build this ios-app and after day-long search I found nothing useful. I already can get the facebook data. now I have to build a connection with mongodb. and there I should call a method (http://myurl.com/api/auth). This method assumes the storage of facebook-data: -name -Birthday ... -and the url of the facebook-profile-picture. I have to call up another method that stores the image in amazon s3 (using the url). The method is called "createPhoto" and so must be called: http://myurl.com/api/createPhoto Now I'm looking for example code and how I can solve it best. I would be very happy if you could suggest me some links or codes. Thank you very much in advance
563e0f692d1761a701f0f84b	X	Any ideas?
563e0f692d1761a701f0f84c	X	You could implement the front-end in pretty much anything that you can code to speak native S3 multipart upload... which is the approach I'd recommend for this, because of stability. With a multipart upload, "you" (meaning the developer, not the end user, I would suggest) choose a part size, minimum 5MB per part, and the file can be no larger that 10,000 "parts", each exactly the same size (the one "you" selected at the beginning of the upload, except for the last part, which would be however many bytes are left over at the end... so the ultimatel maximum size of the uploaded file depends on the part-size you choose. The size of a "part" essentially becomes your restartable/retryable block size (win!)... so your front-end implementation can infinitely resend a failed part until it goes through correctly. Parts don't even have to be uploaded in order, they can be uploaded in parallel, and if you upload the same part more than once, the newer one replaces the older one, and with each block, S3 returns a checksum that you compare to your locally calculated one. The object doesn't become visible in S3 until you finalize the upload. When you finalize the upload, if S3 hasn't got all the parts (which is should, because they were all acknowledged when they uploaded) then the finalize call will fail. The one thing you do have to keep in mind, though, is that multipart uploads apparently never time out, and if they are "never" either finalized/completed nor actively aborted by the client utility, you will pay for the storage of the uploaded blocks of the incomplete uploads. So, you want to implement an automated back-end process that periodically calls ListMultipartUploads to identify and abort those uploads that for whatever reason were never finished or canceled, and abort them. I don't know how helpful this is as an answer to your overall question, but developing a custom front-end tool should not be a complicated matter -- the S3 API is very straightforward. I can say this, because I developed a utility to do this (for my internal use -- this isn't a product plug). I may one day release it as open source, but it likely wouldn't suit your needs anyway -- its essentially a command-line utility that can be used by automated/scheduled processes to stream ("pipe") the output of a program directly into S3 as a series of multipart parts (the files are large, so my default part-size is 64MB), and when the input stream is closed by the program generating the output, it detects this and finalizes the upload. :) I use it to stream live database backups, passed through a compression program, directly into S3 as they are generated, without ever needing those massive files to exist anywhere on any hard drive. Your desire to have a smooth experience for your clients, in my opinion, highly commends S3 multipart for the role, and if you know how to code in anything that can generate a desktop or browser-based UI, can read local desktop filesystems, and has libraries for HTTP and SHA/HMAC, then you can write a client to do this that looks and feels exactly the way you need it to. You wouldn't need to set up anything manually in AWS for each client, so long as you have a back-end system that authenticates the client utility to you, perhaps by a username and password sent over an SSL connection to an application on a web server, and then provides the client utility with automatically-generated temporary AWS credentials that the client utility can use to do the uploading.
563e0f692d1761a701f0f84d	X	Something like S3Browser would work. It has a GUI, a command line and works with large files. You can use IAM to create a group, grant that group access to a specific S3 bucket using a policy, then add IAM users to that group. Your IAM group policy would look something like this : Adding an IAM user to this group will allow them to use S3Browser and only have read-write access to YOUR_BUCKET_NAME. However, they will see a list of your other buckets, just not be able to read/write to them. You'll also need to generate an AWS Access Key and Secret for each IAM user and provide those 2 items to whoever is using S3Browser.
563e0f692d1761a701f0f84e	X	Thanks for the answer, found the solution. I added three libraries - aws-java-sdk-s3, aws-java-sdk-kms and aws-java-sdk-core
563e0f692d1761a701f0f84f	X	I am working on amazon S3 file upload operation in java. Amazon provides aws java sdk for this purpose but the thing is the sdk is of 20 mb including its dependencies. What I want is, a library which have only functionality of S3 related operation. P.S. I read that amazon provides REST API also. So, did anyone tried that api in java. If yes, please help me by sharing the code snippet. Thanks
563e0f692d1761a701f0f850	X	aws now provides separate libraries too. If you are using maven then you can include just the s3 library. Below is the link for the same. http://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3/1.9.33
563e0f692d1761a701f0f851	X	What version of boto are you using?
563e0f692d1761a701f0f852	X	I'm using AWS Cloudtrail. I would like to display the audit trail for a particular file on S3 using Python. How do I do this?
563e0f692d1761a701f0f853	X	AWS CloudTrail does not keep an audit of accesses made to Amazon S3 objects. CloudTrail only records API calls that related to Buckets. See: Logging Amazon S3 API Calls By Using AWS CloudTrail To obtain information about accesses made to Amazon S3 objects, enable Server Access Logging: In order to track requests for access to your bucket, you can enable access logging. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and error code, if any. Access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill. Access logs are similar to web server logs, showing each access to objects within Amazon S3.
563e0f6a2d1761a701f0f854	X	Does it really have to use S3? Or can I specify other sources of input?
563e0f6a2d1761a701f0f855	X	@beefjerky yes, you do have to. see link to faq in edited answer
563e0f6a2d1761a701f0f856	X	Seems a bit weird to restrict input source to S3 though, but yeah, it does mention it in general faq - "In the request form, you specify the name of your cluster, the location in Amazon S3 of your input data, your processing application...". Thanks!
563e0f6a2d1761a701f0f857	X	I'm just beginning to learn about Big Data and I'm interested in Hadoop. I'm planning on building a simple analytics system to make sense of certain events that occurs in my site. So I'm planning to have code (both front and back end) to trigger some events that would queue messages (most likely with RabbitMQ). These messages will then be processed by a consumer that would write the data continuously to HDFS. Then, I can run a map reduce job at any time to analyze the current data set. I'm leaning towards Amazon EMR for the Hadoop functionality. So my question is this, from my server running the consumer, how do I save the data to HDFS? I know there's a command like "hadoop dfs -copyFromLocal", but how do I use this across servers? Is there a tool available? Has anyone tried a similar thing? I would love to hear about your implementations. Details and examples would be very much helpful. Thanks!
563e0f6a2d1761a701f0f858	X	If you mention EMR, it's takes input from a folder in s3 storage, so you can use your preffered language library to push data to s3 to analyse it later with EMR jobs. For example, in python one can use boto. There are even drivers allowing you to mount s3 storage as a device, but some time ago all of them were too buggy to use them in production systems. May be thing have changed with time. EMR FAQ: Q: How do I get my data into Amazon S3? You can use Amazon S3 APIs to upload data to Amazon S3. Alternatively, you can use many open source or commercial clients to easily upload data to Amazon S3. Note that emr (as well as s3) implies additional costs, and that it's usage is justified for really big data. Also note that it is always benefical to have relatively large files both in terms of Hadoop performance and storage costs.
563e0f6a2d1761a701f0f859	X	I am using the Node.js JavaScript API for Amazon AWS S3, and would like to set objects to expire a specified number of days after the objects are created. That is, if I create and upload a new object, I want it to automatically delete itself 100 days or so from now. Is this possible to set expiration for deletion on a per-object basis? The documentation indicates this may be possible: Amazon S3 provides an Expiration action that you can specify in your lifecycle configuration to expire objects. … When an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously. There may be a lag between the expiration date and the date at which Amazon S3 removes an object. You are not charged for storage time associated with an object that has expired. However, it seems that I would have to set this expiration in the bucket configuration, and not per-object when I upload/create them. The JavaScript SDK documentation indicates that I can set an Expires parameter when creating an object, but this seems to be for the Expires HTTP header when S3 returns the object for subsequent GET requests. Is there a way to set the expiration date of an object when creating it?
563e0f6a2d1761a701f0f85a	X	You can not set expiration rules on each object individually. To define object expiration rules, you have to define a bucket lifecycle configuration. To do this with the node.js API, see the putBucketLifecycle call. You can also check out the REST API docs for the bucket lifecycle PUT operation.
563e0f6a2d1761a701f0f85b	X	I plan to store images on Amazon S3 how to retrieve from Amazon S3 : 1)file size 2)image height 3)image width ?
563e0f6a2d1761a701f0f85c	X	Getting the file size is possible by reading the Content-Length response header to a simple HEAD request for your file. Maybe your client can help you with this query. More info on the S3 API docs. Amazon S3 just provides you with storage, (almost) nothing more. Image dimensions are not accessible through the API. You have to get the whole file, and calculate its dimensions yourself. I'd advise you to store this information in the database when uploading the files to S3, if applicable.
563e0f6a2d1761a701f0f85d	X	You can store image dimensions in user-defined metadata when uploading your images and later read this data using REST API. Refer to this page for more information about user-defined metadata: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html
563e0f6b2d1761a701f0f85e	X	I am using Amazon S3 in Yii2 framework For now i am getting this type of error: I am using this demo: Demo How can i resolve this error? Comment if any information required.
563e0f6b2d1761a701f0f85f	X	This is a thoughtful and well-written response.
563e0f6b2d1761a701f0f860	X	I've been putting in some research around REST. I noticed that the Amazon S3 API uses mainly http headers for their REST interface. This was a surprise to me, since I assumed that the interface would work mainly off request parameters. My question is this: Should I develop my REST interface using mainly http headers, or should I be using request parameters?
563e0f6b2d1761a701f0f861	X	The question mainly is whether the parameters defined are part of the resource identifier (URI) or not. if so, then you would use the request parameters otherwise HTTP custom headers. For example, passing the id of the album in a music gallery must be part of the URI. Remember, for example /employee/id/45 (Or /employee?id=45, REST does not have a prejudice against query string parameters or for clean slash separated URIs) identifies one resource. Now you could use content-negotiation by sending request header content-type: text/plain or content-type: image/jpg to get the info or the image. In this respect, resource is deemed to be the same and header only used to define format of the resource. Generally, I am not a big fan of HTTP custom headers. This usually assumes the client to have a prior knowledge of the server implementation (not discoverable through natural HTTP means, i.e. hypermedia) which always is considered a REST anti-pattern HTTP headers usually define aspects of HTTP orthogonal to what is to be achieved in the process of request/response. Authorization header (really a misnomer, must have been authentication) is a classic example.
563e0f6b2d1761a701f0f862	X	Thanks! It works! But some consideration I haven't made, and I will be explained below...
563e0f6b2d1761a701f0f863	X	By the way, I'm having a weird validation message on uploading remote image from facebook/twitter users, I don't know if you have a couple of minutes to watch it StackOverflow Question
563e0f6b2d1761a701f0f864	X	This led me to the answer - funny thing though - the region in the url of my bucket in the aws console was different from the region listed under properties - so word the wise - double check that. It solved this issue for moi.
563e0f6b2d1761a701f0f865	X	For me, this simplest answer worked as well: stackoverflow.com/questions/10630430/…
563e0f6c2d1761a701f0f866	X	+1 @Jackson_Sandland, I had the exact same case – really dazzling because for S3 the console's Regions dropdown menu says they don't matter... but then they do for uploads!
563e0f6c2d1761a701f0f867	X	Im trying to upload images to S3 on Ruby on Rails using carrierwave and fog gems, images are uploaded correctly but when I try tu save the model containing information about the image that was just uploaded Im getting this error: User model: AvatarUploader: User controller carriwerwave initializer gemfile
563e0f6c2d1761a701f0f868	X	This is a frequently encountered issue: You are trying to access a bucket in region us-west-1, however, for legacy reasons the default Amazon S3 region in most/all AWS SDKs is US Standard, which automatically routes requests to facilities in Northern Virginia or the Pacific Northwest using network maps (see Regions and Endpoints for details). Therefore you simply need to specify the endpoint of your buckets region explicitly before using the S3 API, e.g. for us-west-1:
563e0f6c2d1761a701f0f869	X	Thanks again to Steffen Opel! But some consideration I haven't made, my region is US Standard, therefore, my carrierwave initializer looks like this: # :region => # NOT NEEDED BY US STANDARD :endpoint => 'https://s3.amazonaws.com' This link was the key :D
563e0f6c2d1761a701f0f86a	X	this will make my bucket public? Then will i be able to download the files as well?
563e0f6c2d1761a701f0f86b	X	@SyedSalmanRazaZaidi see my updated answer.
563e0f6c2d1761a701f0f86c	X	Yes i know this,but what if i have thousand objects it's nonsense to make 1000 objects publically like that
563e0f6c2d1761a701f0f86d	X	i want to make it publically available through request or Signed URL(i dont have idea of both)
563e0f6c2d1761a701f0f86e	X	I've looked in first link,my request is making similar as it explained :(
563e0f6c2d1761a701f0f86f	X	I am working on Amazon S3 sdk for storing files on cloud server,i am using codeplex's threesharp(http://threesharp.codeplex.com) for implementing this, I have successfully uploaded file on server now i have to download it, and for this i have to download it with the URL eg https://s3.amazonaws.com/MyBucket/Filename I can download the uploaded file but it is appearing blank, if i upload a text file then after downloading it's showing nothing in it,same as images and other files. I have read on Amazon S3 documentation that i'll have to make the object publically readable(http://docs.amazonwebservices.com/AmazonS3/latest/gsg/OpeningAnObject.html) i dont have any idea how to achieve this. How can i accomplish the download functionality? Threesharp project is a desktop based and i am working on web based application
563e0f6c2d1761a701f0f870	X	During file upload set proper ACL: Eg.: Amazon S3 provides a rich set of mechanisms for you to manage access to your buckets and objects. Check this for detail: Amazon S3 Bucket Public Access Considerations Also, You can Download Explorer for Amazon S3 (Eg. CloudBerry Explorer for Amazon S3) & they you can assign appropriate rights to your buckets. CloudBerry Explorer for Amazon S3: Data Access Feature: Bucket Policy Editor Create and edit conditional rules for managing access to the buckets and objects. ACL Editor Manage access permission to any of your objects by setting up 'Access Control List'. ACL will also apply to all 'child objects' inside S3 buckets. Also, you can do the same using Amazon S3 admin console. Eg.
563e0f6c2d1761a701f0f871	X	Have you tried the following: edit: have you taken a look here: How to set the permission on files at the time of Upload through Amazon s3 API and here: How to set a bucket's ACL on S3? It might guide you in the right direction
563e0f6c2d1761a701f0f872	X	I have used amazon v2 iOS latest api to upload videos to s3 BUCKET , i use there latest class called AWSS3TransferUtility https://mobile.awsblog.com/post/Tx283AGGIL76PKP/Amazon-S3-Transfer-Utility-for-iOS i managed upload videos successfully , but now i cant get the checksum of the uploaded file in here etag returns as null any particular reason that this doesnt work
563e0f6d2d1761a701f0f873	X	I think it's important to point out that what you're describing as a service here would not be considered a domain service in DDD terms. The usual advice of "services should not be injected into repositories", while good advice, doesn't quite apply to this scenario.
563e0f6d2d1761a701f0f874	X	+1 it's important to make the distinction between a domain service and technical infrastructure.
563e0f6d2d1761a701f0f875	X	PROBLEM: I am pulling in third party data from an XML feed. I need to take that data and convert it into entities. POINTS FOR DISCUSSION: My question is how to use Services and Repositories in this case. So, for example, I could create a Service that pulls the data from the feed, and then inject that service into the Repository which could then use it to pull the data and convert it into entities. But I'm not sure if that is the right approach? The repository could have the logic to pull in the data and then map it to entities, but I don't think the repository should be handling that logic? Or should it? From a DDD separation of concerns perspective, how should this best be architected?
563e0f6d2d1761a701f0f876	X	I could create a Service that pulls the data from the feed, and then inject that service into the Repository. This would not be considered a domain service in DDD, since it has nothing to do with the domain. It's purely a technical infrastructure concern. My question is how to use Services and Repositories in this case. It sounds like you may have two separate concerns here. The repository obviously provides access to the data in the terms of the bounded context, but I'm betting there's some additional data manipulation/transformation going on in between the XML data and your repository... In DDD terms this would be considered an anti-corruption layer. You have no control over the external data source, and you want to prevent the format of this external data from corrupting the integrity of your carefully designed domain model. It's ok to inject an anti-corruption layer into a repository - as long as its purpose is well defined as such. This would not be considered a domain service since it has nothing to do with the domain, it's a pure fabrication driven by the nature of the external data source.
563e0f6d2d1761a701f0f877	X	No. Repositories should not have any domain logic what so ever except provinding persistance ignorance. But the repository itself could have any kind of conversions between the data entities and the domain enteties. It can also use any number of tables or databases to be able to build the requested domain entities.
563e0f6d2d1761a701f0f878	X	Services should not be injected in the repositories, but the contrary. If your repository is not tightly coupled to your database (as most implementations seems to be), you could have: Another approach: fetch the data and convert to entities in the service layer, and then pass the entities to the repository for persistence.
563e0f6d2d1761a701f0f879	X	Thanks, very helpful !
563e0f6d2d1761a701f0f87a	X	I have configured life cycle policy in S3, some of objects in S3 are stored in Glacier class, some of object are still in S3, now I am trying to restore objects from Glacier, I can get object list in S3 by java AWS SDK, how can I know which object is in Glacier storage by AWS SDK? The reason is when I try restore an object not in Glacier, I will have a exception. I wanna avoid this.
563e0f6d2d1761a701f0f87b	X	The ListObjects Amazon S3 API call can be used to obtain a list of objects in a given bucket. The list of files returned includes a StorageClass field. It can be one of: The GLACIER storage class indicates that the contents of the object is currently in Glacier.
563e0f6d2d1761a701f0f87c	X	What do you mean by "when the file system is mounted"?
563e0f6e2d1761a701f0f87d	X	I have a requirement to list the file from an Amazon S3 location: The colorpics directory contains multiple sub-directories for years like: So the ls command should list the images for all the years dynamically and it should list only from next level sub-directory /640/. If I give /*/ it works when the file system is mounted. But when I try to list from the bucket it fails. Is there anyway I can achieve it?
563e0f6e2d1761a701f0f87e	X	The Amazon S3 API can return the content of a bucket based on a prefix such as s3://static.abc.com/colorpics/, however it cannot have a wildcard mid-pattern. This is probably why s3cmd cannot provide that functionality. By the way, these days it is better to use the official AWS Command-Line Interface (CLI), which has a matching aws s3 ls command.
563e0f6e2d1761a701f0f87f	X	The "s3 region" might be expecting only the region part, not the entire hostname, e.g. "s3-us-west-2" (without .amazonaws.com on the end). I'm guessing, here.
563e0f6e2d1761a701f0f880	X	The only tool I could find, I forked and tried to update to include the S3_REGION because I was getting $ The bucket you are attempting to access must be addressed using the specified endpoint These are all the variables I am passing to access the bucket. https://github.com/rounders/heroku-s3assets has not been update in a while so Im assuming I just can't find where the actual error is breaking either in Heroku tools, or the older aws-s3 gem. Anyone have any method to pull down production assets to Heroku server from AmazonS3?
563e0f6e2d1761a701f0f881	X	I think I mis-understood you, so editing now...maybe experiment with something simpler: http://priyankapathak.wordpress.com/2012/12/28/download-assets-from-amazon-s3-via-ruby/ My search returned this info: The Amazon S3 bucket specified in the COPY command must be in the same region as the cluster. If your Amazon S3 bucket and your cluster are in different regions, you will receive an error similar to the following: You can create an Amazon S3 bucket in a specific region either by selecting the region when you create the bucket by using the Amazon S3 Management Console, or by specifying an endpoint when you create the bucket using the Amazon S3 API or CLI. For more information, see Uploading files to Amazon S3. For more information about Amazon S3 regions, see Buckets and Regions in the Amazon Simple Storage Service Developer Guide. Alternatively, you can specify the region using the REGION option with the COPY command. http://docs.aws.amazon.com/redshift/latest/dg/s3serviceexception-error.html
563e0f6e2d1761a701f0f882	X	So it turns out that gem was all but useless. I've gotten further to my goal of downloading all my s3 assets to public/system - but still can not figure out how to download them to my correct local rails directory using the aws s3 docs - http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html I probably just need to read more unix commands and scp them over individually or something. Any ideas?
563e0f6e2d1761a701f0f883	X	there is a better answer then the marked one...
563e0f6e2d1761a701f0f884	X	Warning! - check out @Roberto's post below. It can be done now.
563e0f6e2d1761a701f0f885	X	I am storing many images in Amazon S3, using a ruby lib (http://amazon.rubyforge.org/) I don't care the photos older than 1 week, then to free the space in S3 I have to delete those photos. I know there is a method to delete the object in a certain bucket: Is there a way to automatically delete the image older than a week ? If it does Not exist, I'll have to write a daemon to do that :-( Thank you UPDATE: now it is possible, check the Roberto's answer.
563e0f6e2d1761a701f0f886	X	Unfortunately, Amazon doesn't offer an API for automatic deletion based on a specific set of criteria. You'll need to write a daemon that goes through all of the photos and and selects just those that meet your criteria, and then delete them one by one.
563e0f6e2d1761a701f0f887	X	You can use the Amazon S3 Object Expiration policy Amazon S3 - Object Expiration | AWS Blog If you use S3 to store log files or other files that have a limited lifetime, you probably had to build some sort of mechanism in-house to track object ages and to initiate a bulk deletion process from time to time. Although our new Multi-Object deletion function will help you to make this process faster and easier, we want to go ever farther. S3's new Object Expiration function allows you to define rules to schedule the removal of your objects after a pre-defined time period. The rules are specified in the Lifecycle Configuration policy that you apply to a bucket. You can update this policy through the S3 API or from the AWS Management Console. Object Expiration | AWS S3 Documentation Some objects that you store in an Amazon S3 bucket might have a well-defined lifetime. For example, you might be uploading periodic logs to your bucket, but you might need to retain those logs for a specific amount of time. You can use using the Object Lifecycle Management to specify a lifetime for objects in your bucket; when the lifetime of an object expires, Amazon S3 queues the objects for deletion. Ps: Click on the links for more information.
563e0f6f2d1761a701f0f888	X	If you have access to a local database, it's easy to simply log each image (you may be doing this already depending on your application), and then you can perform a simple query to retrieve the entire list and delete them each. This is much faster than querying S3 directly, but does require local storage of some kind.
563e0f6f2d1761a701f0f889	X	How do I get a list of all objects in a bucket of Amazon S3? I'm using Zend Framework which has Amazon S3 library built-in. But it doesn't have method to retrieve all objects instead of 1000 limit objects. Here's how I do it: I've found some workaround on this, which provided by Zend docs itself. It uses getIterator() method to list ALL objects in a bucket. BUT, this method doesn't exist in Zend S3 Library I'm using now, unless I update it, which might cause some problem to existing system I'm developing. I've searched for the source code of getIterator but couldn't find it. Is there any other way around this? Edit: Okay, getIterator is in Guzzle\Service. Now what? My current Zend framework doesn't have this library (it's outdated). How do I import it?
563e0f6f2d1761a701f0f88a	X	Currently I am working on a project where we need to send a set of photo's and video's to a S3 amazon server. The flow is like this: -first we ask api to start a transfer and we get an id back (api call) -transfer id -> request file upload at api -> file_id as response (api call) -file id -> request chunk upload at api -> amazon data as response (api call) -upload chunck -> in the NSURLSession in configured in backgroundConfiguration (5mb each upload) -finish file upload after last chunck-upload(api call) -finish transfer after last file-upload(api call) We need to use the api and make calls to it.. also when the app is running in the background. So what I thought was to use the AFNetworking 2.0 that can upload files in the background and then runs a completion block. In that completion block the code is like: So the difficulty is that we like to execute code and also try do a upload task in the background of iOS7 When the app is connected to the debugger (Xcode) the above sample works. But without it provides this error in the console: So thats not cool :( Is there a way to run code and also uploading the files in the background..?? Do you have any experience with this?? Or do we ask to much of iOS7?? I hope you can share your thoughts. Thanks, Kind Regards, Bart Schoon
563e0f6f2d1761a701f0f88b	X	The error that you see on your console is the 30 second limit enforced by iOS for background tasks. Once a NSURLSessionTask has completed in the background. Your app is launched in the background and your app receives a call in the appDelegate. At this point you should be the storing the completion handler, queue up your next upload task and call the completion handler so a new snapshot can be taken and your app is put back to sleep. Now i am not sure when exactly the 30 second limit gets enforced. In earlier versions of iOS 7 version, it was enforced only if your app was relaunched after being kicked out of memory but in the latest version (7.0.3) it sometimes gets enforced even in suspended mode.As usual, there isn't much information in the Apple documentation so we need to figure it out based on trial and error. I think your problem is that your letting the upload continue in the background without ever calling the completion handler and thats why your app crashes. I don't think NSURLSession is meant to upload tons of file continuously in the background. This is how your upload should be working, assuming we are performing a background and all your api calls are being made in the background. 1) Create and resume a task to call the api to get the ID - call the completion handler. 2) Task finishes in the background and your app is launched. Lets assume you have 30 seconds. You should be first parsing the response that you will get you the ID. Then setup the next task to request the file upload, call completion handler. 3) Tasks finishes in the background and your app is launched again. Now you will be parsing the response giving you the file id and then you setup your next task for chunked upload, call the completion handler. So basically to summarize, while in the background, you should be calling the completion handler after every task you setup. If you don't, eventually your app will crash. Calling the completion handler terribly slows down your upload because then you are relying on iOS to relaunch your app whenever it feels like it but at this point i haven't seen a better solution
563e0f6f2d1761a701f0f88c	X	I am creating an app where there will be a lot of photos (like a lot) which would be constantly updated. I was thinking that I should use a 3rd party cloud server. Does anyone have any recommendations on how I can approach this? Thanks!
563e0f6f2d1761a701f0f88d	X	I would recommend Amazon S3 storage, they also have good API's for almost any programming language. See http://aws.amazon.com/s3/
563e0f6f2d1761a701f0f88e	X	I am using Amazon AWS API to read objects from bucket. Bucket has following structure Here is my code Its throwing exception as SignatureDoesNotMatch. I am using AWSSDK API version 2.3.14.0. I can read and download same file using S3 browser.
563e0f702d1761a701f0f88f	X	Which web-services platform are you using? ASMX, WCF (RESTful or SOAP?), ASP.NET MVC Web-API, etc?
563e0f702d1761a701f0f890	X	SDO REST API...
563e0f702d1761a701f0f891	X	Amazon S3......
563e0f702d1761a701f0f892	X	You are trying to upload zip file so content type should be application/octet-stream
563e0f702d1761a701f0f893	X	what about encoding it. would it still be what i have? byte[] bytes = Encoding.ASCII.GetBytes(parameter);
563e0f702d1761a701f0f894	X	thanks GSerjo, I'm going to give this a try
563e0f702d1761a701f0f895	X	I edited my question. Please take a look.
563e0f702d1761a701f0f896	X	The problem with this code is that the file, once it is uploaded, is not the correct format. I'm trying to upload a .zip file. public string HttpPost(string uri, string parameter) { WebRequest webRequest = WebRequest.Create(uri);
563e0f702d1761a701f0f897	X	This example how to upload file in MyBucket Take a look on Amazon S3 REST API
563e0f702d1761a701f0f898	X	I'm trying to find out what is the best storage service for my specific problem. The Application I'm developing a mobile app with Xamarin (.NET). Each user has to register and log in to use my service. Each user can be in several Groups where he hast the permission to store files in (each file about 200kb). My Backend is a EC2 instance hosting Cassandra as my database. The Problems I think about using AWS S3 for storing the files. Question #1: Should i directly upload to S3 or should i upload to EC2, handle the permissions and then store it in S3. When using direct upload to S3, i have the advantage of much less bandwith used on my EC2 instance. For direct uploading i have to provide a Token Vending Machine, which has two modes for providing the credentials i need to interact with S3: anonymous and identity. As i read the anonymous approach is mostly user for read-only scenarios. But for the identity approach the user has to register in a browser windows, which is absolutely nothing that i want for my users. The application initiates communication with the TVM by bringing up a browser on the mobile device to enable the user to register a user name and password with the TVM. The TVM derives a secret key from the password. The TVM stores the user name and secret key for future reference. Is it even possible to handle the permissions i need(each user can only upload and download files to groups which he belongs to)only with assigning AWS permissions to the TVM credentials? Question #2: Should i maybe consider storing each file directly in cassandra, since every file is only about 200kb? Problem here is, that the same files could be accessed several times per second.
563e0f702d1761a701f0f899	X	I would use S3. That way you don't have to worry about bandwidth and permissions on the file. You do have interact with the Amazon S3 and IAM Service (Their authorization service). You can do this through the API and your language of choice (Python, Ruby, Java, etc) If you are concerned about being tied to Amazon you can potentially setup something like OpenStack Storage (compatible with the S3 API) in your own datacenter and move your data to it. The files would still be handled by your initial application since your code would be "S3 compatible"
563e0f712d1761a701f0f89a	X	Bummer. "container" basically translates to "bucket" for S3. I was trying to pass the params object via POST but the devil was in the details i.e. the HTTP POST path for upload was looking for the bucket/container in the path itself. /api/Storage/abc/upload meant 'abc' was the bucket.
563e0f712d1761a701f0f89b	X	This definitely nudged me to think in the right direction. Appreciate the help! I have added a comment to my post above.
563e0f712d1761a701f0f89c	X	I have hit a roadblock using the loopback-component-storage with Amazon S3. As a test, I am trying to upload a file to S3 from my browser app, which is calling my loopback API on the backend. My server config for datasources.json looks like: My API endpoint is: ‘/api/Storage’ The error response I am getting from the API is as follows: How do i pass the {“params”: {“Bucket”: “bucket-name”}} parameter to my loopback REST API? Please advice. Thanks much!
563e0f712d1761a701f0f89d	X	AFAIK Buckets are known as Containers in the loopback-component-storage or pkgcloud world. You can specify a container in your URL params. If your target is /api/Storage then you'll specify your container in that path with something like /api/Storage/container1/upload as the format is PATH/:DATASOURCE/:CONTAINER/:ACTION. Take a look at the tests here for more examples: https://github.com/strongloop/loopback-component-storage/blob/4e4a8f44be01e4bc1c30019303997e61491141d4/test/upload-download.test.js#L157
563e0f712d1761a701f0f89e	X	i had already done this things. But still having same problem as above.
563e0f712d1761a701f0f89f	X	Which version of GWT are you using?
563e0f712d1761a701f0f8a0	X	i used GWT 2.4.0
563e0f712d1761a701f0f8a1	X	You cannot use gwt-s3-api-0.9.3.jar with GWT 2.4 . You can fork gwt-s3-api and sanitize/upgrade it to make it compatible with latest GWT before using it in your project.
563e0f712d1761a701f0f8a2	X	This is my code and also include jar file gwt-s3-api-0.9.3. Its give an error as follow:
563e0f712d1761a701f0f8a3	X	You are using mismatched version of jars. com.google.gwt.user.client.HTTPRequest was available in older version of GWT gwt-servlet.jar and gwt-user.jar . It got deprecated in GWT 1.5 and in latest GWT it is eliminated. You should be using GWT RequestBuilder. In Java code. Reference - http://google-web-toolkit.googlecode.com/svn/javadoc/1.5/com/google/gwt/user/client/HTTPRequest.html The third party jar you are using is very old and not updated for 3 years. In your case the MockS3OnlineStorageService.java inside the gwt-s3-api-0.9.3.jar is using HTTPRequest class which is no longer supported in GWT 2.4
563e0f712d1761a701f0f8a4	X	I am trying to set up rudimentary whitelisting for domains. Basically a customer purchases a script that will be served from S3 or another cloud platform and be tied to a single domain. For that I want to automatically whitelist 1 domain per customer license. From what I've read so far, should I opt for S3 I would have to modify my CORS configuration programatically, which is doable according to this question via the REST API: http://salesforce.stackexchange.com/questions/50839/is-it-possible-to-set-amazon-s3-cors-programmatically Now I am wondering: A) Is this the right approach moving forward? (security/technically, not fundamentally) B) Can I deploy something like this with Heroku or are there any other platforms that would simplify this process?
563e0f722d1761a701f0f8a5	X	i have this working code to delete files and folders from s3. how would you delete using wildcard * ?
563e0f722d1761a701f0f8a6	X	Presumably using wildcard * means you want to delete all objects at once rather than one at a time? This is possible via delete_all_objects($bucket, $pcre), where $pcreis an optional Perl-Compatible Regular Expression (PCRE) to filter the names against (default is PCRE_ALL, which is "/.*/i"), e.g.: I've chosen # rather than the usual / as the pattern enclosing delimiter to avoid escaping (not a problem with the single slash here, but it get's messy soon for more complex cases), see Delimiters for details. Please note that deleting multiple objects had not been possible in the past at the Amazon S3 API level and simulated in the AWS SDK for PHP with a for loop within delete_all_objects() as well, i.e. it used one request per object still; fortunately, Amazon has finally introduced Amazon S3 - Multi-Object Delete in December 2011: Amazon S3's new Multi-Object Delete gives you the ability to delete up to 1000 objects from an S3 bucket with a single request. Support for S3 Multi Object Delete has been added to the AWS SDK for PHP shortly thereafter accordingly, see AWS SDK for PHP 1.4.8 "Zanarkand": The AmazonS3 class now allows you to delete multiple Amazon S3 objects using a single HTTP request. This has been exposed as the delete_objects() method, and the delete_all_objects() and delete_all_object_versions() methods have been rewritten to leverage this new Amazon S3 feature. An example for a dedicated multi-object delete (i.e. without wildcards) is shown as well:
563e0f722d1761a701f0f8a7	X	Here is how to delete by prefix (as close to wildcard as I have gotten). call like: _delete_by_prefix_amazon('pdfs/1-')
563e0f722d1761a701f0f8a8	X	Thank you for your response. I'm new to S3 Uploads (as you could probably tell), and the Web API's documentation doesn't really tell me what to do with which of the values. I will get it managed to recreate a Form Upload similar to the one in the Example.
563e0f722d1761a701f0f8a9	X	From a Web API, I receive the following information about an Amazon S3 Bucket I am allowed to upload a File to: Because I am not the owner of the Bucket, I am provided with the s3_policy and s3_signature values, which, according to the AWS Upload Examples, can be used to authenticate a Put request to a Bucket. However, in AWS's official Java SDK I'm using, I can't seem to find a way to perform this authentication. My code: I do understand that I need to use the s3_signature and s3_policy I'm given at some point, but how do I do so to authenticate my PutObjectRequest? Thanks in advance, CrushedPixel
563e0f722d1761a701f0f8aa	X	I don't think you're going to use the SDK for this operation. It's possible that the SDK will do what you need at this step, but it seems unlikely, since the SDK would typically take the access key and secret as arguments, and generate the signature, rather than accepting the signature as an argument. What you describe is an upload policy document, not a bucket policy. That policy, the signature, and your file, would all go into an HTTP POST (not PUT) request of type multipart/form-data -- a form post -- as shown in the documentation page you cited. All you should need is an HTTP user agent. You'd also need to craft the rest of the form, including all of the other fields in the policy, which you should be able to access by base64-decoding it. The form also requires the AWSAccessKeyId, which looks something like "AKIAWTFBBQEXAMPLE", which is -- maybe -- what you are calling the "s3_key," although in S3 terminology, the "(object) key" refers to the path and filename. This seems like an odd set of parameters to receive from a web API, particularly if they are expecting you to generate the form yourself.
563e0f722d1761a701f0f8ab	X	I am creating a website for my projects and I am creating a website to keep them and to allow people to download them from there. How would I perform a download request from s3 from an EC2?
563e0f722d1761a701f0f8ac	X	You need to do the following things. 1) Create a IAM user for the bucket in Amazon s3 where you have your files. 2) Create security policy according to your requirement and associate the policy with the IAM user. 3) Download the file into Amazon ec2 instance using s3 api calls, use the credentials of IAM user to access the bucket files. Refer to the below link to use s3 bucket file: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html 4) Send the file from Ec2 to your user for download.
563e0f722d1761a701f0f8ad	X	Look at S3 static website hosting. Ideally, if your web page is simple enough (some HTML, CSS, JS and links to downloadable project material) then you may not need the complexity of web servers running on EC2. If you genuinely need EC2, for example because you need server-side code, then pick the language/environment of your choice (JavaScript/Node.js, Python/Django, Java/Tomcat etc.) and then read the S3 SDK for the chosen language. It's pretty simple to download objects from S3 using the SDKs, or even the command-line tools.
563e0f722d1761a701f0f8ae	X	The AWS S3 documentation says this about getBucketLocation() which is needed for this to work properly (see michael's notes): "To view the location constraint of a bucket, the user must be the bucket owner." Because we don't own the buckets (we are just given read permissions), this solution seems untenable, and so we just revert to streaming the file through our servers in the case of periods.
563e0f722d1761a701f0f8af	X	I don't know which regions our customers host their buckets, so this sounds problematic. Actually if I manipulated the url, I would request a presigned one first, and rearrange it, but from you say, that sounds just as problematic region-wise... Amazon doesn't have to pass a region qualified url back. Thanks for alerting me to the region issue, I wouldn't have realized it was a problem.
563e0f722d1761a701f0f8b0	X	We will probably just tell our customers not to use periods in their bucket names. Seems odd that Amazon allows it, because of this.
563e0f732d1761a701f0f8b1	X	@Joel the reason the dots are allowed is that if aren't using HTTPS, then you can CNAME example-bucket.example.com -> example-bucket.example.com.s3.amazonaws.com and S3 will pick up the bucket name from the Host: HTTP header. The same applies, with a different CNAME target, for the S3 static web site hosting feature... the bucket name has to match the fully-qualified hostname. Much of the issue is probably related to the way S3 has evolved over the years.
563e0f732d1761a701f0f8b2	X	Also, you can interrogate S3 to learn the location of a bucket, explained in this answer: stackoverflow.com/questions/27091816/…
563e0f732d1761a701f0f8b3	X	I hope this doesn't make it sound like I don't appreciate your help, but this is a hack or maybe better stated as a "workaround". The fact that Amazon returns a url they know can't possibly work, even though they have the information to do make it correct (they have the bucket name, they know it has periods, they know what region it belongs to, they know they are returning SSL url...). They really should fix this. Not your fault, and thanks for the help working around their issue.
563e0f732d1761a701f0f8b4	X	I'm generating presigned urls but its problematic for bucket names with periods and SSL because of the *.s3.amazonaws certificate, as described here: http://shlomoswidler.com/2009/08/amazon-s3-gotcha-using-virtual-host.html Is there a way to generate urls with the following format?: http://s3.amazonaws.com/bucket.name/key I didn't see an option in the API. I'm guessing I can do it "manually" by rearranging the url, but I'd rather avoid a hack if its not necessary.
563e0f732d1761a701f0f8b5	X	There isn't any official way to do this, but string manipulation isn't as sketchy as it seems, since the possible patterns are limited. Bucket names can't contain a slash, and finding the other elements is pretty safe. One important consideration is the region, and how it impacts the valid combinations of URL. Consider "lolcat.jpg" in bucket "example-bucket"... These two urls are equivalent only if the bucket is in the US-Standard region. If not, the top one works, but the bottom one will return an error message telling you that you are using the wrong endpoint. For other regions, you have to use the correct regional endpoint. If us-west-2, this would actually be: So, you have to know the bucket's region before you can transpose elements in the URL. If you are using Signature Version 4, then you already know the region, because it's needed in order to generate the signature. With Signature Version 2, you can even rearrange the position of the bucket name in the URL after signing the URL without invalidating the signature, because of the details of that algorithm.
563e0f732d1761a701f0f8b6	X	OP, are these filenames on the local computer? If so, you can use os.path.isdir to tell whether they're directories. If not, you have to use some approximation (e.g. endswith("/")).
563e0f732d1761a701f0f8b7	X	@katrielalex No they are on amazon s3
563e0f732d1761a701f0f8b8	X	...unless the API can decide to strip the trailing slash...
563e0f732d1761a701f0f8b9	X	@katrielalex: At which point you are up a creek without a paddle and no means to distinguish reliably between files and directories anyway.
563e0f732d1761a701f0f8ba	X	I'd love to hear what is not helpful or wrong about my answer, to deserve a downvote. That way I can improve my answer!
563e0f732d1761a701f0f8bb	X	@MartijnPieters: Up a creek where os.path.isdir doesn't exist? Not my downvote btw.
563e0f732d1761a701f0f8bc	X	@Junuxx: I've edited the title of the question, which was indeed misleading. The body of the question on the other hand shows this is not about local filenames.
563e0f732d1761a701f0f8bd	X	If not advanced nearly in sync one of the tee object will act as a cache.
563e0f732d1761a701f0f8be	X	what is isdir in above code.
563e0f742d1761a701f0f8bf	X	@user1958218: isdir is the result of p.endswith("/")
563e0f742d1761a701f0f8c0	X	but where r u assigning that value to isdir. is that system variable avaiable in python or we need to assign value to it. i can't see where u r assigning value to it
563e0f742d1761a701f0f8c1	X	@user1958218: it is called "sequence unpacking". Here, it is used with a for-loop e.g., for x, y in [(1,2), (3,4)]: print(x) – each tuple is unpacked into (x,y) i.e., it prints 1 and 3.
563e0f742d1761a701f0f8c2	X	This requires the input to be sorted. As it happens, the sample is sorted, but how do you know the real results are still sorted with directories listed last?
563e0f742d1761a701f0f8c3	X	Good point, I've added sorting to my answer.
563e0f742d1761a701f0f8c4	X	I have the list of strings from the Amazon S3 API service which contain the full file path, like this: I want to put partition folders and files into different lists. How can I divide them? I was thinking of regex like this: is there any better way?
563e0f742d1761a701f0f8c5	X	Don't use a regular expression; just use .endswith('/'): .endswith() performs better than a regular expression and is simpler to boot:
563e0f742d1761a701f0f8c6	X	From Filter a list into two parts, an iterable version: It allows to consume an infinite stream of paths from the service if both dirs and files generators are advanced nearly in sync.
563e0f742d1761a701f0f8c7	X	You could use itertools module for item grouping:
563e0f742d1761a701f0f8c8	X	I am using the carrierwave gem to manage file uploads in my rails 3 app, however, I am not able to connect to my amazon s3 bucket. I have followed the instructions on the wiki yet they are not quite detailed enough, for example where do I store my s3 credentials?
563e0f742d1761a701f0f8c9	X	Put something like this in an initializer. You can store your credentials right in the file, if you want (and the code is private). Or from a separate file, or the database, up to you. The following would load a config file and allow different configurations based on the env.
563e0f742d1761a701f0f8ca	X	Check out this quick blog post I wrote about how to do it. Basically there are a few steps, each of which is pretty complicated: Hope this helps!
563e0f752d1761a701f0f8cb	X	Thanks for the tip, Geoff, but unfortunately it's not the case. I've tried to post picture 'a248.e.akamai.net/assets.github.com/images/modules/header/…; and it worked. I've also compated the response headers, Amazon correctly returns Content-Type: image/png.
563e0f752d1761a701f0f8cc	X	I am trying to create a Facebook wall post using Facebook Graph API. The payload is: The post is created properly but the problem in picture which does not displays at all. When I was using public Amazon S3 URLs like http://s3.amazonaws.com/picyou-andrey-development/images/GejoFV/GejoFV.png picture was displayed correctly. Any chance to use dynamic URLs as a 'picture' param with Facebook? Thanks in advance. Update: Found a guy on the Facebook developer forum with exactly the same problem: http://forum.developers.facebook.net/viewtopic.php?pid=302856
563e0f752d1761a701f0f8cd	X	Could it be that the signature part of the url has some chararcters in it that have special meaning in urls - %2B (+) and %3D (=)? I have run into a problem with these urls in some video players and solved it by making sure that the url doesn't have any of those characters. You can do this by generating the url in a loop and adding a second onto the expiry time everytime to ensure the signature changes. Repeat until the signature is 'valid' psuedocode: In my experience it only takes an iteration or two for the url to be valid.
563e0f752d1761a701f0f8ce	X	are you sure your using the correct function to upload something.... i also have a AWS bucket and a simular php library to upload files.... i use the following function $s3->putObjectFile('folderOnMyServer/picture.jpg', $bucketName, 'someFolder/picture.jpg')
563e0f752d1761a701f0f8cf	X	do you use the same S3 PhP library?
563e0f752d1761a701f0f8d0	X	i dont think so but normally they are all pretty much the same.... i got mine from: undesigned.org.za/2007/10/22/amazon-s3-php-class
563e0f752d1761a701f0f8d1	X	i think yout outObject is equal to putObject in my library but then your storing a object.... i dont know if your goal is to store your picture as a object or as a file (jpg or whatever).... also what if you put some error_log("log something") in your code to see where it starts to fail.... maybe it doesnt execute the function at all????
563e0f752d1761a701f0f8d2	X	yeah, I want to store the picture on s3 that the user chooses to upload
563e0f752d1761a701f0f8d3	X	so I added the code but as soon as I paste in my credentials into the (awsAccessKey, awsSecretKey); field the script crashes again.
563e0f752d1761a701f0f8d4	X	oke... so if it only crashes WITH the s3 code and works fine without.... then maybe you can do these 3 things for debugging porposes.....can you check the following.... first: // Check for CURL if (!extension_loaded('curl') && !@dl(PHP_SHLIB_SUFFIX == 'so' ? 'curl.so' : 'php_curl.dll')) exit("\nERROR: CURL extension not loaded\n\n"); .... also do you have the tool "S3 browser" here you can double check if you can connect with your accesskey and secret.... and very last thing... whats your current PHP version?
563e0f752d1761a701f0f8d5	X	Sorry for the delay, I'm running on the latest version of PHP. I also checked, my credentials are correct. I also have CURL installed. Do I paste the credentials in the S3.php file or in to the actual file I'm using at the moment?
563e0f752d1761a701f0f8d6	X	you should be able to use the code as pasted above, i defined my key and secret in 1 place, you can also just replace it with some strings, so no need to put them into the S3.php , just include the file and instantiate it and use the putObjectFile (if you want to dump a jpg or whatever) or putObject (if you want to make use objects, i never it myself though)....... so this still means your script keeps crashing right? pretty sure this is not a AWS problem, any change to just use run the AWS code seperate to double confirm the AWS if fine?
563e0f752d1761a701f0f8d7	X	The script crashes as soon as I paste in my credentials into $s3 = new S3(awsAccessKey, awsSecretKey);
563e0f752d1761a701f0f8d8	X	So my goal was it to implement Amazon S3 image uploads to the PhPDolphin script, unfortunately I've run into a few Issues, if I add the code in the script just doesn't load and since the script doesn't have an error log I'm clueless as to what went wrong, for licensing reasons I'm unable to publish the entire script here but I will post a snipped of the affected area. /includes/classes.php [Default (Just a small snippet of the 4000 lines of code within this file)] And then my edited version that is supposed to upload the images automatically to Amazon S3. /includes/classes.php [edited] (the s3 code is on the far bottom of the snippet) And yes I did add my own access and secret key :) Any help would be greatly appreciated and will be credited! Links to the products and API used in this: [PhPDolphin] [S3.php API on Github]
563e0f752d1761a701f0f8d9	X	This is how you can use the library i am using, i hope it will fix your problems (make sure the folder on the bucket actually exists otherwise just upload something to the root of the bucket)
563e0f762d1761a701f0f8da	X	Digging a bit further, I realized that Amazon Cloudfront is more like a CDN. Am I correct ? Also, does that mean, if I don't want my audio files to be distributed across a delivery network and want to always stream from a static location, Can I do without Cloudfront ? Will I be able to stream my audio only from Amazon S3 ?
563e0f762d1761a701f0f8db	X	So, as long as the file is on the same server as the web application, there shouldn't be any issues. But once I move the files to Amazon, the app shall start to download the files to the VPS and then play ??
563e0f762d1761a701f0f8dc	X	Not only Amazon, be it any remote server or storage service.. Streaming of multimedia objects are done, just not to keep the end user waiting for a long time for the object to download. Take Youtube videos as example.
563e0f762d1761a701f0f8dd	X	@Nannakuhtum - This is not correct. If your point your player at a url on S3, it will be served directly to the browser and will not go via the web server. Also, Youtube uses progressive download for its video service and not streaming. See blog.mydeo.com/2009/01/12/… for more info on streaming vs progressive download
563e0f762d1761a701f0f8de	X	@GeoffAppleford That is not fully incorrect.. The mention of Application server should be replaced with the client where the video is playing.. The blog that u referred was really helpful. An extract from the blog: "A temporary copy of the video file is then stored on the local computer so that the viewer can watch the file over and over without having to download the file each time." About youtube videos, it is not using progressive download, If it is so one cannot seek to the preferred position in the video and not a copy of the video is saved in the client.
563e0f762d1761a701f0f8df	X	@Nannakuhtum - youtube absolutely does use progressive download. Google it. And, you can seek within progressive download videos as long as the server supports byte-range requests.
563e0f762d1761a701f0f8e0	X	Hmm. I got your point, but you would suggest me to use cloudfront, won't you ? Because without using streaming distribution, it is going to be a download of the media rather than stream of the media, isn't it ?
563e0f762d1761a701f0f8e1	X	Cloudfront can be setup as a streaming distribution or as a standard CDN using progressive download. Unless you have a specific reason to use a streaming distribution, I suggest you stick to progressive download. While it will still be a download, rather than pure streaming, the media will start playing as soon as enough has downloaded - eg you dont need to download the entire file before playing it.
563e0f762d1761a701f0f8e2	X	I am new to Streaming Media. I am working on a web application built using Symfony 1.4. It features audio players and I am using jPlayer for the player. I use ffmpeg for encoding the audio files. Currently, I am storing my audio files on the development server. However, I would want to use Amazon S3 Storage Service for storing my audio files. While going through the information available over the WWW and Amzon's site, I came to realize that it would require a streaming distribution service viz. Amazon Cloudfront. At present, I am not using any streaming distribution while I play audio from my server. Is it necessary to use Amazon Cloudfront ? Can't I directly serve my audio files from Amazon S3 by providing a URL as http://s3.mybucket.com/XXX ? What are the consequences of serving files directly from Amazon S3 and not using Cloudfront ? A demo of the player that I am using can be seen here: http://audiodip.org/project/detailsProject/id/5
563e0f762d1761a701f0f8e3	X	If the audio file has to be retrieved from a remote server(in your case S3) you should read and play it as a streaming audio. Otherwise it is like downloading the full audio file to the application server and then playing it. Cloudfront is a content delivery service with which streaming comes of no extra cost. So it is better use the streaming service from cloudfront. and you will have to enable your audio player to play streaming data.
563e0f762d1761a701f0f8e4	X	Can't I directly serve my audio files from Amazon S3 by providing a URL as http://s3.mybucket.com/XXX Yes, you can. If your files on S3 are public then your url will be in the format http://bucket.s3.amazonaws.com/filename, otherwise you need to create a pre-signed url using a GUI tool or one of the AWS S3 API. Just use that url as the audio source in your player. It will be served directly to the browser and will not go via your web server. You could put Cloudfront in front of S3 and it may improve performance but it is not necessary. You certainly don't need to use a streaming distribution.
563e0f762d1761a701f0f8e5	X	Ok thanks, the documentation was indeed confusing. Hope they change it :)
563e0f762d1761a701f0f8e6	X	I receive the same response from Amazon S3 with nil values and my object IS NOT removed
563e0f762d1761a701f0f8e7	X	Im trying to delete an object on S3 using the ruby aws-sdk (version 2). It works fine, but it returns this Which doesnt make sense because in the documentation it says the response should be of the type: resp.delete_marker #=> true/false resp.version_id #=> String resp.request_charged #=> String, one of "requester" Why am I becoming nil? I want to know if the object was deleted or not. I am getting that response both when i succeed in deleting the object and when I dont. This is the code Im using to delete the object:
563e0f762d1761a701f0f8e8	X	Your delete object was successful. The Amazon S3 API only returns those values under certain circumstances. In this case, your object was not in a versioned bucket (no version id or delete marker boolean), and is not configured for request-payer. As a general rule, if the SDK does no raise an error from the response, it is successful. In this case, the API reference documentation may be confusing as it does not clearly indicate that these values may be nil.
563e0f762d1761a701f0f8e9	X	Are your S3 permissions on the bucket/files set appropriately? It sounds like you might have it restricted in a way that allows you to see it and not whomever else you have viewing (e.g. by IP address).
563e0f772d1761a701f0f8ea	X	There are only four options: list, upload/delete, view permissions, edit permissions. I'm not sure which of these will allow anyone to see the images on the website. I'm still very new to S3.
563e0f772d1761a701f0f8eb	X	I'm using carrierwave and fog, and in the initializer I have config.root = Rails.root.join('tmp') config.cache_dir = 'carrierwave' config.fog_credentials = { :provider => 'AWS', :aws_access_key_id => 'ACCESSKEY', :aws_secret_access_key => 'SECRETKEY' } config.fog_directory = 'BUCKETNAME' config.fog_public = true
563e0f772d1761a701f0f8ec	X	When I view the source of the page and click on the image link, firefox brings up a page saying this connection is untrusted and that the certificate is invalid. How do I fix this?
563e0f772d1761a701f0f8ed	X	@Kyle: if you view in non-SSL mode, does it show the image?
563e0f772d1761a701f0f8ee	X	I figured it out, I wasn't storing the correct url, I thought it was the carrierwave uploader's .url method, but it actually was something different when I looked on S3. I changed it to the format of S3 and it worked perfectly!
563e0f772d1761a701f0f8ef	X	CarrierWave was doing the bucketname followed by the standard s3 stuff, but when I looked on S3 it said it was the reverse.
563e0f772d1761a701f0f8f0	X	I recently added photo uploading to a web application that I'm hosting on heroku. I'm using S3 for storage and it's working great, however when other users go onto the site, they see a photo missing icon rather than the photo. On my machine however, I see the photos on the site. Any clues as to what is happening? Thanks!
563e0f772d1761a701f0f8f1	X	When you upload your photos to s3 you need to set their access level to public using whatever s3 lib your using. Here's snippet from s3 api docs: x-amz-acl The canned ACL to apply, to the object that is created after completing multipart upload. For more information, go to REST Access Policy in the Amazon S3 Developer Guide. Type: String Default: private Valid Values: private | public-read | public-read-write | authenticated-read | bucket-owner-read | bucket-owner-full-control http://docs.amazonwebservices.com/AmazonS3/latest/API/mpUploadInitiate.html
563e0f782d1761a701f0f8f2	X	Hello, I appreciate your help, but my intention is not for private use (or have to pay for it). Sorry but that can not solve my problem
563e0f782d1761a701f0f8f3	X	I'm searching along months (maybe 1 year) a way to upload a file to mediafire. I know mediafire use flash, y know mediafire API is so private... But i know too a program called "File&Image uploader" that can upload files to mediafire (HOW?!) my knowledge about networks are very, VERY basics, but i could make a Ruby script to upload images to Imageshack using the API and the "rest_client" gem, so maybe... i can do the same for mediafire... (With help) I tried all... ALL!: curl, wget, wput, ssl, ruby example scripts, python example scripts, perl example scripts, and a lot of unuseful CLI apps for linux and windows thati don't remember the name now... After all, at the moment i don't know the method to upload a file to mediafire. Well, at this point, My questions are: 1ª - Is it possible to upload a file (no matther if in my account or in "free" mode) to mediafire, using ruby or python? And anyone could give me a brief example or tell me the easiest way? 2º - If not possible to mediafire, could you tell me a FREE and GOOD server (I mean without recaptcha) with ability to upload a file using the rest_client gem ? (or another easy way) Thankyou for read.
563e0f782d1761a701f0f8f4	X	So, I'm sure this answer won't be optimal to what the writer desires, but here goes anyhow. If you want to upload files with media content you should really be taking a look at Amazon's S3 API. http://aws.amazon.com/s3/ Yes, you may have to pay at a certain point, but the free point on S3 will equal or exceed the free point on mediafire. The difference is, that on every level S3 is a better delivery method for your media files, and if you're attempting to abuse free offers you will have an equally frustrating time everywhere
563e0f782d1761a701f0f8f5	X	"something wrong with Amazon S3" seems unlikely. How many uploads are you doing, per second, approximately?
563e0f792d1761a701f0f8f6	X	@Michael-sqlbot Its max of 5 per second
563e0f792d1761a701f0f8f7	X	I'm getting the same error at the moment. I'm talking to one of their support people. Will comment/answer back when I have an answer.
563e0f792d1761a701f0f8f8	X	The reply, after a looooong wait, was that it's our S3 account that's at fault. The error is from Amazon but I think FP send back their own string, else I'd expect more than 10 results from Google!
563e0f792d1761a701f0f8f9	X	Yeh, I guess this is kind of limitation from S3, I found it working very next day.
563e0f792d1761a701f0f8fa	X	I'm using filepicker's rest API to upload images, https://developers.filepicker.io/docs/web/rest/#blob-store It was working fine for few uploads, then it started throwing the error - RateLimit Exception for Amazon IO Error I think something wrong with amazon S3, any idea please ?
563e0f792d1761a701f0f8fb	X	not ideal, but unfortunately it doesn't appear that the rightscale or aws ruby sdks support this without dropping to the http level.
563e0f792d1761a701f0f8fc	X	I think it is possible to add this feature within aws-sdk. I didn't have time to complete it yet, but take a look at this gist. It raises an authentication error on s3.client.set_bucket_website.
563e0f792d1761a701f0f8fd	X	hjblok - great thinking! Using the appfog ruby sdk totally solved it!!! Thank you!!
563e0f7a2d1761a701f0f8fe	X	Unfortunately you seem to have misunderstood the question. I want to turn a bucket into a website, using the above described Amazon API. I am looking for a ruby library that supports this operation on a bucket object. The code is obviously not required to run "on S3", as it is a storage solution only, just as you correctly remarked.
563e0f7a2d1761a701f0f8ff	X	Hey Jeevan, Alojscha is trying to make the equivalent of this http request with a ruby sdk wrapper: docs.amazonwebservices.com/AmazonS3/latest/API/…
563e0f7a2d1761a701f0f900	X	I want to set up an Amazon S3 bucket as a website as described here: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTBucketPUTwebsite.html?r=5271 but using an ruby API, preferably the aws-sdk for ruby. Is there a possibility to do that / a library that already supports that? Could not find anything in aws-sdk and right-aws, but maybe I was just blind?
563e0f7a2d1761a701f0f901	X	It is possible to configure your bucket as website using an ruby API. I did found a solution, but this uses the aws-s3 gem, not the aws-sdk gem. I found this solution in the ponyhost gem: EDIT you could also use the fog gem to accomplish this.
563e0f7a2d1761a701f0f902	X	Current version of AWS SDK for Ruby has the method #configure_website for bucket objects. So something like this would work: (the block to configure_website may be omitted if you don't need to set non-default options)
563e0f7a2d1761a701f0f903	X	@Aljoscha AWS S3 is just a storage solution, to store all your files. It doesnt provide any kind of run time solution. You need to have a Ec2 instance to host your ruby based application or either to use ruby API. You can just host a static web site on S3 but cant run any kind of app.
563e0f7b2d1761a701f0f904	X	I have been tasked with copying s3 objects from one bucket to another. The bucket contains millions of objects. The object should only be physically copied if either of two conditions are met. The object does not exist in the target bucket or; The object in the source bucket has changed and no longer is identical to its counterpart in the destination bucket. I am using the 1.5.3 version of the AWS SDK for .NET and I cannot change versions.
563e0f7b2d1761a701f0f905	X	The AWS Command Line Interface (CLI) has an in-built sync operation that will copy files if they do not exist or if they have changed. If you wanted to write your own version of this sync functionality you would have to list objects from both locations, look for differences and then use the Amazon S3 copy API call to copy files between buckets. Using the CLI is a lot simpler!
563e0f7b2d1761a701f0f906	X	This has just saved me a lot of time. Thanks.
563e0f7b2d1761a701f0f907	X	This is not actually the case. It is possible to pipe/stream to s3! you just need to know the size of the upload. If your client can provide that then you can indeed use pipe to upload to s3 without a nasty hard drive write. I'm writing a cli and intermediary server that will upload to s3. Because I control both the client and server I can determine the file size before upload. I think there may be other edge cases like mine that should not be dismissed. I use knox to stream to s3 with a put request.
563e0f7b2d1761a701f0f908	X	@CharlesTWall3 This is a very valid comment, I didn't think about that at the time - I was thinking about a server-side only solution. Feel free to post an answer if you manage to get something working, I'll happily vote for your solution. You may also want to edit this answer. Thanks!
563e0f7b2d1761a701f0f909	X	Will do. Cheerio.
563e0f7b2d1761a701f0f90a	X	@gulthor - thanks for your input. For my situation, i was interested in streaming from mongodb via node app (no browser). Found solution by using "s3-upload-stream" NPM module. It uses S3 multipart API and therefore does NOT require the overall filesize up front. It works in chunks and passes their size automatically. Only took a few mins to copy the example code off the readme and plugin to my app. Gotta love the convenience of NodeJS modules community. There are quite few old SOF posts still out there trying to do this using bespoke solutions that are not ideal. Thanks again for suggestion.
563e0f7b2d1761a701f0f90b	X	I'm trying to stream upload a file submitted via a form directly to an Amazon S3 bucket, using aws-sdk or knox. Form handling is done with formidable. My question is: how do I properly use formidable with aws-sdk (or knox) using each of these libraries' latest features for handling streams? I'm aware that this topic has already been asked here in different flavors, ie: However, I believe the answers are a bit outdated and/or off topic (ie. CORS support, which I don't wish to use for now for various reasons) and/or, most importantly, make no reference to the latest features from either aws-sdk (see: https://github.com/aws/aws-sdk-js/issues/13#issuecomment-16085442) or knox (notably putStream() or its readableStream.pipe(req) variant, both explained in the doc). After hours of struggling, I came to the conclusion that I needed some help (disclaimer: I'm quite a newbie with streams). HTML form: Express bodyParser middleware is configured this way: POST request handler: However, I'm getting the following error: { [RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.] message: 'Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.', code: 'RequestTimeout', name: 'RequestTimeout', statusCode: 400, retryable: false } A knox version of handlePart() function tailored this way also miserably fails: I also get a big res object with a 400 statusCode somewhere. Region is configured to eu-west-1 in both case. Additional notes: node 0.10.12 latest formidable from npm (1.0.14) latest aws-sdk from npm (1.3.1) latest knox from npm (0.8.3)
563e0f7c2d1761a701f0f90c	X	Well, according to the creator of Formidable, direct streaming to Amazon S3 is impossible : The S3 API requires you to provide the size of new files when creating them. This information is not available for multipart/form-data files until they have been fully received. This means streaming is impossible. Indeed, form.bytesExpected refers to the size of the whole form, and not the size of the single file. The data must therefore either hit the memory or the disk on the server first before being uploaded to S3.
563e0f7c2d1761a701f0f90d	X	Any possibility of providing the actual code, that compiles? all of the examples have errors...
563e0f7c2d1761a701f0f90e	X	What do you mean the actual code? The entire file?
563e0f7c2d1761a701f0f90f	X	For example, your samples are missing a required semicolon.
563e0f7c2d1761a701f0f910	X	I am on a phone. I was expecting something to be missing. Sorry. On my actual code there are no 'compiling' errors.
563e0f7c2d1761a701f0f911	X	Notice: $filename = "${file_name}"; and $filename = '${file_name}'; is completely different, as the second does not evaluate $file_name as a variable!
563e0f7c2d1761a701f0f912	X	Where do I set that piece of code? Just above $params->key = ?. According to your the variable $filanem, it is a scope because if I do $params->key = rand_string(5).'${filename}'; it returns the Dogfilename.png. Correct me if I misunderstood 'Scope'.
563e0f7c2d1761a701f0f913	X	Result: i.imgur.com/fmGoD.jpg
563e0f7c2d1761a701f0f914	X	Okay now I get your point. I'll try and do that. I need to find exactly where the putObject is
563e0f7c2d1761a701f0f915	X	Just another point, you can upload normally, and then rename when uploaded?
563e0f7c2d1761a701f0f916	X	As the question states, it is uploaded onto Amazon S3 and thus I am not able to do that. I can upload, copy, replace, delete previous (that will cost me 4 connections instead of 1).
563e0f7c2d1761a701f0f917	X	I get this: 3Sf5f.
563e0f7c2d1761a701f0f918	X	I changed the $filename to '${filename}' and I get this: 3Sf5f.myimage.png
563e0f7c2d1761a701f0f919	X	This means that somewhere down the line, your $filename is losing its structure. Look further up in your code, are you changing $filename anywhere?
563e0f7c2d1761a701f0f91a	X	Notice I'm appending $extension to the params key, not $filename. If you want to override filename after you've got the extension, just do $filename = $extension.
563e0f7c2d1761a701f0f91b	X	No filename exists in the entire file for some reason. The entire app is based on 1 file.
563e0f7c2d1761a701f0f91c	X	Returns nothing sir
563e0f7d2d1761a701f0f91d	X	do you have access to change the html form file?
563e0f7d2d1761a701f0f91e	X	I do have access and your answer returns nothing sir.
563e0f7d2d1761a701f0f91f	X	the variable $file must contain the temp location of the uploaded file.
563e0f7d2d1761a701f0f920	X	$file in this case is the contents of $_FILES['form_field_name']['tmp_name']
563e0f7d2d1761a701f0f921	X	$file for me returns nothing
563e0f7d2d1761a701f0f922	X	It echo's empty so the 2nd part of your answer doesn't work
563e0f7d2d1761a701f0f923	X	I played around with some variables and $filename = "${filename}"; is the equivalent of saying $filename = $filename; It doesn't do anything at all, you are setting a variable equal to itself. The variable $filename never even gets set in that function (getHttpUploadPostParams()). I'm guess from the comments on the function that the filename is in an element of $headers. Try doing a print_r($headers); die; before $filename = "${filename}"; and see what you get;
563e0f7d2d1761a701f0f924	X	Wrong. $filename = "${filename}"; is not the same as $filename = $filename. Because I am using $filename further down and the first way i get the actual filename, the second way I dont.
563e0f7d2d1761a701f0f925	X	It is the same thing create a php file and just put this in it: $filename = 'test'; $filename2 = "${filename}"; echo "filename2: $filename2"; You will get "filename2: test" as the output. I understand you are not getting the same thing but there is some other problem in your code. Did you do a print_r($headers); if so what did you get?
563e0f7d2d1761a701f0f926	X	Returns nothing.
563e0f7d2d1761a701f0f927	X	@jQuerybeast Sorry. I changed that. I wrote the code too fast. :/
563e0f7d2d1761a701f0f928	X	This works but this is not the case. If I change the image.png to "$filename" (which returns the actual filename), return nothing.
563e0f7d2d1761a701f0f929	X	@jQuerybeast Nop. You rename the image.png to $new_name, so it is a random string with extension appended.
563e0f7d2d1761a701f0f92a	X	How do you know it will be .png?. Obviously my question is for all sort of extensions.
563e0f7d2d1761a701f0f92b	X	So on $params->key = $filename; what will I call alongside?
563e0f7d2d1761a701f0f92c	X	This is my entire HTML file: pastebin.com/qXCVr3eD
563e0f7d2d1761a701f0f92d	X	Invalid according to Policy: Extra input fields: ext
563e0f7d2d1761a701f0f92e	X	you could use this directly $params->key = $filename.$_POST['ext']; but I see you can't add an extra field. in that case this solution is not possible. here the extension is evaluated and sent as a part of form data.
563e0f7d2d1761a701f0f92f	X	Short or longer number dont bother that much. Again, your answer wont work if you've read all the answer etc. What if I upload a file with no extension?
563e0f7d2d1761a701f0f930	X	so the problem is in the s3 upload class, i never used this class , but i used this one: the Zend Framework s3 class (you could use it in standalone mode) give a try if you are desperate... framework.zend.com/manual/en/zend.service.amazon.s3.html
563e0f7e2d1761a701f0f931	X	Will that class allow me to reset to anything at any time?
563e0f7e2d1761a701f0f932	X	what do yo mean about "reset"?
563e0f7e2d1761a701f0f933	X	Im really sorry I was probably thinking of something else. I meant, will that class allow me to change the filename, into any random screen and the file extension? Because it seems my problem comes from Amazon's end.
563e0f7e2d1761a701f0f934	X	Doesn't work. Was one of my many tests. Returns nothing
563e0f7e2d1761a701f0f935	X	Have you confirmed if your $filename variable is a valid string?
563e0f7e2d1761a701f0f936	X	I did. I am not sure if my confirmation was correct. If I do $filename = 'someimage.png', I get its file extension
563e0f7e2d1761a701f0f937	X	AFAICT, it appears your issue has more to do with this variable then getting the file extension part.
563e0f7e2d1761a701f0f938	X	This is the entire file: pastebin.com/EXfJwpny It is the only class that exists in my app. Search for $filename. Only one exists in the class
563e0f7e2d1761a701f0f939	X	syntax error, unexpected T_UNSET
563e0f7e2d1761a701f0f93a	X	Missing Semicolon.
563e0f7e2d1761a701f0f93b	X	The entire file: pastebin.com/EXfJwpny
563e0f7e2d1761a701f0f93c	X	Warning: implode() [function.implode]: Invalid arguments passed on your edit
563e0f7e2d1761a701f0f93d	X	Wow. I get this: gVcwVcfcd208495d565ef66e7dff9f98764da. Note the file extension still missing. This is frustrating
563e0f7e2d1761a701f0f93e	X	Doesn't work. Only the first one.
563e0f7e2d1761a701f0f93f	X	After the first line try vardump($temp). Maybe there is something in your string that doesn't normally show up.
563e0f7e2d1761a701f0f940	X	I am using the Amazon S3 API to upload files and I am changing the name of the file each time I upload. So for example: Dog.png > 3Sf5f.png Now I got the random part working as such: So I set the random_string to the name parameter as such: Now my problem is that this wont show any extension. So the file will upload as 3Sf5f instead of 3Sf5f.png. The variable $filename gives me the full name of the file with its extension. If I use $params->key = rand_string(5).'${filename}'; I get: So I tried to retrieve the $filename extension and apply it. I tried more than 30 methods without any positive one. For example I tried the $path_info(), I tried substr(strrchr($file_name,'.'),1); any many more. All of them give me either 3Sf5fDog.png or just 3Sf5f. An example of what I tried: . . The entire class file: http://pastebin.com/QAwJphmW (there are no other files for the entire script). What I'm I doing wrong? This is really frustrating.
563e0f7e2d1761a701f0f941	X	The variable $filename (and thus "${filename}") is NOT IN SCOPE at line 1053 of your code (line numbering based on raw paste from pastebin). So, no matter what you do, you'll never find the extension of a variable that does not exist. And I've finally worked out what you're doing. I presume this is an extension of PHP: Rename file before upload Simple answer: you can't do it as you envisage.Why - the '$filename' is not parsed at the time that URL is created, but the variable is passed to Amazon S3 and handled there. The solution So, the only option I can think of is to have use the "successRedirect" parameter to point to another URL. That URL will receive the "bucket" and "key" as query parameters from Amazon (http://doc.s3.amazonaws.com/proposals/post.html#Dealing_with_Success). Point that to a PHP script that renames the file on Amazon S3 (copy + delete), then redirects the user to another success screen. So, in your code, line 34, That will do exactly what you want. In response to your comments "Is this the only way - what about the costs as Amazon charge per request?" Delete requests are free. No data transfer costs when moving on the same bucket (or even in the same region). So this solution (which is the only way without you transferring to an intermediate server, renaming and uploading) it doubles the cost of upload a from 1c per 1000 uploads to 2c per 1000 uploads. It's taken me 10 minutes @ $200/hour to find that out and respond = $33 = 1,666,666 uploads! Costs pale a bit when you do the maths :) Compare with the other solution: do a post to an webserver, rename the file and then upload from the webserver: you move all the bandwidth from the clinet tdirectly to yourself - twice. And this also introduces risk and increased possible failure points. In response to "Doesn't work. I you upload a file then the old one gets deleted" I would assusme this is not a problem as you upload a file and then rename it within a second or two. But if you want ot gurantee each file gets uploaded, then you need to do a lot more than create a random filename anyway:
563e0f7e2d1761a701f0f942	X	This explode() divides up the file name into an array with periods as delimiters, and then grabs the last piece of the array (incase a file name is foo.bar.jpg), and puts a period in front of it. This should get you the desired extension to append it to the rand_string(5).
563e0f7e2d1761a701f0f943	X	I think something as simple as below should work to extract file extension from the file-name:
563e0f7e2d1761a701f0f944	X	if you're uploading images try this
563e0f7e2d1761a701f0f945	X	What happends if you: Do you get something like 'Dog.png'? If you don't there is something wrong in the way you are getting the filename. If you do get something like 'Dog.png', here is what I use to get the file extension. Then you should be able to do this:
563e0f7e2d1761a701f0f946	X	You need to first find out what the original extension is and not rename the entire file. So keep the extension and rename de file name. Assuming you have image name in $image_name:
563e0f7e2d1761a701f0f947	X	ok here's another try that I used when I had trouble getting the extension on the server side. what I did was, I used javascript to extract the file extension and the send it via post. in the next php file you can directly use $_POST['ext'] as extension. hope that helped. let me know if you have any trouble implementing this
563e0f7f2d1761a701f0f948	X	i am using this in my websites (and works fine for years): your function generates too short filenames (5 characters), this way creates longer filenames, avoiding to collide the file names. example output: aff5a25e84311485d4eedea7e5f24a4f.png
563e0f7f2d1761a701f0f949	X	It appears what's actually going on is rather than fully producing the filename right now, you're in effect passing a very small 'program' through the interface so it can then produce the filename later (when the variable $filename exists and is in scope). The other side of the interface eventually executes that 'program' you pass in, which produces the modified filename. (Of course passing a 'program' to something else to execute later doesn't tend to make debugging real easy:-) (It's of course up to you whether you want to "make this work" or "do it a different way". "Different ways" typically involve renaming or copying the file yourself before you even try to invoke the upload interface, and are described in other answers.) If you decide you want to "make it work", then the entire filename parameter needs to be a program, rather than just part of it. This somewhat uncommon functionality typically involves enclosing the entire string in single quotes. (You also need to do something about existing single quote marks inside the string so they don't terminate the quoted string too soon. One way is to quote each of them with a backslash. Another way that may look cleaner and usually works is to replace them with double quotes.) In other words, I believe the code below will work for you (I haven't got the right environment to test it, so I'm not sure). (Once you get it working, you might want to revisit your naming scheme. Perhaps the name needs to be a little longer. Or perhaps it should include some identifiable information {like today's date, or the original name of the file}. You may hit on something like $file_base.rand_string(7).$file_extension.
563e0f7f2d1761a701f0f94a	X	Untested, but simple enough to work: will return the extension part (without the '.')
563e0f7f2d1761a701f0f94b	X	A simple solution to re-name a file and compute the extension: Note, that md5() is always 32 bytes long and non unique regarding the computed value. For for many practical instances, it's unique enough. Addendum Additionally, you may use this solution to trace variable changes: A sample use case:
563e0f7f2d1761a701f0f94c	X	How about this?
563e0f802d1761a701f0f94d	X	No exception is thrown? What is the status code of the request?
563e0f802d1761a701f0f94e	X	No exception, the image is uploaded and response is 200 OK.
563e0f812d1761a701f0f94f	X	When you say the image was uploaded successfully, did you also compare the md5sum? After uploading successfully, try comparing the md5sum of the object you uploaded. This way you can be damn sure that the object was not changed at all. Also you say when I try open the image but it is damaged.. are you trying to view it on desktop?
563e0f812d1761a701f0f950	X	I try to open the browser and on the desktop too!
563e0f812d1761a701f0f951	X	@A.Anderson Do you have a sample (any public link?) of the image/object you are trying to upload?
563e0f812d1761a701f0f952	X	Thanks for answer, I believe that be some wrong with my request body, because the request is performed ok, but I'm using Retrofit 2, TypedFile was removed from lib, and I cant make downgrade, I need use the version 2.
563e0f812d1761a701f0f953	X	Have you considered leaving off the explicit Content-Type and Content-Length headers, and letting Retrofit obtain them from the RequestBody ?
563e0f812d1761a701f0f954	X	ok, I'll try this!
563e0f812d1761a701f0f955	X	This does not work AWS api rest requires that this header is sent.
563e0f812d1761a701f0f956	X	It make sense, I'll trying this approach later!
563e0f812d1761a701f0f957	X	So I tried, but does not work well. The image is not sent right!
563e0f812d1761a701f0f958	X	their approach seems ok, I figured out that the s3 AWS need that content is to be sent in the request body and not as multipart.
563e0f812d1761a701f0f959	X	I'm trying upload a Image from my Android APP to Amazon AWS S3 and I need use AWS Restful API. I'm using Retrofit 2 to make to the request. My application is connecting successfully with Amazon S3 and performing the request as expected, but when I try to view the Image from the Bucket, the picture does not open. I downloaded the Image to my pc and tried to open but keep getting the message that the image is corrupted. Lets see my complete code bellow. My Gradle dependencies Here is created a File and starts the request Retrofit Interface Utils class to the mount the credentials Lastly the method to make a request I appreciate any helps, thanks in advance!
563e0f812d1761a701f0f95a	X	I haven't used Retrofit 2, just Retrofit 1, so YMMV, but I believe that the typical way to do what you're trying to do is to use TypedFile where you are attempting to use RequestBody. I'm guessing that Retrofit uses RequestBody internally. You would create the TypedFile something like: and your interface would be: There's a decent example at https://futurestud.io/blog/retrofit-how-to-upload-files/
563e0f812d1761a701f0f95b	X	You are sending a multipart payload, but forcing the Content-type to be image/jpeg. Your jpg is corrupt because S3 probably saved the multipart headers into your jpg file since you told it the whole message was a JPG. Since you do not actually have multiple parts to send, you can drop the Multipart annotation and use Body instead of Part for your RequestBody You should also be able to remove explicitly setting the Content-type and Content-length headers.
563e0f812d1761a701f0f95c	X	I have the same problem, and as I use Fiddler checked the HTTP request content, I found retrofit 2.0.0 beta1 has a different with 1.9.0. In my problem, the different of HTTP request content prevent server get the correct data. In order to make a same HTTP request content, i do next steps using retrofit 2.0.0 deta1. In the retrofit service, add a form-data header for the http request; int retrofit 2.0.0 deta1, the header using @Multipart will get a data like this: Content-Type: multipart/mixed as the deafult value is mixed, and has no boundary title. Do not using @Multipart to upload file, just using @Body RequestBody if you using @Multipart to request Server, you have to pass param(file) through @Part(key), then a new problem you will get. May be retrofit 2.0.0beta1 has a BUG ..., @Multipart generate a bad http request compile with 1.9.0. When you call the method, you need pass MultipartRequestBody to @Body RequestBody Using MultipartBuilder to create a MultipartRequestBody, when you new MultipartBuilder, call this consturt: the param is you set int @headers(boundary=) This method will help form a data like below int HTTP request content: Content-Disposition: form-data; name="imgFile"; filename="IMG_20150911_113029.jpg" Content-Type: image/jpg Content-Length: 1179469 RequestBody value is what you has generate in your code. I just resolve this problem temporary. Hope can help you!
563e0f852d1761a701f0f95d	X	Do you control the file and folder names? If so, you could have your script probe how many images there are (i.e. check if 012/99.jpg exists etc) to avoid having a list of files.
563e0f852d1761a701f0f95e	X	it was my first choice (yes, I can choose the file name I prefer), and I still love it very much, but I'd like to show always the last modified files first AND I can't load all files data since they are too many and it would be slow. I though to choose consecutive numbers as filenames, but it would be difficult to handle in case of modification/erase of a file (last modified should always be the first in the slideshow)
563e0f852d1761a701f0f95f	X	I don't see a way to order the results with that SOAP API call. So it still doesn't seem to satisfy your need.
563e0f852d1761a701f0f960	X	From my test, it seems that the xml is ordered by last modification, even if I didn't found a clear indication about that.
563e0f852d1761a701f0f961	X	If that's the case then I'd say this is your best bet as much as I dislike SOAP.
563e0f852d1761a701f0f962	X	Eventually I will use the REST API GET to create list.html file, however, thank you.
563e0f852d1761a701f0f963	X	It could be THE solution, but I'd like to get an ordered list with newest (and last modified)files first and it should not deliver the entire list, since it could be very large. I have to think how to choose the filename to make it possible through the "prefix" parameter
563e0f852d1761a701f0f964	X	Hypothesis: I have thousands of images into different folders in an amazon S3 bucket. I'd like to make them accessibile to unlogged users as slideshow, but I don't want to deal with db and server poor performance (in case of too many users at the same time) , so I'd like to use only javascript. The problem is that I should however deliver to the client the file list, since I can't use XMLHttpRequest to fetch and parse the xml file that Amazon provides when you try to browse a bucket because (I expect) the browsing page should be located on my webserver. I think I should write some server-side code to create,after every upload/modification, an updated filelist to share with users, but I'm not sure it's a good idea. Can anybody suggest me the best way to proceed? Happy New Year!
563e0f852d1761a701f0f965	X	Possible answer, tell me what do you think about: Amazon provides ListBucket operation http://docs.amazonwebservices.com/AmazonS3/latest/API/SOAPListBucket.html I can choose how many results to get at once using max-keys and marker (for pagination) parameters (example: http://download.terracotta.org/?max-keys=5). I will obtain a xml file (as smallas I want) that I can parse locally with js in a "list.html" file, for example. I could then include this list.html file (that should print just the definition of an array of images) in a iframe included in my slideshow.html file on my webserver. Too dirty?
563e0f852d1761a701f0f966	X	The Amazon S3 JavaScript API has a method, bucket.list() that will list the contents of a bucket.
563e0f862d1761a701f0f967	X	Thanks for the answer Geoff, unfortunately it doesn't seem to work either. I'll do a bit more fiddling around and see if I've overlooked something somewhere.
563e0f862d1761a701f0f968	X	@davee - Well in that case, I suspect its because that library hasn't been updated to support the new server side encryption. Why not use the official PHP SDK?
563e0f862d1761a701f0f969	X	I thought that it could have been the class too, but then I looked at the source and see the headers are set like so: if (is_array($requestHeaders)) foreach ($requestHeaders as $h => $v) $rest->setHeader($h, $v); so in theory adding the server-side encryption should work. This leads me to think that perhaps amazon hasn't fully implemented the feature across all its systems yet. Also, unfortunately the code I inherited heavily relies on this class and I don't have the time to update all the code. I'll see if I can get a response from someone from amazon and post my findings here. Cheers.
563e0f862d1761a701f0f96a	X	I should note that the above works perfectly now.
563e0f862d1761a701f0f96b	X	I have decide to avail of amazons new server-side encryption with s3, however, I have run into a problem which I am unable to resolve. I am using the s3 PHP class found here : https://github.com/tpyo/amazon-s3-php-class I had been using this code to put objects originally (and it was working) : I then did as instructed here : http://docs.amazonwebservices.com/AmazonS3/latest/API/index.html?RESTObjectPUT.html and added the 'x-amz-server-side​-encryption' request header. But now when I try to put an object it fails without error. My new code is : ); Has anybody experimented with this new feature or can anyone see an error in the code. Cheers.
563e0f862d1761a701f0f96c	X	That header should be part of the $metaHeaders array and not $requestHeaders array. Here's the method definition from the docs: You might also consider using the SDK for PHP?
563e0f862d1761a701f0f96d	X	We can upload files with encryption using the code following $s3->create_object($bucket_name,$destination,array( 'acl'=>AmazonS3::ACL_PUBLIC, 'fileUpload' => $file_local, 'encryption'=>"AES256")); And you can download latest sdk from here
563e0f862d1761a701f0f96e	X	With the official SDK: Source: http://docs.aws.amazon.com/AmazonS3/latest/dev/SSEUsingPHPSDK.html
563e0f862d1761a701f0f96f	X	Why don't you just implement an ordered dictionary?
563e0f862d1761a701f0f970	X	OK, but then how would I parse a String representing a JSON object into an OrderedDictionary? Why not just use an Array of tuples instead of an OrderedDictionary? I've updated my question.
563e0f862d1761a701f0f971	X	In Swift, is it possible to parse a String representing a JSON object that only contains strings into an Array of tuples [(String, String)] (not a Dictionary<String, String>)? I'm programming my iPhone app to forward the presigned post response from my server to Amazon S3, which requires the clients to preserve the ordering of the fields. This is an example JSON string, with whitespace added for presentation: According to Amazon S3: API Reference: Authenticating Requests in Browser-Based Uploads Using POST, I think it's safe to assume that none of the strings will contain ", :, or whitespace characters. So, I guess my question is: How do I parse a String like the one above into an Array of tuples [(String, String)]?
563e0f862d1761a701f0f972	X	it looks quite simple to try and find out..
563e0f872d1761a701f0f973	X	If i create a temporary url to an s3 object, can I then change the domain to my cloudfront distribution and have the content still be available from cloudfront?
563e0f872d1761a701f0f974	X	It won't work if you simply change domain to CloudFront distribution in your S3 temporary url. The approach is different with CloudFront. See the Serving Private Content chapter of CloudFront Developer Guide.. The most relevant part: Use the CloudFront control API to create a CloudFront origin access identity. For more information, see Creating a CloudFront Origin Access Identity. Use the Amazon S3 API (or your favorite Amazon S3 tool) to update the ACL on your private objects to give read permission to the CloudFront origin access identity you just created. For a list of Amazon S3 tools you can use, go to Amazon CloudFront Developer Tools. For more information about setting the ACL, see Modifying the ACL on Your Private Content Objects. Set up a private content distribution or streaming distribution (either create a new one or update an existing distribution). For more information, see Setting Up a Private Content Distribution and Streaming Distribution. Use the Amazon S3 API (or your favorite Amazon S3 tool) to update the ACL on your private objects to remove any read permission grants for the public, leaving the read permission for the CloudFront origin access identity. For more information, see Modifying the ACL on Your Private Content Objects. You can stop here if you simply want to serve private content with basic URLs. Continue if you want to use signed URLs. Use the AWS web site to create a key pair and download the private key, which you'll use to sign the URLs. For more information about creating your key pair, see Creating a Key Pair. Update your private content distribution or streaming distribution to specify that the distribution's URLs must be signed, and who can sign them. For more information, see Requiring Signed URLs. Create a signed URL to give the end user. For more information, see Creating a Signed URL.
563e0f872d1761a701f0f975	X	Just tried it out, they do work. Note this is not for signed urls. just the general url to the object.
563e0f872d1761a701f0f976	X	I'm developing a Cordova application to interact with Amazon S3. I have a png image stored in S3. I'm using getObject function from AWS Javascript SDK to retrieve that image. I have an array of uint8array in the data.body property of the retrieved object. Is there any way to save this array as an image file? maybe with File System plugin from Cordova? or maybe something like a FileWriter( as FileReader from Mozilla Web API Interfaces)
563e0f872d1761a701f0f977	X	what is the error message?
563e0f872d1761a701f0f978	X	It throws an error on the require keyword.
563e0f872d1761a701f0f979	X	I'm trying to call a method listObjects on my Amazon S3 Bucket using the Node.js SDK. However I am getting an error on the keyword require. The API I am using is this and this. I also have installed AWS-SDK via NPM. The version of node I have is v0.10.18. Assistance is appreciated.
563e0f872d1761a701f0f97a	X	You first have to install the aws sdk by calling "npm install aws-sdk" in a command-line, from your script directory.
563e0f872d1761a701f0f97b	X	I have uploaded several files to Amazon S3 using boto. However, I failed to set a lifecycle using statement (I know this can be done using the AWS Management Console, but I need to allow each user to decide how long want to keep the file). The boto API reference for S3 properly documents configure_lifecycle(lifecycle_config, headers=None) as the solution, but I'm unable to configure this. Can anyone correct my code? Thanks!
563e0f872d1761a701f0f97c	X	You aren't showing where "lifecycle_config" comes from in this example. However, what you should do is create a Lifecycle object, like this: See class boto.s3.lifecycle for details about the Lifecycle object and what the above parameters mean. Once you have a Lifecycle object, you can then use that in the call to configure_lifecycle(), like this:
563e0f872d1761a701f0f97d	X	I am trying this code to upload file of size 107MB. but file does not get upload on s3, when I check log on s3, it shows me for some parts of this file, "IncompleteBody". How do I deal with that??
563e0f872d1761a701f0f97e	X	This is what it shows in log : REST.PUT.PART 6/4/5_34fa2f47f8f2e3d/645_d48b407b9d44efc.mp4 "PUT /my.bucket/6/4/5_34fa2f47f8f2e3d/645_d48b407b9d44efc.mp4?partNumber=5&uploadId=C‌​9utxlgdMqWCKrYuOEWyE2TOANRjx9r8YnDL3YV53kHVjEAGQ55U3IjqxodIAJRlbdQS8Fd5kWhHdlL_8k‌​g-- HTTP/1.1" 400 IncompleteBody 260 5242880 3356 - "-" "aws-sdk-php2/2.8.20 Guzzle/3.9.3 curl/7.30.0 PHP/5.3.28" I have also tried this solution " docs.aws.amazon.com/AmazonS3/latest/dev/usingHLmpuPHP.html"; but its also giving same problem
563e0f882d1761a701f0f97f	X	I tried to upload large file into Amazon s3 using PHP. I have found nice solutions on various forums but these solutions are for SDK version 1 . http://docs.aws.amazon.com/AmazonS3/latest/dev/LLuploadFilePHP.html Of course, I have found examples on Amazon API documentation. This example expects file on local disk and can not handle with input stream. I couldn't find similar examples for the SDK for PHPv2 as shown in first link. Did someone solved similar problem successfully?
563e0f882d1761a701f0f980	X	I recently just prepared a code sample for this. In this example I am using a file, but you can use a stream as well.
563e0f882d1761a701f0f981	X	Thank you for your answer and explanation!
563e0f882d1761a701f0f982	X	You're welcome @osc! Welcome to SO and don't forget to upvote & accept answers if we've helped you out!
563e0f882d1761a701f0f983	X	I tried to upvote but it says that I dont have enough points!
563e0f882d1761a701f0f984	X	My doubt is the title of my question: I'm studying AWS, and I'm not understanding if when we use the S3 Glacier Storage, are we then using the Amazon Glacier Service or is the Glacier Storage Service just a property of Amazon S3?
563e0f882d1761a701f0f985	X	Yes. Data from an S3 bucket can copy data into Glacier archive storage. This is exactly the same as using a Glacier storage archive directly behind the scenes. When using the lifecycle properties of an S3 bucket and creating an archive, you are using both services together. They have different use-cases, speed & access capabilities, and pricing. They work together very seamlessly, but are in fact separate services. You can take a look at the Amazon S3 FAQ's Amazon Glacier section to get some additional info. With that being said: be aware that if you archive FROM S3 to Glacier then you'll be using only the S3 APIs to access the Glacier archive. They are still in fact different services, but when you perform an archive operation from an S3 bucket it creates a mapping for you. However, if you create an archive directly in Glacier bypassing S3, then you can use the separate Glacier API.
563e0f882d1761a701f0f986	X	My site is using photos stored on Amazon s3, but I have the following problem; Chrome sometimes loads the image, and sometimes doesn-t load the same image. When it doesn-t load I get this in the console... Firefox also sometimes loads the image and sometimes doesn't load the same image. I haven't checked other browsers. my s3 bucket metadata is set to my upload php code is as follows NB all photos have been reworked as jpg's (extension .jpg) I have looked at this SO question, but I have the API call set properly (right?) Amazon S3 is not serving files correctly NB2 because amazon saves the link to the file as HTTPS this I have gets blocked, so I call the link using HTTP only.
563e0f882d1761a701f0f987	X	Thanks! It works. I did it in some other (wierd) way:
563e0f882d1761a701f0f988	X	<code> s3Client.putObject(targetBucketName, key ,s3Client.getObject(sourceBucketName, key).getObjectContent() ,s3Client.getObjectMetadata(sourceBucketName, key));</code>
563e0f882d1761a701f0f989	X	@aviad - I'm not sure, but that looks like you are downloading the file to your computer before uploading again to the new bucket, rather than just copying it within S3 itself.
563e0f882d1761a701f0f98a	X	Does anybody know is it possible to programatically transfer files stored on amazon s3 from one region to another?
563e0f882d1761a701f0f98b	X	This is easily accomplished using the Amazon S3 API to copy the object from one bucket to another. It doesn't matter that the buckets are in different regions. Here's an example using the Rest API. Or if you prefer, the SDKs can do the same thing. Here's a .Net SDK example. If you mean that you want to change a buckets region, you would have to: Of course most of the major S3 GUI tools can also copy objects between buckets and regions too.
563e0f892d1761a701f0f98c	X	What about setting version to earlier than latest? Didn't try it, just a shot in the dark.
563e0f892d1761a701f0f98d	X	@zaak Yes, I tried that as well. No luck.
563e0f892d1761a701f0f98e	X	Can you compare how the gsutil auth header (viewable with "gsutil -D ...") differs from the one sent by the PHP SDK?
563e0f892d1761a701f0f98f	X	Same problem here, eventually introduced my own GoogleStorageServiceProvider which uses the league/flysystem-aws-s3-v2 library instead of v3
563e0f892d1761a701f0f990	X	@JorisBlaak Thanks. I'm going to avoid Google Storage for now until they either provide better documentation or update their support for the lastest stable Amazon SDK.
563e0f8a2d1761a701f0f991	X	I feel pretty odd posting this here but since SO is the only official channel for Google API support, I guess I need to ask it here. If you have API, tool usage, or other software development-related questions, search for and post questions on Stack Overflow, using the official google-cloud-storage tag. Ok, so here goes. I've spent the better part of two days working on trying to get Google Storage to work on the v3 version (latest) of Amazon's PHP SDK. I can't use an older version of the SDK because I'm trying to stick to Laravel 5.1's filesystem without having to write a brand new driver for Google Storage. I believe this is within the spirit of what Google advertises for Google Storage: https://cloud.google.com/storage/docs/migrating In a simple migration from Amazon S3 to Google Cloud Storage, you can use your existing tools and libraries for generating authenticated REST requests to Amazon S3, to also send authenticated requests to Google Cloud Storage. The changes you need to make to your existing tools and libraries are described in this section. To get set up for a simple migration do the following: Set a default Google project. Get a developer key. In your existing tools or libraries, make the following changes: Change the request endpoint to use the Google Cloud Storage request endpoint. Replace the Amazon Web Services (AWS) access and secret key with the corresponding Google Cloud Storage access key and secret key (collectively called your Google developer key). That's it! At this point you can start using your existing tools and libraries to send keyed-hash message authentication code (HMAC) requests to Google Cloud Storage. What a pitch! Let's give it a try using Interoperability credentials that work using gsutil. Doesn't work. You get an "Incorrect Authentication Header". Let's take a look at that header. AWS4-HMAC-SHA256 Credential=GOOGGUxxxxxxxxxxx/20150611/US/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=9c7de4xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx I created a SO post about this at this point, and someone suggested that I add 'signature' => 'v2'. Google Storage Incorrect Authorization Header with Amazon S3 PHP SDK v3 Let's try that: No luck. Same error. The authorization header hasn't changed. Let's look at S3Client's code and see how 'signature' gets used: It doesn't. So now we're deviating from S3's official documentation because they say the same thing: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/configuration.html It's not 'signature', it's 'signature_version'. Let's change that to v2. At least we get a different error this time! UnresolvedSignatureException in SignatureProvider.php line 61: Unable to resolve a signature for v2/s3/US. Valid signature versions include v4 and anonymous. So, after toying with this for two days it looks like it isn't possible, at least not with the ease that Google wants you to believe in their pitch. I can't get this to work at all, so I'm hoping someone here can shed some light on this. Either I missed something important, or Google is falsely advertising that Google Storage works using Amazon's S3 SDK and wasting developers' time. I'm thinking that maybe we have to manually hijack the authorization header, but that's outside of my expertise. Any help would be greatly appreciated.
563e0f8a2d1761a701f0f992	X	So I'm going to post later than never and say that I'm using laravel 5.1, you have to make a choice on whether you want to go with AWS or GCS at the time of configuration as you can't install two versions of the aws php client (which limits your choice of flysystem to v2 or v3. GCS requires v2 for the time being while the s3 implementation in laravel requires v3. I spent hours trying to get v3 to work with GCS but the authentication headers are different so I didn't bother. You do have to provide your own provider, but it's not that difficult to setup. Just create GcsAppsServiceProvider.php in app/Providers Essentially in your config you're just duplicating the s3 config in filesystem.php and changing region -> base_url. You're also free to change the default and cloud to gcs to support google cloud storage. Last but not least you also need to add the provider to the provider list in app.php The AWS S3 adapter also mentions that GCS supports it in their documentation.
563e0f8a2d1761a701f0f993	X	I could use the system.getInfo("deviceID") for local as well, which I think is the better approach.
563e0f8a2d1761a701f0f994	X	I am currently developing a mobile app using the Corona SDK and Lua. I need to store information about each user and load the information for the current user when they load the app. What is the best method to store this information for each user, and how would I get this data on app load. I was thinking about using sqlLite and having a single row for each user. However when the user re loads the app I would have no way of accessing the data for the current user because when the app loads I would need something to index the database. Is there any way I can get some information from the mobile device on app load to index the database? Any ideas or suggestions?
563e0f8a2d1761a701f0f995	X	I'm not clear on where the database is stored. If it's remote on a server just use the device ID system.getInfo( "deviceID") If it's local and you have multiple local users then use a login. You could use a registration process for either instance and store The registration keys for automatic access when launching the app.
563e0f8a2d1761a701f0f996	X	take a look at amazon's services. like S3,simpleDB, dynamoDB.. Some implementations are already available in Code Exchange http://developer.anscamobile.com/code/amazon-simpledb-api http://developer.anscamobile.com/code/amazon-s3-rest-api-implementation-corona
563e0f8a2d1761a701f0f997	X	Is there a way to create a CloudFront signed url that limits the number of times that a file can be downloaded? According to this post Controlling number of downloads on Amazon S3, you can get the number of file downloads via the cloudfront api (but it cant find any reference to this on the amazon site) Has anyone managed to achieve this via CloudFront?
563e0f8a2d1761a701f0f998	X	Yes, with CloudFront you can serve Private Content. Basically you can protect your content in two ways: Require that your users use special CloudFront signed URLs to access your content, not the standard CloudFront public URLs. Require that your users access your Amazon S3 content using CloudFront URLs, not Amazon S3 URLs. When you create signed URLs for your objects, you can specify:
563e0f8a2d1761a701f0f999	X	i've used s3fuse in the past to 'mount' s3 as a filesystem. I thought that when you did a mkdir /var/mys3mount/newfolder/ it only created the folder in the local file system, and not in S3. It was giving the appearance of a folder existing but s3 wouldn't see it until you put an object there. Then giving you the full path in the folder concept.
563e0f8a2d1761a701f0f99a	X	Hi Greg, I've seen the same behavior with riofs when using mkdir locally, but I've also seen that when a prefix object exists on the S3 bucket, riofs will create a local directory to mirror this. This can be useful when automating things around these "mounts".
563e0f8b2d1761a701f0f99b	X	It is usually explained that there are no folders/directories on S3, which is true, however there are PREFIX objects. In some cases - e.g. using riofs to "mount" a filesystem to S3 - it could actually be useful to know how to create one of these. Does anyone know if there is a "correct" way to do this with the AWS CLI? It's likely possible using the low-level API aws s3api ... Related SO posts: amazon S3 boto - how to create folder? Creating a folder via s3cmd (Amazon S3) P.S. I also want to point out that in the AWS console this action is actually named "Create Folder...", so it's not really fair to tell people that there is no "concept" of a folder on S3. Many thanks!
563e0f8b2d1761a701f0f99c	X	There really, really aren't directories in S3. However, as you point out, there are prefixes and delimiters and the default delimiter is / which allows you to get a pretty convincing simulation of a hierarchical directory structure in an S3 bucket. But the bucket is still just a flat space containing objects with key names. If you want to create a directory you have to create an object with a key whose name includes or ends with a delimiter character (/ by default). So your technique described above may not feel right but it is the only way. And whoever came up with the Create Folder idea in the console should be ashamed of themselves. It causes a lot of confusion.
563e0f8b2d1761a701f0f99d	X	After some quick fiddling around I found this seems to work: aws s3api put-object --bucket test --key dir-test/ But it only works if you include the "/" at the end of the argument to --key. That part just didn't feel right... surely there's a better way?
563e0f8b2d1761a701f0f99e	X	Thanks David. I've spent some time with the getting started guide and the docs and samples but so far it's been a slow process to get to a point of seeing suitable sample code. Amazon seem to have written the docs for people who are going to learn everything and spend a lot of time with it - I don't have that luxury, I just want to shove a file in, and do other stuff. Looks like I need to put a lot more time in which is a shame.
563e0f8b2d1761a701f0f99f	X	I know what you mean. I'm going on three days now trying to find an easy to understand example for uploading images securely to s3.
563e0f8b2d1761a701f0f9a0	X	To be honest I think they should improve the documentation!
563e0f8b2d1761a701f0f9a1	X	I need to upload a bitmap to Amazon S3. I have never used S3, and the docs are proving less than helpful as I can't see anything to cover this specific requirement. Unfortunately I'm struggling to find time on this project to spend a whole day learning how it all hangs together so hoping one of you kind people can give me some pointers. Can you point to me to a source of reference that explains how to push a file to S3, and get a URL reference in return? More specifically: - Where do the credentials go when using the S3 Android SDK? - Do I need to create a bucket before uploading a file, or can they exist outside buckets? - Which SDK method do I use to push a bitmap up to S3? - Am I right in thinking I need the CORE and S3 libs to do what I need, and no others?
563e0f8b2d1761a701f0f9a2	X	Take a look at the Amazon S3 API documentation to get a feel for what can and can't be done with Amazon S3. Note that there are two APIs, a simpler REST API and a more-involved SOAP API. You can write your own code to make HTTP requests to interact with the REST API, or use a SOAP library to consume the SOAP API. All of the Amazon services have these standard API endpoints (REST, SOAP) and in theory you can write a client in any programming language! Fortunately for Android developers, Amazon have released a (Beta) SDK that does all of this work for you. There's a Getting Started guide and Javadocs too. With this SDK you should be able to integrate S3 with your application in a matter of hours. The Getting Started guide comes with a full sample and shows how to supply the required credentials. Conceptually, Amazon S3 stores data in Buckets where a bucket contains Objects. Generally you'll use one bucket per application, and add as many objects as you like. S3 doesn't support or have any concept of folders, but you can put slashes (/) in your object names.
563e0f8b2d1761a701f0f9a3	X	
563e0f8b2d1761a701f0f9a4	X	Thanks, I did it.
563e0f8b2d1761a701f0f9a5	X	I have a PHP REST API that hosts all images in the Amazon S3. I'm looking for a plugin, or trick, to resize the images using GET params. For example: I found this plugin, but a member of my team said it is ASP.NET based and doesn't fit to my PHP API project. Should I use a script hosted in EC2 to resize those images? Is there other way? Ideas are welcome. Thanks!
563e0f8b2d1761a701f0f9a6	X	I suggest setting up your own PHP service to resize images based on the query string values, as you describe. Yes, the PHP service could be hosted on AWS EC2 or another hosting platform. The service would need to receive the query string such as: http://bodruk.com/images/image.jpg?width=300&height=300 This would need to be configured (perhaps using mod_rewrite [1]) to receive the name of the image (example: 'image.jpg') and pass the query string size values into your PHP script. The script would then find your image on S3, resize it using an image library (such as ImageMagick / PHP GD or PHPThumb [2]) save it (or not) back to S3 and also pass the image data back through on the original request. I wish you good fortune! [1] https://httpd.apache.org/docs/current/mod/mod_rewrite.html [2] http://phpthumb.sourceforge.net/
563e0f8c2d1761a701f0f9a7	X	Could be a memory issue... how much are you giving php?
563e0f8c2d1761a701f0f9a8	X	The weird thing is that it doesn't seem to be producing an error in the log....The only error it generates is that its missing the favicon..
563e0f8c2d1761a701f0f9a9	X	I'm configuring a new Drupal installation, and I installed the MediaMover module so that I could take media and put it on Amazon S3. However, when I try to enable the S3 module within Media Mover and hit Save Settings, it results in a Server 500 Error every time. Is there something I might be missing that would cause this? It says its only dependency is the MediaMover api, which is installed and eneabled. Or maybe some configuration that is needed that I have missed...
563e0f8c2d1761a701f0f9aa	X	The details of a 500 error can be found in your server's logs: it can be any number of issues, ranging from a server misconfiguration to permissions to bugs with the module itself. Once you identify what is actually happening, and if you deterimine it's not your server's configuration, you're going to want to file an issue on Media Mover's issue queue.
563e0f8c2d1761a701f0f9ab	X	your answer is Exceptions :) Look at PHP docs about them ;)
563e0f8c2d1761a701f0f9ac	X	just so you do not have to google it: php.net/manual/en/language.exceptions.php
563e0f8c2d1761a701f0f9ad	X	@Floris consider accepting my answer to prevent this question from getting more unneeded attention, or if my answer didn't solve your problem, use the comments section to ask for further detail.
563e0f8c2d1761a701f0f9ae	X	I have a long running script which gets a (long) array of folders (with subarray of files in that folder) where I have to do several actions on each file. What is the best way to make sure I make all actions successful? And how to handle unsuccessful actions? Lets say what will happen if my mysql server is unavailable or like the Amazon S3 API is not working correctly. pseudocode of my script:
563e0f8c2d1761a701f0f9af	X	As mentioned, what you could do is throw and catch Exceptions. So for instance, if you iterate over files in a folder using a foreach, doing something with those files, on an error, you can throw an Exception and it will stop code execution till it is catched. So maybe you want to use a logger instead. Since it is 2014, you probably want to use a DIC to inject a logger service or otherwise, you can just use a singleton (only considering the great flaws that brings) that stores your errors. So either way you have this service that stores every error. At the end you just check if it has any errors and then act accordingly.
563e0f8c2d1761a701f0f9b0	X	I have implemented Amazon API to upload and download data from amazon server. It does not use any specific URL like traditional web service call but instead it used bucket and secret keys. With this keys I assume that it protects unauthorised access to amazon web service. But my question is when we try to upload data to s3 server or download data from it by using amazon API... will the data transfer to/from server securely? The code for downloading the data is like this: I tried to look into amazon documentation and forums but there is no clear thing mentioned. I have found that we can encrypt the data with amazon API and can store it to server but I haven't found how exactly amazon's API transfer the data.
563e0f8d2d1761a701f0f9b1	X	Please post detailed questions here
563e0f8d2d1761a701f0f9b2	X	I am building gallery app that get image url from api via php (laravel5) server. But the image url is a link from S3 Amazon. Should I set cache header on the server or S3?
563e0f8d2d1761a701f0f9b3	X	thanks for this - ok so I see i am confused.. you are right that the data is in ebs. I guess i want to get data from ebs out of Amazon and down to my local machine. To do this I have to create an s3 bucket, mount it to the ec2 instance then copy the data from ebs into the bucket and from there down to my local machine? Holy moly that's complicated is it not?
563e0f8d2d1761a701f0f9b4	X	@utunga - alternatively you might want to reuse your SSH credentials to access the instance via SSH File Transfer Protocol (SFTP); most FTP programs support this these days, good ones are e.g. WinSCP, Cyberduck or Filezilla.
563e0f8d2d1761a701f0f9b5	X	again, thanks for the comments Stefan. I have found out earlier on that the same credentials file approach that works just fine with ssh is not working with scp. when i try to transfer even the smallest file I get a 'Permission Denied' error. From what i've read elsewhere amazon doesn't allow scp - hence the existence of things like s3cmd ? But perhaps I'm still confused. have you or has anyone successfully used scp or sftp with amazon s3 accounts connecting offsite ?
563e0f8d2d1761a701f0f9b6	X	@utunga - SCP/SFTP support has nothing to do with AWS, the earlier SCP and its successor SFTP differ significantly though; each can be disabled/enabled separately within the typical SSH daemon, but I haven't encountered a situation where SFTP didn't just work out of the box for quite a while (mostly Ubuntu 12.04, see also my related answer to Uploading files on Amazon EC2) - given you inherited the instance, it might just be disabled there?
563e0f8d2d1761a701f0f9b7	X	@utunga - You are indeed still messing services up a bit btw., I suggest to make yourself familiar with these: Amazon S3 is storage for the Internet. You need a dedicated S3 client to interact with this, this has nothing to do with (S)FTP - nowadays many former (S)FTP only programs have support for the S3 API build in as well though. Amazon EC2 is a web service that provides resizable compute capacity in the cloud, i.e. it provides virtual machines with your choice of OS (Windows/Unix/Linux/...).
563e0f8d2d1761a701f0f9b8	X	I've inherited an already configured ec2 instance and am trying to download data from it. I have set up S3Browser with relevant credentials but just need the name of the external bucket to connect to. I can ssh to the machine and see that the bucket with the data is already mounted - thusly *some numbers changed to protect the innocent.. But what I need is the name of the bucket for - say the /ebs mount point - to enter into s3browser. I realise this is kind of going backwards... but there must be a way. If not where can I find information on available s3 buckets?
563e0f8d2d1761a701f0f9b9	X	You might eventually be confusing a few AWS concepts, at least the information you provided seems to be inconsistent with your question at first sight. While it is indeed possible to mount an Amazon S3 bucket on an Amazon EC2 instance (see e.g s3fs, which is a FUSE-based file system backed by Amazon S3), the name of the mount point in question suggests that this is an Amazon Elastic Block Store (EBS) volume instead. If that would be the case, you can only access the data via the EC2 instance where the volume is attached to and not via external tools.   No, snapshots are only available through the Amazon EC2 APIs.  This is most easily done via the AWS Management Console, which allows you to Access and manage Amazon Web Services through a simple and intuitive web-based user interface.
563e0f8e2d1761a701f0f9ba	X	We all know you can do direct uploads to amazon S3 using a form. See: http://aws.amazon.com/articles/1434/ I would love to replicate this functionality using an API, but without storing the uploaded file on our webserver. I know if you would stream the file through - for example - PHP, your file is stored in the TMP directory before it is uploaded to S3. But I want to avoid that. Isn't there a way that could work like this: I know this might sound far-fetched, and possibly not possible at all. But I wanted to see if someone thinks there might be a remote possibility that it could work in a way we haven't though about yet.
563e0f8e2d1761a701f0f9bb	X	Have a look at S3-Curl. It's a python wrapper that handles AWS keys and headers to properly generate the write CURL commands for the REST API for various amazon services (S3 included). You could look inside the source of the .pl file to get an idea of how to create the curl requests yourself (only if you don't want to use s3-curl and have a restriction that you can only use curl directly). You could use this in combination with Amazon's STS to generate a temporary token granting access for that particular upload. In this case, your modified flow would be:
563e0f8e2d1761a701f0f9bc	X	In my case I cannot use Amazon's SDK as my host is refusing to install it for me.
563e0f8e2d1761a701f0f9bd	X	You don't need to have them install it. Just put it on your PHP include path (or, if you're really desperate, just in the domain directory).
563e0f8e2d1761a701f0f9be	X	Hi, do you have a link to a tutoria to learn how to go about it please? Thanks
563e0f8e2d1761a701f0f9bf	X	I am trying to copy a 1TB file from one bucket to another. I know that this can be done easily if I log into the AWS S3 panel but I would like to do it using PHP. I am using the following AWS S3 class from github I am using it in my PHP code as follows: I'm getting no error_log. What am I doing wrong that I am missing, please?
563e0f8e2d1761a701f0f9c0	X	At 1 TB, the object is too large to copy in a single operation. To quote from the S3 REST API documentation: You can store individual objects of up to 5 TB in Amazon S3. You create a copy of your object up to 5 GB in size in a single atomic operation using this API. However, for copying an object greater than 5 GB, you must use the multipart upload API. Unfortunately, it doesn't appear that the S3 class you're using supports multipart uploads, so you'll need to use something else. I'd strongly recommend that you use Amazon's AWS SDK for PHP — it's a bit bigger and more complex than the one you're using right now, but it supports the entirety of the S3 API (as well as other AWS services!), so it'll be able to handle this operation.
563e0f8e2d1761a701f0f9c1	X	I need to create a HMAC-SHA1 signature to send my policy document to amazon web services s3 API. Is there a way of producing this using codenameone's API or do I need to use native java to accomplish this?
563e0f8e2d1761a701f0f9c2	X	Did you try the bouncy castle cn1lib? It should offer support for HMAC-SHA1.
563e0f8f2d1761a701f0f9c3	X	We are doing a COPY Object request with Directive Replace. It works for some files but we found that some files are still not encrypted. I think I will need to try your first solution to run a script every day.
563e0f8f2d1761a701f0f9c4	X	Thanks Julio, this makes sense, seems like the only way is to change the headers in the request. I was hoping there would be some way to do this through a system wide custom boto configuration or something, having a proxy server seems overengineered. Anyway I would think this is a pretty common use case though, people want a totally secure bucket. Hopefully this will be an option in future releases.
563e0f8f2d1761a701f0f9c5	X	I want to set an S3 bucket policy so that all requests to upload to that bucket will use server side encryption, even if it is not specified in the request header. I have seen this post (Amazon S3 Server Side Encryption Bucket Policy problems) where someone has managed to set a bucket policy that denies all put requests that don't specify server side encryption, but I don't want to deny, I want the puts to succeed but use server side encryption. My issue is with streaming the output from EMR to my S3 bucket, I don't control the code that is making the requests, and it seems to me that server side encryption must be specified on a per request basis.
563e0f8f2d1761a701f0f9c6	X	IMHO There is no way to automatically tell Amazon S3 to turn on SSE for every PUT requests. So, what I would investigate is the following : write a script that list your bucket for each object, get the meta data if SSE is not enabled, use the PUT COPY API (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) to add SSE "(...) When copying an object, you can preserve most of the metadata (default) or specify new metadata (...)" If the PUT operation succeeded, use the DELETE object API to delete the original object Then run that script on an hourly or daily basis, depending on your business requirements. You can use S3 API in Python (http://boto.readthedocs.org/en/latest/ref/s3.html) to make it easier to write the script. If this "change-after-write" solution is not valid for you business wise, you can work at different level (aligned with Julio's answer above) use a proxy between your API client and S3 API (like a reverse proxy on your site), and configure it to add the SSE HTTP header for every PUT / POST requests. Developer must go through the proxy and not be authorised to issue requests against S3 API endpoints write a wrapper library to add the SSE meta data automatically and oblige developer to use your library on top of the SDK. The later today are a matter of discipline in the organisation, as it is not easy to enforce them at a technical level. Seb
563e0f8f2d1761a701f0f9c7	X	S3 will not perform this automatically, you would have to create a workaround. I would suggest passing requests thru a proxy that would "enrich" them adding the proper header. To do that i'd try (in order): 1- Apache Camel Content Enrich 2- NGXINX / HTTPD mod_proxy 3- Custom code I bet there is also a very smart ruby http lib for that too :)
563e0f8f2d1761a701f0f9c8	X	I'm working on an automated mechanism for our EBS volumes to be backed up on a daily basis. Regarding that can you please tell me how to take backup of snapshots, how to move it to s3 and then how to take incremental back up?
563e0f8f2d1761a701f0f9c9	X	Your apparently haven't yet realized the full potential of Amazon EBS, insofar your requirements are mostly build in already, see sections Features of Amazon EBS volumes as well as Amazon EBS Snapshots: Amazon EBS also provides the ability to create point-in-time snapshots of volumes, which are persisted to Amazon S3. These snapshots can be used as the starting point for new Amazon EBS volumes, and protect data for long-term durability. [...] [emphasis mine] and Amazon EBS provides the ability to back up point-in-time snapshots of your data to Amazon S3 for durable recovery. Amazon EBS snapshots are incremental backups, meaning that only the blocks on the device that have changed since your last snapshot will be saved. [...] [emphasis mine] So you neither need to move EBS snapshots to S3 nor handle their incremental nature yourself and the only thing missing is the scheduled usage of the respective APIs, which can be achieved in one of the following two ways: Good luck! In fact it isn't even possible to access EBS snapshots in S3 outside of the aforementioned API, see the FAQ Will I be able to access my snapshots using the regular Amazon S3 APIs?: No, snapshots are only available through the Amazon EC2 APIs. You might want to review the other EBS related FAQs in section Amazon Elastic Block Storage (EBS) within the Amazon EC2 FAQs as well.
563e0f8f2d1761a701f0f9ca	X	Check out http://skeddly.com - they have a feature for doing automated backups.
563e0f8f2d1761a701f0f9cb	X	I am one of the developer of Bucket Explorer, it is amazon s3 tool. Bucket Explorer upload only updated or new files on Amazon s3 bucket. It does not upload identical files on s3. So when whenever you upload/download it will upload/download only new or updated data. Bucket Explorer
563e0f8f2d1761a701f0f9cc	X	Thanks, I did not realize that you could append to the blob after the initial write. I think this largely solves my problem, although since yesterday I've also been looking at the Page mode. Specifically just appending pages to the BLOB. Do you see any issues with that approach?
563e0f8f2d1761a701f0f9cd	X	I believe page blobs need to have predetermined (fixed) length, so you'll need to create a very large page blob and then gradually fill it. Block blobs can instead just get longer and longer (but you have to commit the entire list of blocks each time, so that list may get really long).
563e0f902d1761a701f0f9ce	X	You only pay for the pages you have written though so there is no issue in just making a page blob that is a couple of TB and then just appending to it. You might need to keep the length somewhere though, perhaps at the beginning of the file.
563e0f902d1761a701f0f9cf	X	What are the cloud data storage APIs that accomodate streaming data well? Specifically, a constant data stream that: 1) has no known end and is continually appended to and 2) can be read from at any time. Due to the nature of distributed access, the big cloud storage options like Amazon S3, Google Storage for Developers, and Windows Azure Blobs do not seem to support streaming data. Current beliefs: 1) Amazon S3 does not allow append operations to objects (only replace). The multipart upload API allows a "streaming" upload, but it requires to be "finalized" once completely written. 2) Google Storage objects are immutable, so same thing. 3) Windows Azure blog storage has has block storage, but like Amazon S3 multipart upload, requires the blocks to be "finalized" so an open-ended stream is not possible. Any ideas?
563e0f902d1761a701f0f9d0	X	With Windows Azure blob storage, you can keep appending to the same blob (and committing the block list after each write) as long as you want, and you can request any byte range when reading. However, you still wouldn't get the behavior of a single HTTP request with data continually streaming down. (You'd have to request a range and then make another request for the next range, etc. In other words, at any given time, the blob has finite length.) Building your own code to front-end the data (socket-based or maybe a chunked HTTP response) may be your only option, if I'm understanding the requirements correctly.
563e0f902d1761a701f0f9d1	X	What you want is a Windows Azure Page Blob, rather than a Block Blob. For info about page blobs see: http://msdn.microsoft.com/en-us/library/windowsazure/ee691964.aspx. With a Page Blob you will be able to append to an existing blob, the main consideration is that you have to write whole 512 byte pages, so if you appending to an existing file you may have to also send up to 511 bytes of existing data from the end of your file.
563e0f902d1761a701f0f9d2	X	Why do you think that Amazon SDK does not work on GAE?
563e0f902d1761a701f0f9d3	X	Thank you for your answer. Fortunately there is an experimental feature that allows creating files programatically in the Blobstore (code.google.com/appengine/docs/java/blobstore/…) - so that part will work. But what is the file download limit in your scenarion? 1. create file programatically in Blobstore, 2. create entry in BigTable for key, 3. user calls servlet which will serve file from Blobstore. Is the file size dowload limit 32MB in this case?
563e0f902d1761a701f0f9d4	X	You can get maximum of 32MB with one api call to the blobstore service, if that's not enough for you consider to use google storage option.
563e0f902d1761a701f0f9d5	X	There's no limit on the size of programmatically created blobs, and users can download them all at once when you serve them using the blob serving API.
563e0f902d1761a701f0f9d6	X	I am creating XML files in my GAE web application and I would like to host them somewhere. The link has to be consistent HOST_URL + filename. So Amazon S3 looks like it would work - I can upload a file and the URL is pointing directly to the file. Now my question is - how can I upload files from GAE to S3? The Amazon SDK does not work on GAE. What is the upload limit from GAE? Is it 1MB or 32MB? Can you provide maybe a sample HTTP request for uploading data directly to S3? Would be using the Blobstore API easier? What is the file-size limit for uploading a file that is created in GAE and need to uploaded to Blobstore API directly? Thanks.
563e0f902d1761a701f0f9d7	X	As you have noticed already S3 api's don't work well with GAE. For storing files on GAE you can use either BigTable, Blobstore or google storage so pick the option which best suits your needs. There is a nice article describing them all with code samples here You can save a filename and the blob location in a bigtable. Once you have your data stored on GAE you can create a special servlet which would take the file name, find the correspoding data informatio in bigtable, retrive it from blobstore for example and return it to the user (just an idea).
563e0f912d1761a701f0f9d8	X	Try this. S3::deleteObject('wecombinate','products/images/image1.png');
563e0f912d1761a701f0f9d9	X	@IqbalMalik yes, that's exactly how I am doing it...
563e0f912d1761a701f0f9da	X	This link might have info you need. docs.aws.amazon.com/AmazonS3/latest/dev/…
563e0f912d1761a701f0f9db	X	@IqbalMalik Thanks for the link. I have read the Amazon docs and have found nothing related to my problem unfortunately.
563e0f912d1761a701f0f9dc	X	Try it with a leading slash: $s3->deleteObject('wecombinate', '/products/images/image1.png') and see if that changes anything.
563e0f912d1761a701f0f9dd	X	I'm having trouble deleting an object in one of my buckets which uses slashes in the object name to help with organization. For example, my bucket name is wecombinate and my object name is products/images/image1.png When I try to delete, I get "[BucketNotEmpty] The bucket you tried to delete is not empty" as if I'm trying to delete the whole bucket, which I'm not, I am using the DELETE object REST API request to delete the single item products/images/image1.png. I'm using the popular https://github.com/tpyo/amazon-s3-php-class PHP class to manage S3 and the code seems fine, plus no issues reported on GitHub. The code to do the delete: Is there a known problem with using slashes in the object name? Any other things I might be missing?
563e0f912d1761a701f0f9de	X	You need to add a leading slash to the key to get it working:
563e0f912d1761a701f0f9df	X	How can I set the S3 bucket file header to be the following: content-encoding: gzip content-type: text/css I am using Amazon API using the SDK for .NET. I am uploading a file to S3 using the PutObjectRequest. The problem that when the file is uploaded, the content type and content encoding headers aren't beign updated (I've checked via the files properties on Amazon Console). example: Also tried: What I'm doing wrong?
563e0f912d1761a701f0f9e0	X	you add the content-type to the contentType property in the api like the following //for the content-encoding //add the following header hope this could help
563e0f922d1761a701f0f9e1	X	A brief look at the class makes me think that the library is expecting the first param to be a file path, not file data. If it's not a file or not accessible, it runs trigger_error('S3::inputFile(): Unable to open input file: '.$file, E_USER_WARNING); and returns false.
563e0f922d1761a701f0f9e2	X	@JonStirling - Thank you, you're right. I've been googling around and can't even make out IF there is a way to upload file data to S3..
563e0f922d1761a701f0f9e3	X	@Mortimer check my answer then.
563e0f922d1761a701f0f9e4	X	i highly recommend you to not use that class because it too old 2011 and not updated than since
563e0f922d1761a701f0f9e5	X	I am using PHP CURL to generate a customized PNG image from a REST API. Once this image has loaded I would like to upload it into an AWS S3 Bucket and show the link to it. Here's my script so far: It keeps failing. Now, I think the problem is with where I use putObjectFile - the $data variable represents the image, but maybe it has to be passed in another way? I am using a common PHP Class for S3: http://undesigned.org.za/2007/10/22/amazon-s3-php-class
563e0f922d1761a701f0f9e6	X	Use PHP memory wrapper to store the contents of the image, and use $s3->putObject() method: Proven method (you may need to alter the code a bit) with PHP 5.5 and latest AWS libraries. http://php.net/manual/en/wrappers.php.php
563e0f922d1761a701f0f9e7	X	can you help me with stackoverflow.com/questions/22505525/… ?
563e0f922d1761a701f0f9e8	X	Is anybody using this API? I am trying to connect to Amazon S3 and EC2, following this paper here, but I get stuck on that line: Don't know what to put inside forName function or how to implement or get CloudProvider instance. Thanks.
563e0f922d1761a701f0f9e9	X	It should be like this:
563e0f932d1761a701f0f9ea	X	hi, thanks for the answer. yes, i included the signature on my authorization header. I generated my signature using this: var policyBase64 = Base64.encode(JSON.stringify(POLICY_JSON)); var signature = b64_hmac_sha1(secret, policyBase64);
563e0f932d1761a701f0f9eb	X	Is it working now?
563e0f932d1761a701f0f9ec	X	nope, it still saying SignatureDoesNotMatch. Could you take a look at my policy json? Is there anything missing on my policy. I'm using temporary credentials so I added x-amz-security-token on my header.
563e0f932d1761a701f0f9ed	X	Why are you encoding and signing policy? Did you try these steps http://docs.aws.amazon.com/general/latest/gr/sigv4_signing.html
563e0f932d1761a701f0f9ee	X	im sorry, I'm reading it now. I'm having difficulty understanding a canonical request. Right now I'm using Chrome advance rest client to perform http request to amazon. How do I perform canonical req there?
563e0f932d1761a701f0f9ef	X	I'm trying to implement an unploading of image to amazon s3 using only rest api. I've seen their docs but the problem is I'm only using temporary credential which will expire for about an hour. Below is the response My policy Here what I tried so far: Using chrome advance rest client I entered this url: https://mybucket.s3.amazonaws.com/avatars/test@domain.com with headers of The result was: 403 Forbidden and its saying SignatureDoesNotMatch. Does anyone able to accomplish uploading of object using only Rest Api of s3 (not using of SDK's). The client asked me if its possible to build it using only javascript. Is this possible?
563e0f932d1761a701f0f9f0	X	Should you not sign the content? Check this how to sign. After signing you have to pass the signature value in the Authorization header. Authorization: AWS AWSAccessKeyId:Signature
563e0f932d1761a701f0f9f1	X	I have download files from Google drive and save into my local system by using google drive api with java.My aim is to make a copy of documents from gdrive to amazon s3. I can achieve this by download the Gdrive documents into my local directory and upload into amazon-s3 by using the s3Utility's public void uploadToBucket(int userId, String bucketName, String fileName, File fileData) method. Is there any direct way to achieve this? that is i want to reduce one step. i don't like to download documents into my local.Instead of this i would like to give the gdrive document's downloadurl into s3 method,it will need to save the document into s3. Is it possible? Any Suggestions? sorry the essay type of question
563e0f932d1761a701f0f9f2	X	are you looking for Route 53 (aws.amazon.com/route53)?
563e0f932d1761a701f0f9f3	X	I have the basic knowledge of route53,but I want to be able to create domains.Does Route 53 allow me to create domians?I did not find any API associated with it.
563e0f932d1761a701f0f9f4	X	maybe this: forums.aws.amazon.com/… ?
563e0f932d1761a701f0f9f5	X	the PHP2 SDK reference for Route 53 is here: docs.aws.amazon.com/aws-sdk-php-2/latest/…
563e0f932d1761a701f0f9f6	X	btw, you do not have to use AWS for DNS things. If your host allows you to change DNS settings for your domains then you can use that instead. all you need to do it seems is to point your domain to the bucket.
563e0f932d1761a701f0f9f7	X	Thank you for pointing out the limitation!As you said I need to do a proxy,but can you please elaborate on how exactly I can achieve it?I am still a newbie working on the cloud and aws stuff!
563e0f942d1761a701f0f9f8	X	I updated my original answer to include links to the nginx docs as you could use this server to proxy the requests. This setup is not specific to the cloud or aws, it would be the same to proxy and rewrite requests to any backend server. Those docs will help you get started but you will probably need to do more research on this and post other specific questions, either here or on serverfault if you get stuck on a specific point.
563e0f942d1761a701f0f9f9	X	Just to make sure if I am going the right way,earlier I used cpanel and its services(API's) to create subdomains(on the fly) and transfer files via FTP.Is the approach highlighted by you better or I can go with this one?Offcourse I don't yet have a cpanel on Amzon,bt thats a different problem probably!
563e0f942d1761a701f0f9fa	X	The approach I suggested is not necessarily better in your case. It really depends on your needs and experience. Each method has advantages and disadvantages. As advantages in storing the data directly to S3 and using this proxy method I would mention that your EC2 instance will not need a persistent storage attached to it (EBS), S3 has great data durability and reliability, the load on your server will probably be smaller than when using cPanel so you might use a cheaper instance and it is easier to load balance the load between multiple servers than it is with cPanel.
563e0f942d1761a701f0f9fb	X	I am going through your suggestions,but would the functionality work fast in terms of speed?As subdomain creation will take time and I don't want the user to wait for the results to propagate!!Do you say its the best way to go while using AWS and amazon EC2? I have accepted your answer and will post any specific questions when help needed(I believe I will definitely need it)
563e0f942d1761a701f0f9fc	X	INFO: I am working on an app built in php which facilitates users to create HTML templates and publish them on web.As the templates would be static I thought of using amazon S3 for storing them as it can host static websites and has good infrastructure overall for the application.br/> QUERY: I am facing an issue while publishing it to the web.I want the template to be published as a subdomain on my domain,for eg: I own www.ABC.com,I ask the user to name the template,if he names it as mysite,I publish his template as mysite.ABC.com(similar for all users).Now,I can store the template in the S3 bucket using putObjectFile in the aws s3 api,but I am not sure how can I create a subdomain(on the fly) and publish it on that domain.(I want to automate the process for the user). Also,can I make the bucket as hosting static website using the API? Earlier,I worked with cpanel and cpanel API's allow us to create domains and do a FTP to the domain with the content,I am not clear how can I achieve it here. RESEARCH: The success till now I have achieved is,I have hosted a site using the S3 console.Using the AWS services I have moved the files to a bucket with the name same as the subdomain of the user.I want now to have the bucket endpoint changed to the subdomain. REFERENCE: This website does the same,they create a directory like structure and publish the website on web.I don't know if they host it on Amazon,but I want to achieve something similar.Hope I am clear and get some guidance. Thank you for the attention
563e0f942d1761a701f0f9fd	X	I researched a similar scenario some time ago but I was unable to use S3 for this because of a S3 limitation. To host a static website on S3 on a custom domain or subdomain, you need to create a bucket with a name that matches that domain. And because each S3 account is limited to 100 buckets, you will only be able to host those many domains or subdomains on a single account. Based on the use case you described, I suspect this S3 limitation will also force you to find another solution. In my case, the solution was to set up an EC2 instance that proxies requests to S3 after rewriting them. For example if someone requests: http://mysite.abc.com/file.html that goes through our EC2 server where the request is rewritten and forwarded to S3 as something like: http://ourbucket.s3.amazonaws.com/mysite.abc.com/file.html UPDATE: There are several proxy servers that you could use for this but I would recommend nginx as it worked great in our case. To get started, check out the following nginx docs: http://wiki.nginx.org/HttpRewriteModule http://wiki.nginx.org/HttpProxyModule
563e0f942d1761a701f0f9fe	X	When use scan for 1), it means I have to spend a lot of read capacity units to get all items then extract the result of just two hash primary key? wow ~ it's very expensive! any other choice?
563e0f942d1761a701f0f9ff	X	You can choose which attributes to fetch - meaning that your read throuout won't go crazy if you have big attributes for each item. You can also add conditions to minimize returned results which reduces network IO but doesn't save throughout
563e0f942d1761a701f0fa00	X	it's useful info for me, but I found no description on AWS about the spent read capacity unit of scan depending on the attributes retrieved (it has a similar description on query but not for scan). any reference to prompt? thanks a lot
563e0f942d1761a701f0fa01	X	I would write a post on the AWS DynamoDB forum asking for documentation clarifications.
563e0f942d1761a701f0fa02	X	Could I expect that if I just want the scan to return hash primary key and no any other attributes then each return item size should be smaller than 100 byte. And I need only single read capacity unit for more than 40 items?
563e0f942d1761a701f0fa03	X	I would like to extract the primary key of table to a list , but find no api to do that. for example, as the amazon example thread table, I want to ask how to : 1) get the hash primary key list, in the amazon example thread table it would be an array of ["Amazon DynamoDB", "Amazon S3"] 2) with assigning the hash primary key to "Amazon DynamoDB", I want to get the range primary key list and it would be an array of ["Amazon DynamoDB Thread 1", "Amazon DynamoDB Thread 2"]
563e0f942d1761a701f0fa04	X	For 1 what you want is to run a Scan operation on a table. Scan Gets all the items of the list. Depends on the API you are using, you can get only the hash key or any attributes you want. For 2 what you want is Query - which gets a hash attribute and returns all rows that have the hash attribute (can be more than one). Overview - Query and Scan operations Java mapper reference - Scan and Query
563e0f942d1761a701f0fa05	X	Why the downvote? Do I need to be more/less specific?
563e0f952d1761a701f0fa06	X	Why aren't you using the S3 API instead of trying to use it as a filesystem?
563e0f952d1761a701f0fa07	X	Not the downvoter, but I wonder if he/she was looking for a chunk of code you are having trouble with. Whilst we do have a policy here against discursive questions, the question seems specific enough to me, so +1.
563e0f952d1761a701f0fa08	X	@StevenVondruska I'll look into that. Thanks.
563e0f952d1761a701f0fa09	X	@halfer I figured people were more reacting to the title of the post (I definitely am frustrated!) rather than the content, which is why I tried again. Thanks!
563e0f952d1761a701f0fa0a	X	I don't know why I even tried to mount the S3 bucket on the local filesystem...probably because it was somebody else's idea first.
563e0f952d1761a701f0fa0b	X	+1 for the leaky abstraction article!
563e0f952d1761a701f0fa0c	X	I checked out Gaufrette, thinking it would make s3 integration easier, but really, amazon's php sdk is quite easy to use by itself.
563e0f952d1761a701f0fa0d	X	I'm working on a project that is being hosted on Amazon Web Services. The server setup consists of two EC2 instances, one Elastic Load Balancer and an extra Elastic Block Store on which the web application resides. The project is supposed to use S3 for storage of files that users upload. For the sake of this question, I'll call the S3 bucket static.example.com I have tried using s3fs (https://code.google.com/p/s3fs/wiki/FuseOverAmazon), RioFS (https://github.com/skoobe/riofs) and s3ql (https://code.google.com/p/s3ql/). s3fs will mount the filesystem but won't let me write to the bucket (I asked this question on SO: How can I mount an S3 volume with proper permissions using FUSE). RioFS will mount the filesystem and will let me write to the bucket from the shell, but files that are saved using PHP don't appear in the bucket (I opened an issue with the project on GitHub). s3ql will mount the bucket, but none of the files that are already in the bucket appear in the filesystem. These are the mount commands I used: I've also tried using this S3 class: https://github.com/tpyo/amazon-s3-php-class/ and this FuelPHP specific S3 package: https://github.com/tomschlick/fuel-s3. I was able to get the FuelPHP package to list the available buckets and files, but saving files to the bucket failed (but did not error). Have you ever mounted an S3 bucket on a local linux filesystem and used PHP to write a file to the bucket successfully? What tool(s) did you use? If you used one of the above mentioned tools, what version did you use? EDIT I have been informed that the issue I opened with RioFS on GitHub has been resolved. Although I decided to use the S3 REST API rather than attempting to mount a bucket as a volume, it seems that RioFS may be a viable option these days.
563e0f952d1761a701f0fa0e	X	Have you ever mounted an S3 bucket on a local linux filesystem? No. It's fun for testing, but I wouldn't let it near a production system. It's much better to use a library to communicate with S3. Here's why: The bottom line is that S3 under FUSE is a leaky abstraction. S3 doesn't have (or need) directories. Filesystems weren't built for billions of files. Their permissions models are incompatible. You are wasting a lot of the power of S3 by trying to shoehorn it into a filesystem. Two random PHP libraries for talking to S3: https://github.com/KnpLabs/Gaufrette https://aws.amazon.com/sdkforphp/ - this one is useful if you expand beyond just using S3, or if you need to do any of the fancy requests mentioned above.
563e0f952d1761a701f0fa0f	X	Quite often, it is advantageous to write files to the EBS volume, then force subsequent public requests for the file(s) to route through CloudFront CDN. In that way, if the app must do any transformations to the file, it's much easier to do on the local drive & system, then force requests for the transformed files to pull from the origin via CloudFront. e.g. if your user is uploading an image for an avatar, and the avatar image needs several iterations for size & crop, your app can create these on the local volume, but all public requests for the file will take place through a cloudfront origin-pull request. In that way, you have maximum flexibility to keep the original file (or an optimized version of the file), and any subsequent user requests can either pull an existing version from cloud front edge, or cloud front will route the request back to the app and create any necessary iterations. An elementary example of the above would be WordPress, which creates multiple sized/cropped versions of any graphic image uploaded, in addition to keeping the original (subject to file size restrictions, and/or plugin transformations). CDN-capable WordPress plugins such as W3 Total Cache rewrite requests to pull through CDN, so the app only needs to create unique first-request iterations. Adding browser caching URL versioning (http://domain.tld/file.php?x123) further refines and leverages CDN functionality. If you are concerned about rapid expansion of EBS volume file size or inodes, you can automate a pruning process for seldom-requested files, or aged files.
563e0f952d1761a701f0fa10	X	As per the Amazon multipart upload documentation the Each part must be at least 5 MB in size, except the last part. http://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html Question is how do I upload a file less than 5MB through multipart upload api to AWS S3 bucket. The reason I am asking this that I want to use multipart upload API for all files when uploading to S3
563e0f962d1761a701f0fa11	X	You can still upload it using multipart upload, the same as you would a larger file... but you have to upload it with only one part. The rule enforced by S3 is that all parts except the last part must be >= 5MB. If the first part is also the last part, this rule isn't violated and S3 accepts the small file as a multipart upload.
563e0f962d1761a701f0fa12	X	I have a lot of subdirectories containing a lot of images (millions) on S3. Having the files in these subdirectories has turned out to be a lot of trouble, and since all file names are actually unique, there is no reason why they should reside in subdirectories. So I need to find a fast and scalable way to move all files from the subdirectories into one common directory or alternatively delete the sub directories without deleting the files. Is there a way to do this? I'm on ruby, but open to almost anything
563e0f962d1761a701f0fa13	X	I have added a comment to your other question, explaining why S3 does not have folders, but file name prefixes instead (See Amazon AWS IOS SDK: How to list ALL file names in a FOLDER). With that in mind, you will probably need to use a combination of two S3 API calls in order to achieve what you want: copy a file to a new one (removing the prefix from the file name) and deleting the original. Maybe there is a Ruby S3 SDK or framework out there exposing a rename feature, but under the hood it will likely be a copy/delete. Related question: Amazon S3 boto: how to rename a file in a bucket?
563e0f972d1761a701f0fa14	X	this question might be more suitable for webmasters.stackexchange.com
563e0f972d1761a701f0fa15	X	ok, can you provide recommendation, that would be more useful for thanks?
563e0f972d1761a701f0fa16	X	THANKS Alfasin - I didnt know about that site. Im only new to SO.
563e0f972d1761a701f0fa17	X	I really would recommend that you not ask this question on webmasters. You'll receive a poor reception there as well, for the same reason. Stack Exchange sites, as a whole, aren't suited for this kind of A-B question.
563e0f972d1761a701f0fa18	X	Then could you recommend a website that does, Im guessing QnA websites are all about sharing of Knowledge ?
563e0f972d1761a701f0fa19	X	If anyone else has other info, pros and cons I will vote that to be the correct answer and pass on the points
563e0f972d1761a701f0fa1a	X	For web Sites/Applications and eCommerce, which storage solution is more desirable and Why ? Im very new to Amazon Cloud services I need some direction here.
563e0f972d1761a701f0fa1b	X	S3 likes network storage, you can access it with web api ebs likes hard disk, you can access it in your ec2 you can also see, http://www.differencebetween.net/technology/internet/difference-between-amazon-s3-and-amazon-ebs/
563e0f972d1761a701f0fa1c	X	I found another SO question here Should I persist images on EBS or S3? This pretty much answers my question.
563e0f972d1761a701f0fa1d	X	EBS is meant for transactional data and S3 for everything else. In EBS you can have static IP which is required when we use database access etc.
563e0f972d1761a701f0fa1e	X	I bet creating a custom storage class is the best way to go. The custom class can re-use code from FileSystemStorage and S3BotoStorage.
563e0f972d1761a701f0fa1f	X	This may help you: djangosnippets.org/snippets/1976
563e0f972d1761a701f0fa20	X	I have django application that was using S3BotoStorage backend to store uploaded files on Amazon s3. But in web api services(using django-tastypie) it was taking long time to upload file on s3. As there were request passes through web server and then to amazon s3 storage backend. So, we come with solution to let them upload first on Web server and implement django-celery tasks through which files get uploaded to amazon s3. It is finished and working. But after that we want to modify the url of files to amazon s3 storage location urls. But when we try to modify file_field_obj.storage to s3botostroage. This gets revert it back to Default File Storage as expected. So is there any option we can modify Django Models FileField storage field after uploading files on s3. So, in settings there will be DefaultFileStorage pointing to FileSystemStorage. But if files are on s3 then, they will point to s3 storage locations.
563e0f982d1761a701f0fa21	X	The solution already exists in an app: django-queued-storage It should handle creating celery tasks that uploads between the storage backends.
563e0f982d1761a701f0fa22	X	That's not really an answer. A comment would suffice.
563e0f982d1761a701f0fa23	X	Sorry, but I respectfully disagree - you asked: [...] is there any way to send the notification directly to the SWF, without having a service consuming them and starting the workflow? - I provided an answer stating that there is no such way, including an explanation and a proper reference - I've added that quote to make this more obvious now. Though unfortunate, sometimes a negative answer can also be an answer (depending on the question asked).
563e0f982d1761a701f0fa24	X	Makes sense. Thanks.
563e0f982d1761a701f0fa25	X	I have a workflow which takes a file in an S3 bucket and does a lot of processing and further requests based on the file contents. Currently, clients have to trigger the workflow manually after uploading the file. This seems to be a pretty common use case for me, so is there any way to trigger the workflow as soon as the file is uploaded? I imagine there should be an SNS notification in between, but is there any way to send the notification directly to the SWF, without having a service consuming them and starting the workflow?
563e0f982d1761a701f0fa26	X	AWS has finally launched New Event Notifications for Amazon S3 today, which indeed simply extend the long available PUT Bucket notification API with additional event types for object creation via the S3 APIs such as PUT, POST, and COPY: [...] is there any way to send the notification directly to the SWF, without having a service consuming them and starting the workflow? Unfortunately there is no such way, you indeed need a mediating service - while the PUT Bucket notification has obviously been designed to allow for other types of events too, Amazon S3 doesn't support Amazon SNS notifications for anything but Enabling RRS Lost Object Notifications as of today: This implementation of the PUT operation uses the notification subresource to enable notifications of specified events for a bucket. Currently, the s3:ReducedRedundancyLostObject event is the only event supported for notifications. The s3:ReducedRedundancyLostObject event is triggered when Amazon S3 detects that it has lost all replicas of an object and can no longer service requests for that object. [emphasis mine]
563e0f982d1761a701f0fa27	X	As Steffen Opel said, there is no way to do this right now. However, an alternate route to what his updated answer provided would be to use AWS's new event processing service Lambda (which ATM is in preview). The documentation that shows you how to configure it for S3 is here, but at a high level:
563e0f982d1761a701f0fa28	X	Is there a way to use Fine Uploader to upload to an Amazon S3 bucket by providing the already signed policy document along with the key and the other credentials all at once by overriding the policy post request with our own XML api call? Our company API returns all the credentials including signed policy for the file in one response and is already well established so setting up a signing page is not an option.
563e0f982d1761a701f0fa29	X	This may work for non-chunked uploads since Fine Uploader will only make one request for the signed policy document. However, when uploading files in chunks, the S3 REST API must be used. In that case, a policy document is not used. Instead, a long string of relevant headers for each request must be signed. This signature is then included with the REST call. The headers change with each request, therefore requiring a new signature. If you want to support chunking and concurrent chunking to S3, you'll need to ensure each request is signed separately via a signature server, or make use of an identity provider to handle this client-side, as demonstrated in our documentation at http://docs.fineuploader.com/branch/master/features/no-server-uploads.html.
563e0f982d1761a701f0fa2a	X	Thanks for your answer, I already setup S3FS unfortunately. My folder is read/write enabled, but not allowed for public access, for that you need to set a bucket policy. I managed to solve this by using s3curl. ;)
563e0f982d1761a701f0fa2b	X	I've managed to use S3FS to mount an Amazon S3 folder into my Wordpress site. Basically, my gallery folder for NextGEN gallery is a symlink to a mounted S3FS folder of the bucket, so when I upload an image, the file is automatically added to the S3 bucket. I'm busy writing an Apache rewrite rule to replace the links, to fetch gallery images from S3 instead, without having to hack or change anything with NextGEN, but one problem I'm finding, is that images are not public by default on S3. Is there a way to change a parent folder, to make its children always be public, including new files as they are generated? Is it possible or advisable to use a cron task to manually make a folder public using the S3 command line API?
563e0f982d1761a701f0fa2c	X	I'm the lead developer and maintainer of Open source project RioFS: a userspace filesystem to mount Amazon S3 buckets. Our project is an alternative to “s3fs” project, main advantage comparing to “s3fs” are: simplicity, the speed of operations and bugs-free code. Currently the project is in the “beta” state, but it's been running on several high-loaded fileservers for quite some time. We are seeking for more people to join our project and help with the testing. From our side we offer quick bugs fix and will listen to your requests to add new features. Regarding your issue: if'd you use RioFS, you could mount a bucket and have a write / read access to it using the following command (assuming you have installed RioFS and have exported AWSACCESSKEYID and AWSSECRETACCESSKEY environment variables): (please refer to project description for command line arguments) Please note that the project is still in the development, there are could be still a number of bugs left. If you find that something doesn't work as expected: please fill a issue report on the project's GitHub page. Hope it helps and we are looking forward to seeing you joined our community !
563e0f982d1761a701f0fa2d	X	I downloaded s3curl and used that to add the bucket policy to S3. See this link: http://blog.travelmarx.com/2012/09/working-with-s3curl-and-amazon-s3.html You can generate your bucket policies using the Amazon Policy Generator: http://awspolicygen.s3.amazonaws.com/policygen.html
563e0f992d1761a701f0fa2e	X	Do you have reason to believe that the method can fail without throwing an exception?
563e0f992d1761a701f0fa2f	X	This is pretty similar to what I recently did with the PHP API. I'd capture the originals etag, copy, compare original etag to copied etag, if match, delete original.
563e0f992d1761a701f0fa30	X	Historical note, following up on Dan's comment: It appears that the system for generating etags changes when a multipart upload is used as opposed to a regular upload, so comparison by MD5 might be the better approach.
563e0f992d1761a701f0fa31	X	Since S3 doesn't have immediate consistency, wouldn't this method occasionally show that the object hasn't been copied even though it has?
563e0f992d1761a701f0fa32	X	This is probably just me being unfamiliar with S3, but what explicitly would I check for on the etag string?
563e0f992d1761a701f0fa33	X	If you set it when you made your CopyObjectRequest, you should be able to compare the two for equality. Hope that helps.
563e0f992d1761a701f0fa34	X	Ultimately, if you're unsure, just test it. Use an file you don't care about.
563e0f992d1761a701f0fa35	X	I feel like I am missing some larger context, what is an ETag?
563e0f992d1761a701f0fa36	X	They explain it well in this thread: stackoverflow.com/questions/6591047/…
563e0f992d1761a701f0fa37	X	I'm trying to implement a move operation using the Amazon S3 Java API. The problem I am having is that the CopyObjectResult object returned by the AmazonS3Client.copyObject method doesn't seem to have a clear indicator about wiether the operation was successful or not. Given that after this operation I am going to be deleting a file, I'd want to make sure that the operation was successful. Any suggestions?
563e0f992d1761a701f0fa38	X	This is what I ended up doing, Note that this is Groovy code, but it is extremely similar to how the Java code would work. I don't like having to make two additional operations to check the metadata, so if there is anyway to do this more efficiently let me know.
563e0f992d1761a701f0fa39	X	I'm pretty sure you can just use CopyObjectResult object's getETag method to confirm that, after created, it has a valid ETag, as was made in the CopyObjectRequest setETag method. getETag public String getETag() Gets the ETag value for the new object that was created in the associated CopyObjectRequest. Returns: The ETag value for the new object. See Also: setETag(String) setETag public void setETag(String etag) Sets the ETag value for the new object that was created from the associated copy object request. Parameters: etag - The ETag value for the new object. See Also: getETag() Always trust the data. Been a year since I did a similar function with the Amazon PhP SDK a couple years ago, but it should work.
563e0f9a2d1761a701f0fa3a	X	possible duplicate of Amazon - EC2 cost?
563e0f9a2d1761a701f0fa3b	X	I have gone through AWS docs but they were not much clear, Again I am not much clear.. why is EBS required then if everything is to stored on S3? Also for daily users of about 500 which CPU instance should I purchase?
563e0f9a2d1761a701f0fa3c	X	You can store files on S3 without needing servers. S3 cannot run server-side code like PHP, but it can serve static websites, assets like images and videos, etc. It is an additional offering of AWS that has no direct comparison on a host like Hostgator. EBS is, again, like a traditional hard drive like you'd have on Hostgator.
563e0f9a2d1761a701f0fa3d	X	Sorry to bother you but again why should I host videos on S3 rather then EBS. I get that S3 will be faster and it doesn't run PHP but if I host videos on EBS then it will work just fine right? Also just last thing should I buy small or medium CPU for average of 500 users per day
563e0f9a2d1761a701f0fa3e	X	Re: "should I buy": serverfault.com/questions/384686/…
563e0f9a2d1761a701f0fa3f	X	Re: videos, EBS can handle videos, but S3 is generally a better place for large static files. For one, hosting them off S3 means your server isn't handling that bandwidth - it'll have more bandwidth available for the rest of what it has to do, and S3 has effectively infinite bandwidth for your level of usage. EBS can have disk failures just like any other hard disk, whereas S3 is engineered to be highly redundant.
563e0f9a2d1761a701f0fa40	X	I have PHP based website hosted on hostgator VPS, I have also subscribed to free tier of amazon. Now I am planning to shift website on amazon but I am not quiet sure how much is the capacity of the micro CPU of EC2 and also I have 5 GB of Amazon S3 storage plus 30 GB of amazon EBS Storage. I have data of almost 20 GB and I have video streaming site. Now I am having trouble figuring this out. I know if I run only one instance then it will cost me nothing for whole month. Thanks
563e0f9a2d1761a701f0fa41	X	Not much. Micro instances have severely restricted CPU if you use it much. It means any data your instances send to the outside Internet over 15 GB/month will cost you money. S3 is a storage system. EBS is more like a traditional hard disk on a server. You'd generally host video content off S3 rather than on an instance's EBS hard drive. Up to you. RDS is just an EC2 instance with MySQL installed that Amazon manages for you. If you'd prefer to manage it yourself, install MySQL on an EC2 instance. The AWS docs answer all of this and are well worth perusing.
563e0f9a2d1761a701f0fa42	X	I think you should be able to achive this by using curl and s3cmd. E.g. pipe the output of curl into s3cmd
563e0f9a2d1761a701f0fa43	X	Since my all data is in FTP server, how can i achieve with curl?
563e0f9a2d1761a701f0fa44	X	I would like to copy all my folders and files available from FTP server to Amazon S3 bucket. Tried to find the information on web to find the tools or AWS S3 provides any APIS for copy folders and files from FTP server. Any tools or pointers to links would be helpful
563e0f9a2d1761a701f0fa45	X	The AWS Command Line Interface (CLI) can be used to copy multiple files and folders to Amazon S3. It also has a sync command that can intelligently copy on new or modified files. This has nothing to do with your origin being an FTP server. Rather, it's a means of copying from some source (Windows, Linux, Mac) to Amazon S3. You can call the command from outside of your FTP software (eg as a script triggered hourly).
563e0f9a2d1761a701f0fa46	X	when i put wrong keys it returns output:S3 Object ( ) and put right key and print_r($s3); its again returns S3 Object ( ) then how trace key validation with this.
563e0f9b2d1761a701f0fa47	X	its working for me.
563e0f9b2d1761a701f0fa48	X	the official PHP API functions are little different: create_bucket() and delete_bucket() and to test for true if($return->status == 200). Otherwise works for me +1
563e0f9b2d1761a701f0fa49	X	yep: undesigned.org.za/2007/10/22/amazon-s3-php-class
563e0f9b2d1761a701f0fa4a	X	I'm learning the Amazon S3 PHP API, and I want to save my Amazon keys if these are valid for login into an Amazon account or alert the user if he puts wrong keys via the following code: Now, how can I check whether theses keys are valid or invalid and save them in a database if correct, e.g.: Does the object return any error if keys is valid or invalid? I don't know how get an authentication response, could someone please suggest a solution for it? Thank You.
563e0f9b2d1761a701f0fa4b	X	you may try this
563e0f9b2d1761a701f0fa4c	X	Amazon limits its response to max 1000 objects, as stated in the documentation. Higher values for $maxkey will not work here.
563e0f9b2d1761a701f0fa4d	X	Thanks for clarify. I edited an answer.
563e0f9b2d1761a701f0fa4e	X	This is the method in my S3 library file. [public static function getBucket($bucket, $prefix = null, $marker = null, $maxKeys = null) ]. What change should I make?
563e0f9b2d1761a701f0fa4f	X	With your first request you get 1000 object-keys. Take the last one and pass this as parameter for $marker on your second request to retrieve the next 1000 (max) objects
563e0f9b2d1761a701f0fa50	X	I'm totally new to this. Can you please edit the above code @ itinance
563e0f9b2d1761a701f0fa51	X	To be honestly, as a developer you should be able to call a method with a second parameter and also to read and understand documentation :) Stackoverflow is a platform to give you hints and answer your questions, not to do your work in general
563e0f9b2d1761a701f0fa52	X	I got it. I used iterator method.
563e0f9b2d1761a701f0fa53	X	I started listing the contents of a S3 bucket using foreach loop. But the problem is that it is not listing above 1000 index. The last few files are not listing. I tried getting the length of the array and I found that last few files are not even getting stored in that array. What should I do to list them all?
563e0f9b2d1761a701f0fa54	X	The default number of listed files is set to 1000, so thats why you cannot get more than 1000 files: Here you have a description of a function: Use $marker to set where to start getting files and $maxKeys as a limit. WWW: http://www.drupalcontrib.org/api/drupal/contributions!media_mover!contrib!mm_s3!S3.php/function/S3%3A%3AgetBucket/6 If you use the same library.
563e0f9b2d1761a701f0fa55	X	As stated in the S3 developer documentation: As buckets can contain a virtually unlimited number of keys, the complete results of a list query can be extremely large. To manage large result sets, Amazon S3 API support pagination to split them into multiple responses. Each list keys response returns a page of up to 1,000 keys with an indicator indicating if the response is truncated. You send a series of list keys requests until you have received all the keys. AWS SDK wrapper libraries provide the same pagination. That means you need multi page requests to iterate over multiple "pages". This will depend on the implementation of your "s3"-Class. Which library are you using for s3? If the implementation you are using offers a "getBucket"-Method, there will be an optional parameter $marker. Place here the key of the last object to retrieve the next 1000 objects after that object. Documentation: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e0f9b2d1761a701f0fa56	X	thanks Nicola!!!
563e0f9c2d1761a701f0fa57	X	@JohnZ if you come up with a S3 MediaStorage class and are interested in sharing it please get in touch with me, we could add it to the Eve repo or, at the very least, make an extension out of it.
563e0f9c2d1761a701f0fa58	X	indeed a good idea! will get you posted
563e0f9c2d1761a701f0fa59	X	I know we now can upload/post a media type field to the python-eve REST-API framework and it will be stored in mongodb. Is there a way we can change the storage of those media? E.g. a remote storage server e.g. amazon S3? So that we only store the URL of the image in mongodb instead of the whole image?
563e0f9c2d1761a701f0fa5a	X	While the standard behaviour is to store on GridFS, you can also provide your own MediaStorage subclass which can store wherever you want to (file system, S3, etc.) Something like this would do the trick: Check out the actual MediaStorage class for implementation details and/or see the actual GridFSMediaStorage class for reference.
563e0f9c2d1761a701f0fa5b	X	Is this possible? Is there a programme that will allow this to happen? I have a program that needs to access a lot of data from a central storage, but the likes of Amazon S3 only allows access via RESTful API which is no good for this program. Only UNC or drive letters are acceptable... Help! Bernard
563e0f9c2d1761a701f0fa5c	X	I believe that you can map a Windows drive letter to a WebDav store. There are plenty of online Webdav storage providers.
563e0f9c2d1761a701f0fa5d	X	downvoters please state teh reason, if I violate anything..
563e0f9c2d1761a701f0fa5e	X	which S3 class exactly?
563e0f9c2d1761a701f0fa5f	X	I have seen many posts through which we can upload image on S3, but what if we want to save image from google/facebook , that is something like graph.facebook.com/abc.img (suppose it is a valid image) I have seen a similar post but that uses Node.js but I simply want to use php to save image on s3, any help in this matter is appreciated Thanks
563e0f9c2d1761a701f0fa60	X	You could use this Facebook SDK PHP Class: https://developers.facebook.com/docs/reference/php/4.0.0 And this one for S3: https://github.com/tpyo/amazon-s3-php-class Should be pretty easy ;)
563e0f9d2d1761a701f0fa61	X	Could you check if this works <img ng-src="yanpy.dev.s3.amazonaws.com/img/boats/1/1.jpg">;
563e0f9d2d1761a701f0fa62	X	Configure S3 to allow access. If it is for a public website you might want to enable public access in S3.
563e0f9d2d1761a701f0fa63	X	@Sebastian I made the image public in S3 and it works. However, from the security point of view, is this right? Should it be public?
563e0f9d2d1761a701f0fa64	X	if you have a website which should be publicly accessible you must make it public (read-only of course)
563e0f9d2d1761a701f0fa65	X	Im developing a website with AngularJS in frontend that sends requests to a Rails 4 API backend. I have to manage quite images, so I would like to use Amazon S3 (but Im newbie with this and Im a bit lost). I have uploaded an image to folder in a new bucket (yanpy.dev) I have created in path: How can I get this image to display it in a view in my AngularJS front-end? Im trying something like: But its not working.
563e0f9d2d1761a701f0fa66	X	See @Sebastian comment above. Point for him.
563e0f9d2d1761a701f0fa67	X	I am writing script that connects to amazon S3 storage. The script is supposed to create 2 buckets: Bucket is for data Bucket is for logs I successfully created both buckets but I can't set up logging. Below is shown code I use for enabling bucket logging In amazon AWS API for PHP version 2 is written that Bucket, LoggingEnabled and Type are mandatory. But the documentation does not say how to exactly implement there parameters. Could you please help me with structure of config array for putBucketLogging method?
563e0f9d2d1761a701f0fa68	X	You can also use the service's API documents as a reference, which sometimes contain more details about how to specifically structure some of the data types for requests. The S3 API docs for PUT Bucket Logging have more details about how to specify the grantee. Also, you should not use capital letters in bucket names (See Rules for Bucket Naming).
563e0f9d2d1761a701f0fa69	X	After searching in manuals the php array for method putBucketLogging is However enabling logging fails with exception that tells me I have to set permissions on log bucket...
563e0f9d2d1761a701f0fa6a	X	Thanks bro, you really help me on this one
563e0f9d2d1761a701f0fa6b	X	This will upload the file both times in your S3 bucket, once during pick, once during store, the callback on store is called slowly, so if your user changes pages in between your file will only exist in filepicker's records and not your database. You could call it after pick, but then you will point to the wrong file.
563e0f9d2d1761a701f0fa6c	X	I am trying to use this technique, but the onSuccess callback seems to be returning an intermediate promise value of "temp.txt". How do I save a file without having to dramatically alter the filename to a non-human readable format using filepicker.js ?
563e0f9d2d1761a701f0fa6d	X	I've been uploading files to the amazon s3 using the javascript library filepicker.io and the implementation works ok, the problem I'm facing now is that when users upload files with white spaces or fancy characters those files are not accessible through http, i was wondering if there is any way to apply some kind of renaming to the file prior the uploading face. I'm using the pickAndStore method from the filepicker.io api Thanks.
563e0f9d2d1761a701f0fa6e	X	You can specify a "path" parameter in both the filepicker.store() call and filepicker.pickAndStore(), so if you want to specifically strip the whitespace from the filenames but keep the rest the same, the structure would be:
563e0f9e2d1761a701f0fa6f	X	You can also use path on pickAndStore but you won't be able to preprocess the inkblob.
563e0f9e2d1761a701f0fa70	X	Thanks Ryan, I was reading over the documents last night and what you are saying is correct. I have contacted their team after signing up to the affiliates programme. I shall update this question and links to relevant resources as I progress with this issue.
563e0f9e2d1761a701f0fa71	X	I would like to make requests for ItemSearch using Amazon Product Advertising API with meteor. http://s3.amazonaws.com/awsdocs/Associates/latest/prod-adv-api-dg.pdf https://images-na.ssl-images-amazon.com/images/G/01/webstore_t_d/API/WebstoreAPI_SearchProductUsersGuide.pdf Essentially, I would like the users on my web application to search and select books that they have read which will then be displayed in their profile. As the user types in the field, I would like the api to return a limited number of suggestions. When one item is selected, I would like to store the title and author of the book and url of the books' advertisement page on amazons website. I have been sourcing the documents and branching out from the following two links. I am beginning to understand in an abstract way of how the error and data callbacks work. Everything I am reading is abstract. I need to help in setting up the searchItem feature which falls under Amazon Product Advertising API . I will limit the search index to the 'books' product category. According to amazon, I am effectively advertising for amazon in my use case so I joined their affiliates program https://affiliate-program.amazon.com/ But really, my use case intensions are for my users to list books they have read on their profile page. I need to capture 3 data points (title, author, and, url of the advertising page for the book on amazon.com). Has anybody attempted to use this API? If so, please can you shed light on how you set up to make requests to the API in meteor.
563e0f9e2d1761a701f0fa72	X	The Amazon Product Advertising API does not fall under Amazon Web Services, but instead, Amazon Associates. The AWS SDK does not support non-AWS services (including other Amazon services), and likely never will. You'll need to find an entirely different package for hitting the Amazon Product Advertising API.
563e0f9e2d1761a701f0fa73	X	I should clarify, that the problem with Marcos is that the upload from Filepicker.io when using a service other than local computer upload (ie: Dropbox, File URL, Box, Webcam, etc) is not sending the file to the configured S3 bucket. That's why he is not getting the S3 file key, because Filepicker.io is not uploading the file to S3 when using other options besides local upload. What we need to know is why this is happening and how to fix it.
563e0f9e2d1761a701f0fa74	X	Hey thanks this has solved my problem.
563e0f9e2d1761a701f0fa75	X	In case anyone hits this, you will not receive the key if you are on a free plan.
563e0f9e2d1761a701f0fa76	X	I've been using Filepicker.IO in order to upload files directly from the browser to the amazon s3 and most things are working fine, the only problem i'm facing now is that after the upload is done, i'm not getting the name of the file in the s3. Filepicker js api is returning this object: Usually this object comes with a property named 'key' which has the name of the file in the S3. This happens when the upload is not done from the local computer, if i pick a local file everything works ok, but if i pick a file from any of the providers (e.g Dropbox, Google Drive), i can't get the filename in the S3 server. Thanks.
563e0f9e2d1761a701f0fa77	X	You should make sure that you are using a function that is explicitly storing to S3, for instance filepicker.pickAndStore or filepicker.store. As noted in the filepicker.io pick API documentation, the "key" parameter on fpfiles returned specifically from the .pick() call are deprecated and not meant to be used.
563e0f9e2d1761a701f0fa78	X	I should have explained better that we are using CopyObjectRequest to do this now. I've updated the question to reflect this.
563e0f9e2d1761a701f0fa79	X	The ObjectMetadata.setHeader is marked as internal use only. Have you used this successfully? Our code is only going to be run once, so we won't need to worry about it later if Amazon makes changes. But YMMV.
563e0f9e2d1761a701f0fa7a	X	Hmmh, I wasn't aware of that restriction indeed, but recall having used Expires: at some point a while back; I might be wrong though, insofar I often use other SDKs for interacting with S3 (e.g. C#/Python, which do definitely support this) and could have mixed that up - the code itself doesn't differ from the other setXYZHeader() methods currently (see ObjectMetadata.java), so the restriction would be based on a non visible side effect, if any.
563e0f9e2d1761a701f0fa7b	X	It's probably used by the other header methods internally and internal only because only certain headers will work. They should add a setExpires method on ObjectMetadata as it's the only header that doesn't have it's own method.
563e0f9e2d1761a701f0fa7c	X	The setHeader method solved this for us, thanks!
563e0f9f2d1761a701f0fa7d	X	Thanks. You are absolutely correct Cache-Control supersedes the Expires header. Still, we'd like to include it for HTTP/1.0 clients that do not respect Cache-Control.
563e0f9f2d1761a701f0fa7e	X	I'm updating existing objects in an Amazon S3 bucket to set some metadata. I'd like to set the HTTP Expires header for each object to better handle HTTP/1.0 clients. We're using the AWS Java SDK, which allows for metadata changes to an object without re-uploading the object content. We do this using CopyObjectRequest to copy an object to itself. The ObjectMetadata class allows us to set the Cache-Control, Content-Type and several other headers. But not the Expires header. I know that S3 stores and serves the Expires header for objects PUT using the REST API. Is there a way to do this from the Java SDK? Updated to indicate that we are using CopyObjectRequest
563e0f9f2d1761a701f0fa7f	X	To change the metadata of an existing Amazon S3 object, you need to copy the object to itself and provide the desired new metadata on the fly, see copyObject(): By default, all object metadata for the source object are copied to the new destination object, unless new object metadata in the specified CopyObjectRequest is provided. This can be achieved like so approximately (fragment from the top of my head, so beware): Please be aware of the following easy to miss, but important copyObject() constraint: The Amazon S3 Acccess Control List (ACL) is not copied to the new object. The new object will have the default Amazon S3 ACL, CannedAccessControlList.Private, unless one is explicitly provided in the specified CopyObjectRequest. This is not accounted for in my code fragment yet! Good luck!
563e0f9f2d1761a701f0fa80	X	We were looking for a similar solution and eventually settled for max-age cache-control directive. And we eventually realized that hte cache-control overrides the Expires even if expires is more restrictive. And anyways cache-control met our requirement as well.
563e0f9f2d1761a701f0fa81	X	What is your actual question?
563e0f9f2d1761a701f0fa82	X	1-) What is wrong with my amazon uploading code? 2-) How to implement uploading in background which I mentioned above.
563e0f9f2d1761a701f0fa83	X	Wire up the NSURLSession Progress delegate then you would know- If the upload hangs after say 128 KB transfer then there is a problem in request.
563e0f9f2d1761a701f0fa84	X	I have a generic question, experienced developers may laugh at me when they read that but I couldn't manage it somehow. Basically, I want to upload images from filesystem to my amazon s3 bucket and after that upload links of these images to my server. After I upload images to s3, I got links of them and store them in an array. Then I want to upload those links to my server. But, in order to cover uploading against application suspending I want to make this process run in background. The schema is below. Upload images to s3 -> get s3 links of these images -> upload links to server I'm trying to use presignedURL utility of Amazon SDK since Amazon recommends to use it for background uploading. iOS API docs says NSURLSessionUploadTask does what I'm trying to do. Firstly, I couldn't manage to upload images to s3 with presignedURL (I managed to upload with TransferManagerUploadRequest). Secondly, this process should be running even if application is suspended. But I don't know how to do that actually. I put my "upload-to-s3" code below which is not working right now. Sorry for the complicated question, it indeed shows my mind right now.
563e0f9f2d1761a701f0fa85	X	I think it's cool if it can. Maybe you can send e-mail to support@github.com
563e0f9f2d1761a701f0fa86	X	Is there a way to override the file in the Downloads section on GitHub when uploading a file with the same filename? (e.g. via developer API or the ruby script, etc) The reason is that I want to keep track of the number of downloads. Thanks!
563e0f9f2d1761a701f0fa87	X	I havn't tried this but it's possible that you could replace the file on Amazon S3. I don't know if it will work or if it's a one-time upload token you get without the posibility to delete the file. See the API documentation for uploading a file on Github (which includes using the amazon s3 rest api to upload the file): http://developer.github.com/v3/repos/downloads/ And API documentation for deleing a file on amazon s3: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectDELETE.html And API documentation for putting a file on amazon s3: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectPUT.html
563e0fa02d1761a701f0fa88	X	I have an app using RestKit successfully. I am building an IAP in the app and for iOS5 I needed a place to host the app files for the IAP. I decided using Amazon S3 for that. Now I now that I can integrate amazon API but I wish to keep my app simple, and since I am using RestKit I just want to use it to download the files. Is there a guide or explanation on how to generate a bucket url with expiration and secrets ? Thanks Shani
563e0fa02d1761a701f0fa89	X	Sure: all the information you need is in the Authenticating REST Requests documentation page. Also, it's not entirely clear from your question, but I hope you're putting the URL generation in some web app somewhere that you control, rather than directly embedding it in the IOS app. I also hope you're using IAM to restrict that key to the appropriate permissions level regardless.
563e0fa02d1761a701f0fa8a	X	I'm currently serving up static images to my site via Amazon Cloudfront and for some reason my images won't update when I try to overwrite them with an updated image. The old image continues to display. I've even tried deleting the entire images folder and uploading the newest ones without success. The only thing that works is renaming the image. Anyone else experience this?
563e0fa02d1761a701f0fa8b	X	recently amazon s3 announced new feature called content invalidation. it allows you to invalidate some file just with a one call. check cloudfront api references for more details.
563e0fa02d1761a701f0fa8c	X	Actually I think these are too many questions for one question. But some of them are really interesting.
563e0fa02d1761a701f0fa8d	X	Yeah, actually the is only one question: Where we can find materials about Amazon MapReduce best practices for logs analysis? Updated the description.
563e0fa02d1761a701f0fa8e	X	I'm parsing access logs generated by Apache, Nginx, Darwin (video streaming server) and aggregating statistics for each delivered file by date / referrer / useragent. Tons of logs generated every hour and that number likely to be increased dramatically in near future - so processing that kind of data in distributed manner via Amazon Elastic MapReduce sounds reasonable. Right now I'm ready with mappers and reducers to process my data and tested the whole process with the following flow: I've done that manually according to thousands of tutorials that are googlable on the Internet about Amazon ERM. What should I do next? What is a best approach to automate this process? What are common practices for: Sure in most cases it depends on your infrastructure and application architecture. Sure I can implement that everything with my custom solution, possibly re-investing a lot of thing that are already used by others somewhere. But there should be some kind of common practices that I would like to become familiar with. I think that this topic can be useful for many people who trying to process access logs with Amazon Elastic MapReduce, but wasn't able to find good materials about best practices to handle that. UPD: Just to clarify here is the single final question: What are best practices for logs processing powered by Amazon Elastic MapReduce? Related posts: Getting data in and out of Elastic MapReduce HDFS
563e0fa22d1761a701f0fa8f	X	That's a very very wide open question, but here are some thoughts you could consider: Hope that gives you some clues.
563e0fa32d1761a701f0fa90	X	I have made a ruby on rails application with amw S3. I can upload the photo, but I try to delete the photo, even though the photo reference is deleted in database, the photo is still in S3. How do I actually delete files from S3?
563e0fa32d1761a701f0fa91	X	Your question isn't correctly specified. If you could provide more information, it would be great. But here are some options how to handle files on Amazon S3. According to this documentation you can use method delete(see the implementation) from the Ruby Library for Amazon's Simple Storage Service's (S3) REST API. So the should look like this: Very helpful could be also this question. Anyway I highly recommend you to use Paperclip or Carrierwave when uploading files to Amazon S3. Some helpful articles:
563e0fb32d1761a701f0fa92	X	I've been trying to get Authorization for Amazon's s3 rest api going. It's pretty damn complicated. Because I'm trying to make a simple GET request from an admin page on my website, I'm just trying to do this through Javascript. Here are the instructions for constructing the Signature for the Authorization header: To keep us sane, they give us a few examples, with the following givens: The output for this in their docs is bWq2s1WEIj+Ydj0vQ697zp+IXMU=. Based on the following I am getting ZGVjNzNmNTE0MGU4OWQxYTg3NTg0M2MxZDM5NjIyZDI0MGQxZGY0ZQ==: I used code.google.com's CryptoJS.HmacSHA1 function for the SHA1 hashing. My final Signature function looks like this: What is going wrong here???
563e0fb32d1761a701f0fa93	X	I actually found the answer from an SO question with reference to google's older (2.0) CrytpoJs library. You need: Then you create your signature as so: I couldn't find a way to to get Strings instead of Bits in the new version.
563e0fb32d1761a701f0fa94	X	EC2 instances are persistent if you create them as EBS backed volumes, which you should almost always do. stackoverflow.com/a/3630707/141172
563e0fb32d1761a701f0fa95	X	Hi, Michael - Are you saying my app should directly call S3 iOS API to save photo in S3? I saw a post saying S3 is "eventual consistency" which means after a successful photo upload, the photo may not be immediately available for read. But the iPhone app does need read the photo from server to display on the screen right away. This way the iPhone app will interpret the upload as a "failure" and will try to upload again.
563e0fb32d1761a701f0fa96	X	Eric - please check the following stackoverflow.com/questions/2288402/…
563e0fb32d1761a701f0fa97	X	Hi @100calorie, S3 provides read-after-write consistency in most of the regions (see here), so bear this in mind while picking the region for your buckets.
563e0fb52d1761a701f0fa98	X	I am creating an iPhone app to allow users upload & share photos. Currently the photos uploaded are stored in my 1and1 cloud server I subscribed. Now I want to try AWS. I have subscribed a free tier AWS Linux EC2 and set up php/mysql. My question is, for scalability purpose, where should I store user pictures: EC2 or S3? And how to connect EC2 with S3 so user uploaded photos will be stored in S3? My understanding is that when user upload a photo to my EC2 instance, it is stored in EC2 and it will fill the space soon since I have only 5GB space. With limited knowledge of AWS, my question may sound st**d but any help and advice will be appreciated!
563e0fb52d1761a701f0fa99	X	You should store your pictures in S3, data stored within your EC2 instances are not persistent. Use AWS SDK to upload data to S3.
563e2e4161a801306526704e	X	For this use case I would use S3. The advantage of using S3 backing for your pictures is that you can easily use Amazon's Cloud Front CDN with S3 as the origin (you can also use your EC2 instance, but that involves more work). And how to connect EC2 with S3 so user uploaded photos will be stored in S3 There is an S3 API for PHP http://aws.amazon.com/sdkforphp/
563e2e4161a801306526704f	X	I think this might be what you are looking for stackoverflow.com/questions/8310462/…
563e2e4161a8013065267050	X	HI, thanks for the links. can you suggest some video tutorials to get me started? I am totally new to this and find the AWS forums a little overwhelming.
563e2e4161a8013065267051	X	@user1652930 Afraid I can't. There might be some out there but I can't stand video tutorials personally and thus never remember them when I see them.
563e2e4161a8013065267052	X	I trying to explore AWS S3 and I found out that we can store data and have a URL for a file which can be used on a website, but my intention is to store files on S3 and have users of my website post and retrieve files to/from S3 without my intervention. I am trying to have my server and JSP/Servlets pages on EC2 on which Tomcat (and MySQL server) will be running. Is this possible and if yes, how can i achieve this. Thanks, SD
563e2e4161a8013065267053	X	Yes, it's possible. A full answer to this question is tantamount to a consulting gig, but some resources that should get you started:
563e2e4161a8013065267054	X	Also, check-out this article on using SimpleDB with Ruby geekin.gs/using-amazon-aws-simpledb-with-ruby-roundup
563e2e4161a8013065267055	X	Are there any online services/servers that could store information like: So that it could be retrieved by Ruby script?
563e2e4261a8013065267056	X	For a simple key/value store in the cloud check out Amazon SimpleDB For complex relational data use a database. If you want a database in the cloud check-out Amazon RDS.
563e2e4261a8013065267057	X	Yes, they are also known as databases. You can set up your own db if you have a server, or you can try and find someone who will host databases for you (try Googling "Free MySQL" for example)
563e2e4261a8013065267058	X	Amazon S3 is an online server that stores your variables, properties, files online and allows you to retrieve them via kind of API.
563e2e4261a8013065267059	X	With a small amount of knowledge about how git works, you could easily set up a 1-file rack or Sinatra application on heroku to do this.
563e2e4261a801306526705a	X	OpenKeyval Simple Key/Value storage. And it's Free. Not secure though.
563e2e4261a801306526705b	X	I need to find a storage service that can programatically (REST API) send (FTP) a file to a third party service. I was thinking of using Amazon S3, but I found a previous similar question here: Sending file from S3 to third party FTP server using CloudFront and apparently it cant be done. What I want to avoid is downloading the file to my app server and then upload it to the third party server. I want to send a command to the storage server to send it directly to the third party server. If I can't find a storage service that delivers this functionality, pricing is my second priority. Right now I'm thinking either box.net, sugarsync or dropbox, as well as amazon S3 since all of these provide a REST API I can use. Thank you for your help!
563e2e4261a801306526705c	X	Thanks will give it a go but it looks like what i need
563e2e4261a801306526705d	X	I am now at the point where i cant get any further with out some help.I am trying to host files on the cloud and then access those files via code (C#). So far i have tried Rapidshare and Skydrive and have been unable to get either working at all. Below is a few things that i am trying to do or rather must be able to do with the cloud storage. I don't really mind having to pay as long as the price is not ridiculous.Any help at all will be much appreciated. Thanks Stalkerh
563e2e4261a801306526705e	X	Why don't you look at Amazon S3 they do what you want, are cheap and have a C# API wrapping their web service (But ThreeSharp is better).
563e2e4361a801306526705f	X	Yes you're right. I think it was the backslashes that caused the issue.
563e2e4361a8013065267060	X	I'm using the .NET API straight from Amazon to upload some files to S3. However, I'm getting the exception message: The request signature we calculated does not match the signature you provided. Check your key and signing method. My code is as follows: Is there anything immediately obvious I'm doing wrong? The ProcessFiles method just returns a list of filenames in that directory.
563e2e4361a8013065267061	X	Does ProcessFiles return just the filename or the complete path? Regardless, it's unlikely that FilePath and Key should be set to the same thing. FilePath should be set to the full path of the local file to upload. eg c:\Dev\pktest\myfile.txt Key is the name of the file to store on S3. eg myfile.txt. Or if you want keep the same path structure: Dev/pktest/myfile.txt (note the forward slashes)
563e2e4361a8013065267062	X	Amazon supports a single byte range request Response header from the above request correctly shows Content-Length: 500 However if you add another range It ignores the range request and gives the content length of the whole file Content-Length: 1274819234 Does anyone know if amazon s3 supports multiple byte range requests? Or a workaround?
563e2e4361a8013065267063	X	According to the doc as well as api still Amazon S3 does not supports multiple byte range requests. Can you tell us the use-case ?
563e2e4361a8013065267064	X	(1) I can list the files on a folder this way: However, this: or this: does not work. How can I query the files on the root folder? (2) The code above returns the list of files in a folder. How can I get the list of folders within a folder? I there any parameter I can set to get this list? Note: I know that this is a 'flat' file system, similar to Amazon S3. However, both (cloudfiles and S3) provides a way to work with 'folder'. In S3 is easy. In cloudfiles (with the .net API) I could not find how to do this. Any hint will be highly appreciated.
563e2e4361a8013065267065	X	This has just been fixed with the latest push and closes issue #51 on github Link to downloadable package Hope this helps.
563e2e4461a8013065267066	X	Great Chris, i didn't know that. Just 15cents per Gig/month? Are you sure ? Is sooo good to be true =)
563e2e4461a8013065267067	X	I bought this service Chris, great tip you gave me. I already learn how upload files using API, now i'm trying to figure out how to retrieve the url from the object i store there. Thanks dude.
563e2e4461a8013065267068	X	Can i upload some image in Smug Mug ?
563e2e4461a8013065267069	X	Yes, it is similar to Flickr and Picasa. The API would let you upload. But, truly, it is more for photo sharing, not sure if there are access restrictions, bandwidth limits or ??. You should also check out DropBox (I highly recommend it), it will let you have a Public section for files.
563e2e4461a801306526706a	X	That's ok Harscware i bought Amazon S3 service, great service dude. Thanks by the help.
563e2e4461a801306526706b	X	i'm looking for some api that i can upload an image in their server and then retrieve the url to store in my MySQL. Any advice ? Best regards, Valter Henrique.
563e2e4461a801306526706c	X	Amazon S3 has a nice API, does exactly what you need and is very cheap. 15 cents per Gig/month. http://aws.amazon.com
563e2e4461a801306526706d	X	I wonder if the Smug Mug Java API might do the trick. Surely, you should check out the DropBox API (with DropBox you get 2GB storage free, files are sync'd to your desktop).
563e2e4461a801306526706e	X	Google App Engine is the first that comes to mind. You have to enable billing, but it's free if you stay under a certain quota. You can use their Blobstore to serve files up to 2 gigabytes, and it's very fast. A quick search found this overview and guide, but Google's own documentation is excellent.
563e2e4561a801306526706f	X	I'm designing a RESTful API that should handle binary file uploads. Should this be done the same way as html form upload or is there a better way?
563e2e4561a8013065267070	X	Take a look at the Amazon api for an idea. It uses a PUT query and then through sendREST it sends the content. Uploading files to Amazon S3 with REST API
563e2e4561a8013065267071	X	A good way is to upload the binary information using streams. You could have a look at the JeCARS client project. To be exact the JC_RESTComm.java class performs the upload.
563e2e4561a8013065267072	X	"Taking an input and applying a one-way function is called "hashing" the input, and what Amazon stores on their system is a "hash" of your secret key" - If Amazon stores a HASH of your secret key, how is it possible for Amazon to HASH the text sent to them ?
563e2e4561a8013065267073	X	First you say "what Amazon stores on their system is a "hash" of your secret key" and then later "Amazon looks up their copy of the secret key". These seem to contradict each other. I believe the first statement is wrong.
563e2e4561a8013065267074	X	foursquare for example have a client ID and a client secret you get when you sign up. And I need to send both to get a response. So isnt this just going to be able to be seen and intercepted on the 'wire'. They dont make mention of making or forcing https. There is also a common scenario of a serverless App these days so it was a) send the request to FS via the server (still able to see intercepted) b) send via client browser to FS (very easy to spot the client key and secret.) What am I missing...the secret is surely like a password you must never SEND it http always https.
563e2e4561a8013065267075	X	This url tells more details of Amazon S3 Auth implementation - docs.aws.amazon.com/AmazonS3/latest/dev/S3_Authentication2.html
563e2e4561a8013065267076	X	"Theoretically, any mathematical functions that maps one thing to another can be reversed" - Thats not true, hash functions are the example. it is very easy to show. lets say we have a function that turns words into numbers, based on sum of values(a=1, b=2, c=3 etc). Eg "SO" would be 18 + 14 = 32. So we have changed SO into 32 but if i reveal this function to somebody, and give him number 32, there is no way he can know if our basic word was "SO" or "ZF"(26+6) or one of dozens other possibilities
563e2e4661a8013065267077	X	I am just starting to think about how api keys and secret keys work. Just 2 days ago I signed up for Amazon S3 and installed the S3Fox Plugin. They asked me for both my Access Key and Secret Access Key, both of which require me to login to access. So I'm wondering, if they're asking me for my secret key, they must be storing it somewhere right? Isn't that basically the same thing as asking me for my credit card numbers or password and storing that in their own database? How are secret keys and api keys supposed to work? How secret do they need to be? Are these applications that use the secret keys storing it somehow? Thanks for the insight.
563e2e4661a8013065267078	X	Basically elaborating on what's outlined here. Here's how it works: let's say we have a function that takes a number from zero through nine, adds three and, if the result is greater than ten, subtracts ten. So f(2) = 5, f(8) = 1, etc. Now, we can make another function, call it f', that goes backwards, by adding seven instead of three. f'(5) = 2, f'(1) = 8, etc. That's an example of a two-way function and its inverse. Theoretically, any mathematical functions that maps one thing to another can be reversed. In practice, though, you can make a function that scrambles its input so well that it's incredibly difficult to reverse. Taking an input and applying a one-way function is called "hashing" the input, and what Amazon stores on their system is a "hash" of your secret key. SHA1 is an example of this kind of "one-way" function, it's also hardened against attacks. The HMAC function builds on established hash functions to use a known key to authenticate a string of text. It works like this: The difference between this and PKI is that this method is RESTful, allowing a minimum number of exchanges between your system and Amazon's servers. Isn't that basically the same thing as asking me for my credit card numbers or password and storing that in their own database? Yes, though the damage someone can do with S3 seems to be limited to draining your account. How secret do they need to be? Are these applications that use the secret keys storing it somehow? At some point, you're going to have to load the secret key, and with most Unix based systems, if an attacker can get root access they can get the key. If you encrypt the key, you have to have code to decrypt it, and at some point the decryption code has to be plain text so it can be executed. This is the same problem DRM has, except that you own the computer. In many cases, I just put secret keys in a file with limited permissions, and take the usual precautions to prevent my system from being rooted. There are a few tricks to make it work properly with a multiuser system, such as avoiding temporary files and such.
563e2e4661a8013065267079	X	Public Key Cryptography is used to defend against very specific attacks, some of which are common. In short this is complex math that allows one to verify that at individual has both the Public and Private Key pair while only knowing the public key. This is very different from a credit card or static password. As an example if you are authenticating with an OpenSSH server then the server doesn't need the private key. Ideally if Amazon's API database where to be compromised the attacker would have a list of public keys and would be unable to access the user's API using this information. However ideal systems are not always put into practice and i don't know for sure if Amazon is protecting against this attack vector, but they should be. In public key authentication is statistically immune to brute force. Passwords are often dictionary words which can be broken relativity fast. However a private key is a massive number that isn't easy to guess. If the attacker had the public key then they could perform many guesses "offline" on a super computer, but even then it would take a lot of time and money to break the key.
563e2e4661a801306526707a	X	Pause upload? There isn't a way to do that in PHP. PHP just uses commands to upload a file and they are just executed, you cannot alter them.
563e2e4661a801306526707b	X	Is any other way to do the pause and resume functionality ?? but that should work with the help of PHP langauage..Because I have a php web application , from there I need to give the upload data , pause and resume option...
563e2e4761a801306526707c	X	Like I said. I never heard about pause upload in PHP. PHP is a server side scripting. As soon as you start a process you cannot pause it from client side: eg. Browser
563e2e4761a801306526707d	X	Hi Lorga, Thanks for reply.. Do u know how the amazon is using the pause and resume functionality inside the console..(Means when we login into Amazon console panel, they have direct upload option for any data.)????
563e2e4761a801306526707e	X	Iorga ... with i :). Don't know ... but I'm sure it's not PHP :)
563e2e4761a801306526707f	X	I am trying to upload the file to amazon s3 bucket using the PHP-REST API and my code is working fine..but I want to know how can I implement the pause and resume functionality by using the Rest API. currently I am following one blog post for uploading the file:- http://www.anyexample.com/programming/php/uploading_files_to_amazon_s3_with_rest_api.xml So, I need to implement the reusme and pause fucntionality..Plz help me on this..
563e2e4761a8013065267080	X	I think your answer is better, supposing the apache commons library is available.
563e2e4761a8013065267081	X	Trying to pull an image off of Amazon S3 (returns S3ObjectInputStream) and send it to the mandrill email api (takes a base64-encoded string). How can this be done in Scala?
563e2e4761a8013065267082	X	Here is one solution, there are probably others more efficient. You could (and should) also replace the sun.misc encoder by the apache commons Base64 for a better compatibility.
563e2e4761a8013065267083	X	I also managed to do it just using the Apache commons; not sure which approach is better, but figured I'd leave this answer for the record:
563e2e4761a8013065267084	X	Here's a simple encoder/decoder I wrote that you can include as source. So, no external dependencies. The interface is a bit more scala-esque: import io.github.marklister.base64.Base64._ // Same as Lomig Mégard's answer val b64 = bytes.toBase64
563e2e4761a8013065267085	X	did you make any progress with this? I have a different scenario whereby knox appears to do all the setup (checking debug etc) but the registered callback functions never get invoked from req response or error events. Spent 2 hours double checking all settings. The URL generated by knox from input configuration is all correct. s3cmd works fine with same settings.
563e2e4761a8013065267086	X	To the best of my knowledge, I'm not behind a proxy here at work. As with any corporate connection, I'm sure there are a handful of devices between my desktop and AWS, however I haven't had to worry about configuring proxy servers for any other connections from my desktop. The region parameter is an interesting idea. While we are hosting out of us-east, I wonder if it would help to make that explicit. I only see 'us-standard' as a possible value in the link you shared, however, so it seems unlikely that this will help. I'll give it a try!
563e2e4861a8013065267087	X	us-standard is the "default". i see, "us-east-1" is missing in that list. set the region option to "us-east-1", that should fix your problem!!!
563e2e4861a8013065267088	X	As the knox docs say, see Amazon's endpoint documentation for the up–to–date list (it seems us-standard is no longer a region name; use us-east-1).
563e2e4861a8013065267089	X	I'm trying to integrate an S3 deployment step into my Grunt toolchain to upload the newly built file out to AWS. However, the step always fails silently (claims to succeed but doesn't do anything), and while debugging the results I've found a few different points along the way that are getting hung up. I'm using grunt-s3 as the package that handles the grunt commands, which in turn calls the knox package which wraps Amazon's S3 API. Here's where things are falling apart: 1) There's a point in the logic where knox uses the fs package to try to get the size of the file it's about to upload via fs.stat(file, callback). Near as I can tell, the process dies somewhere under the node.js layer between the fs.stat call and the callback getting invoked. I have set breakpoints and 'debugger' statements all over the place in the callback logic and neither node-inspector nor the IntelliJ debugger can seem to catch the process after fs.stat() is called. 2) If I hack the knox plugin and change the fs.stat call to fs.statSync(), the process successfully moves forward. However, later in the process I can see knox set up the expected PUT URL with S3 to upload the file and then call stream.pipe() to upload the file. Nothing seems to happen as a result of the stream.pipe() call, and I can't see any activity on WireShark that indicates an upload between my computer and AWS taking place. However, if I use the command line tool s3cmd to do the upload, the file uploads fine. I'm about ready to ditch grunt for this step and move to directly invoking s3cmd, but I'd love to do it the grunt way if possible. Anyone have any suggestions as to what might be happening during these two steps? Thanks!
563e2e4861a801306526708a	X	are you sitting behind i proxy? if so, knox will not work. if not, how does your s3-config look like? another important thing to notice is the location of your bucket. manually setting the region (in my example "eu-west-1") helped for me, because knox sets region to "us-standard" per default. see a list of possible values here, check in your bucket-properties where yours is located, and set that value manually! here a (for me) working config: }
563e2e4861a801306526708b	X	Thank you for your such a good explanation! Probably we will go with CORS approach and create workaround for IE.
563e2e4861a801306526708c	X	Glad to. If this solves your problem, please mark it as answered.
563e2e4861a801306526708d	X	With AWS services we have the Web application running from the S3 bucket and accessing the data through the REST API from Load Balancer (which is set of Node.js applications running on EC2 instance). Currently we have specified URL's as following: But having this setup brought us a set of problems since requests are CORS with this setup. We could workaround CORS with special headers, but that doesn't work with all browsers. What we want to achieve is running API on the same domain but with different path: One of the ideas was to attach the API Load Balancer to the CDN and forward all request to Load Balancer if query is coming on the "/api/*" path. But that doesn't work since our API is using not only HEAD and GET requests, but also POST, PUT, DELETE. Another idea is using second EC2 instance instead of S3 bucket to host website (using some web server like nginx or apache). But that gives too much overhead when everything is in place already (S3 static content hosting). Also if using this scenario we wouldn't get all the benefits of Amazon CloudFront performance. So, could your recommend how to combine Load Balancer and S3, so they would run on same domain, but with different paths? (API on somedomain.com/api and Web App on somedomain.com) Thank you!
563e2e4861a801306526708e	X	You can't have an EC2 instance and an S3 bucket with the same host name. Consider what happens when a web browser makes a request to that host name. DNS resolves it to an IP address (or addresses) and the packets of the request are delivered to that address. The address either terminates at the EC2 instance or the S3 bucket, not both. As I understand your situation, you have static web pages hosted on S3 that include JavaScript code that makes various HTTP requests to the EC2 instance. If the S3 web pages are on a different host than the EC2 instance then the same origin policy will prevent the browser from even attempting some of the requests. The only solutions I can see are: The first method is no good, because you almost might as well not use S3 at all in that case. The second case should be okay for you. It should work in any browser, because it's not really CORS. So no CORS headers are needed. But it's tricky. The third, CORS, approach should be just fine. Your EC2 instance just has to return the proper headers telling web pages from the S3 bucket that it's safe for them to talk to the EC2 instance.
563e2e4861a801306526708f	X	I need help with aws s3 Rest auth. I have the next code: I need help i'm geeting the next error: (403) Forbiden. The authentication is done following the steps on this link: http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html. The credentials are: PublicKey: 25496a25b6554f54b5e6 PrivateKey: 551a656b548e8466f555d540156b5a You just need to use the date for the SHA1, there is no need to use the entire the request string. Example headers: Date: Mon, 26 Mar 2007 19:37:58 +0000 Authorization: FXST AKIAIOSFODNN7EXAMPLE:frJIUN8DYpKDtOLCwo//yllqDzg=
563e2e4861a8013065267090	X	Your problem seems to be your canonicalString and your HttpWebRequest , I had the same problem, please see the solution in this post: Amazon S3 REST API 403 error c# That's sad how Amazon API is badly documented
563e2e4861a8013065267091	X	Be careful how you use http statuses like 401, 409 and 412 - they have particular meaning in the HTTP protocol and aren't codes you can just decide to use in some generalised error scenario because you like the way the wording sounds :) '422 unprocessable entity' is probably what you're looking for as a general-purpose "while it was syntactically valid for its media type, we were unable to accommodate the semantics of the submitted request entity" tools.ietf.org/html/rfc4918#section-11.2
563e2e4961a8013065267092	X	Note that 422 is a WebDAV specific extension. The 400 status code would be more appropriate.
563e2e4961a8013065267093	X	It's an extension defined in WebDAV, but still a generic HTTP status code.
563e2e4961a8013065267094	X	If you 200 and an extra header for the success, you are tunneling a protocol over HTTP, instead of using HTTP properly. Don't do that.
563e2e4961a8013065267095	X	Among the data my application sends to a third-party SOA server are complex XMLs. The server owner does provide the XML schemas (.xsd) and, since the server rejects invalid XMLs with a meaningless message, I need to validate them locally before sending. I could use a stand-alone XML schema validator but they are slow, mainly because of the time required to parse the schema files. So I wrote my own schema validator (in Java, if that matters) in the form of an HTTP Server which caches the already parsed schemas. The problem is: many things can go wrong in the course of the validation process. Other than unexpected exceptions and successful validation: Since it's an HTTP Server I'd like to provide the client with meaningful status codes. Should the server answer with a 400 error (Bad request) for all the above cases? Or they have nothing to do with HTTP and it should answer 200 with a message in the body? Any other suggestion? Update: the main application is written in Ruby, which doesn't have a good xml schema validation library, so a separate validation server is not over-engineering.
563e2e4a61a8013065267096	X	It's a perfectly valid thinking to map error situations in the validation process to meaningful HTTP status codes. I suppose you send the XML file to your validation server as a POST content using the URI to determine a specific schema for validation. So here are some suggestions for error mappings:
563e2e4a61a8013065267097	X	Status code 422 ("Unprocessable Entity") sounds close enough: "The 422 (Unprocessable Entity) status code means the server understands the content type of the request entity (hence a 415(Unsupported Media Type) status code is inappropriate), and the syntax of the request entity is correct (thus a 400 (Bad Request) status code is inappropriate) but was unable to process the contained instructions. For example, this error condition may occur if an XML request body contains well-formed (i.e., syntactically correct), but semantically erroneous, XML instructions."
563e2e4a61a8013065267098	X	Amazon could be used as a model for how to map http status codes to real application level conditions: http://docs.amazonwebservices.com/AWSImportExport/latest/API/index.html?Errors.html (see Amazon S3 Status Codes heading)
563e2e4a61a8013065267099	X	Say you're posting XML files to a resource, eg like so: POST /validator Content-type: application/xml If the request entity fails to parse as the media type it was submitted as (ie as application/xml), 400 Bad Request is the right status. If it parses syntactically as the media type it was submitted as, but it doesn't validate against some desired schema, or otherwise has semantics which make it unprocessable by the resource it's submitted to - then 422 Unprocessable Entity is the best status (although you should probably accompany it by some more specific error information in the error response; also note it's technically defined in an extension to HTTP, WebDAV, although is quite widely used in HTTP APIs and more appropriate than any of the other HTTP error statuses when there's a semantic error with a submitted entity). If it's being submitted as a media type which implies a particular schema on top of xml (eg as application/xhtml+xml) then you can use 400 Bad Request if it fails to validate against that schema. But if its media type is plain XML then I'd argue that the schema isn't part of the media type, although it's a bit of a grey area; if the xml file specifies its schema you could maybe interpret validation as being part of the syntactic requirements for application/xml. If you're submitting the XML files via a multipart/form or application/x-www-form-urlencoded form submissions, then you'd have to use 422 Unprocessable Entity for all problems with the XML file; 400 would only be appropriate if there's a syntactic problem with the basic form upload.
563e2e4a61a801306526709a	X	From w3c: 400 = The request could not be understood by the server due to malformed syntax. I wouldn't serve that up unless it was actually the case that the server could not understand the request. If you're just getting invalid xml, serve a 200 and explain why things are not working. Regards Fake
563e2e4a61a801306526709b	X	I'd go with 400 Bad request and a more specific message in the body (possibly with a secondary error code in a header, like X-Parse-Error: 10451 for easier processing)
563e2e4a61a801306526709c	X	That sounds like a neat idea, but the HTTP status codes don't really provide an "operation failed" case. I would return HTTP 200 with an X-Validation-Result: true/false header, using the body for any text or "reason" as necessary. Save the HTTP 4xx for HTTP-level errors, not application-level errors. It's kind of a shame and a double-standard, though. Many applications use HTTP authentication, and they're able to return HTTP 401 Not Authorized or 403 Forbidden from the application level. It would be convenient and sensible to have some sort of blanket HTTP 4xx Request Rejected that you could use.
563e2e4b61a801306526709d	X	I am using the store function from filepicker's JavaScript API to upload files to Amazon S3, like so: The files are fairly big (several megabytes). Therefore, the users should have the option to suspend the upload after it has been started. Here's the question: How would you go about stopping the upload? Looking at the API documentation, the only thing I can think of is finishing the upload and deleting the file from the server afterwards. That feels wrong, though. Any suggestions?
563e2e4b61a801306526709e	X	There is currently no way of suspending or canceling an upload once it's in progress.
563e2e4b61a801306526709f	X	Please provide some sample code so that we can see what your PHP script is doing.
563e2e4c61a80130652670a0	X	How can you be sure that the delay is caused by your PHP script and not your browser/location? You might need to run a few benchmarks before you can be 100% sure that PHP is to blame. It might be a server hardware issue (out of your hands) or a local software issue (out of your hands for other users).
563e2e4c61a80130652670a1	X	the readfile() function takes 3 seconds to completely read the .mp3 file, tested using microtime() functions
563e2e4c61a80130652670a2	X	I want to allow only registered user's to download a .mp3 file. So, I decided to hide the actual location of the .mp3 file and allow downloads using http://example.com/donwload.php?mp3_name=1 The File donwload.php checks weather the user is logged in and then uses readfile() to read the file location in a folder that is not shown the the user. The problem here is that, accessing http://example.com/donwload.php?mp3_name=1 for a .mp3 file of 500kb takes 3 seconds to load (the save dialog box to appear) Is there any other way to do so or read files quickly in PHP? Thanks Akash
563e2e4c61a80130652670a3	X	Really short answer, as I'm running out. (Will provide more information later). This answer also takes an alterntive approach to what you are trying to do. If possible you should use a system that has a dedicated ACL backing all of the files which are stored on it. For instance, if you go with Amazon S3, then you can provide your own ACL for each object that is stored within a bucket, and you can also generate links on the fly that are valid and signed for only X number of minutes. Given your scenario, what you could do is store every MP3 file that you have on something like Amazon S3 (There are others out there so don't feel like you have to use S3), and then when a user makes a purchase and the transaction is confirmed, you can use the S3 API to generate a link for the image. It would be something like : get_object_url( 'my-mp3s.com', 'albums/Foo/bar.mpg') You will then get a URL which you can provide to the customer. Alternatively, you can ask Amazon to generate a URL that expired within 15 minutes.
563e2e4d61a80130652670a4	X	You could use file_get_contents instead of readfile() http://php.net/manual/en/function.file-get-contents.php or the X-Sendfile apache module.
563e2e4d61a80130652670a5	X	If you have your mp3 on a different server you could install lighttpd with modsecdownload or run lighttpd on a different port if you have only one machine only http://redmine.lighttpd.net/wiki/1/Docs:ModSecDownload so you than you could generate only for logged in users a download link, which is valid for few mins - hours, and they can download it. I think this is better, cause it will use less memory than reading the file and sending it to the user, if you have much traffic. Hope this helps!
563e2e4d61a80130652670a6	X	Not adding an answer unfortunately: I see the same pros and cons of each route that you do. I can help with the S3 REST APIs, though; ASIHTTPRequest has slick S3 support: allseeing-i.com/ASIHTTPRequest/S3
563e2e4d61a80130652670a7	X	Matthew, thanks for the tip on ASIHTTPRequest. Is there any reason it's preferred over the iOS Beta SDK that Amazon provides for AWS?
563e2e4d61a80130652670a8	X	No reason that I know of. I'm just had good success with the ASI library in a wide variety of data transfer situations, so haven't bothered with anything else.
563e2e4d61a80130652670a9	X	Joe, I don't think I can use iOS Keychain because it's meant to store sensitive data in a secure way so it can't be extracted by other users or malicious apps. However, the user themselves can extract items from it. See this: blog.crackpassword.com/2010/08/peeking-inside-keychain-secrets
563e2e4e61a80130652670aa	X	Update: Looks like there are ultimately 2 ways to do this. First, it can be proxied through my server which has downsides of placing my server in the middle of every transaction. Advantage of this is there are fewer points of error with multiple legs of communication. Second approach is to use "pre-signed URLs" with AWS that Adrian Petrescu pointed out.
563e2e4e61a80130652670ab	X	Thanks for the tip on the AWS IAM. Too bad it's still in beta. Can you elaborate on the "presigned URLs" solution you mention? Is this just using HTTP POST with json policy doc?
563e2e4e61a80130652670ac	X	Hey TMC, I added more details (and two links) about presigned URLs to my answer. Hope that helps :)
563e2e4e61a80130652670ad	X	In my quick read, the client would have to request the pre-signed URL from my server since it's based on the AWS secret key. Then it would use that pre-signed URL to do it's file upload. So essentially, this is no different than the HTTP POST method mentioned earlier, correct?
563e2e4e61a80130652670ae	X	You would still need a server of your own, but by using presigned URLs this server's job is much easier -- all he has to do is return a URL, not do the upload himself as hipplar is suggesting. That's a huge difference.
563e2e4e61a80130652670af	X	Presumably after the upload is completed to S3, the client should tell the server it was successful? Additionally, why in the world does the AWS iOS SDK have "S3GetPreSignedURLRequest" which requires the access key to be on the client?
563e2e4e61a80130652670b0	X	Shadowmatter is right. Uploading directly to S3 is the better option. His code worked great. I've placed my fork into a gist that contains some example Python code in addition to the Objective-C code. This is also a great way to get around Heroku's 30 second Request Timeout issue.
563e2e4e61a80130652670b1	X	I'm trying to implement the above but keep getting 405 error - method not allowed. Do I need to modify the bucket's policy to enable the above?
563e2e4e61a80130652670b2	X	@maethorr Although I'm no longer on this project, I would expect a bad policy to generate a 401 or 403 error code. My best guess is that you're accidentally using POST and not PUT?
563e2e4e61a80130652670b3	X	@shadowmatter found out the issue. I PUT onto the wrong end-point (the website endpoint <bucketname>.s3-website-ap-southeast-2.amazonaws.com instead of just <bucketname>.s3.amazonaws.com). Thank you for your reply.
563e2e4e61a80130652670b4	X	Major +1 for the EC2<->S3 info.
563e2e4e61a80130652670b5	X	We run in Azure so moving to EC2 not an option. I mentioned in my original post that my server being proxy doesn't appear to be the only way since amazon supports uploads by HTTP POST with json policy files. If there is a way to get away from my server being the middleman that is the ideal approach for obvious reasons.
563e2e4e61a80130652670b6	X	I prefer to upload the image directly but still store a reference and meta data on my own web server db.
563e2e4f61a80130652670b7	X	Joe, amazon provides an iPhone SDK which explains Dat's confusion.
563e2e4f61a80130652670b8	X	I want to allow users of an iPhone app to upload photos and use Amazon S3. There are 2 ways I see going about this: For option 1, the security is straightforward. I don't ever have to tell the iPhone my S3 secret. Downside is that everything is proxied through our server for uploads which sort of defeats the purpose of going to S3. For option 2, in theory it's better but the issue is how do you enable the iPhone (or any mobile app on a different platform) to write into my S3 bucket without giving it my secret? Additionally, I'm not sure if this is a good design or not because the flow would be: iphone uploads to S3, gets the URL, then tells the server what the URL is so it can add it to our database to reference in the future. However, since the communication is separated into 2 legs (iphone->S3 vs iPhone->My-Server) it leaves it fragile as a non-atomic operation. I've found some older info that references using Browser-based Uploads using POST but unsure if that is still the recommended approach. I'm hoping for a better solution where we can just use the REST APIs rather than relying on POST. I've also see the AWS iOS Beta SDK, but their docs didn't help much and I found an Amazon article that was equally unhelpful as it cautions you on what not to do, but doesn't tell you an alternative approach: The mobile AWS SDKs sign the API requests sent to Amazon Web Services (AWS) in order to validate the identity of the AWS account making the request. Otherwise, a malicious developer could easily make requests to another developer's infrastructure. The requests are signed using an AWS Access Key ID and a Secret Access Key that AWS provides. The Secret Access Key is similar to a password, and it is extremely important to keep secret. Tip: You can view all your AWS security credentials, including Access Key ID and Secret Access Key, on the AWS web site at http://aws.amazon.com/security-credentials. Embedding credentials in source code is problematic for software, including mobile applications, because malicious users can de-compile the software or view the source code to retrieve the Secret Access Key. Does anyone have any advice on the best architectural design and flow for this? Update: The more I dig into this, it seems that a bunch of pople recommend using the HTTP POST method with the json policy file as described here: http://docs.amazonwebservices.com/AmazonS3/2006-03-01/dev/index.html?UsingHTTPPOST.html. With this, the flow would be something like (1) iPhone makes request to my server, asking for policy file (2) server generates json policy file and gives back to client (3) iPhone does HTTP POST of photo + json policy to S3. I hate that I'm using HTTP POST in an apparently kludgy way but it appears to be better because it removes the need for my server to store the photo at all.
563e2e4f61a80130652670b9	X	I've discussed this issue on the AWS forums before. As I say there, the proper solution for accessing AWS from a mobile device is to use the AWS Identity and Access Management service to generate temporary, limited-privilege access keys for each user. The service is great, but it's still in beta for now and it's not part of the mobile SDK yet. I have a feeling once this thing is released for good, you'll see it out on the mobile SDK immediately afterwards. Until then, generate presigned URLs for your users, or proxy through your own server like some others have suggested. The presigned URL will allow your users to temporarily GET or PUT to an S3 object in one of your buckets without actually having your credentials (they are hashed into the signature). You can read about the details here. EDIT: I've implemented a proper solution for this problem, using the preview beta of IAM. It's available on GitHub, and you can read about it here.
563e2e4f61a80130652670ba	X	You can upload directly from your iPhone to S3 using the REST API, and having the server be responsible for generating the part of the Authorization header value that requires the secret key. This way, you don't risk exposing the access key to anyone with a jailbroken iPhone, while you don't put the burden of uploading the file on the server. The exact details of the request to make can be found under "Example Object PUT" of "Signing and Authenticating REST Requests". I strongly recommend reading that document before proceeding any further. The following code, written in Python, generates the part of the Authorization header value that is derived from your S3 secret access key. You should substitute your own secret access key and bucket name in virtual host form for _S3_SECRET and _S3_BUCKET_NAME below, respectively: Calling this with the filename foo.txt and content-type text/plain yields: Note that if you run this code, the time returned will be different, and so the encoded HMAC digest will be different. Now the iPhone client just has to issue a PUT request to S3 using the returned date and HMAC digest. Assuming that Then you can do the following to create the NSURLRequest: Next you can issue the request. If you're using the excellent AFNetworking library, then you can wrap request in an AFXMLRequestOperation object using XMLDocumentRequestOperationWithRequest:success:failure:, and then invoking its start method. Don't forget to release headers and request when done. Note that the client got the value of the Date header from the server. This is because, as Amazon describes under "Time Stamp Requirement": "A valid time stamp (using either the HTTP Date header or an x-amz-date alternative) is mandatory for authenticated requests. Furthermore, the client time-stamp included with an authenticated request must be within 15 minutes of the Amazon S3 system time when the request is received. If not, the request will fail with the RequestTimeTooSkewed error status code. " So instead of relying on the client having the correct time in order for the request to succeed, rely on the server, which should be using NTP (and a daemon like ntpd).
563e2e4f61a80130652670bb	X	Upload to your server and then post to S3. From an architecture standpoint you will want to do this from your server. There are many things that could go wrong during the data transfer you can handle better on the server and if you want to store any data about the image you are sending to S3 you are probably going to have a server side call anyway. Plus, your Secret Access Key is stored in a more secure environment. Your own. If you are worried about scalability and you are going to be doing a considerable number of S3 transfers I would consider hosting your server on and EC2 instance. Transferring data between the two is free (given you store you data in following data centers). There is no Data Transfer charge for data transferred between Amazon EC2 and Amazon S3 within the same Region or for data transferred between the Amazon EC2 Northern Virginia Region and the Amazon S3 US Standard Region." Amazon Simple Storage Service (Amazon S3) There are many post here on SO Amazon - EC2 cost? (example) about the pros and cons of using EC2.
563e2e4f61a80130652670bc	X	I m confused. Why would amazon come up w/ the ios sdk to upload data to s3 then tell us not to use it (Embedding credentials in source code is problematic for software, including mobile applications, because malicious users can de-compile the software or view the source code to retrieve the Secret Access Key)???
563e2e4f61a80130652670bd	X	they might have provided the sdk for the purpose that maybe the application could permit authentication to individual s3 accounts? e.g an app that lets users store files in their own (user's) bucket instead of provider? i feel a security flaw in merging the keys with application and distributing it. anyone can (mis)use them once the keys are revealed anyhow (its never secure when you're giving it out). on the other hand, keeping the functionality reserved to server will keep your keys transparent to user,isn't it?
563e2e5061a80130652670be	X	possible duplicate of Retrieve bucket's objects without knowing bucket's region with AWS S3 REST API
563e2e5061a80130652670bf	X	I want to obtain the location of a bucket in Amazon S3. According to the documentation I should use the API GET location (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html). My problem is that, again according to the API, I have to include an authorization header which in the latest version (v4) needs the region where the bucket is located (http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html). So, how can I sign the authorization, giving that I don't know the region where the bucket is located?
563e2e5061a80130652670c0	X	Could you give some sample code of such thin S3 wrapper?
563e2e5061a80130652670c1	X	I'm writing an application in Java that will upload a file up to AWS S3. The file will be given to the application in an argument, not hardcoded. I'd like to write tests to ensure that the file actually gets uploaded to S3. The test will be written before the code for TDD. (I have actually already written the code, but I'd like to ingrain TDD practices into all of my work as a habit) How exactly would I go about doing this? I will be using JUnit as that's what I'm most familiar with. Thanks in advance for any help.
563e2e5061a80130652670c2	X	The actual uploading and the tests that are doing it are part of your integration testing, not the unit testing. If you wrap the S3 API in a very thin class, you will mock that class for unit testing of your business classes, and you will use the real implementation for integration testing. If you have decided, your business classes to take directly the AmazonS3 interface, then for unit testing you have to mock that one. The actual exploratory testing (learning and verifying) if and how amazon s3 works is what you actually do in separate experimental setup. P.S. I do not recommend using the AmazonS3 interface directly in your business classes, rather, wrap it in a thin interface of yours, so that if you decide to change the 'back-end storage' you can easily change it.
563e2e5061a80130652670c3	X	I'm not a Java programmer but you probably want to look into mocking. There is a SoapUI tool called MockService that appears to allow mocking of an external service like those provided by AWS.
563e2e5061a80130652670c4	X	Op De Cirkel answer is good in unit testing scope but if you are writing framework support or simply need to run the AWS S3 calls during your tests, you can run any service that offer AWS compatible APIs. OpenStack is one of them and can be run in a virtual machine (see DevStack). Or you can choose from a variety of test-oriented tools that provide AWS compatible APIs. Here are some that expose S3 service:
563e2e5161a80130652670c5	X	Map a directory that points to your desired root folder under your applications root
563e2e5161a80130652670c6	X	How do I do that? Any examples?
563e2e5161a80130652670c7	X	Tried this and it didn't seem to do anything. Anyway, it looks like there already is a virtual directory for my site in my applicationHost.config file that is pointing to the correct directory. It still seems to be looking for the file in the inetsrv directory I mentioned.
563e2e5161a80130652670c8	X	Can you post some code?
563e2e5161a80130652670c9	X	K, edited the question with my controller code
563e2e5161a80130652670ca	X	is request.videopath an absolute path or is it a relative path? an example?
563e2e5161a80130652670cb	X	Relative. It would be a string like 'myVideo.flv' or 'files/myVideo.flv'.
563e2e5161a80130652670cc	X	I have a Web Api Application that performs a file upload to Amazon S3 when I POST a file path to it. How do I change the root directory this file path is relative to? Right now, if I send myVideo.flv as the file path, my app tries to find the file to upload at c:\windows\system32\inetsrv\myVideo.flv. I'd like it to look for the file at c:\MyApp\files\myVideo.flv. Is this something I change in the app config or iis? Here is my controller method:
563e2e5161a80130652670cd	X	Open IIS, Expand Sites and find your web site, Right click on the web site and select "Add Virtual Directory". This directory can be pointed to anywhere on the machine. In your app, you would read/write to this virtual directory under the root of the application. Also be mindful of permissions, to make sure the users accessing the site have proper permissions to that virtual directory.
563e2e5161a80130652670ce	X	Uploading to S3 is a pain in the ass.. I would not recommend that to anyone.
563e2e5161a80130652670cf	X	after creating a instance in amazon cloud using webservice in java i need to transfer a executable file or war file via program from my local machine to the newly created instance in amazon and i want to execute that excetuable,i tried and found that there is something called createbucket in ec2 api and using that we can upload the file to that and we can transfer that reference using PutObjectRequest i can transfer the reference to a remote computer in amazon do it is possible or if it is wrong please suggest me the correct way to proceed for file transfer from my local machine to the amazon ec2.
563e2e5161a80130652670d0	X	The basic suggestion is, you shouldn't transfer the file(s) with CreateBucket, which is actually an S3 API. Use scp may be a better solution. Amazon S3, which you are trying to use with CreateBucket, is a data storage service mainly for flexible, public (with authentication) file sharing. You can use REST or SOAP APIs to access the data, but cannot really read/write it in EC2 instances as if it's in local harddisk. To access file system in EC2 instances, that really depends on your operating system (on EC2). If it's running Linux, scp is a mature choice. You can use Java to directly invoke scp, if you are using Linux locally, or pscp if you are using Windows. If the EC2 instance is running Windows, one choice is to host an SSH/SFTP environment with FreeSSHD, and then proceed like Linux. Another option is use Shared Folder and regular file copy.
563e2e5261a80130652670d1	X	Is your code running behind some kind of firewall?
563e2e5261a80130652670d2	X	@Rohit: Yes, the code is running with firewall, is firewall related to this?
563e2e5261a80130652670d3	X	I'm not an expert on firewall settings. But it looks like your firewall is signing the incoming data again with ssl certificates hosted on firewall server. That causes the host name mismatch on your code. Check for it with your systems engineer
563e2e5261a80130652670d4	X	I'contact our system engineer and try to figure this out, thank you @Rohit!
563e2e5261a80130652670d5	X	When using JAVA API of SQS (sdk version: 1.9.30), sometimes I got following exceptions: There are some question like mine (i.e SSL problems with S3/AWS using the Java API: "hostname in certificate didn't match"), but seems not the same one. Because in my situation the exception is like: [sqs url] != [my company's url] looks like when I try to connect to AWS server, the sdk connect to my company's server for some reason, and make the SSL handshake fail. There are other one got the same question (https://github.com/Upplication/Amazon-S3-FileSystem-NIO2/issues/40), but still have no answer. Is this a bug of SDK and any solution for this?
563e2e5261a80130652670d6	X	yes thats what the backticks are for
563e2e5261a80130652670d7	X	I'll recommend not to use reserved words, but if you must use them, then always use backticks.
563e2e5261a80130652670d8	X	It's safe - it wouldn't work otherwise. But it's a good idea not to use reserved words if you can, because it makes the code that bit harder to follow.
563e2e5261a80130652670d9	X	I recount instances wherein a SQL Function returned results with pure numeric column-names. The situation aggravates with the wrappers of various scripting environment such as PHP. Consider using aliases through AS aliasname in those cases.
563e2e5261a80130652670da	X	This question already has an answer here: I need to return a multidimensional array from a query with the id keys named key. (needed for object keys in amazon S3 API) example: The problem: key is a reserved name in MySQL. I have to use the name key. The following query gives an error but adding backticks around key doesn't give errors Is it safe to continue like this or is there a better way? I was thinking to rename the id key in the array in PHP but doing it in the query seems faster
563e2e5261a80130652670db	X	As stated, use backticks. From the MYSQL Docs Reserved words are permitted as identifiers if you quote them as described in Section 9.2, “Schema Object Names”:
563e2e5261a80130652670dc	X	To be away from reserved keyword around table field in query always considered as the best way...If you are using in reserved keyword in query, then backtick allow you to use reserved keyword... As backtick is not defined in ANSI SQL standard, it'll probably create problem when you migrate from MySQL environment...
563e2e5361a80130652670dd	X	I am using Microsoft MVC I have written a view that uploads files to the Amazon S3 API. I would like a progress bar in my view to show the progress of the processing of that action in my controller not the uploading of the file in particular. I have tried a few JQUERY/AJAX uploaders but every time I loose the value of The HttpPostedFileBase and the value of the file is null. I basically need to find a progress bar function that supports Post and multipart/form-data. Code is below ASPX Controller action
563e2e5361a80130652670de	X	Here is an example that I think will help you: Asynchronous Process With Progress Bar
563e2e5361a80130652670df	X	To clarify, I have one folder in my bucket that's available to public where the text file is located at.
563e2e5361a80130652670e0	X	The second link is something that looks like it's going to work out great for me. Thanks again
563e2e5361a80130652670e1	X	yeah. IIRC that's sigv3. you can find sigv4 examples but the signing is more complicated
563e2e5361a80130652670e2	X	here is also a sigv4 example: gist.github.com/vszakats/2917d28a951844ab80b1
563e2e5361a80130652670e3	X	As the title says, is it possible to upload to S3 via shell script without aws-cli-tools? If so, how? What I'm trying to do is read from a txt file on S3 (which is public, so no authentication is required). But I want to be able to overwrite whatever is in the file (which is just a number). Thanks in advance, Fadi
563e2e5361a80130652670e4	X	Yes you can! You basically emulate the api calls the SDK would do for you through standard linux cmd utils. Look at: https://aws.amazon.com/code/Amazon-S3/943 and/or http://tmont.com/blargh/2014/1/uploading-to-s3-in-bash
563e2e5361a80130652670e5	X	I use s3cmd which is a command line tool written in Python. It uses the (restful) web APIs. would be the interesting bits: Synchronize a directory tree to S3 s3cmd sync LOCAL_DIR s3://BUCKET[/PREFIX] or s3://BUCKET[/PREFIX] LOCAL_DIR
563e2e5461a80130652670e6	X	It's also worth noting a caveat with reading spot price history from the API - it will only show changes between your start+end time. Some instance types aren't supported in certain availability zones (and detecting this is quite hard since if you ask for the last 24 hours of spot price data an unchanged price would look like an unsupported instance type in that region (or vice versa)
563e2e5461a80130652670e7	X	Note that as of April 2014, those URLs are deprecated and the pricing info is outdated. They're now at a0.awsstatic.com/pricing/1/ec2/linux-od.min.js and a0.awsstatic.com/pricing/1/s3/pricing-storage-s3.min.js. If you look in the source of any pricing page and search for "json" you'll find the appropriate links.
563e2e5461a80130652670e8	X	@TimDorr can you edit the post, or post your answer if the above is now outdated?
563e2e5461a80130652670e9	X	Nice! I was going to get around to this eventually myself, but you've saved me the trouble.
563e2e5461a80130652670ea	X	Very nice but i noted it doesnt include the previous generation instances like t1.micro - is there another .js file that contains those prices ?
563e2e5461a80130652670eb	X	i wasn't able to find anything for base pricing, however deltacloud is a very interesting project for cross-cloud information and management deltacloud.org
563e2e5461a80130652670ec	X	I also did not see a general API for pricing. The closest I found was the spot price history that brokenbeatnik described.
563e2e5461a80130652670ed	X	Are there any API's that have up-to-date pricing on Amazon Web Services? Something that can be queried, for example, for the latest price S3 for a given region, or EC2, etc. thanks
563e2e5561a80130652670ee	X	This is something I have asked for (via AWS evangelists and surveys) previously, but hasn't been forthcoming. I guess the AWS folks have more interesting innovations on their horizon. As pointed out by @brokenbeatnik, there is an API for spot-price history. API docs here: http://docs.amazonwebservices.com/AWSEC2/latest/APIReference/ApiReference-query-DescribeSpotPriceHistory.html I find it odd that the spot-price history has an official API, but that they didn't do this for on-demand services at the same time. Anyway, to answer the question, yes you can query the advertised AWS pricing... The best I can come up with is from examining the (client-side) source of the various services' pricing pages. Therein you'll find that the tables are built in JS and populated with JSON data, data that you can GET yourself. E.g.: That's only half the battle though, next you have to pick apart the object format to get at the values you want, e.g., in Python this gets the Hi-CPU On-Demand Extra-Large Linux Instance pricing for Virginia: Disclaimer: Obviously this is not an AWS sanctioned API and as such I wouldn't recommend expecting stability of the data format or even continued existence of the source. But it's there, and it beats transcribing the pricing data into static config/source files!
563e2e5561a80130652670ef	X	For the people who wanted to use the data from the amazon api who uses things like "t1.micro" here is a translation array
563e2e5561a80130652670f0	X	I've create a quick & dirty API in Python for accessing the pricing data in those JSON files and converting it to the relevant values (the right translations and the right instance types). You can get the code here: https://github.com/erans/ec2instancespricing And read a bit more about it here: http://forecastcloudy.net/2012/04/03/quick-dirty-api-for-accessing-amazon-web-services-aws-ec2-pricing-data/ You can use this file as a module and call the functions to get a Python dictionary with the results, or you can use it as a command line tool to get the output is a human readable table, JSON or CSV to use in combination with other command line tools.
563e2e5561a80130652670f1	X	There is a nice API available via the link below which you can query for AWS pricing. http://info.awsstream.com If you play around a bit with the filters, you can see how to construct a query to return the specific information you are after e.g. region, instance type etc. For example, to return a json containing the EC2 pricing for the eu-west-1 region linux instances, you can format your query as per below. http://info.awsstream.com/instances.json?region=eu-west-1&os=linux Just replace json with xml in the query above to return the information in an xml format. Note - similar to the URL's posted by other contributors above, I don't believe this is an officially sanctioned AWS API. However, based on a number of spot checks I've made over the last couple of days I can confirm that at time of posting the pricing information seems to be correct.
563e2e5561a80130652670f2	X	I don't believe there's an API that covers general current prices for the standard services. However, for EC2 in particular, you can see spot price history so that you don't have to guess what the market price for a spot instance is. More on this is available at: http://docs.amazonwebservices.com/AWSEC2/latest/DeveloperGuide/using-spot-instances-history.html
563e2e5561a80130652670f3	X	I too needed an API to retrieve AWS pricing. I was surprised to find nothing especially given the large number of APIs available for AWS resources. My preferred language is Ruby so I wrote a Gem to called AWSCosts that provides programmatic access to AWS pricing. Here is an example of how to find the on demand price for a m1.medium Linux instance. AWSCosts.region('us-east-1').ec2.on_demand(:linux).price('m1.medium')
563e2e5561a80130652670f4	X	I made a Gist of forward and reverse names in Yaml should anyone need them for Rails, etc.
563e2e5561a80130652670f5	X	Another quick & dirty, but with a conversion to a more convenient final data format
563e2e5561a80130652670f6	X	Here is another unsanctioned "api" which covers reserved instances: http://aws.amazon.com/ec2/pricing/pricing-reserved-instances.json
563e2e5661a80130652670f7	X	There is no pricing api, but there are very nice price mentioned above. In the addition to the ec2 price ripper I'd like to share my rds and elasticache price rippers: https://github.com/evgeny-gridasov/rdsinstancespricing https://github.com/evgeny-gridasov/elasticachepricing
563e2e5661a80130652670f8	X	There is a reply to a similar question which lists all the .js files containing the prices, which are barely JSON files (with only a callback(...); statement to remove). Here is an exemple for Linux On Demand prices : http://aws-assets-pricing-prod.s3.amazonaws.com/pricing/ec2/linux-od.js (Get the full list directly on that reply)
563e2e5661a80130652670f9	X	For those who need the comprehensive AWS instance pricing data (EC2, RDS, ElastiCache and Redshift), here is the Python module grown from the one suggested above by Eran Sandler: https://github.com/ilia-semenov/awspricingfull It contains previous generation instances as well as current generation ones (including newest d2 family), reserved and on-demand pricing. JSON, Table and CSV formats available.
563e2e5661a80130652670fa	X	Some quick questions: Thanks
563e2e5661a80130652670fb	X	Amazon S3 is an object store, not a filesystem. It has a specific set of APIs for uploading, listing, downloading, etc but it does not behave like a normal filesystem. There are some utilities that can mount S3 as a filesystem (eg Expandrive, Cloudberry Drive, s3fs), but in the background these utilities are actually translating requests into API calls. This can cause some issues -- for example, you can modify a 100MB file on a local disk by just writing one by to disk. If you wish to modify one byte on S3, you must upload the whole object again. This can cause synchronization problems between your computer and S3, so such methods are not recommended for production situations. (However, they're a great way of uploading/downloading initial data.) A good in-between option is to use the AWS Command-Line Interface (CLI), which has commands such as aws s3 cp and aws s3 sync, which are reliable ways to upload/download/sync files with Amazon S3. To answer your questions... Amazon S3 does not support a "soft link" (symbolic link). Amazon S3 is an object store, not a file system, so it only contains objects. Objects can also have meta-data that is often for cache control, redirection, classification, etc. Amazon S3 does not support directories (sort of). Amazon S3 objects are kept within buckets, and the buckets are 'flat' -- they do not contains directories/sub-folders. However, it does maintain the illusion of directories. For example, if file bar.jpg is stored in the foo directory, then the Key (filename) of the object is foo/bar.jpg. This makes the object 'appear' to be in the foo directory, but that's not how it is stored. The AWS Management Console maintains this illusion by allowing users to create and open Folders, but the actual data is stored 'flat'. This leads to some interesting behaviours: The above-mentioned utilities take all this into account when allowing an Amazon S3 bucket to be mounted. They translate 'normal' filesystem commands into Amazon S3 API calls, but they can't do everything (eg they might emulate renaming a file but they typically won't let you rename a directory).
563e2e5861a80130652670fc	X	Yes, that is an interesting alternative. For the OP's statistical analysis this might be less useful -- all strings would be have the same length.
563e2e5961a80130652670fd	X	Ugh, that sounds like a great way to annoy users.
563e2e5a61a80130652670fe	X	Yeah, that's the idea. It's basically to do the job for the leechers a bit harder.
563e2e5a61a80130652670ff	X	Thank you for your answer. Indeed it sounds like exactly what I need. I think I gotta do a lot of research, hiring someone very experienced to do this, or simply move onto using Rapidshare. On the other hand, I know Amazon has vast amounts of bandwith, but what I want to achieve with this is stopping leechers from downloading hundreds of files non-stop. We are not gonna charge for the files, therefore this is very likely to happen.
563e2e5a61a8013065267100	X	Do you feel comfortable downloading the AWS SDK for PHP and putting it on your web server? And do you have a working MySQL database on that server? If you can do that, then turning $user_can_download = true into a real check wouldn't be much work..don't have time to do that now but it may be a nice Breakfast Coding Exercise tomorrow morning :) Of course, making it foolproof and integrating it with Joomla would take quite a bit longer, and you'd really have to hire someone to do that...but a rudimentary implentation of what @Pushpesh described is easy enough to put together
563e2e5a61a8013065267101	X	Yeah, I've thought about hiring someone to do this actually, but before that I wanted to check how easy/difficult is this to do, and if there are already solutions created that I could implement. Regarding the query string authentication, I have read about it, and I have a component for Joomla that it says it uses that for serving the files. However, this component falls short on doing all the rest.
563e2e5a61a8013065267102	X	You can 'easily' generate time limited URLs for your files, and make them expire in about 5 mins or so. Then only make a time limited URL for a user if they have not recently downloaded. Which means cookies and IP addresses, and other tricks - several people usually share an IP, and cookies can be cleared, and people can try another browser, etc. So the unique thing is perhaps hard... You will need a ruby or PHP programmer, etc to do this.
563e2e5a61a8013065267103	X	I must say this is the first time I ask anything here, and I'm not a developer, so please be patient with my lack of knownledge. This requirement is for a website I am creating with some friends, so it's not that I'm making money with this. This is the problem: I want to implement some kind of restriction to downloads, very much in the same way Rapidshare or any other file sharing service does: The user should be able to download only 1 file simultaneously The user should wait before being able to download another file, let's say 2 hours. However, I am not trying to create a file sharing website. I am going to upload all the files to Amazon S3, and the only thing I need is to be able to restrict the downloads. I will create the links to the files. I don't care if users are registered or not, they should be able to download anyway. The website is built in Joomla!, which uses Apache + MySQL. The files would be located at Amazon's servers. My question is the following. Is there any way to implement this in a not-so-extremely-complicated way? Do you know some script or web-service that could help me get this done? I have looked around, but the only thing I've found are Payment gateways, and we don't plan to charge for downloads. Any help will be much appreciated. Thanks! UPDATE: I solved this problem using this script: http://www.vibralogix.com/linklokurl/features.php
563e2e5a61a8013065267104	X	As far as I know, there is no way to check on the current status of a download from S3. Having said that, S3 really does have plenty of bandwidth available, so I wouldn't worry too much about overloading their servers :) Just last week, Amazon announced that S3 is now serving an average of 650,000 objects / second. If you want to implement something like @Pushpesh's solution in PHP, one solution would be to use the Amazon SDK for PHP and do something like this: This uses the get_object_url function, which generates pre-signed URLs that allow you to let others download files you've set to private in S3 without making these files publicly available. As you can see, the link this generates will only be valid for 10 minutes, and it's a unique link. So you can safely let people download from this link without having to worry about people spreading the link: the link will have expired. The only way people can get a new, valid link is to go through your download script, which will refuse to generate a new link if the IP/user that is trying to initiate a download has already exceeded their usage limit. It's important that you set these files to private in S3, though: if you make them publicly available, this won't do much good. You probably also want to take a look at the docs for the S3 API that generates these pre-signed URLs.
563e2e5b61a8013065267105	X	Only 2 ways comes to my mind - you either copy a file with unique hash and let apache serve it.. then you don't have any control over when user actually ends his download (or starts). Useful for big files. Another way is to pass it through php. Still, you would need to kill download session in case user stops download.
563e2e5b61a8013065267106	X	If there is no plugin for that you won't be able to do it the easy way by adding a script or copy & paste "some" code. So either hire somebody or you'll need to learn what you need to approach the task an your own, in other words learn how to program. Your question already contains the steps you need to implement: Record who is downloading what and when and keep track of the status of the download. I have not tried to track a download status before but I'm not sure if it is possible to get the status somehow directly from the server that is sending the file. But you can get it from the client: Download Status with PHP and JavaScript I'm further not sure if this will work properly in your scenario because the file will come from S3. S3 itself has a so called feature "query string protection": With Query string authentication, you have the ability to share Amazon S3 objects through URLs that are valid for a predefined expiration time. So you need to lookup the S3 API to figure out how to implement this. What you could try is to send an ajax request to your server when the user clicked the download link, send the amazon s3 link your server generated back as a response and have the client side javascript somehow trigger that file download then.
563e2e5b61a8013065267107	X	You can monitor user downloads by their ip address, store it in a database along with the time at which the user downloaded and the session id (with hashes of course) and check this before each download request. If the current time is less than 2 hours within the same session, block requests, else give them the download. Table Structure: This is a very basic implementation. I'm sure more robust methods exist. Hope this helps.
563e2e5b61a8013065267108	X	Please edit the question and demostrate the the effort you have tried this far: your source code, exceptions you are getting, etc.
563e2e5b61a8013065267109	X	Hello and welcome to StackOverflow. Please take some time to read the help page, especially the sections named "What topics can I ask about here?" and "What types of questions should I avoid asking?". And more importantly, please read the Stack Overflow question checklist. You might also want to learn about Minimal, Complete, and Verifiable Examples.
563e2e5b61a801306526710a	X	Thank you! I am doing exactly that but I'm gonna host my pictures at Amazon S3.
563e2e5c61a801306526710b	X	I am using Github API to create an issue (specifically, with requests module in python). I need to include the picture in the issue content, but I could't find any way that worked for me. Could you please suggest something that works from python? Thanks Edit: It could be really easily done by Github markdowns for linking the pictures that I stored at Amazon S3.
563e2e5c61a801306526710c	X	Assuming you are already able to post things to the Issues Page of a given repository then follow these steps to post the image. I too at first found it annoying to post pictures to github... Github uses markdown and with markdown you can theoretically use an image sourced on your computer (this I believe only works on markdown that you host) but you can also provide a link or image source which is on the internet... particularly convenient for posting pictures to github... is github or you can use dropbox, picasa, photobucket, instagram, or flixster or 20 other image hosting services on the internet, but the point is you then once the image has an href="http://photobucket.com/myimage/3549604690" you can now host that into your markdown file. Like so: Meaning in my case: Notice that it works on stackOverflow as well. For guides on markdown syntax: http://daringfireball.net/projects/markdown/syntax#img https://guides.github.com/features/mastering-markdown/ So STEP BY STEP (if you want to do it all through Github) and not apply any other APIs then have the user: Create a new Github Repo or use their existing repo. (Your choice). Make a post request to insert the image inside the repository's files. Post the issue to the repository and now use the url of the image where it was hosted. From what I've seen when images are added to a repository they go to the path: https://github.com/myUser/PictureHoster/blob/master/myscreenshot.jpg. Python should brush my teeth! I tried to get it to work and here: ![Screen shot of Python stuff](https://github.com/myUser/PictureHoster/blob/master/myscreenshot.jpg "Screen Shot") it is not working. . (Or something like that to find out I would go to your github page click on repositories click on the image inside your repo then right click on it once it's open and do copy Image URL). Basically to use a different picture hosting service you do the same right click on it inside your flixster account and copy/paste that image url.
563e2e5c61a801306526710d	X	I'm building a business solution that has an ios app backed up with Web API 2. I'm using AWS (Amazon Web Services) to host the API. The ios app can take pictures. Pictures will be associated with a user in SQL Server. So I have a User table and Pictures table in my DB. I was thinking of using S3 to save the images. So how can I go about doing this? the ios app can upload the image to S3 but how can i relate the image to a user?
563e2e5c61a801306526710e	X	You can either store the full s3 image url in your database and read the url from the db to then find the image on s3, or else use some sort of folder naming convention on s3 that would let you determine who's image file it is based on the filename along, i.e.: etc.
563e2e5c61a801306526710f	X	When using plain auth credentials I can do: ... to access BlobStoreContext for S3. In native Amazon java api I can use Security Token Service (STS) to assume role and use temporary credentials to access S3 or any other AWS service. How do I do this in jclouds?
563e2e5d61a8013065267110	X	I figured it out. This code snippet allows to assume role and use temp credentials to access S3:
563e2e5d61a8013065267111	X	+1. Good Job. You can accept your own answer.
563e2e5d61a8013065267112	X	It says I have to wait a day. :) Which I usually do anyway in case someone has a better answer.
563e2e5d61a8013065267113	X	Small correction, you're missing a comma after the "NotResource":[...] property to be valid JSON
563e2e5d61a8013065267114	X	Edited that part of the code block. Thank you.
563e2e5d61a8013065267115	X	I have a bucket filled with contents that need to be mostly public. However, there is one folder (aka "prefix") that should only be accessible by an authenticated IAM user. When I try to save this policy I get the following error messages from AWS: Obviously this error applies specifically to the second statement. Is it not possible to use the "s3:prefix" condition with the "s3:GetObject" action? Is it possible to take one portion of a public bucket and make it accessible only to authenticated users? In case it matters, this bucket will only be accessed read-only via api. This question is similar to Amazon S3 bucket policy for public restrictions only, except I am trying to solve the problem by taking a different approach.
563e2e5d61a8013065267116	X	After much digging through AWS documentation, as well as many trial and error permutations in the policy editor, I think I have found an adequate solution. Apparently, AWS provides an option called NotResource (not found in the Policy Generator currently). With this, I do not even need to play around with conditions. This means that the following statement will work in a bucket policy:
563e2e5d61a8013065267117	X	i've tried decoding the data with Javascript's atob function, but i'll try it server side as well.
563e2e5d61a8013065267118	X	wow... that actually did it. thanks!
563e2e5e61a8013065267119	X	I've got a drag and drop function which takes the file that's been dropped on it and converts it to Base64 data. Before, it was uploading to Imgur, whose API supports Base64 uploads, and now I'm working on moving to Amazon S3. I've seen examples of people using XMLHTTP requests and CORS to upload data to S3, I'm using Amazon's AWS S3 SDK gem to avoid having to sign policies and other things, as the gem does that for me. So what I've done is send the Base64 data to a local controller metod which uses the gem to upload to S3. The other posts using Ajax i've seen show that S3 supports raw data uploads, but the gem doesn't seem to, as whenever I view the uploads i get broken images. Am I uploading it incorrectly? Is the data in the wrong format? I've tried the basic Base64, atob Base64, and blob urls, but nothing works so far. JS: Controller method: Edit: To be clear, I've tried a couple of different formats, the one displayed above is decoded base64. Regular Base64 looks like this: and a blob url looks like this:
563e2e5e61a801306526711a	X	Am I reading this right that you are: If that's the case, you need to decode the data in step 2 before sending it on to S3. Something like this might work:
563e2e5e61a801306526711b	X	I want to create an application that needs to store xml and jpg files. Do you know any service on the internet that allows me to store files(jpg most important) and retrieve the files from the service when I need them? I'm looking for something like flickr but with the option of manipulating files trough webservices.
563e2e5e61a801306526711c	X	You could maybe use Amazon S3: http://aws.amazon.com/s3/ You can use their API to manipulate your files from different environments:
563e2e5e61a801306526711d	X	Please post your code
563e2e5e61a801306526711e	X	Do you have permission to rename the file?
563e2e5f61a801306526711f	X	Define "not working".
563e2e5f61a8013065267120	X	Are you sure the first argument exists? file_exists($path) == TRUE ?
563e2e5f61a8013065267121	X	After update: s3:// is not really a normal file. I suppose you're using Services_Amazon_S3?
563e2e5f61a8013065267122	X	Yes.i am using stream wrapper. can u please give me the code(how to rename the files in bucket)
563e2e6061a8013065267123	X	@user, I will not "give you the code" -- you are either discarding the return value or otherwise have an error in your own rename call. Is the return value true, or false? If it's returning false, then the rename failed at the S3 level. If it's returning true, then S3 says the rename worked.
563e2e6061a8013065267124	X	@Charles the link you provided is broken
563e2e6061a8013065267125	X	@tq, fixed. It's irksome that they did not redirect their svn repo viewer to the new canonical file locations on github.
563e2e6061a8013065267126	X	How to rename the file using PHP(in linux). I am using rename(oldfile,newfile), but not working.
563e2e6061a8013065267127	X	It looks like you're using a stream wrapper for Amazon S3. It's up to the individual, custom wrapper code to implement rename functionality. If your wrapper isn't doing the rename, then either the code it's using is buggy, or it doesn't implement that functionality and either isn't reporting it, or you aren't checking the return code from the rename function call and it's returning false to signify failure. You will probably need to actually use the normal S3 API to perform your file rename. If the function is actually returning true, you should file a bug with the people that provided the stream wrapper library. Edit: If you are using PEAR's Services_Amazon_S3 as suggested in the comments, then the stream wrapper it provides does do rename using the rename method starting at about line 570. Edit2: After examining the code further, if there are failures, you will see warnings emitted from the stream wrapper. Perhaps you don't have error_reporting cranked up all the way to -1?
563e2e6061a8013065267128	X	I think this question would be best posed to Amazon Customer / Technical Support. But if you do get an answer there, please do post the solution here as well :)
563e2e6061a8013065267129	X	I already asked this question on AWS forum, however still waiting for an answer. Thought of checking out the bigger community here. :) Either ways, I will keep posted if I have it working.
563e2e6061a801306526712a	X	This looks like a bug in the SDK generation. Also, friendly reminder, never use your root credentials anywhere, create admin IAM user instead.
563e2e6061a801306526712b	X	I have created sample GET and POST APIs on Amazon API Gateway following their official documentation. I have generated JS SDK for these APIs, which I am using to call these APIs from a client-side JS file hosted on S3. This works flawlessly without any 'Authorization Type'. Now, when I set 'Authorization Type' for GET method as 'IAM', I am required to pass IAM credentials in order for it to work. In spite of passing my AWS account's root credentials, I am getting this in the response headers: And finally it returns a 403 error code. My question is: Has anyone successfully attempted to use generated javascript SDK from Amazon API Gateway with IAM authentication? Can you point where I might be going wrong? Thanks.
563e2e6061a801306526712c	X	I was able to resolve this with the help of few folks on AWS Forum. It appears that the API Gateway GET method expects an empty body. By default, if you are following the README sample that comes with generated JS SDK, passing 'undefined' or just '{}' inside the body to GET causes a non-matching payload and this results in an incorrect signature being calculated. As of now, I just made a small tweak in the /lib/apiGatewayCore/sigV4Client.js by hardcoding the body = ''. This should be a temporary workout as this may affect your other API Gateway methods that require a filled 'body'. In my case, I only had GET methods.
563e2e6161a801306526712d	X	I'm posting an image to Amazon S3 using mattt's AFAmazonS3Manager library, basically by following the example there. The upload works fine, but when I download the file from S3, it has those headers that make the file invalid. When I remove the headers, the file becomes valid and I can open it with Preview or Photoshop. The code to create the image is as follow: I'm using a PUT request, as per Amazon S3's REST API reference. Any idea what I could do to store a valid image on S3?
563e2e6161a801306526712e	X	Simple i want to apply image compression using PNG/JPEG/Bitmap file. Android we have Bitmap.CompressFormat to compressed our bitmap file and use for further operation. Bitmap.CompressFormat class allow to compress in 3 format as below : My query is i want to compress file in any on of below format : I have found some image compression library like ImageIo & ImageMagick but didn't get any success. I want to use this file to upload on AmazonServer. Please guide me how to achieve this or is there any other option to upload image on amazon server. Thanks for your time.
563e2e6161a801306526712f	X	I don't know about those file's compression but i created this class to upload files programatically into an Amazon s3 bucket that uses the Amazon SDK api: And for usage with your file:
563e2e6161a8013065267130	X	Ok no one seems to care about this...with SO that is hardly ever the case :). Needed to access my server-side presigned post credentials from a mobile client and was a bit confused till I came across this. Thanks.
563e2e6161a8013065267131	X	I am creating an API for a backend service with Rails 4. The service needs to upload an image file to an amazon s3 bucket. I'd like to use a direct upload url, so that the clients manage the uploads to s3 and the server is not kept busy. Currently I have the following prototypical rails action This generates such an url: Now I try to post the test.png to this url with the following: curl -v -T test.png "url" and I get the following error response: I believe the problem comes from the fact, that the specified X-Amz-SignedHeaders Header is wrong. I am not sure which headers are used by default from the amazon rails sdk gem. How should I change my url generation, so that a mobile client can just take the url and post a file to it?
563e2e6161a8013065267132	X	Ok no one seems to care about this, but if by any chance someone stumbles upon this, here is a solution: in config/initializers/aws.rb in your model/controller/concern/or whatever Then you can use a mobile client to upload or via curl Note that you will have to add the x-amz-acl: public-read header if you used the public-read cal option.
563e2e6261a8013065267133	X	Duplicate of this question, see it for answer. LINK
563e2e6261a8013065267134	X	this is incorrect. You can set the 'download filename' by using the headers in the original question. See my answer.
563e2e6261a8013065267135	X	@Geoff; Thank you for the info. As I wrote on my comment, I never tried it tho and of course their classes / APIs could bring solution as in your answer. Now how do we contact him? He accepted my answer as a solution while there is a way to do that.
563e2e6261a8013065267136	X	I am using amazon S3 service with PHP by using this API https://github.com/tpyo/amazon-s3-php-class I am passing the url to client like this So when the client clicks or paste the URL into browser , the file downloaded with the name of filename_11052011111924.zip.But I stored my original filename in DB. So is it possible to download when passing the URL alone to the client and download with original file name.I am not sure whether this will help me.
563e2e6261a8013065267137	X	I don't think that will work (I never tried it though). You might need to download the file to your server first, later use headers, once it is completed (or after sometime later with some bot or cron) you can delete the file(s). This approach will be using your bandwidth.
563e2e6261a8013065267138	X	If you set the headers that you listed on your file when you upload it to S3, you will be able to download the file with the original filename. (you can also set these on existing files in S3 - see the AWS docs) I'm not sure if your library supports this but you can do it with the AWS S3 SDK. Something like (I don't know php so check the syntax): Update You can also adjust certain headers each time you generate a new url. See http://docs.amazonwebservices.com/AWSSDKforPHP/latest/#m=AmazonS3/get_object_url
563e2e6261a8013065267139	X	Yes, you can tell to AWS how output file must be named: Note: we encode file name!
563e2e6261a801306526713a	X	I don't see how deploying a Django app would be any different just because it uses Angular. Angular is exclusively static files from the point of view of Django.
563e2e6261a801306526713b	X	It's better to deploy static files to S3 and access them through CloudFront. Read about why CDN is useful.
563e2e6261a801306526713c	X	I am able to deploy (It is not different than deploying any django app) I wanted to know about the more Proper way like compressors and things like that to make loading faster (which aren't always obvious for beginners ) and whether to use docker containers or not.
563e2e6361a801306526713d	X	I have an app similar to this with django REST for API backend and AngularJS as client. The angularJS app resides in the static folder. I can't wrap my head around how to properly deploy this to beanstalk. This answer gives a good idea on how to do so. Also others suggest using Docker like in this tutorial So is it best to just have the angularJS under the django static folder deployed to amazon S3, or should I have a docker container and deploy this as a whole to AWS beanstalk?
563e2e6361a801306526713e	X	I am working on rails 4 application where I can upload images with description. I can upload images on web application but I want to test the json API. I want to upload image from HTTP POST on postman Rest client of chrome browser, But I cant seem to make it to work. Here's my complete Pins controller, here I am using paperclip 4.2 gem for image upload and amazon s3 storage
563e2e6361a801306526713f	X	I actually tried this method -- I've just come to the conclusion that S3 can not support deleting buckets right now, and that with it's horrendous access speed leaves an extremely bitter taste in my mouth for S3.
563e2e6361a8013065267140	X	s3cmd del s3cmd ls s3://Mybigbucket/somepattern | awk '{print $4}' . Yeah this is painful
563e2e6361a8013065267141	X	even listing the keys at 1000 time or whatever the number was -- that took forever -- more than an afternoon and I finally killed it after I got bored and noticing that my heap was way too overfilled.
563e2e6361a8013065267142	X	I don't think there is an API call to just get the number of items. Probably you've used a tool that also gets the contents of the files - that's why it took so long. Just use Fiddler or some other tool to send the GET bucket request (see the REST API link in my answer). It shouldn't take long to get the xml back. I am afraid that I don't have such a big bucket to test it myself.
563e2e6461a8013065267143	X	Where is the second way to count?
563e2e6461a8013065267144	X	So I know this is a common question but there just doesn't seem to be any good answers for it. I have a bucket with gobs (I have no clue how many) number of files in them. They are all within 2k a piece. 1) How do I figure out how many of these files I have WITHOUT listing them? I've used the s3cmd.rb, aws/s3, and jets3t stuff and the best I can find is a command to count the first 1000 records (really performing GETS on them). I've been using jets3t's applet as well cause it's really nice to work with but even that I can't list all my objects cause I run out of heap space. (presumably cause it is peforming GETS on all of them and keeping them in memory) 2) How can I just delete a bucket? The best thing I've seen is a paralleized delete loop and that has problems cause sometimes it tries to delete the same file. This is what all the 'deleteall' commands that I've ran across do. What do you guys do who have boasted about hosting millions of images/txts?? What happens when you want to remove it? 3) Lastly, are there alternate answers to this? All of these files are txt/xml files so I'm not even sure S3 is such a concern -- maybe I should move this to a document database of sorts?? What it boils down to is that the amazon S3 API is just straight out missing 2 very important operations -- COUNT and DEL_BUCKET. (actually there is a delete bucket command but it only works when the bucket is empty) If someone comes up with a method that does not suck to do these two operations I'd gladly give up lots of bounty. UPDATE Just to answer a few questions. The reason I ask this was I have been for the past year or so been storing hundreds of thousands, more like millions of 2k txt and xml documents. The last time, a couple of months ago, I wished to delete the bucket it literally took DAYS to do so because the bucket has to be empty before you can delete it. This was such a pain in the ass I am fearing ever having to do this again without API support for it. UPDATE this rocks the house! http://github.com/SFEley/s3nuke/ I rm'd a good couple gigs worth of 1-2k files within minutes.
563e2e6461a8013065267145	X	I am most certainly not one of those 'guys do who have boasted about hosting millions of images/txts', as I only have a few thousand, and this may not be the answer you are looking for, but I looked at this a while back. From what I remember, there is an API command called HEAD which gets information about an object rather than retrieving the complete object which is what GET does, which may help in counting the objects. As far as deleting Buckets, at the time I was looking, the API definitely stated that the bucket had to be empty, so you need to delete all the objects first. But, I never used either of these commands, because I was using S3 as a backup and in the end I wrote a few routines that uploaded the files I wanted to S3 (so that part was automated), but never bothered with the restore/delete/file management side of the equation. For that use Bucket Explorer which did all I need. In my case, it wasn't worth spending time when for $50 I can get a program that does all I need. There are probably others that do the same (eg CloudBerry) In your case, with Bucket Explorer, you can right click on a bucket and select delete or right click and select properties and it will count the number of objects and the size they take up. It certainly does not download the whole object. (Eg the last bucket I looked it was 12Gb and around 500 files and it would take hours to download 12GB whereas the size and count is returned in a second or two). And if there is a limit, then it certainly isn't 1000. Hope this helps.
563e2e6761a8013065267146	X	"List" won't retrieve the data. I use s3cmd (a python script) and I would have done something like this: But first check how many bucketfiles_ files you get. There will be one s3cmd running per file. It will take a while, but not days.
563e2e6761a8013065267147	X	1) Regarding your first question, you can list the items on a bucket without actually retrieving them. You can do that both with the SOAP and the REST API. As you can see, you can define the maximum number of items to list and the position to start the listing from (the marker). Read more about it here. I do not know of any implementation of the paging, but especially for the REST interface it would be very easy to implement it in any language. 2) I believe the only way to delete a bucket is to first empty it from all items. See alse this question. 3) I would say that S3 is very well suited for storing a large number of files. It depends however on what you want to do. Do you plan to also store binary files? Do you need to perform any queries or just listing the files is enough?
563e2e6761a8013065267148	X	I've had the same problem with deleting hundreds of thousands of files from a bucket. It may be worthwhile to fire up an EC2 instance to run the parallel delete because the latency to S3 is low. I think there's some money to be made hosting a bunch of EC2 servers and charging people to delete buckets quickly. (At least until Amazon gets around to changing the API)
563e2e6761a8013065267149	X	Old thread, but still relevant as I was looking for the answer until I just figured this out. I wanted a file count using a GUI-based tool (i.e. no code). I happen to already use a tool called 3Hub for drag & drop transfers to and from S3. I wanted to know how many files I had in a particular bucket (I don't think billing breaks it down by buckets). I had 20521 files in the bucket and did the file count in less than a minute. I'd like to know if anyone's found a better way since this would take some time on hundreds of thousands of files.
563e2e6761a801306526714a	X	To count objects in an S3 bucket: Go to AWS Billing, then reports, then AWS Usage reports. Select Amazon Simple Storage Service, then Operation StandardStorage. Download a CSV file that includes a UsageType of StorageObjectCount that lists the item count for each bucket.
563e2e6761a801306526714b	X	Can I make a PHP script, using Youtube API or any other legal way, to directly copy mp4 files from my own youtube channel to Amazon S3 or other media host? I need this because my client wants me to make native iPhone / Android apps that will stream the videos from his channel. I know I can do it manually from Youtube web, but the problem is that I can only download vids locally and then again upload to Amazon which seems like a waste of time and assets.
563e2e6861a801306526714c	X	This is really stating the obvious but, I really hope you're not actually using the private key posted here.
563e2e6861a801306526714d	X	I'm trying to serve time-limited links to private content on a Cloudfront-enabled Amazon S3 bucket. I would like to be able to use the AWS PHP API But I keep getting this access denied message The cdn_private_key is a string containing the RSA private key which looks sort of like this: I may be doing something wrong there, but I would've expected to get an error about the key or signature instead of an access denied message. I have also tried manually signing using the following method, but get the same error, only with different HostId and RequestId:
563e2e6861a801306526714e	X	I've eventually solved the problem. It seems that Amazon aren't very clear about this ... hidden deep in the bowels of AWS documentation you are instructed to set the bucket permissions on the S3 bucket to allow CloudFront access to it. Further confusion ensues when you have to set the Principal property on the policy, as it suggests you need to get the Canonical User ID. However, this is NOT the Canonical User ID for your AWS account found on the Security Credentials page .. it is instead found in the "Origin Access Identity" link on the CloudFront console. Here is how to do it .... First create/obtain the CloudFront Origin Access Identity like this:- Now apply the policy to the bucket to allow CloudFront access:-
563e2e6961a801306526714f	X	Assume 200,000 images in a flat Amazon S3 bucket. The bucket looks something like this: (a 6 digit hash followed by a count, followed by the extension) If I need all files matching 000001-*.jpg, what's the most efficient way to get that? In PHP I'd use rglob($path,'{000001-*.jpg}',GLOB_BRACE) to get an array of matches, but I don't think that works remotely. I can get a list of all files in the bucket, then find matches in the array, but that seems like an expensive request. What do you recommend?
563e2e6961a8013065267150	X	Amazon provides a way to do this directly using the S3 api. You can use the prefix option when calling listing S3 objects to only return objects that begin with the prefix. eg using the AWS SDK for PHP: You might also find the delimiter option useful - you could use that to get a list of all the unique 6 digit hashes.
563e2e6961a8013065267151	X	I have an application to upload files to Amazon S3 using AWS .net API. My query is can we use throttling while uploading the data to S3? If I upload too many files from a single machine of small size say < 5MB then my entire bandwidth is choked up. So is there a way to manage the bandwidth while uploading the data. We also tried with Low level .net API and high level API but there is no Attribute or API to set throttling. Then I tried using Throttled Stream class of .net but this works smooth if I keep chunk size of 512KB and my current Network is of 2MBPS, I'm still not sure is it because of throttled stream class or because of low chunk size that it works fine. Below is my code: I followed the below link http://www.codeproject.com/Articles/18243/Bandwidth-throttling
563e2e6a61a8013065267152	X	Why don't you just run on heroku or something?
563e2e6a61a8013065267153	X	See I don't know what that is, thats the problem, theres so many different ways to do these types of things, its hard to find / know the best efficient path to take
563e2e6a61a8013065267154	X	Well, now you know heroku :)
563e2e6a61a8013065267155	X	Dang, expensive huh? lol
563e2e6a61a8013065267156	X	So what should I be looking into to do what I need? I'm new to aws btw, I know nothing yet.
563e2e6a61a8013065267157	X	You can use the EC2 free tier to get free EC2 hosting within limits. Added a link.
563e2e6a61a8013065267158	X	Would my videos be stored on EC2 now as well? Would I need RDS anymore
563e2e6b61a8013065267159	X	@EricJ. I believe that's free for one year though.
563e2e6b61a801306526715a	X	The best place to store videos is probably in S3, because you can optionally use CloudFront, Amazon's Content Delivery Network, with S3 to deliver the videos to your customers quickly no matter where in the world they may be. You can though store them in an EBS backed file system in EC2. See stackoverflow.com/a/3630707/141172
563e2e6b61a801306526715b	X	So what should I be looking into?
563e2e6b61a801306526715c	X	Personally, I have been using AWS services for years and honestly wouldn't think of hosting anything other than a trivial website or application on anything else.
563e2e6b61a801306526715d	X	I know in amazon's documentation it says S3 is not really made for server side scripting but to rather use EC2 instead. I don't need and will not use a operating system that EC2 provides to handle my server calls, it just seems pricey and seems to be an overkill. Basically, I have a couple php files that handle writing data to a RDS database and uploading videos to S3. Is it wrong for me to have my php files in S3 and allow static web hosting so that my iphone api can call the php scripts?
563e2e6b61a801306526715e	X	Static web hosting means just that... no server-side script execution. You cannot run PHP scripts on S3. You can host a static website on Amazon S3. On a static website, individual web pages include static content. They may also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting. http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html Check out the EC2 free tier.
563e2e6b61a801306526715f	X	S3 has no way of actually executing your PHP files. It is just file storage that just happens to also be able to also serve up files in response to basic HTTP requests. But it can only serve up static content in this manner. There is however nothing that says you need to use EC2 for your web application. You can use whatever you want for that, though you will likely see bandwidth cost penalties around S3 that you may be able to avoid using EC2.
563e2e6c61a8013065267160	X	I use larvel 4 with AWS sdk. I want to add another AWS service with different key and region. when I used only one AWS service for example SES I put my key in /app/config/aws/aws-sdk-php-laravel just in the array 'secret' => 'xxxxxxxxxxxxx', all works well now I want add another service for example S3. so I create custom config file in the same directory: configaws.php (with chmod 777): and use this line in my config.php file 'config_file' => 'configaws.php', the problem is that Laravel can't find this file, I got exception 'Unable to open configaws.php for reading'. my simple question is where I should put my configaws.php file? because I try a lot of options and nothing works for me. is there another option to use 2 different keys for amazon services?
563e2e6c61a8013065267161	X	if you are using Laravel 4.1+, The best way store sensitive information using dot files. For "real" applications, it is advisable to keep all of your sensitive configuration out of your configuration files. Things such as database passwords, Stripe API keys, and encryption keys should be kept out of your configuration files whenever possible. So, where should we place them? Thankfully, Laravel provides a very simple solution to protecting these types of configuration items using "dot" files. Assume your environment is production or you can check your environment by reading the following doc. http://laravel.com/docs/configuration#environment-configuration For production environment, create a .env.php in the project root of your application and add the AWS information like so. Now, when you need the api information, use the PHP's super global variables $_ENV or $_SERVER to retrieve the information. For example, to retrieve the Amazon s3 api info, you can do the following: For more information please check the Laravel documentation: http://laravel.com/docs/configuration#protecting-sensitive-configuration
563e2e6c61a8013065267162	X	Thanks for the reply. I keep getting Unknown error 500 from Facebook which most probably caused by the URL from Paperclip has extension like .../my-image.jpg?140583423. I tried hard-coding the URL without that extension and works. Any way to disable that extension?
563e2e6c61a8013065267163	X	If I understood your comment, @user.photo.url(:origin).split('?')[0] can work for you
563e2e6c61a8013065267164	X	Thanks! the traditional split didn't even cross my mind
563e2e6c61a8013065267165	X	I'm using Koala for Facebook API and Paperclip for Amazon S3. I already finished the code for S3 Upload, but having problem for Facebook's upload. Here's my simplified code: I keep getting this error on last line: I think the way I set file is wrong but I can't find other way to do it. Thanks before.
563e2e6c61a8013065267166	X	While uploading picture on facebook using URL, you just need to send picture url directly (you don't need to send binary data). Update API call as: Some other ways to use put_picture method: Source: Koala gem.
563e2e6d61a8013065267167	X	You may want to try CloudBuddy Personal m1.mycloudbuddy.com/index.html. It runs only on windows though.
563e2e6d61a8013065267168	X	tl;dr - Is there a robust S3 ACL management tool, possibly for use with CloudFront? I'm working on a personal private content distribution (via CloudFront) but obviously the AWS Console is severely lacking in this regard. I know there are a handful of S3 clients out there, but none of them really do much for advanced ACL. To avoid having to use the AWS cli tools or to write wrappers for the API for everything (this is for configuring long-term systems, not for anything that would need to be done programmatically), I'm looking for one that has the best ACL support. OR, if anyone has suggestions for managing CloudFront and custom ACLs (specifically for adding canonical user IDs/OriginAccessIdentities to buckets), I'm totally open to that too. On a side note, the AWS docs mention the following: Once you have a private content distribution, you must grant your CloudFront origin access identity read access to the private content. You do this by modifying the Amazon S3 ACL on each of the objects (not on the bucket). which seems, er, exceptionally hard to maintain for a system that could potentially be used as swap (sic) storage for protected assets and modified on a regular basis (tens+ of times per day). Am I misreading that, or is it really intended to be that static and explicit?
563e2e6d61a8013065267169	X	Thanks for the suggestions, but I can't use those (Mac - didn't mention, not your fault). I ended up going with Bucket Explorer, FWIW.
563e2e6d61a801306526716a	X	Cyberduck for Mac & Windows supports ACL editing. Refer to http://trac.cyberduck.ch/wiki/help/en/howto/s3.
563e2e6d61a801306526716b	X	Thanks Nick - let's see if I can get the multipart upload API to play nicely with app-engine :)
563e2e6d61a801306526716c	X	@JohnIdol Have you considered just writing the code yourself? The RESTful API is pretty straightforward - the only complexity is computing the header signature, and there's probably a library that will do that for you.
563e2e6d61a801306526716d	X	I am keeping that for the all-else-fails scenario :) Now I am giving it another shot with the 'low-level' aws API approach (the 'high level' approach fails with all sorts of 'access denied' issues).
563e2e6d61a801306526716e	X	thanks man -- much appreciated :)
563e2e6d61a801306526716f	X	I am trying to upload a file to amazon S3 from GAE. I tried the official amazon sdk (jetS3t, built on top of the lower-level sdk), just to find out that even if you can get it to work locally by setting permissions on the local JVM it is not supported for GAE crypto-related reasons once you deploy it. Then out of desperation I found that some good soul forked the official low-level amazon sdk so that it would work with GAE. This kind of works (even though I can see some strage NullPointer exceptions being thrown here and there) and the file gets uploaded ... but if the file size exceeds 5MB I am getting a error from within the API: I don't fully understand this as the current GAE limitations seem to be 32MB on file size upload and 1MB on request/response while my problem is occurring only when the file is around 5MB or bigger. I think my only alternative left is jclouds, but I am having trouble finding examples of uploading files to S3 using the BlobStore library. Does anyone have experience/examples to share of S3 file upload with jClouds? And am I likely to incur in the same urlfetch.Fetch() was too large error? Any help appreciated.
563e2e6e61a8013065267170	X	URLFetch requests are limited to 5MB, as documented here. The only solutions that will work are those that involve breaking up a large payload into smaller chunks. Fortunately, S3 provides a multipart upload API.
563e2e6e61a8013065267171	X	On the release notes for 1.5.0, I read In response to popular demand, the HTTP request and response sizes have been increased to 32 MB. So, request and response, URL fetch is not mentioned. Indeed, looking at URL Fetch documentation, it says it's max 5 Mb.
563e2e6e61a8013065267172	X	thanks Michael, but when i changed to https, the code in first line to "POST s3.amazonaws.com HTTP/1.1". the AWS server still returns Method not Allowed.
563e2e6e61a8013065267173	X	i want to use the AWS S3 by soap, i checked the aws website, there is an example to list all the bucket.http://docs.aws.amazon.com/AmazonS3/latest/API/SOAPListAllMyBuckets.html. so when i use the soap template. it returns "405 Method Not Allowed". any help? thanks this is what is sent to aws s3 server.
563e2e6e61a8013065267174	X	You need to use HTTPS, not HTTP. Note that SOAP requests, both authenticated and anonymous, must be sent to Amazon S3 using SSL. Amazon S3 returns an error when you send a SOAP request over HTTP. http://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html Note also that the SOAP interface is deprecated.
563e2e6e61a8013065267175	X	Sony, thanks for your help. Especialy Amazon Elastic Transcoder. I will have to process many videos, so i think thats an interesting alternative.
563e2e6e61a8013065267176	X	users will load up videos from ios app, android app and vía webupload to my server. so i get a lot of different video formats, which i need to encode for showing them in the website. As we now, video encoding is not a simple thing. I´m doing it up to now on the webserver itself, using ffmpeg with php. That was ok for the Beta Version, but now i need a professional service. The encoding takes a lot of server cpu time and trying to cover all the video formats is practically imposible for me. From what i found out up to now, the most profesional solution seems to be a online/cloud service like like transloadit.com, or zencoder.com or encoding.com. There are more, but that's what o foung up to now. Does anyone have experience with these or other similar services and can tell something about the advantages and disadvantages of each of them? I also consider using Amazon S3 bucket to save the uploaded videos. Some (or all) of the named services, deliver the encoded videos also to a S3 bucket if wanted. And last but not least i'd like to get also some help in which player is recommended to use. The videos will be called from desktop, tablets and smartphones vía a web app. The alternatives i see up to now are jwplayer.com, flowplayer.org and vid.ly player.
563e2e6e61a8013065267177	X	One very highlevel third party service which I used in the past if Wistia.com. They have APIs to upload videos (most format, I guess) to them. Once you upload, you need to wait for them to be transcoded and ready to be streamed - You can check this status via API again. Then you will get an HTML embed code (as from youtube), using which you can stream the videos on your website. This basically abstracts lot of things for you (no need to deal with jwplayer, transcoder etc) Hoeever, You can use AWS elastic transcoder for transcoding if you want to do it the developer way. You should be able to call the APIs from anywhere (even from your web-server). You can configure S3 such that your users can directly upload to S3, Transcoder picks it from S3, converts and puts it back in S3 again. API Reference - Amazon Elastic Transcoder This example helps you setup a video trans-coding pipeline.
563e2e6e61a8013065267178	X	There is also http://www.bitcodin.com as cloud-based transcoding service, which can be used via an API to automate processes. There is a full tutorial on the usage together with Amazon S3 and CloudFront: http://www.bitcodin.com/blog/2015/02/create-mpeg-dash-hls-content-for-amazon-s3-and-cloudfront/
563e2e6e61a8013065267179	X	I have used both amazon's elastic_encoder and zencoder.com both if them are great I personally would prefer zencoder.com which is efficient and much faster but its cons are that it is costlier. As you have mentioned that you are using amazons s3 bucket for uploading video then it is worth mentioning that amazons elastic transcoder works smoothly with s3 and is lot cheaper then zencoder. Moreover zencoder supports more video formats than elasctic trancoder. At work I am using elastic transcoder as primary trascoding but when it fails I user zencoder.com for most part elastic transcoder should do fine. For implementation of aws elastic encoder (django and python stack) and this post which explains implementation for zencoder.Hope it helps.
563e2e6e61a801306526717a	X	I am new bee to AWS Bucket. Please consider my content in the post. I have more than 100 GB of files stored on a Windows Azure. It has some what folowing structure: As now we are migrating another server. We need to copy these all files and its contents on AWS Bucket using C#. Also, i have a all the URLS of these links to a text file. So, can we copy the files directly from AZURE to AWS Bucket? or need to Download it on local and then need to Upload to AWS Bucket? Any suggestions? Help Appreciated!
563e2e6f61a801306526717b	X	Looking at Amazon S3's Put Object REST API documentation, I believe it doesn't support fetching a URL's contents and saving that as an object (which Azure Blob Service does support BTW). So your only option would be to download blobs from Azure Storage to local computer and then upload them into Amazon S3.
563e2e6f61a801306526717c	X	Besides HTTP 503, you can also use HTTP 421 - There are too many connections from your internet address. (But my personal favorite is HTTP 418 - I'm a teapot. :-))
563e2e6f61a801306526717d	X	Twitter is returning a non standard HTTP 420
563e2e6f61a801306526717e	X	RFC 2616 defines none of these 4xx's and says "The 4xx class of status code is intended for cases in which the client seems to have erred", which seems inappropriate here, where the client has done nothing wrong and it's the server that's overloaded.
563e2e6f61a801306526717f	X	In Twitter's case, they have expressedly told clients that there is a 150 request per hour limit. So in a way, the client is breaking the agreement.
563e2e6f61a8013065267180	X	Handshake uses status code 429.
563e2e6f61a8013065267181	X	I want to limit clients to an upper limit of number of calls to my REST APIs. What should I return to inform clients that they've been throttled ? Amazon S3 is returning HTTP 503 with an error code SlowDown to inform clients. What do you advise ?
563e2e6f61a8013065267182	X	Since RFC 2616 documents status 503 as (my emphasis): The server is currently unable to handle the request due to a temporary overloading or maintenance of the server. The implication is that this is a temporary condition which will be alleviated after some delay. If known, the length of the delay MAY be indicated in a Retry-After header. it seems a reasonable approach, especially with a Retry-After header.
563e2e6f61a8013065267183	X	You need to use the S3GetObjectMetadataRequest and return the contentLength property on the response object. What have you tried? Where's your code?
563e2e6f61a8013065267184	X	Please don't depreciate new comers even if you are not supporting, it is not a problem but don't discourage thanks @EFeit
563e2e7061a8013065267185	X	My comment was meant to help, not discourage. I provided you with a good place to start. Editing your question to include your code will not only help you get a more valuable response, but it will help SO users in the future who encounter the same problem.
563e2e7061a8013065267186	X	How to get s3 bucket/folder size for android and ios i can get folder size using .net c# code from the following link but i couldn't find a code or api to get the same result in ios and android How to check the size of the sub-folder for a folder inside a Amazon S3 bucket Thank you.
563e2e7061a8013065267187	X	This is my method used to get folder size from a s3 bucket android and java code :
563e2e7061a8013065267188	X	Search here or on a search engine for "Create REST service PHP". It doesn't matter too much what language your service is written in, so use the one you know best.
563e2e7061a8013065267189	X	I need to create public api for my website, like http://instagram.com/developer/ API or dropbox API. I want to use oAuth2.0. I need very basic functionality like getting user information, upload ,download, get, delete data etc. I need to have SDKs in different languages like PHP, Java etc. My website is in PHP 5 and the data stored is on dropbox and Amazon S3. I also have an option to create my API server in Java, is that good option. I am totally new to API creation and need suggestions to start API. Flow should be like this user create app on my website and get api,key and secret and then use get access tokens for users and then making calls. Is there any opensource library to create such service and defining endpoints and APi explorer etc. Thank You.
563e2e7061a801306526718a	X	Here are a couple of usefull sites you might want to look in: OAuth demo application: http://brentertainment.com/oauth2/ Demo application on github: https://github.com/bshaffer/oauth2-demo-php/ OAuth tutorial: https://github.com/bshaffer/oauth2-server-php/ This will get you started with OAuth. But your not there yet, OAuth is just a tool to get you there. A good API requires a lot of thinking, because developers will probably implement your API it needs to be good from the start. i suggest you google a bit on good API practices. Try youtube, search for API. You'll find a lot of interesting videos about this topic, for example: Wikipedia on rest services: http://en.wikipedia.org/wiki/Representational_State_Transfer Google on APIS: http://www.youtube.com/watch?v=aAb7hSCtvGw The API Guys: http://www.youtube.com/user/apigee
563e2f3661a801306526718b	X	Help is appreciated Thanks!
563e2f3661a801306526718c	X	Is this still the case? No way to send a verifiable token from the client (JS) to your web-service which can then be turned into credentials?
563e2f3661a801306526718d	X	@BenSmith Unfortunately, yes. While the Amazon Cognito vended OpenID Connect token can be verified by your backend, this would be simple bearer token security and would not be recommended.
563e2f3661a801306526718e	X	@BenSmith Please see my update.
563e2f3661a801306526718f	X	Oh, how timely! Cheers @BobKinney.
563e2f3661a8013065267190	X	If have successfully developed and used Developer Authentication with Amazon Cognito. I have tried to upload images to S3 and download and display from there. What I want to do now is to secure my own webservice with the Cognito API. This is how Amazon WebServices are used, e.g. I want to secure my personal web services with the Cognito Security. Is it possible to secure developers personal webservices using Amazon Cognito?
563e2f3661a8013065267191	X	Cognito is a mechanism for acquring AWS credentials to access AWS services. Currently there is nothing in Cognito that would allow you to secure your own API. This is a request we have heard from other customers and will certainly take it into account as we plan out our feature roadmap. Update 2015-07-09 AWS has announced Amazon API Gateway. Using API Gateway you can build a REST interface to your existing API (or to AWS Lamdba functions) secured with credentials retrieved via an Amazon Cognito authflow. See this blog post for additional announcement details.
563e2f3661a8013065267192	X	Are you using an instance profile to get credentials to the instance?
563e2f3761a8013065267193	X	Not sure about your question, but I use environment variable in my config to get those credentials and they are set in my elastic beanstalk configuration. The user that manage the elastic beanstalk have full access to the S3.
563e2f3761a8013065267194	X	Everything is setup on Amazon, I don't think they would have setup the elastic beanstalk with the wrong time. For the s3, I have no control over there. Thanks, This could had been the issue.
563e2f3761a8013065267195	X	how would you Make sure your local or server has the correct time setup.
563e2f3761a8013065267196	X	I currently have a Rails API with AngularJS frontend that upload picture to Amazon S3. But when I do, I get that error: AWS::S3::Errors::ExpiredToken The provided token has expired.. The strange thing is that if I reupload a file right away, it works. I guess when the token expired, it try to get a new one and the upload works on the second time. My code is pretty basic, no need to share. I included basic paperclip functionality into my model and my configuration file are fine too. Any Idea ?
563e2f3761a8013065267197	X	I think your problems are due to IAM permissions. Make sure your permissions are on the same users as your elastic bean stalk.
563e2f3761a8013065267198	X	Make sure your local or server has the correct time setup. If you server is few minutes ahead of the AWS server it will fail the first time but will work few minutes after.
563e2f3761a8013065267199	X	Which S3 API are you using for PHP? Please provide info about which API you're using. Also so people have something concrete to work with please include a snippet of code.
563e2f3761a801306526719a	X	Using aws-sdk. I just called the method getobject in s3.
563e2f3761a801306526719b	X	I have a file in Amazon S3 and the API returns an object. I want to transfer the files from Amazon to my local servers directory. The object has a url address too. It is just not clear to me how to use this object to transfer the file.
563e2f3761a801306526719c	X	In PHP this is how you code it. I hope this helps.
563e2f3861a801306526719d	X	I've tried cloudberry explorer PRO and it will take 300 hours!! (and this is not downloading the files, either). Parallel.For sounds like it could be a big winner .. i'll give that a go..
563e2f3861a801306526719e	X	I have 2x S3 Amazon buckets and I wish to move a list of 10K or so items from the Old Stuff bucket (which is just a subset of the data in that bucket) to the New Stuff bucket. I'm not sure of the best way to do this. I was thinking of leveraging their REST API but nothing stood out that could do this. Secondly, I'm not sure that their API would handle bulk moving - so then I would need some suggestions about how to best fire off 10K odd REST api requests... Any code examples would need to be preferred in .NET please. Lastly, if someone suggests an Open Source library to do this.. can they please explain if the method(s) handle bulk requests .. and if not .. how can i handle so many requests in a short time.
563e2f3861a801306526719f	X	Is this just a one off move? Why not use a GUI tool such Cloudberry Explorer or BucketExplorer. I'm pretty sure both can do parallel operations. If you want or need to do it programmatically, you can use the AWS .NET SDK's CopyObject method to copy files between buckets. Then delete the original file to complete the move. You could wrap this up in a Parallel.For or any of the other built in libraries for parallel/async operations. See Task Parallel Library. I guess there's nothing stopping you using these libraries to make multiple request to the REST API in parallel too.
563e2f3861a80130652671a0	X	i tried this, but seems that doesn't work with S3, i'm still having the same problem.
563e2f3861a80130652671a1	X	@shadow_of__soul Hmn. That is a quandary, then! As you say, you have a desire for more debugging information that you can use to trace the issue. Are you able to run your client in a debugger (for example, in the debugging facilities provided by IntelliJ or Eclipse) and set a breakpoint directly before the exception fires? For that matter, what happens when you just catch (Exception e) (which will notably catch RuntimeException) when you attempt the connection? I'm sorry to go with the standard advice here, but I don't have good bounds on what you've tried yet.
563e2f3861a80130652671a2	X	the problem is that this only happen in the server, locally the code works perfectly. is there anyway to run a debugger from a command line? (i'm running the app wtih java -jar app.jar)
563e2f3861a80130652671a3	X	@shadow_of__soul Yes! You'll want to look here for that functionality. If local or remote debugging in jdb isn't sufficient, I'm sure we can hash something else out that fits your needs. :)
563e2f3861a80130652671a4	X	i'm coding a command line tool to manage the S3 service. on my local machine, everything works but on the server where it should be executed, fails with the following message: i make the connection with the following code: clientConf only sets the protocol to HTTP, as i suspected that maybe could be a problem to connect to HTTPS but i'm having the same result. now, the server have the following configuration: debian 6 64 bits LAMP installed from source openssl installed from source java installed from distribution packages packages this is the network configuration: wget, telnet, curl, everything works, except this, i have 3 network interfaces as i have 2 SSL and another ip for the other sites. how i should configure the clientConf to make this work? is a java problem? a network problem? at least, how i can get more debug info? i tried to catch the AmazonClientException exception but doesn't work. Thanks in advance :) Regards.
563e2f3861a80130652671a5	X	This has been reported as a bug in the Amazon S3 API. Quoth ZachM@AWS: This appears to be a bug in the SDK. The problem is that the client configuration object is shared with the Security Token Service client that DynamoDB uses to establish a session, and it (unlike Dynamo) doesn't accept the HTTP protocol. There are a couple workarounds: 1) Create your own instance of STSSessionCredentialsProvider and provide it to your DynamoDB client, or 2) Instead of specifying the protocol in the ClientConfiguration, specify it with a call to setEndpoint("http://...") We'll discuss solutions for this bug. I would recommend using one of the workarounds for now. Good luck getting your connection to work successfully. (Additional documentation and workarounds)
563e2f3861a80130652671a6	X	I think I need to do it myself..using a strange tool will not save me from future problems. To be onest the problem is very strange. Any idea where the EBS AMI instance is actually saved?..I mean physical location? When creating a AMI using the ec2-bundle-vol, I have access to the AMI(and the Manifest.xml) and I can upload it to the bucket...but in the case of creating a EBS AMI in the AWS console..where is the EBS AMI saved, can it be backuped on the S3? Any tips would be very helpful.
563e2f3861a80130652671a7	X	I've created an AMI(EBS AMI) using the Amazon AWS console. That AMI has 2 snapshots attached to it. Now I want to backup that AMI to a S3 bucket. Is this possible? I actually need to do this to be able to then move that AMI to a bucket in a different region and register that AMI for use in that different region. Any clues?
563e2f3861a80130652671a8	X	My initial answer still applies concerning the question as such (see below), however, given you actually need to do this to be able to then move that AMI to [...] a different region, you will be pleased that AWS has just released Cross Region EC2 AMI Copy to address this long standing feature request: AMI Copy enables you to easily copy your Amazon Machine Images between AWS Regions. AMI Copy helps enable several key scenarios including: Now I want to backup that AMI to a S3 bucket. Is this possible? While Amazon EBS indeed provides the ability to create point-in-time snapshots of volumes, which are persisted to Amazon S3, this operation is outside of your control and entirely handled by EC2, see the respective FAQ Will I be able to access my snapshots using the regular Amazon S3 APIs?: No, snapshots are only available through the Amazon EC2 APIs. You can achieve your goal by following Eric Hammond's elaborate article Copying EBS Boot AMIs Between EC2 Regions, which guides you through all required steps (quite some though).
563e2f3961a80130652671a9	X	That is not a trivial task. I have seen this site referenced in many blogs and references, but I have not used it myself. You might want to try CloudScripts and in particular for your needs this particular script: https://cloudyscripts.com/tool/show/4 Hope this helps.
563e2f3961a80130652671aa	X	@IIa Thanks It works
563e2f3961a80130652671ab	X	Is there a method to write/create a text file to S3 bucket in AWS SDK?
563e2f3961a80130652671ac	X	This may helpful. I use ZenS3 (https://github.com/cyberbuff/ZenS3). It has a method putObjectString(). Just pass string to putObjectString method. It will create a file in S3 Bucket. Make sure your bucket should be in US region.
563e2f3961a80130652671ad	X	Amazon S3 works via HTTP REST API. There are methods in the SDK's to write a string to a file in S3: for example the set_contents_from_string method. There are different S3 clients that provide an ease of use to S3 like CloudBerry and DragonDisk
563e2f3961a80130652671ae	X	When ever we upload files to S3 (using a clinet like cloudberry), the files date changes to the upload date. Is there a way to keep the current date/time?
563e2f3961a80130652671af	X	You cannot change the LastModified field. But you can add custom metadata (Amazon calls this "user metadata") to store the information: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html You can store the date here and then retrieve it using the S3 API.
563e2f3961a80130652671b0	X	You can access SkyDrive over WebDAV, would that work for you?
563e2f3961a80130652671b1	X	@Kevin: Hmm interesting. Yes, I think WebDAV is possible on Linux with C#/mono.
563e2f3961a80130652671b2	X	You don't even need your code to handle the WebDAV part. Just mount the WebDAV share and then your code can just pretend it's a local folder.
563e2f3a61a80130652671b3	X	FREE for life without any benefits? Oh yes, I want free food, free medicines, free traveling. Come on, they are not charging millions, they are charging in cents.
563e2f3a61a80130652671b4	X	Parse gives you 1GB storage for free.
563e2f3a61a80130652671b5	X	Anybody knows a FREE (some free volme, 2-5 GB i mean - continuously, not just the first year like Amazon S3) cloud storage provider which has an API that does NOT use oauth ? I need that to make regular backups to from a console service (scheduled web-browser and user-input free application). I have looked at DropBox, Google Drive, SkyDrive and UbuntuOne. DropBox & Google use oAuth, which I can't use (oAuth is NOT web-browser & user input free). UbuntuOne's "API" is a horrible mess - simply unusable. SkyDrive SDK needs Windows. I have my data on a Linux server, and that's not gonna change.
563e2f3a61a80130652671b6	X	This is off-topic for StackOverflow - you're asking opinions on external resources (hosting), plus opinions on various frameworks.
563e2f3a61a80130652671b7	X	Thanks Joran! I've heard about using Python and Flask. I do have several questions about this though. Where would this be usually stored, I'm planning on using parse (database), and s3 (file storage). Mind you, I have little to no experience with backend, but need to know just enough to get my mvp up and running. Also, how secure is flask in terms of preventing third-party apps from using the private api?
563e2f3b61a80130652671b8	X	as secure as you make it ... you would raise an AuthenticationError when validating if it did not meet any of your criteria ... you can run this anywhere you can install flask and run python ...
563e2f3b61a80130652671b9	X	digital ocean works good ... its pretty easy to setup apache to server it (there are a million and one tutorials out there ...) or you could serve it with a more multithreaded application like gunicorn + nginx (Also a lot of tutorials but a little harder to setup)
563e2f3b61a80130652671ba	X	in this example you can just run it locally to test until it works how you want (maybe a couple hours) ... then get it on a host
563e2f3b61a80130652671bb	X	chat.stackoverflow.com/rooms/87666/…
563e2f3b61a80130652671bc	X	I'm attempting to build a private API to connect to my mobile app. One use would be to make a call with a string parameter, have the api run the string through several nlp python scripts, and return back some json. What would be a good place to start in terms of api services and resources? So far I've heard that I can use Django Rest Framework for this, but I wanted to make sure to ask people with more experience. Also what's the best place to host it, including the scripts (my hosting with namecheap, amazon s3, etc)
563e2f3b61a80130652671bd	X	your question is very vague and short on details ... that said flask is probably the easiest to get up and running when its time to pick a host you should choose one that meets your needs I like dreamhost alot ... however I have recently used digital ocean with great sucess ... you can also just run it on a local linux (or windows box) and use something like noip.com to point a domain name to your box
563e2f3b61a80130652671be	X	Have you used AWS CLI ?
563e2f3b61a80130652671bf	X	Yes! That is it! If you know your folder, or path, you can to that path by adding in the ...prefix=your/path relative to the bucket. It works
563e2f3b61a80130652671c0	X	I am searching for a specific file in a S3 bucket that has a lot of files. In my application I get an error of 403 access denied, and with s3cmd I am getting an error of 403 (Forbidden) if I try to get a file from the bucket. My problem is that I am not sure if the permissions are the problem (because I can get other files) or the file isn't present on the bucket. I have started to search in the Amazon console interface, but I am scrolling for hours and I have not arrived at "4...." (I am still at "39...") and the file I am looking for is in a folder "C03215". So, is there a faster way to verify that the file exists on the bucket? Or is there a way to do auto-scrolling and meanwhile doing something else (because if I do not scroll nothing new is loading)? P.S.: I have no permission to list with s3cmd
563e2f3b61a80130652671c1	X	Regarding accelerating the scrolling in the console Like you I have many thousands of objects that takes an eternity to scroll through to in the console. I recently discovered though how to jump straight to a specific path/folder in the console that is going to save my mouse finger and my sanity! This will only work for folders though not the actual leaf objects themselves. In the URL bar of your browser when viewing a bucket you will see something like: If you append your object's path after the prefix and hit enter you assume that it should jump to that object but it does nothing (in chrome at least). However if you append your object's path after the prefix, hit enter and then hit refresh (f5) the console will reload at your specified location. e.g. There was much joy in our office when this was figured out!
563e2f3b61a80130652671c2	X	The only "faster way" is to have the s3:ListBucket permission on the bucket, because, as you have noticed, S3's response to a GET request is intentionally ambiguous if you don't. If the object you request does not exist, the error Amazon S3 returns depends on whether you also have the s3:ListBucket permission. If you have the s3:ListBucket permission on the bucket, Amazon S3 will return an HTTP status code 404 ("no such key") error. If you don’t have the s3:ListBucket permission, Amazon S3 will return an HTTP status code 403 ("access denied") error. http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html Also, there's not a way to accelerate scrolling in the console.
563e2f3c61a80130652671c3	X	Recommendation questions are off-topic, see help center. Research this site and you'll find tons of techniques. Try to implement one and come back if you get stuck.
563e2f3c61a80130652671c4	X	Sounds promising. I'll give it a try. I found this Wordpress plugin: wordpress.org/plugins/nmedia-user-file-uploader/screenshots but the free version has too few features. Before paying 30 bucks, i'm gonna try writing exactly what i want by myself ;)
563e2f3c61a80130652671c5	X	Moving to an other provider is not possible, so this solution doesnt work for me. Thanks anyways ;)
563e2f3c61a80130652671c6	X	You wouldn't be moving to another provider. Amazon S3 is a cloud service that you can use with any provider you desire.
563e2f3c61a80130652671c7	X	I want to create a landing page for a movie festival. Visitors should be able to upload their videos on our FTP server. We're running a Wordpress setup, but we don't need to use it for the landing page. Do you guys know any tools, plugins, scripts etc, that do this job?
563e2f3c61a80130652671c8	X	Have a look at this, http://www.w3schools.com/php/php_file_upload.asp it might help. I made something similar recently, and used php... don't know about using wordpress though.
563e2f3c61a80130652671c9	X	One of the easiest ways to do it would probably be to use Amazon S3, they have an easy to use API for PHP and you can pay for exact usage that way. Your new video page would have a simple file upload section, then use PHP to save uploaded vids to Amazon S3 via their API and then you can display the videos embedded and the urls are actually links to the files on Amazon's servers.
563e2f3c61a80130652671ca	X	You exactly hit the problem. Is the resource name just a prefix? So assuming I have buckets 'com.mydomain.xxx' and 'com.mydomain.yyy'. Would it be possible to grant access to all operations to both buckets (including content) via 'arn:aws:s3:::com.mydomain.*'?
563e2f3c61a80130652671cb	X	The exact syntax and meaning of the resource specification via ARNs vary by service, see e.g. Syntax and Examples of ARNs - for S3 you can use prefix (or rather a path) for objects indeed, but the bucket must be specified in full for the time being and can't include wildcards, see e.g. the AWS team's response to IAM statement for s3 bucket wildcard?.
563e2f3c61a80130652671cc	X	I have a Amazon S3 bucket and would like to make it available to scripts on a certain machine, whithout the need to deploy login credentials. So my plan was to allow anonymous access only from the IP of that machine. I'm quite new to the Amazon cloud and bucket policies look like the way to go. I added the following policy to my bucket: But anonymous access still does not work. For testing, I granted access to "Everyone" in the S3 management console. That works fine, but is obviously not what I want to do. ;-) Any hint what I'm doing wrong and how to get this working? My use case is some data processing using EC2 and S3, so access control by IP would be much simpler than fiddling around with user accounts. If there's a simpler solution, I'm open for suggestions.
563e2f3c61a80130652671cd	X	But anonymous access still does not work. What operation still does not work exactly, do you by chance just try to list the objects in the bucket? Quite often a use case implicitly involves Amazon S3 API calls also addressing different resource types besides the Resource explicitly targeted by the policy already. Specifically, you'll need to be aware of the difference between Operations on the Service (e.g. ListAllMyBuckets), Operations on Buckets (e.g. ListBucket) and Operations on Objects (e.g. GetObject). In particular, the Resource specification of your policy currently addresses the objects within the bucket only (arn:aws:s3:::name_of_my_bucket/*), which implies that you cannot list objects in the bucket (you should be able to put/get/delete objects though in case) - in order to also allow listing of the objects in the bucket via ListBucket you would need to amend your policy as follows accordingly:
563e2f3d61a80130652671ce	X	I updated this module to support asynchronous IO, so it now has fewer caveats.
563e2f3d61a80130652671cf	X	Does anyone know a way to upload image files directly from a Corona app to an Amazon S3 bucket? I found this article helpful on how to upload to a server by base64 encoding the image first: http://developer.anscamobile.com/code/how-upload-image-server-multipartform-data To my knowledge though, this method will not work uploading directly to S3. Any thoughts?
563e2f3d61a80130652671d0	X	I had a similar problem and implemented S3 support myself. See: http://developer.anscamobile.com/code/amazon-s3-rest-api-implementation-corona Note that there are a lot of caveats (it uses blocking I/O, which is kind of a non-starter, and the async I/O provided by Corona doesn't have enough functionality/fidelity to talk to the S3 REST API, at least not yet).
563e2f3e61a80130652671d1	X	I do still need to tunnel through my server because of the compression I'll be applying to the output.
563e2f3e61a80130652671d2	X	I have an example which I'm trying to create which, preferably using Django (or some other comparable framework), will immediately compress uploaded contents chunk-by-chunk into a strange compression format (be it LZMA, 7zip, etc.) which is then written out to another upload request to S3. Essentially, this is what will happen: Step 3 is optional; I could store the file locally and have a message queue do the uploads in a deferred way. Is step 2 possible using a framework like Django? Is there a low-level way of accessing the incoming data in a file-like object?
563e2f3e61a80130652671d3	X	The Django Request object provides a file-like interface so you can stream data from it. But, since Django always reads the whole Request into memory (or a temporary File if the file upload is too large) you can only use this API after the whole request is received. If your temporary storage directory is big enough and you do not mind buffering the data on your server you do not need to do anything special. Just upload the data to S3 inside the view. Be careful with timeouts though. If the upload to S3 takes too long the browser will receive a timeout. Therefore I would recommend moving the temporary files to a more permanent directory and initiating the upload via a worker queue like Celery. If you want to stream directly from the client into Amazon S3 via your server I recommend using gevent. Using gevent you could write a simple greenlet that reads from a queue and writes to S3. This queue is filled by the original greenlet which reads from the request. You could use a special upload URL like http://upload.example.com/ where you deploy that special server. The Django functions can be used from outside the Django framework if you set the DJANGO_SETTINGS_MODULE environment variable and take care of some things that the middlewares normally do for you (db connect/disconnect, transaction begin/commit/rollback, session handling, etc.). It is even possible to run your custom WSGI app and Django together in the same WSGI container. Just wrap the Django WSGI app and intercept requests to /upload/. In this case I would recommend using gunicorn with the gevent worker-class as server. I am not too familiar with the Amazon S3 API, but as far as I know you can also generate a temporary token for file uploads directly from your users. That way you would not need to tunnel the data through your server at all. Edit: You can indeed allow anonymous uploads to your buckets. See this question which talks about this topic: S3 - Anonymous Upload - Key prefix
563e2f4061a80130652671d4	X	We have build an implementation of S3 auth for client authentication on top of spring security but i have no idea if that's portable to shiro. It's not open source but i think we could share the code if you are interested.
563e2f4061a80130652671d5	X	Another option could be restlet: restlet.org/learn/guide/2.1/core/security
563e2f4061a80130652671d6	X	Is there a Java security framework that provides Amazon like REST authentication? (I am not talking about a Client for AWS, but use the same algorithm to authenticate my users.) From the documentation: The Amazon S3 REST API uses a custom HTTP scheme based on a keyed-HMAC (Hash Message Authentication Code) for authentication. To authenticate a request, you first concatenate selected elements of the request to form a string. You then use your AWS Secret Access Key to calculate the HMAC of that string. Informally, we call this process "signing the request," and we call the output of the HMAC algorithm the "signature" because it simulates the security properties of a real signature. Finally, you add this signature as a parameter of the request, using the syntax described in this section. If not, it may be possible to implement something on top of Apache Shiro, but I don't know if I am able to properly implement the algorithm above.
563e30ac61a80130652671d7	X	I started using fog storage for a project. I do the most simple actions: upload an object, get the object, delete the object. My code looks something like this: In all cases there's a 1st step to get the directory, which does a request to the storage engine (it returns nil if the directory doesn't exists). Then there's another step to do whatever I'd like to do (in case of delete there's even a 3rd step in the middle). However if I look at let's say the Amazon S3 API, it's clear that deleting an object doesn't need 3 requests to amazon. Is there a way to use fog but make it do less requests to the storage provider?
563e30ac61a80130652671d8	X	I think this was already answered on the mailing list, but if you use #new on directories/files it will give you just a local reference (vs #get which does a lookup). That should get you what you want, though it may raise errors if the file or directory does not exist.
563e30ac61a80130652671d9	X	I ended up scripting the operation with the AWS SDK in .NET
563e30ac61a80130652671da	X	@MattDell can you add the .NET answer to this question?
563e30ac61a80130652671db	X	@balexandre, I've added my .NET code below
563e30ad61a80130652671dc	X	What sucks about this is that Amazon isn't very clear on whether the copy command was successful or not, so the delete after the operation seems dangerous.
563e30ad61a80130652671dd	X	Just to be clear, I was referring specifically to the Java API. I've opened a separate question stackoverflow.com/questions/17581582
563e30ad61a80130652671de	X	This should be up voted to the top of the list. It's the proper way to sync buckets and the most up to date in all these answers.
563e30ad61a80130652671df	X	If you have trouble with 403 access denied errors, see this blog post. It helped. alfielapeter.com/posts/…
563e30ad61a80130652671e0	X	is this server side?
563e30ad61a80130652671e1	X	Server side? There is no server side for s3. All commands are performed from a remote client.
563e30ad61a80130652671e2	X	This command seems to work just fine over the internet, by the way!
563e30ae61a80130652671e3	X	The "server side" question is valid. Does the s3cmd transfer shunt all data over to the client, or is it a direct S3 to S3 transfer? If the former, it would be preferable to run this in the AWS cloud to avoid the external WAN transfers.
563e30ae61a80130652671e4	X	The copying happens all remotely on S3.
563e30ae61a80130652671e5	X	That's seems like a good solution. but what happens if you have different credentials for the 2 buckets?
563e30ae61a80130652671e6	X	The credentials are for the execution of the copy command. Those single credentials require appropriate read/write permissions in the source/target buckets. To copy between accounts, then you need to use a bucket policy to allow access to the bucket from the other account's credentials.
563e30ae61a80130652671e7	X	I'd like to copy some files from a production bucket to a development bucket daily. For example: Copy productionbucket/feed/feedname/date to developmentbucket/feed/feedname/date Because the files I want are so deep in the folder structure, it's too time consuming to go to each folder and copy/paste. I've played around with mounting drives to each bucket and writing a windows batch script, but that is very slow and it unnecessarily downloads all the files/folders to the local server and back up again.
563e30ae61a80130652671e8	X	As pointed out by alberge (+1), nowadays the excellent AWS Command Line Interface provides the most versatile approach for interacting with (almost) all things AWS - it meanwhile covers most services' APIs and also features higher level S3 commands for dealing with your use case specifically, see the AWS CLI reference for S3:   The following sync command syncs objects under a specified prefix and bucket to objects under another specified prefix and bucket by copying s3 objects. [...]   Moving files between S3 buckets can be achieved by means of the PUT Object - Copy API (followed by DELETE Object): This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3. A PUT copy operation is the same as performing a GET and then a PUT. Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket. There are respective samples for all existing AWS SDKs available, see Copying Objects in a Single Operation. Naturally, a scripting based solution would be the obvious first choice here, so Copy an Object Using the AWS SDK for Ruby might be a good starting point; if you prefer Python instead, the same can be achieved via boto as well of course, see method copy_key() within boto's S3 API documentation. PUT Object only copies files, so you'll need to explicitly delete a file via DELETE Object still after a successful copy operation, but that will be just another few lines once the overall script handling the bucket and file names is in place (there are respective examples as well, see e.g. Deleting One Object Per Request).
563e30ae61a80130652671e9	X	The new official AWS CLI natively supports most of the functionality of s3cmd. I'd previously been using s3cmd or the ruby AWS SDK to do things like this, but the official CLI works great for this. http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html
563e30ae61a80130652671ea	X	To move/copy from one bucket to another or the same bucket I use s3cmd tool and works fine. For instance:
563e30ae61a80130652671eb	X	If you have a unix host within AWS, then use s3cmd from s3tools.org. Set up permissions so that your key as read access to your development bucket. Then run:
563e30ae61a80130652671ec	X	.NET Example as requested: with client being something like There might be a better way, but it's just some quick code I wrote to get some files transferred.
563e30ae61a80130652671ed	X	Here is a ruby class for performing this: https://gist.github.com/4080793 Example usage:
563e30af61a80130652671ee	X	We had this exact problem with our ETL jobs at Snowplow, so we extracted our parallel file-copy code (Ruby, built on top of Fog), into its own Ruby gem, called Sluice: https://github.com/snowplow/sluice Sluice also handles S3 file delete, move and download; all parallelised and with automatic re-try if an operation fails (which it does surprisingly often). I hope it's useful!
563e30af61a80130652671ef	X	I know this is an old thread but for others who reach there my suggestion is to create a scheduled job to copy content from production bucket to development one. You can use If you use .NET this article might help you http://www.codewithasp.net/2015/03/aws-s3-copy-object-from-one-bucket-or.html
563e30af61a80130652671f0	X	I read over that document and I don't see where it lets me tell S3 to download the file from a public URL anywhere. Am I just missing something?
563e30af61a80130652671f1	X	Look over the document carefully. If you use S3 API, just add header x-amz-acl:public-read in your put object request, then you can get the file from public url: http://[bucket name].s3.amazonaws.com/[file name]
563e30af61a80130652671f2	X	If you use s3cmd: s3cmd put [file name] s3://[bucket name] -P (-P means public)
563e30af61a80130652671f3	X	Thanks! I will try it shortly and confirm.
563e30af61a80130652671f4	X	I'm looking at having to transfer a lot of large files from a 3rd party system to a cloud based file storage system, such as Rackspace Cloudfiles or Amazon S3. I'm not limited to just using those two, however. What I'm trying to find is a way to just let those services download the files directly, once provided with a public URL, in order to speed up the transfer and avoid having to setup something in the middle that relays each file. Is there a service out there that has an option like this available via an API or a file list upload?
563e30af61a80130652671f5	X	All modem cloud (object) storage service meet your requirement, I think. For exmaple, AWS S3, supports set canned ACL to public when putting object (uploading file). http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html
563e30af61a80130652671f6	X	You should accept Elie's solution
563e30af61a80130652671f7	X	this is correct. Also for the newer versions, key and secret must be passed like this: stackoverflow.com/a/31582475
563e30af61a80130652671f8	X	I've migrated servers and updated AWS phar, however once i've done that i'm getting the following error: Fatal error: Uncaught exception 'InvalidArgumentException' with message 'Missing required client configuration options: version: (string) A "version" configuration value is required. Specifying a version constraint ensures that your code will not be affected by a breaking change made to the service. For example, when using Amazon S3, you can lock your API version to "2006-03-01". Your build of the SDK has the following version(s) of "email": * "2010-12-01" You may provide "latest" to the "version" configuration value to utilize the most recent available API version that your client's API provider can find. Note: Using 'latest' in a production application is not recommended. A list of available API versions can be found on each client's API documentation page: http://docs.aws.amazon.com/aws-sdk-php/v3/api/index.html. If you are unable to load a specific API version, then you may need to update your copy of the SDK.' in phar:////includes/3rdparty/aws/aws.phar/Aws/ in phar:////includes/3rdparty/aws/aws.phar/Aws/ClientResolver.php on line 328 I've tried adding it via different method and looking into the actual documentation without any luck. Here's my code right now: Any help would be appreciated!
563e30af61a80130652671f9	X	Apparenty, the 'version' field is mandatory now, so you must pass it to the factory.
563e30b161a80130652671fa	X	need some modifications in this files for saving images(or images path) and drop down options: What are those modifications?
563e30b161a80130652671fb	X	how to save images into mysql database with out leaving this code(or adding some code with this)
563e30b161a80130652671fc	X	then i ask one. when a user submit some images how to save that images and path into sql server and how to retrieve?
563e30b161a80130652671fd	X	@user2481198 see my updated answer.
563e30b161a80130652671fe	X	I have a form written in PHP. The form data which a user fills in is saved directly into a MySQL database, except for the images and the drop-down options. My problems are: The form looks like this. And this is the HTML code for the form: (form.html) and my process form is looking like this (process.php) and mysql database table is looking like this Kindly help me.
563e30b161a80130652671ff	X	I would highly recommend that you don't save the images in the MySQL database. Instead I would suggest you upload the images to your server or use a service like Amazon S3 / RackSpace Cloud Files and then store the link to that specific image in your database. Large websites like Facebook / Twitter / etc don't store images in the DB. It add's unnecessary workload to the DB when it's not required. You can use the following to upload your image and save it into MySQL: To retrieve the data you would just write a SELECT statement to fetch the data back. You will then have a column in your table that contains the image URL. You can then drop the image URL into an <img src="LOCATION" /> tag to display it
563e30b161a8013065267200	X	You would save them like everything else. When you add your categories to the database and fetch them to generate the dropdown lists you can reference the ID, if not you should save the option value itself and try to match your stored option with the existing categories while generating the dropdown-list. You should change your database structure like this: So you can save as much images per user as you want. If you would do it your way, you would limit yourself to three images per user. When you want to move your categories you can add the cols to you Owner_detail table where you reference the chosen category / dropdown-option.
563e30b261a8013065267201	X	Could the remote-server-url.com just download the file itself? It would be much easier for you and you will be able to get rid of the extra network round-trip "download the file to my server and send it again to a remote one".
563e30b261a8013065267202	X	Awesome answer on your question is here: stackoverflow.com/a/12282709/1426097, with 0-responsibility on your server :)
563e30b261a8013065267203	X	@AlexeyShein, unfortunately I don't have any control over what the remote server does. It's just an API that's listening for a file payload.
563e30b261a8013065267204	X	@dimakura I'll give that a try, but I'm not sure that's right. I can already get a public-facing URL of the files in S3 from inside the S3 dashboard. I can even set permissions to make them public. Is that answer you linked doing it differently? I wasn't sure from reading. Thanks for the advice.
563e30b261a8013065267205	X	@BoomShadow it's ideal answer for number of reasons. (1) It allows you to keep your files private (no need for setting them from dashboard). (2) It removes load from your server. (3) It also gives a recipe how to mask amazon server, to look like user actually downloads link from you.
563e30b261a8013065267206	X	Wow. This makes a lot of sense. I'm going to give this a try. I'm marking it as the accepted answer as it looks like exactly what I need. Thanks so much! You are truly fantastic.
563e30b261a8013065267207	X	sure. lmk if this works. if you run in any snags along the way I can actually try to put together a complete code sample.
563e30b261a8013065267208	X	I've been hunting around and can't seem to find a good solution for this. My Rails app stores it's files in Amazon S3. I now need to send them to a remote (3rd party) service. I'm using RestClient to post to the 3rd party server like this: It works for local files, but how can I send a remote file from S3 directly to this 3rd party service? I found an answer here where someone was using open-uri: ruby reading files from S3 with open-URI I tested that for myself, and it worked. But, I've read a comment here that says open-uri simply loads the remote file into memory. See last comment on this answer: http://stackoverflow.com/a/264239/2785592 This wouldn't be ideal, as I'm handling potentially large video files. I've also read somewhere the RestClient loads even local files into memory; again, this isn't ideal. Does anyone know if that's true? Surely I can't be the only one that has this problem. I know I could download the S3 file locally before sending it, but I was hoping to save on time & bandwidth. Also, if RestClient truly does load even local files to memory, than downloading it locally doesn't save me anything. Heh heh. Any advice would be much appreciated. Thanks :) Update: The remote server is just an API that responds to post requests. I don't have the ability to change anything on their end.
563e30b261a8013065267209	X	Take a look at: https://github.com/rest-client/rest-client/blob/master/lib/restclient/payload.rb RestClient definitely supports streamed uploads. The condition is that in payload you pass something that is not a string or a hash, and that something you pass in responds to read and size. (so basically a stream). On the S3 side, you basically need to grab a stream, not read the whole object before sending it. You use http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Client.html#get_object-instance_method and you say you want to get an IO object in the response target (not a string). For this purpose you may use an IO.pipe you pass in the reader to the RestClient::Payload.generate and use that as your payload. If the reading part is slower than the writing part you may still read a lot in memory. you want, when writing to only do accept the amount you are willing to buffer in memory. You can read the size of the stream with writer.stat.size (inside the fork) and spin on it once it gets past a certain size.
563e30b361a801306526720a	X	Any luck with either of the posted solutions?
563e30b361a801306526720b	X	I have a rake task that creates a CSV file. Right now I am storing this file into my /tmp folder (I am using Heroku). This CSV file is not associated with any model, it's just some data I pull from several APIs, combined with some data from my models. I would like to download this file from Heroku, but this seems not possible. So, my question is: Which gem am I trying to look for in order to upload that file to Amazon S3? I have seen gems like Paperclip, but that seems to be associated with a model, and that is not my case. I just want to upload that CSV file that I will have in /tmp, into my Amazon S3 bucket. Thanks
563e30b361a801306526720c	X	You can use aws-s3 gem You should define the exact path of your tmp file, for example:
563e30b361a801306526720d	X	CarrierWave can directly interface your Ruby application with S3 directly via the Fog library. Rather than operating on the model level, CarrierWave utilizes a Uploader class where you can cull from across your APIs and datasources, which is precisely what you're trying to accomplish.
563e30b361a801306526720e	X	I'm developing a website in EC2 and I have a lampp server in the original /opt/lampp folder. The thing is that I store all my images related to the website including users' profile images there(/opt/lampp/htdocs). I doubt this is the most efficient way? I have links to the images in my MySQL server. I actually have no idea of what Amazon EBS and Amazon S3 is, how can I utulize it?
563e30b361a801306526720f	X	EBS is like an external usb hard drive. You can easily access content from filesystem (/mnt/). S3 is more like an API based cloud storage. You'll have much more work to integrate it into your system. You have a pretty good summary here : http://www.differencebetween.net/technology/internet/difference-between-amazon-s3-and-amazon-ebs/ Google has a lot of infos about this.
563e30b361a8013065267210	X	Amazon S3 can include a Content-MD5 as part of the header string to prevent the MITM attack you describe.
563e30b461a8013065267211	X	MD5 is a very weak hash function and it's usage has been discouraged for many years now: en.wikipedia.org/wiki/MD5. Use SHA2 nowadays. MD5 is lipstick on a pig with an identity crisis.
563e30b461a8013065267212	X	Startcom provides free SSL certificates that don't throw certificate warnings in major browsers
563e30b461a8013065267213	X	@Henrik MD5 is weak but the content hash will be worthless in a few minutes...far quicker than anyone (well 99.99999% of people) can make any practical use of it.
563e30b461a8013065267214	X	@SeanKAnderson (rant: I find it absurd how people talk about 99.99999%s when the internet is under siege by spy agencies which have automated A LOT of attacks already at 2008 -- it's such a strange way to deal with a real issue -- "Naaah, won't be a problem; for my grandma to wouldn't be able to hack it"
563e30b461a8013065267215	X	Exactly. The only thing SSL verifies as far as your API is concerned is that the call it's dealing with hasn't been messed with en route. The API still has no idea who's talking to it or whether or not they should have access at all.
563e30b461a8013065267216	X	Good answer. I would also recommend having a look at these excellent resources .. owasp.org/index.php/Web_Service_Security_Cheat_Sheet and owasp.org/index.php/REST_Security_Cheat_Sheet (DRAFT)
563e30b461a8013065267217	X	Just a minor point, but using SSL also has the additional benefit of preventing eavesdropping and man in the middle attacks.
563e30b461a8013065267218	X	@Les Hazlewood Could you explain how HTTP Basic authentication over Https can help to determine server knows whom its talking to?
563e30b461a8013065267219	X	@Les Hazlewood here I asked it in a question; tnx stackoverflow.com/questions/14043397/…
563e30b561a801306526721a	X	You could get a GoDaddy ssl certificate for like $30 a year I think. I was shocked to see how much the Verisign SSL certs go for ($600 a year or something if I remember correctly?) But the GoDaddy option is perfectly feasible.
563e30b561a801306526721b	X	Unless you are using SSL/TLS mutual authentication, and the cert used by the user/client is trusted by the server, then you have not authenticated the user to the server/application. You would need to do something more to authenticate the user to the server/application.
563e30b561a801306526721c	X	Ryan: SSL encryption these days takes a pretty tiny amount of processing power compared to what you'd use to generate a response with a web app framework like Django or Rails etc.
563e30b561a801306526721d	X	certs from startcom are free and widely recognized. cacert.org is an open alternative with less recognition
563e30b561a801306526721e	X	This doesn't address the question, which is about authentication.
563e30b561a801306526721f	X	Self-signed certs are free, but AFAIK you still need a static IP.
563e30b661a8013065267220	X	@dF there is no requirement of having a static IP except for certain licensing requirements of commercial paid for certificates.
563e30b661a8013065267221	X	If you have control of the certificate stores on both ends (clientes & server) this may be a viable option but... certificate management and distribution is probably much more complex in production than in a development environment. Be sure to understand the complexities to this alternative before commiting to it.
563e30b661a8013065267222	X	Coming up with a string that will generate the same md5 hash as the valid content may be much easier than it should be, but coming up with an evil version of valid content that hashes to the same value is still prohibitively difficult. This is why md5 isn't used for password hashes anymore, but is still used to verify downloads.
563e30b661a8013065267223	X	Background: I'm designing the authentication scheme for a REST web service. This doesn't "really" need to be secure (it's more of a personal project) but I want to make it as secure as possible as an exercise/learning experience. I don't want to use SSL since I don't want the hassle and, mostly, the expense of setting it up. These SO questions were especially useful to get me started: I'm thinking of using a simplified version of Amazon S3's authentication (I like OAuth but it seems too complicated for my needs). I'm adding a randomly generated nonce, supplied by the server, to the request, to prevent replay attacks. To get to the question: Both S3 and OAuth rely on signing the request URL along with a few selected headers. Neither of them sign the request body for POST or PUT requests. Isn't this vulnerable to a man-in-the-middle attack, which keeps the url and headers and replaces the request body with any data the attacker wants? It seems like I can guard against this by including a hash of the request body in the string that gets signed. Is this secure?
563e30b661a8013065267224	X	A previous answer only mentioned SSL in the context of data transfer and didn't actually cover authentication. You're really asking about securely authenticating REST API clients. Unless you're using TLS client authentication, SSL alone is NOT a viable authentication mechanism for a REST API. SSL without client authc only authenticates the server, which is irrelevant for most REST APIs because you really want to authenticate the client. If you don't use TLS client authentication, you'll need to use something like a digest-based authentication scheme (like Amazon Web Service's custom scheme) or OAuth 1.0a or even HTTP Basic authentication (but over SSL only). These schemes authenticate that the request was sent by someone expected. TLS (SSL) (without client authentication) ensures that the data sent over the wire remains untampered. They are separate - but complementary - concerns. For those interested, I've expanded on an SO question about HTTP Authentication Schemes and how they work.
563e30b661a8013065267225	X	REST means working with the standards of the web, and the standard for "secure" transfer on the web is SSL. Anything else is going to be kind of funky and require extra deployment effort for clients, which will have to have encryption libraries available. Once you commit to SSL, there's really nothing fancy required for authentication in principle. You can again go with web standards and use HTTP Basic auth (username and secret token sent along with each request) as it's much simpler than an elaborate signing protocol, and still effective in the context of a secure connection. You just need to be sure the password never goes over plain text; so if the password is ever received over a plain text connection, you might even disable the password and mail the developer. You should also ensure the credentials aren't logged anywhere upon receipt, just as you wouldn't log a regular password. HTTP Digest is a safer approach as it prevents the secret token being passed along; instead, it's a hash the server can verify on the other end. Though it may be overkill for less sensitive applications if you've taken the precautions mentioned above. After all, the user's password is already transmitted in plain-text when they log in (unless you're doing some fancy JavaScript encryption in the browser), and likewise their cookies on each request. Note that with APIs, it's better for the client to be passing tokens - randomly generated strings - instead of the password the developer logs into the website with. So the developer should be able to log into your site and generate new tokens that can be used for API verification. The main reason to use a token is that it can be replaced if it's compromised, whereas if the password is compromised, the owner could log into the developer's account and do anything they want with it. A further advantage of tokens is you can issue multiple tokens to the same developers. Perhaps because they have multiple apps or because they want tokens with different access levels. (Updated to cover implications of making the connection SSL-only.)
563e30b661a8013065267226	X	Or you could use the known solution to this problem and use SSL. Self-signed certs are free and its a personal project right?
563e30b761a8013065267227	X	If you require the hash of the body as one of the parameters in the URL and that URL is signed via a private key, then a man-in-the-middle attack would only be able to replace the body with content that would generate the same hash. Easy to do with MD5 hash values now at least and when SHA-1 is broken, well, you get the picture. To secure the body from tampering, you would need to require a signature of the body, which a man-in-the-middle attack would be less likely to be able to break since they wouldn't know the private key that generates the signature.
563e30b761a8013065267228	X	In fact, the original S3 auth does allow for the content to be signed, albeit with a weak MD5 signature. You can simply enforce their optional practice of including a Content-MD5 header in the HMAC (string to be signed). http://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html Their new v4 authentication scheme is more secure. http://docs.aws.amazon.com/general/latest/gr/signature-version-4.html
563e30b761a8013065267229	X	Remember that your suggestions makes it difficult for clients to communicate with the server. They need to understand your innovative solution and encrypt the data accordingly, this model is not so good for public API (unless you are amazon\yahoo\google..). Anyways, if you must encrypt the body content I would suggest you to check out existing standards and solutions like: XML encryption (W3C standard) XML Security
563e30b861a801306526722a	X	i'm looking for an API on Amazon (not S3!) that will give me notifications about certain events. For example: I know there is the Event Notification Service API which really seems to be out of date. Further, there seems to be the the Instant Order Processing Notification for Amazon Checkout, which has an empty documentation!? (click link) I'm looking for something like the eBay Platform Notification API which gives me all these events and sends data to an URL i define on those occuring events.
563e30b861a801306526722b	X	You can get the 'Instant Order Processing Notification' document here It only provides provides three messages though:
563e30b861a801306526722c	X	There is none. There is only a payment gateway IPN, if you use their payment processing. This is unbelievable? that a company so large, cannot send a simple notice of a sale to us via IPN, like paypal and Ebay.
563e30b961a801306526722d	X	Related: stackoverflow.com/questions/6669109
563e30b961a801306526722e	X	Is anyone aware of a tool that will automatically deploy a Rails app static assets to Rackspace Cloud Files or Amazon Cloud Front? In my perfect world capistrano would automatically upload everything in javascripts, stylesheets, and images then override the default image_tag and script_tags to route to the appropriate CDN path. It would be great if the deploy task created a new container with each deploy like cap creates a new release directory, or maybe it should use the same containers and keep a cached file with the hashes of all the deployed assets and only deploy new assets to take advantage of long CDN TTLs.
563e30b961a801306526722f	X	I'm not aware of anything, but you could probably script something to do this without too much work. The Fog gem provides an agnostic API for pushing files to Amazon S3 and Rackspace Cloud files, among others.
563e30b961a8013065267230	X	I haven't done it myself yet, but I think it can be done with rsync as a capistrano task. Have look at this. http://railscasts.com/episodes/133-capistrano-tasks
563e30b961a8013065267231	X	I have previously used Rackspace Cloud Files CloudFuse for Linux http://www.rackspace.com/knowledge_center/article/mounting-rackspace-cloud-files-to-linux-using-cloudfuse. It allows you to mount your cloud files containers so that they can be written using standard file system operations, which makes for simple scripting in your deploy scripts. You'll obviously want to take care of keeping the machine that does this secure.
563e30b961a8013065267232	X	...I'm highly doubtful of this (a connection requires 2 parties) my only advice would be to look to see if a site like flickr has an api you could tap into to do this ... unless someone else on S.O has a hack to do so
563e30b961a8013065267233	X	How to incorporate an api into the above code? Any clue on that?
563e30b961a8013065267234	X	I am a newbie in the programming world,It could be of great help if you provide me with some more details. I have edited the question by inclusion of some code. All I need to ensure is that android doesn't give an out of memory error. Is there any ready made website and port number that accepts the file?
563e30b961a8013065267235	X	if it is just about memory issues, you could direct your stream into a file on the sd-card.
563e30b961a8013065267236	X	I haven't wrote any server side code till now. Is there any way I can try writing an android program where I can write the file being uploaded from the mobile to a remote server? Additional details:- My code - All I need to make sure is that android doesn't run out of memory exception.
563e30b961a8013065267237	X	There are several options. One is to use webDAV or FTP on server. But nowadays there is a lot of file storage services which you can access with RESTful API like Amazon S3
563e30ba61a8013065267238	X	It seems you want to mock a Server. Mocking HTTP Server is a good point to start from.
563e30ba61a8013065267239	X	If your using php to do it (ie server side script) your server must relay the file. check the api for a client side implementation (ie javascript).
563e30ba61a801306526723a	X	That is a way to redirect the request, but all the headers that he outputs before 'Location' are meaningless since after the location header a new HTTP request is being generated that has no relation with the previous one (as HTTP is a stateless protocol).
563e30ba61a801306526723b	X	When you put a file into S3 you tell it the content-type, it will work out transfer-encoding by itself, and will use the filename exactly as-is, so it will work fine. If your files aren't public on S3, then you'll need to generate the secure, time-limited URL (S3 Docs detail this...) and then redirect to that.
563e30ba61a801306526723c	X	So, currently, when I open one of the MP4s in my S3, (in Firefox anyway), it embeds the MP4 and begins playing it. What content-type would I need to call it, and how would I do so?
563e30ba61a801306526723d	X	I have a bunch of videos stored on my Amazon S3 storage. I'm working on creating a PHP script, very similar to the one here, where users can download the videos to their hard drive. I'd like to use something like this: However, I am under the impression that this increases the bandwidth because the video will be coming through my server. Any ideas on how I might be able to force the download of these videos, while avoiding reading it first through my own server? Many thanks!
563e30ba61a801306526723e	X	Take a look at the S3 API Docs, and note the header values that you can set. Amazon will send these when the file is requested: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html (the same parameters can be sent with a POST to update an existing object)
563e30ba61a801306526723f	X	The php script you mentioned will work ok, but the main downside is that every time a visitor on your site requests a file, your own servers will load it from the S3 and then relay that data to the browser. For low traffic sites, it's probably not a big deal, but for high traffic ones, you definitely want to avoid running everything through your own servers. Luckily, there's a fairly straight-forward way to set your files to be forced to download from the S3. You just want to set the content-type and content-disposition (just setting content-disposition will work in some browsers, but setting both should work in all browsers). This code is assuming that you're using the Amazon S3 PHP class from Undesigned:  // set S3 auth and get your bucket listing // then loop through the bucket and copy each file over itself, replacing the "request headers": S3::copyObject($bucketName, $filename, $bucketName, $filename, "public-read", array(), array("Content-Type" => "application/octet-stream", "Content-Disposition" => "attachment")); ?> Now all your files will be forced to download. You may need to clear your cache to see the change. And obviously, don't do that on any file that you actually do want to be loaded "inline" in the browser. The nice part with this solution is that applications that load media files directly (like let's say an mp3 player in Flash) don't care about the content-type or content-disposition, so you can still play your files in the browser and then link to download that same file. If the user already finished loading the file in flash, they'll most likely still have it in their cache, which means their download will be super quick and it won't even cost you any extra bandwidth charges from the S3.
563e30ba61a8013065267240	X	
563e30ba61a8013065267241	X	I know this is an old thread, but within the past year, the S3 team has added support for request parameters, which allow you to override certain HTTP headers upon request. You could, for example, upload an SVG image as image/svg+xml, but then override the headers when retrieving as application/octet-stream to force a download. Using the AWS SDK for PHP, you can use the Aws\S3\S3Client::createPresignedUrl() method to achieve this.
563e30bb61a8013065267242	X	Heroku also charges for data that passes through their dynos (over a certain amount, anyway). Look for a storage solution that will allow your uploads to bypass their servers, or use a Heroku dyno on EC2-east and use S3; transfers are free between the two. Very-low-usage (dev level) S3 is free, too, and the prices scale nicely from there.
563e30bb61a8013065267243	X	thanks, I will open up an Amazon S3 account. Is there a good conventional and safe path to use though? The images are uploaded by users with their posts so that's the only type of access I need. It will also be very low usage at the moment as I don't have any users
563e30bb61a8013065267244	X	I'm using carrierwave and I want to change the directory where images are stored. Right now the image URL is /uploads/modelname/image/51/nameoffile.jpg the store_dir in ImageUploader is I definitely do not want the modelname to show Is there an accepted ideal path where images should be stored on heroku?
563e30bb61a8013065267245	X	Heroku doesn't allow uploads to their servers. You need to use another storage medium, like Amazon's S3. I'm actually using Parse's (www.parse.com) API to store images on their solution. But it depends how you need access to your images.
563e30bb61a8013065267246	X	You can upload files to the Heroku dyno filesystems but the filesystem are perishable and not shared among your dynos. Here's a Gist showing how to make Carrierwave store uploaded file in AWS S3 which is a better option: https://gist.github.com/cblunt/1303386 Here's a Heroku guide for accomplishing this with PaperClip: https://devcenter.heroku.com/articles/paperclip-s3
563e30bb61a8013065267247	X	I'm working with an android device and Amazon's S3 storage system. I have a main user account and I've creates sub-users to access the storage on the behalf of my user. Typically, I would use do the following to access S3 with my main user: Where ACCESS_KEY_ID and SECRET_KEY are given for my main user's account. I am trying to do this with my IAM user "Alice". This user was given it own ACCESS_KEY_ID. Using the above code, I user Alice's ACCESS_KEY_ID with the same SECRET_KEY from above. The user Alice has no special permissions or restrictions, thus I assume the user can create buckets. When createBucket is called, there is an error: I cannot seem to find another way to authenticate via Android API to S3 using an IAM user. ANy help is appreciated, thanks.
563e30bb61a8013065267248	X	Your IAM user will have a different SECRET_KEY than your root account. When you created the the IAM account you should have received both an ACCESS_KEY_ID and SECRET_KEY for the account which must be used in combination. The error you received is indicative of a key mismatch or a typo in one of the keys.
563e30bc61a8013065267249	X	I'm using the Java API to upload and download files in Amazon S3. It was working perfectly until about a week ago, and it just stopped working altogether even though I hadn't touched the code. I'm wondering if there's a solution to this. Thanks! Over here's the code that was worked fine: And here's the error: I'd done a bit of research, and some had suggested that the problem could be with the permissions. However, I think I've followed the instructions here: http://docs.aws.amazon.com/AWSToolkitEclipse/latest/GettingStartedGuide/tke_setup_creds.html#d0e387 as well as I could. I'm really pretty stuck, guys. Thanks, and any help at all will be appreciated!
563e30bc61a801306526724a	X	I had same problem. The solution turned out to be the device wrong date. Try to keep your device date updated. The reason is that SSL certificates have issuance/expiry date that are being verified against your device date.
563e30bc61a801306526724b	X	I think we will generate the signature server side. I don't want to include the accounts secret key in a javascript file with the app.
563e30bc61a801306526724c	X	Im doing some research to build an iOS and Android app using titanium appcelerator. The only requirement I have not being able to confirm is the possibility to upload a photo directly to Amazon S3 in a way that is compatible with iOS and Android devices.
563e30bc61a801306526724d	X	Amazon S3 has provided a REST API so your can do it by creating a Titanium.Network.HTTPClient. For your upload case, you need to provide a PUT request to S3. If you want to do it in this way, you may need to include your S3 secret key in your client for signing your request.
563e30bc61a801306526724e	X	You might ask the developers of Titanium Appcelerator. But if you're not writing an HTML5 app, you can definitely do this with the ASIHTTPRequest framework.
563e30bc61a801306526724f	X	dropbox, box.net, amazon drive?
563e30bc61a8013065267250	X	Do those have API's that make it easy to integrate with a web app?
563e30bc61a8013065267251	X	There are many services available for delivering content to users. I am looking for the reverse: a service for receiving content uploads from users. Specifically, I'm building a web app where the files uploaded may be anywhere from 10MB to 100MB. I expect there are many issues around receiving this much data, and what I'm looking for is a service that handles those issues for me.
563e30bd61a8013065267252	X	You can use Amazon S3 to receive uploads from users. You can generate a signed HTML form that will let your users upload files to a designated bucket; you can control the expiry of the form using a policy document. Amazon S3 Developer Guide: Browser-Based Uploads Using POST Article: Browser Uploads to S3 using HTML POST Forms
563e30bd61a8013065267253	X	Uploadcare is built to ease your pain of receiving, storing, processing and distributing files and images from your users. Give it a try, there's a free plan.
563e30bd61a8013065267254	X	You can actually solve this without paying a third party by using a HTML5/Flash uploader in your web app. I particularly recommend Plupload (free, open-source). It's a javascript library that uses a choice of client frontends (HTML5, Flash, Silverlight, etc) to handle uploading the file(s) in a way that doesn't trigger browser timeouts and allows the user to see the progress of their uploads (demo with 10Mb limit). Other than the script you only require a backend on the server to receive the data. You'll want to watch out for anything that could trigger a server-side timeout like PHP's MAX_FILE_SIZE or request timeouts. The plupload package comes with example frontend and backend scripts so you should be up and running fairly quickly. I've uploaded 80Mb files using plupload so I know it works. It's going to be slow on a typical DSL connection but no upload service is immune to that.
563e30bd61a8013065267255	X	Seems like you're looking for something like Amazon S3 or Google Storage. Both have dedicated API's and you only pay for the storage that you use. They both also have libraries avaliable in various languages to make integration easier.
563e30bd61a8013065267256	X	A couple other options: https://www.yousendit.com/ http://www.uploadthingy.com/
563e30bd61a8013065267257	X	I'm not sure, but I'd be surprised if you could get your images into Google Image Search if you prevent hotlinking.
563e30bd61a8013065267258	X	@sharth: Good point.I just searched, there is no referrer in Googlebot. Only one agent: Googlebot-Image/1.0.
563e30bd61a8013065267259	X	Did you succeed in preventing hotlinking? Cheers.
563e30bd61a801306526725a	X	Is there any command line tool to upload an image to blobstore?
563e30bd61a801306526725b	X	@DocWiki No, but the blobstore APIs are available over remote_api, so you could write one fairly simply.
563e30bd61a801306526725c	X	Since you are here, I want to know something about Blobstore. I know there is a 30 secs per request limit in app engine. Will this limit apply when I upload a video to the app engine Blobstore? The max single file size for Blobstore is 2GB, and if I upload via an HTML form, it may take hours. Will the 30 secs per request limit apply?
563e30be61a801306526725d	X	@DocWiki The 30 second execution time limit only applies to the time your code actually spends executing - which doesn't begin until the user has sent the entire request, and ends as soon as you send your response (before they receive it).
563e30be61a801306526725e	X	LOL What are you talking about? FYI My English sucks
563e30be61a801306526725f	X	Wow. 1) The point of hotlink prevention is to prevent users from linking directly to your resources by making it unusable by other users. The users who are sending the referer headers are not your adversaries, the people who linked to your images are, and they have no control over other users' browsers. 2) I'm pretty sure Roosevelt and Churchill didn't use disks, since they weren't invented until 30 years after the end of world war 2. 3) What you're talking about is One Time Pads, and completely irrelevant to the question at hand. 4) Don't invent your own crypto. Just don't.
563e30be61a8013065267260	X	It's been drawn to my attention that you probably meant vinyl records when you said 'discs', which is accurate. It's still pretty much irrelevant to the OP's problem, though.
563e30be61a8013065267261	X	Is this an ironic way of saying "don't worry about hot-linking"?
563e30be61a8013065267262	X	And fetch and return the image from a third-party service on every response? Sure, if you love high bandwidth bills, do this.
563e30be61a8013065267263	X	I implied google app engine blobstore, since as far as I know short of storing static images through app deployment that's the only way I know of storing images there. I guess you have a point in that I didn't specifically say blobstore since that was part of his question...
563e30be61a8013065267264	X	Then you're not really "returning the image from another location", are you? That was what led me to believe you were talking about fetching the image from elsewhere.
563e30be61a8013065267265	X	I meant to say that you can specify "examplewebsite.com/images/image1234.png" when the image's url is whatever the blobstore url is. Google's bandwidth charges are very reasonable for small to medium website's to serve images directly imho. =)
563e30be61a8013065267266	X	Well, the blobstore lets you serve images under any URL you want - the only 'blobstore URLs' are upload URLs and get_serving_url ones. I agree that App Engine's bandwidth charges are reasonable - I was more worried about the OP paying three times that for every request.
563e30be61a8013065267267	X	I am going to build a site using Google App Engine. My public site contains thousands of pictures. I want to store these pictures in the Cloud: Google Storage or Amazon S3 or Google App Engine BlobStore. The problem is image hotlinking. As for Google Storage, I googled and I cant find a way to prevent image hotlinking. (I like its command line tool gsutil very much though) Amazon S3 has "Query String Authentication" which generates expiring image urls. But this is very bad for SEO, isnt it? Constantly changing the URL would have quite negative affects as it takes upwards of a year to get an image, and its related URL, into Google Images. I am rather sure changing this URL would have an immediate negative affect when GoogleBot comes around to say hi. (UPDATE: A better way to preven image hotlinking in Amazon S3 by referrer is using Bucket Policy. Details here: http://www.naveen.info/2011/03/25/amazon-s3-hotlink-prevention-with-bucket-policies/) Google App Engine BlobStore? I have to upload the images via Web Interfaces manually and it generates changing urls too. (update: Due to my ignorance about Blobstore, I just made a mistake. By using Google App Engine BlobStore, you can use whatever url to serve the image you want.) What I need is simple referrer protection: Only show the image when the referrer is my site. Are there some better ways to prevent image hotlinking. I dont want to file bankruptcy due to the extremely high cost of cloud bandwidth. UPDATE: Still difficult to choose from the three, each of them have pros and cons. BlobStore seems to be the ultimate choice.
563e30be61a8013065267268	X	The easiest option would be to use the blobstore. You can provide whatever upload interface you want - it's up to you to write it - and the blobstore doesn't constrain your download URLs, only your upload ones. You can serve blobstore images under any URL simply by setting the appropriate headers, or you can use get_serving_url to take advantage of the built-in fast image serving support, which generates cryptic but consistent URLs (but doesn't let you do referer checks). I would suggest giving some consideration to whether this is a real, practical problem you're facing, though. The bandwidth consumed by a few hotlinked images is pretty minimal by today's standards, and it's not a particularly common practice in the first place. As @sharth points out in the comments, it's likely to impact SEO too, since image search tends to show images in their own windows in addition to linking to the page that hosted them.
563e30be61a8013065267269	X	Whenever I get back to coding for statistical web services, I had to generate images and charts dynamically. The images generated would depend on the request parameter, state of data repository, and some header info. Therefore if I were you, I would write a REST web service to serve the images. Not too difficult. It's pretty ticklish too because if you don't like a particular ip address, you could show cartoon of tongue-out-of-cheek (or animated gif of OBL samba dancing while getting bombed) rather than the image for the data request. For your case you would check the referer (or referrer) at the http header, right? I am doubtful because people can and will hide, blank out or even fake the referer field in the http header. So, not only check the referer field, but create a data field where the value changes. The value could be a simple value matching. During the world war, Roosevelt and Churchill communicated thro encryption. They each had an identical stack of disks, which somehow contained the encryption mechanism. After each conversation, both would discard the disk (and never reused) so that the next time they spoke again, they reach for the next disk on the stack. Instead of a stack of disks, your image consumers and your image provider would carry the same stack of 32 bit tokens. 32 bits would give you ~4 billion ten minute periods. The stack is randomly sequenced. Since it is well known that "random generators" are not truly random and actually algorithmic in a way which can be predicted if supplied a sufficiently long sequence, you should either use a "true random generator" or resequence the stack every week. Due to latency issues, your provider would accept tokens from the current period, the last period and the next period. Where period = sector. Your ajax client (presumably gwt) on your browser would get an updated token from the server every ten minutes. The ajax client would use that token to request for images. Your image provider service would reject a stale token and your ajax client would have to request a fresh token from the server. It is not a fireproof method but it is shatterproof, so that it could reduce/discourage the number of spam requests (nearly to zero, I presume). The way I generate "truly random" sequences is again quick and dirty. I further work on an algorithmically generated "random" sequence by spending a few minutes manually throwing in a few monkey wrenches by manually resequencing or deleting values of the sequences. That would mess up any algorithmic predictability. Perhaps, you could write a monkey wrench thrower. But an algorithmic monkey wrench thrower would simply be adding a predictable algorithm above another predictable algorithm which does not reduce the overall predictability at all. You could further obsessively constrict the situation by employing cyclic redundancy matching as a quick and dirty "encrypted" token matching mechanism. Let us say you have a circle divided into 8 equidistant sectors. You would have a 3 digit binary number to be able to address anyone of all the 8 sectors. Imagine each sector is further subdivided into 8 subsectors, so that now you will be able to address each subsector with an additional 3 bytes, making a total of six bytes. You plan to change the matching value every 10 minutes. Your image provider and all your approved consumers will have the same stack of sector addresses. Every ten minutes they throw away the sector address and use the next one. When a consumer sends your provider a matching value, it does not send the sector address but the subsector address. So that as long as your provider receives a subsector address belonging to the currently accepted sector, the provider service would respond with the correct image. But the subsector address is remapped through an obfuscation sequencing algorithm. so that each subsector address within the same sector do not look similar at all. In that way, not all browsers would receive the same token value or highly similar token value. Let us say that you have 16bit sector addresses and each sector has 16 bit subsector addresses, making up a 32 bit token. Which means you can afford to have 65536 concurrent browser clients carrying the same token sector but where no two token has the same low predictability value. So that you could assign a token subsector value for every session id. Unless you have more than 65536 concurrent sessions to your image provider service, no two session ids would need to share the same subsector token address. In that way, unless a spammer had access to session id faking equipment/facilities, there would be no way your image provider could be spammed except thro denial of service attack. Low predictability means that there is low probability for a snooper or peeper to concoct an acceptable token to spam your image provider service. Certainly, normal bots would not be able to get thro - unless you had really offended the ANNONYMOUS group and they decided to spam your server out of sheer fun. And even then if you had thrown monkey wrenches into the sector address stack and subsector maps, it would be really difficult to predict a next token. BTW, Cyclic Redundancy matching is actually an error correction technique and not so much an encryption technique.
563e30be61a801306526726a	X	Simpler version of geek's essay, build a handler in google app engine to fetch and server the images. You can modify your headers to specify png or whatever, but you're returning the image from another location. You can then examine your request referrer information in the handler and take appropriate action if somebody is trying to access that image "hotlinked". Of course, because you're never exposing the actual image, it would be impossible to hotlink. =)
563e30be61a801306526726b	X	You should know that the File API is still experimental, check out this issue: http://code.google.com/p/googleappengine/issues/detail?id=6888#c20 I'm working on a startup which is moving out from Blobstore to Amazon S3
563e30bf61a801306526726c	X	Thanks for that @ejdyksen. The solution I've devised used exactly that (I haven't updated the question with my answer)! So my solution was to do authenticated URLs for GET requests. However, when a user contributes content, he creates resources to a specific /bucket/user/objectname location using federated IAM token (temporary credentials that expire) with the policy attached to allow for /bucket/user/* write access. so no user in the system can do harm to other users' content. Seems to work okay. Appreciate your answer.
563e30bf61a801306526726d	X	It's just a temporary url, not one time url.
563e30bf61a801306526726e	X	@okwap you're right. "One time" is not very accurate, as it can be used an unlimited number of times before the expiration. I updated the answer to reflect that.
563e30bf61a801306526726f	X	If you're using v2 of the aws-sdk-ruby, note that the methods are somewhat different : docs.aws.amazon.com/sdkforruby/api/Aws/S3/…
563e30bf61a8013065267270	X	Isn't it a risk that the user can see my access key? There is also the secret key (converted)
563e30bf61a8013065267271	X	Thanks for the blog post with your solution, @dineth. I asked a question on Amazon's forum looking for Sample Code to this exact solution you blogged about (forums.aws.amazon.com/thread.jspa?threadID=125851&tstart=0) and their answer pointed in the direction you developed. It would be great if you could post the key parts of your code in Ruby as described. In any case, thanks again for pointing the right path.
563e30bf61a8013065267272	X	I was interested, but the URL is a 404 now.
563e30bf61a8013065267273	X	I updated the URL to my old tumblr blog's post. Something got messed up while moving to Squarespace.
563e30bf61a8013065267274	X	I am new to writing Rails and APIs. I need some help with S3 storage solution. Here's my problem. I am writing an API for an iOS app where the users login with the Facebook API on iOS. The server validates the user against the token Facebook issues to the iOS user and issues a temporary Session token. From this point the user needs to download content that is stored in S3. This content only belongs to the user and a subset of his friends. This user can add more content to S3 which can be accessed by the same bunch of people. I guess it is similar to attaching a file to a Facebook group... There are 2 ways a user can interact with S3... leave it to the server or get the server to issue a temporary S3 token (not sure of the possibilities here) and the user can hit up on the content URLs directly to S3. I found this question talking about the approaches, however, it is really dated (2 yrs ago): Architectural and design question about uploading photos from iPhone app and S3 So the questions: Apologies for multiple questions and I appreciate any insight into the problem. Thanks :)
563e30bf61a8013065267275	X	Using the aws-sdk gem, you can get a temporary signed url for any S3 object by calling url_for: This will give you a signed, temporary use URL for only that object in S3. It expires after 20 minutes (in this example), and it's only good for that one object. If you have lots of objects the client needs, you'll need to issue lots of signed URLs. Or should let the server control all content passing (this solves security of course)? Does this mean I have to download all content to server before handing it down to the connected users? Note that this doesn't mean the server needs to download each object, it only needs to authenticate and authorize specific clients to access specific objects in S3. API docs from Amazon: http://docs.amazonwebservices.com/AmazonS3/latest/dev/RESTAuthentication.html#RESTAuthenticationQueryStringAuth
563e30bf61a8013065267276	X	If anyone is interested, I have wrote about my final solution in a blogpost: http://tmblr.co/ZqibVwRI2W7o (a short explanation is present as a comment on the previous answer).
563e30bf61a8013065267277	X	The above answers use the old aws-sdk-v1 gem rather than the new aws-sdk-resources version 2. The new way is: where your_object_key is the path to your file. If you need to look that up, you would use something like: That information was startlingly difficult to dig up, and I almost just gave up and used the older gem. http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Object.html#presigned_url-instance_method
563e30c061a8013065267278	X	I have the access_key_id and secret_access_key but what do i have to specify in bucket?
563e30c061a8013065267279	X	The name of the bucket you created on S3. You can do it from the AWS Web console.
563e30c061a801306526727a	X	No, I have not created bucket. I just created an account.
563e30c061a801306526727b	X	Then you need to log in into your account aws.amazon.com, then look for AWS Management Console. Look for S3 and create a bucket. And you are good to go. Let me know if you need anything else.
563e30c061a801306526727c	X	Ok. I have seen it and i did not actually finished creating my account because i have not provided the payment details. I tried creating free tier but still it is asking for payment details. why?
563e30c061a801306526727d	X	I have a page where i store images for each post and i store images in files using paperclip but i want to store it in the amazon s3 instead of storing it in folder. How can i do it?
563e30c061a801306526727e	X	Let me try to explain it. add gem 'aws-sdk' to you gemfile. (don't forget to bundle install) create a new file in {APP FOLDER}/config/s3.yml with the following content: access_key_id: xxxxxYOUR_ACCESS_KEY_IDxxxxx enter code heresecret_access_key: xxxxxYOUR_SECRET_ACCESS_KEYxxxxx Update your model accordingly: At last in your views: Any issues let me know. As for the alternatives to S3, Windows Azure Storage and Nimbus.io.
563e30c061a801306526727f	X	There is a fairly thorough documentation about it here, take a look.
563e30c061a8013065267280	X	As per my understanding in Amazon S3 there is nothing called folders to store the data. we have used S3 for string images only but it was stored in the virtual folder S3 termed it as bucket, there are C# API available to read those virtual folders( buckets) information to access the data. there is also AWS SDK available for C#.
563e30c061a8013065267281	X	Here is a minimal howto. For testing and development you can use fake-s3 Descrtiption from github page: "FakeS3 is a lightweight server that responds to the same calls Amazon S3 responds to. It is extremely useful for testing of S3 in a sandbox environment without actually making calls to Amazon, which not only require network, but also cost you precious dollars." I think you should stay with Amazon S3 in production. I do not know a better alernative. It works.
563e30c061a8013065267282	X	I'm using Boto to connect to Amazon S3 in my Python program. I'm able to open a connection and upload files to a bucket. I figured I should then close the connection to release resources and, more important, to avoid any security risks from leaving an open connection hanging around. I assumed I should call the close() method. But I tested this as follows: 1. Open connection. 2. Close connection. 3. Upload file to bucket. I figured step 3 would fail, but the upload worked! So what does close() do? If it doesn't really close the connection, what should I use in place of close()? Or is it just unnecessary to close the connection? I've looked for the answer in the Boto tutorial, the Boto API reference, and this StackOverflow post, but no luck so far. Thanks for your help.
563e30c061a8013065267283	X	Your step 3 worked because boto has code that will automatically re-open closed connections and retry requests on errors. There is little to gain by manually closing the boto connections because they are just HTTP connections and will close automatically after a few minutes of idle time. I wouldn't worry about trying to close them.
563e30c061a8013065267284	X	Under the covers, boto uses httplib. This client library supports HTTP 1.1 Keep-Alive, so it can and should keep the socket open so that it can perform multiple requests over the same connection. connection.close() does not actually close the underlying sockets. Instead, it removes the reference to the underlying pool of httplib connections, which allows the garbage collector to run on them, and that's when the actual socket close happens. Obviously, you also can let the garbage collector run by not keeping a reference to the boto connection itself. But there are performance benefits to reusing the boto connection (e.g. see the Keep-Alive note above). Fortunately, in most cases, you don't have to call connection.close() explicitly. For more details on one case where you DO have to call close, see my answer to the StackOverflow post that's linked in the question.
563e30c161a8013065267285	X	I did something simpler a while ago with a script that generates <form> and allows to send files through POST, but am stuck with PUT.
563e30c161a8013065267286	X	hi, just want your help about updating your code if the credentials I'm using are only temporary security credentials which consist of aws_accesskey, aws_secretkey & security_token? Were you able to accomplish uploading of file using PUT?
563e30c161a8013065267287	X	@WonderingCoder, nope, sorry - I wasn't been able to use PUT and I've forgot about the project. I still would like to learn that though.
563e30c161a8013065267288	X	Why do you need PUT? Isn't POST enough? The documentation strictly talks about POST form, what's the issue with that?
563e30c161a8013065267289	X	@alexcasalboni, quite obvious - you need to reload the page to upload anything. S3 supports CORS and PUT, it's just a matter of learning how it works on an example.
563e30c261a801306526728a	X	I am trying to upload a file using simple PUT method (jQuery if needed), but Amazon documentation seems too superficial for me to understand. There are several examples - few old, few newer (but often overcomplicated). Would there be anyone able to write a simple tutorial. There are three simple parts in here - HTML file with single input[type="file"], PHP file that will generate the signature and most complicated JavaScript that will use File API and AJAX (PUT method as I understand) to store the file on S3. I think many would consider such a working example as a life savior, considering Amazon probably isn't going to make S3 documentation a little noob-friendly.
563e30c261a801306526728b	X	is piping it through head -n 1 an option, or do you need a clean solution?
563e30c261a801306526728c	X	what does it do exactly? well a clean solution would be better, but im taking everything that works... thanks @Fiximan
563e30c261a801306526728d	X	head will just print the head of a file, i.e. the first few (standard is 10) lines, while -n NUMBER will let you select how many exactly. So you would not change what is being retrieved, but what you actually use from it.
563e30c261a801306526728e	X	I'm trying to list and to echo all the files having yesterday's date in their names, on a amazon s3 bucket. I know I can do: which is working just fine, but I need to do several stuff on these files, one bye one. So my code is: it also works fine but instead of retrieve me just the file name it also give me for every single element: the date it was uploaded and its weight: How can I just retrieve the file's name? ie: s3://my-bucket/20150720-1437436434_ip-10-0-1-36_android.log.gz
563e30c261a801306526728f	X	Rather than using s3cmd, these days it is recommend to use the official AWS Command-Line Interface (CLI). It has great features such as --query (to control output) and access to every AWS API call. Here's a sample that would fit your need: Breaking it down: See: list-objects CLI documentation As simpler version of listing objects is available (aws s3 ls BUCKET-NAME) but it has a fixed text output, like s3cmd.
563e30c261a8013065267290	X	I think that only works if the bucket has an ACL, does that still work if you assigned permissions through bucket policies?
563e30c261a8013065267291	X	I know you can try to read the ACLs or Bucket Policies through the Java SDK, but is there any easy way to just check if you have read and/or write permissions to a bucket and/or its contents? I don't see any "haveReadPermissions()" method or anything in the AmazonS3 class, but maybe I'm missing something? I find it hard to believe there's no easy way to check permissions.
563e30c261a8013065267292	X	I think the answer is that there's no fool-proof way to do this, at least not at this time. There are a couple other methods you can use to try to get around this. I originally tried to use the getBucketLocation() method to determine if my given user had read access to the bucket, but it turns out you must be the owner of the bucket to use this method... so that didn't work. For read access, there is another hack you can use. Just use something along the lines of getObject(bucketName, UUID.randomUUID().toString()) - this will throw an exception because you are trying to fetch a key that doesn't exist. Catch the AmazonServiceException (or AmazonS3Exception) and check that the e.getErrorCode() equals "NoSuchKey". If this is the case, then you have read access to the bucket. If it's any other error code, then you don't have access, or the bucket doesn't exist, etc (it could be any number of things). Make sure you explicitly check the ErrorCode not the StatusCode, because you will also get a 404 StatusCode if the bucket doesn't exist (which is the same StatusCode you get when the key doesn't exist). Here's the complete list of S3 error/status codes: http://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html For write access, it's not as simple. The best way is to actually write a small test file to the bucket (and then maybe try to delete it). Besides that, if you want to check for more generic permissions, using a method like PutObjectAcl will determine if your user has "s3:Put*" permissions (you can set the ACL to the exact same as the current ACL by reading it first and then using that to set it).
563e30c261a8013065267293	X	Try getBucketACL(String bucketName) which is a method of AmazonS3.
563e30c261a8013065267294	X	I do not think there is an easier way unless you created your own classes that extended the AWS API. Which might not be a bad idea if you are doing a larger program having to do with Amazon S3.
563e30c261a8013065267295	X	Are there any benchmarks comparing data transfer rates using s3cmd to awscli?
563e30c361a8013065267296	X	There are s3cmd official packages for Ubuntu: packages.ubuntu.com/trusty/s3cmd
563e30c361a8013065267297	X	Also check out s4cmd. While it doesn't have all the features of s3cmd, its performance is definitely better over high-bandwidth connections (e.g. on EC2), since it multithreads the connections. github.com/bloomreach/s4cmd
563e30c361a8013065267298	X	I am thinking about redeploying my static website to Amazon S3. I need to automate the deployment so I was looking for an API for such tasks. I'm a bit confused over the different options. Question: What is the difference between s3cmd, the Python library boto and AWS CLI?
563e30c361a8013065267299	X	s3cmd and AWS CLI are both command line tools. They're well suited if you want to script your deployment through shell scripting (e.g. bash). AWS CLI gives you only the basic commands, but they should be enough to deploy a static website to an S3 bucket. It also has some small advantages such as being pre-installed on Amazon Linux, if that was where you were working from. One AWS CLI command that may be appropriate to sync a local directory to an S3 bucket: Full documentation on this command: http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html s3cmd supports everything AWS CLI does, plus adds some more extended functionality on top, although I'm not sure you would require any of it for your purposes. You can see all its commands here: http://s3tools.org/usage Installation of s3cmd may be a bit more involved because it doesn't seem to be packages for it in any distros main repos. boto is a Python library, and in fact the official AWS Python SDK. The AWS CLI, also being written in Python, actually uses part of the boto library (botocore). It would be well suited only if you were writing your deployment scripts in Python. There are official SDKs for other popular languages (Java, PHP, etc.) should you prefer: http://aws.amazon.com/tools/ The rawest form of access to S3 is through AWS's REST API. Everything else is built upon it at some point. If you feel adventurous, here's the S3 REST API documentation: http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html
563e30c461a801306526729a	X	I studied Amazon web service API reference, I found many services (EC2,Cloudwatch) only support the programming SDK, java/python and so on. Only little services provide RESTful API, such as S3. I think Restful API is more easier to use than programming SDK. Why Amazon didn't provide the Restful API?
563e30c461a801306526729b	X	I agree that REST APIs are preferable to SDKs but, actually, all of the AWS services do expose an HTTPS interface, they're just not "RESTful." They call it the "Query API." http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/Using_Query_API.html http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-query-api.html
563e30c461a801306526729c	X	Well, in fact, all services of AWS in the SDK communicates with POST/GET. In the documentation of the AWS services, they provide the url of each action. The only thing you need to do is read the documentation. See this S3 documentation which you can see how things works: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html
563e30c461a801306526729d	X	Some may say it just comes down to opinion... but imo, there's no reason to prefer lower RESTful API over SDK, when your language of choice has one. The SDK's are hand-crafted by AWS to get the most out of their API's (and who would know how to do so better than them?), and give you abstractions that you can take advantage of. For anything besides a trivial project toying around with AWS, choosing to work with the lower-level API's means that you will end up re-implementing many things the SDK's give you out of the box, aka reinventing the wheel... The SDK's are there to get you productive and working, fast.
563e30c461a801306526729e	X	At the moment i am developing angular with nodejs app, and i ran into a problem where i need to do image upload, also resize image on client side before uploading to the server. I found plupload and when i tested it and it seem to be the right library for me to used. So because the example here https://github.com/moxiecode/plupload/wiki/Upload-to-Amazon-S3 is js and php. This will not be suitable for my app while running in Heroku. So i can't convert php into js because due to API keys will be expose to anyone on client side. I tried doing some CORS but it doesnt seem to work for me as everything now is getting really complicated just to do image upload on the client side. Can someone point me to the easiest method or direction... Thank you
563e30c461a801306526729f	X	Aggghhh... it says 403 forbidden and everything.. it made total sense as soon as you mentioned it.. I'm an idiot.. I changed 404 for 403 and the redirect worked! Thanks so much!
563e30c461a80130652672a0	X	Is it possible to change this S3 behavior & return a default object when the requested object is not found without any kind of error ?
563e30c461a80130652672a1	X	I am trying to get an S3 bucket when it encounters a 404 rather than throwing up a 404 page it redirects to my own server so I can then do something with the error. This is what I have cobbled together, what I think it should do is go to mydomain.com and hit the error.php and let the php script workout the filename the user was trying to access on S3. I would like this to happen no matter what directory the request comes from. When I have an error document defined in website hosting the 404 page shows up and when I don't have a 404 page defined I get an access denied xml error. This is my current redirection rule Can anyone give me a hint as to what I am missing please?
563e30c561a80130652672a2	X	Change the error code: S3 doesn't generate a 404 unless the requesting user is allowed to list the bucket. Instead, it generates a 403 ("Forbidden"), because you're not allowed to know whether the object exists or not. In this case, that's the anonymous user, and you probably don't want to allow anonymous users to list the entire contents of your bucket. If the object you request does not exist, the error Amazon S3 returns depends on whether you also have the s3:ListBucket permission. If you have the s3:ListBucket permission on the bucket, Amazon S3 will return an HTTP status code 404 ("no such key") error. if you don’t have the s3:ListBucket permission, Amazon S3 will return an HTTP status code 403 ("access denied") error. — http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html See also: Amazon S3 Redirect rule - GET data is missing
563e30c561a80130652672a3	X	COPY is a SQL command sent over JDBS/ODBS connection.
563e30c561a80130652672a4	X	If you'are using python, you'll need a postgreSQL binder like psycopg2 to connect to redshift and then run a "copy" command.
563e30c561a80130652672a5	X	Is it possible to load redshift using bulk copy command using python boto package. I do not see a way to do this. It seems a JDBC client is necessary.
563e30c561a80130652672a6	X	No, boto is not used to load data into Amazon Redshift. Data can be loaded into Amazon Redshift from: The python boto package provides access to AWS APIs. These can be used with Redshift to create, snapshot, reboot, describe and resize clusters (plus other commands). However, the process to load data into Redshift (eg via Amazon S3) is done with the COPY command that is called via the normal ODBC or JDBC connection -- just like calling a SELECT command: You could, however, use a standard Python JDBC library to connect to Redshift and execute the COPY command to bulk load data. See also:
563e30c561a80130652672a7	X	sounds interesting, I would like to learn more from people with S3 experience too.
563e30c561a80130652672a8	X	Hi Tuviah, it's been two year since you asked this question, what solution you've used eventually? I'm looking for something similar.
563e30c661a80130652672a9	X	I work for a company which is developing a media asset management website. We've decided to modify our web app to host our media on the cloud. We will license this web application to our customers and allow them to choose whatever cloud service they want. Currently we are using Amazon S3 and I manually upload the files using cloudberry. But now we need to write an application which will mirror whatever is in our local media directory to Amazon. It's all ASP.net and I've researched ThreeSharp and other simple C# cloud APIs for S3. But now I see that Zend and Microsoft have partnered for this simplecloud initiative which apparently only works for PHP. Question: Does anyone know of a simple cloud api (simplecloud.org) which works with all popular cloud services for .NET? I would really hate to have to write the same code to move and delete files three different times for Azure, S3 and RackSpace or have to reinvent the wheel. best, Tuviah
563e30c661a80130652672aa	X	There's some of those on CodePlex:
563e30c661a80130652672ab	X	Have you tried it?
563e30c661a80130652672ac	X	I have a number of large PDFs stored in Amazon S3. Users of my web application need to be able to view these documents within the browser, and I am considering using pdf.js for this purpose. I am aware that Amazon S3 provides a REST API for getting Multiple Parts of an S3 object, allowing a specified range of bytes to be downloaded using the HTTP Range header I believe that pdf.js provides support for making requests with a Range Header to fetch only the bytes for a specific page. Is it therefore possible to download the first few pages of a pdf from S3, render these pages and then request the rest of the pdf from S3 in the background?
563e30c661a80130652672ad	X	As noted in Julien's answer above, WS does not support multipart/form-data POSTing. multipart/form-data is implied by curl -F option in the question. Though if you are free to send the file contents as the request body you can just do WS.url(...).post(fileContentsAsByteArray). Also fit in a .withHeaders(("Content-Type",...)) before sending, so the server knows how to handle it.
563e30c661a80130652672ae	X	In what way can I simulate file upload in play framework? In other words I can upload file to server with such curl command: curl -k -v -H "X-Agile-Authorization: token" -F uploadFile=c:\1.txt -F directory=/testpost -F basename=1.txt https:// api /post/file how can I do the same without curl or browser in play framework. The aim is to upload file from one server to another.
563e30c661a80130652672af	X	Currently, it's not possible to post a multipart/form-data through the WS API. You can easily send a File WS.url(myUrl).post(myFile), but not a form... A workaround should be to use another library, like Apache Http Client. Check this topic on the Play mailing-list: [2.0] multipart/form-data in WS POST
563e30c661a80130652672b0	X	Post on google-groups: With org.apache.http.entity.mime.MultipartEntity, you can manipulate multipart data easily. And then just write it to byte array buffer as you did. Ex: Upload photo to my facebook wall: Dependency: "org.apache.httpcomponents" % "httpclient" % "4.3.1" and "org.apache.httpcomponents" % "httpmime" % "4.3.1". If performance were a concern, you could do the same with WS's underlying client WS.client.
563e30c661a80130652672b1	X	Take a look on the Windows Azure Storage or Amazon S3 REST APIs, then using similar approach you can send files with Play's WebServices API. You just need to construct POST or PUT request and send it. Probably you'll need to care about authentication and/or authorization between both apps yourself (both APIs Windows and Amazon uses HMAC for this task)
563e30c661a80130652672b2	X	The following example allows you to post multipart/form-data. It is a simple version that only works with String values, but it could easily be modified to use other types of data. Usage is like this:
563e30c761a80130652672b3	X	I need to fetch images from different URLs and store it in Google Datastore after compressing it. I understand tinyPNG is the a great API to do the compression but the API only supports Amazon S3. Can someone guide how to accomplish this in Google App Engine.
563e30c761a80130652672b4	X	I have created a bucket in Amazon S3 and have uploaded 2 files in it and made them public. I have the links through which I can access them from anywhere on the Internet. I now want to put some restriction on who can download the files. Can someone please help me with that. I did try the documentation, but got confused. I want that at the time of download using the public link it should ask for some credentials or something to authenticate the user at that time. Is this possible?
563e30c761a80130652672b5	X	By default, all objects in Amazon S3 are private. You can then add permissions so that people can access your objects. This can be done via: As long as at least one of these methods is granting access, your users will be able to access the objects from Amazon S3. 1. Access Control List on individual objects The Make Public option in the Amazon S3 management console will grant Open/Download permissions to all Internet users. This can be used to grant public access to specific objects. 2. Bucket Policy A Bucket Policy can be used to grant access to a whole bucket or a portion of a bucket. It can also be used to specify limits to access. For example, a policy could make a specific directory within a bucket public to users from a specific range of IP addresses, during particular times of the day, and only when accessing the bucket via SSL. A bucket policy is a good way to grant public access to many objects (eg a particular directory) within having to specify permissions on each individual object. It is commonly used for static websites served out of an S3 bucket. 3. IAM Users and Groups This is similar to defining a Bucket Policy, but permissions are assigned to specific Users or Groups of users. Thus, only those users have permission to access the objects. Users must authenticate themselves when accessing the objects, so this is most commonly used when accessing objects via the AWS API, such as using the aws s3 commands from the AWS Command-Line Interface (CLI). Rather than being prompted to authenticate, users must provide the authentication when making the API call. A simple way of doing this is to store user credentials in a local configuration file, which the CLI will automatically use when calling the S3 API. 4. Pre-Signed URL A Pre-Signed URL can be used to grant access to S3 objects as a way of "overriding" access controls. A normally private object can be accessed via a URL by appending an expiry time and signature. This is a great way to serve private content without requiring a web server. Typically, a Pre-Signed URL is constructed by an application when it wishes to grant access to an object. For example, let's say you have a photo-sharing website and a user has authenticated to your website. You now wish to display their pictures in a web page. The pictures are normally private, but your application can generate Pre-Signed URLs that grant them temporary access to the pictures. The Pre-Signed URL will expire after a particular date/time.
563e30c761a80130652672b6	X	should use the new aws cli like @number5 's answer below docs.aws.amazon.com/cli/latest/reference/s3/rm.html
563e30c761a80130652672b7	X	this should be the answer. It's a (new-ish) standard, powerful tool, designed for things just like this question
563e30c761a80130652672b8	X	This does not work for buckets under versioning.
563e30c761a80130652672b9	X	This is deleting the files just fine but its also deleting the bucket after deleting the files. Did I miss anything?
563e30c761a80130652672ba	X	@Naveen as I said above, rm will only delete files but rb --force will delete the files and the bucket.
563e30c761a80130652672bb	X	@number5 I used the following -> aws s3 rm --recursive s3://your_bucket_name and this deletes the bucket as well. I even tried aws s3 rm --recursive s3://your_bucket_name --exclude "" --include ".gz" and this did not help either.
563e30c761a80130652672bc	X	Great idea for using Lifecycle instead of some delete command.
563e30c761a80130652672bd	X	Exactly, let S3 do it for you.
563e30c761a80130652672be	X	You can also apply this to the entire bucket, enabling you to delete the bucket.
563e30c761a80130652672bf	X	Thanks for posting this answer, I was trying to do this exact thing and had put -Key "%_.Key" which doesn't work.
563e30c761a80130652672c0	X	According to the help it is either single-object delete s3cmd del s3://BUCKET/OBJECT or whole bucket delete s3cmd rb s3://BUCKET. There is no s3cmd rm, at least according to s3cmd --help.
563e30c861a80130652672c1	X	I have the following folder structure in S3. Is there a way to recursively remove all files under a certain folder (say foo/bar1 or foo or foo/bar2/1 ..)
563e30c861a80130652672c2	X	This used to require a dedicated API call per key (file), but has been greatly simplified due to the introduction of Amazon S3 - Multi-Object Delete in December 2011: Amazon S3's new Multi-Object Delete gives you the ability to delete up to 1000 objects from an S3 bucket with a single request. See my answer to the related question delete from S3 using api php using wildcard for more on this and respective examples in PHP (the AWS SDK for PHP supports this since version 1.4.8). Most AWS client libraries have meanwhile introduced dedicated support for this functionality one way or another, e.g.: You can achieve this with the excellent boto Python interface to AWS roughly as follows (untested, from the top of my head): This is available since version 1.24 of the AWS SDK for Ruby and the release notes provide an example as well: Or:
563e30c861a80130652672c3	X	With the latest aws-cli python command line tools, to recursively delete all the files in a bucket is just: If what you want is to actually delete the bucket, there is one-step shortcut: which will remove the contents in that bucket recurisvely then delete the bucket.
563e30c861a80130652672c4	X	You might also consider using Amazon S3 Lifecycle to create an expiration for files with the prefix foo/bar1. Open the S3 browser console and click a bucket. Then click Properties and then LifeCycle. Create an expiration rule for all files with the prefix foo/bar1 and set the date to 1 day since file was created. Save and all matching files will be gone within 24 hours. Just don't forget to remove the rule after you're done! No API calls, no third party libraries, apps or scripts. I just deleted several million files this way. A screenshot showing the Lifecycle Rule window (note in this shot the Prefix has been left blank, affecting all keys in the bucket): 
563e30c861a80130652672c5	X	I just removed all files from my bucket by using PowerShell:
563e30c861a80130652672c6	X	With s3cmd package installed on a Linux machine, you can do this s3cmd rm s3://foo/bar --recursive
563e30c861a80130652672c7	X	Best way is to use lifecycle rule to delete whole bucket contents. Programmatically you can use following code (PHP) to PUT lifecycle rule. In above case all the objects will be deleted starting Date - "Today GMT midnight". You can also specify Days as follows. But with Days it will wait for at least 24 hrs (1 day is minimum) to start deleting the bucket contents.
563e30c861a80130652672c8	X	Just seen that Amazon have added Empty Bucket to there AWS console menu. http://docs.aws.amazon.com/AmazonS3/latest/UG/DeletingaBucket.html
563e30c861a80130652672c9	X	I run a site where the content is user generated and storage space is mainly filled via images. Eventually my VPS, though it says it has unlimited space, will get too large and I will be limited. So then I thought to upload the user generated images on a third party storage server, but how will I program how my website will load images from my server, and the new ones from another server, like for example Amazon S3? And suppose I want to do a backup of all the images..... how long does it take to backup 10gb? oh and, what about uploading all the old images onto the third party server and load them from there? how can that be coded? I mean transfer all old images on my server, to S3, then load images from s3
563e30c861a80130652672ca	X	Amazon S3 is certainly a good option for storing images because: An easy way to move existing data into Amazon S3 is to use the Amazon Command Line Interface (CLI). This is free software that can call the AWS API for practically any service, including S3. As per the CLI documentation for S3, you can use commands like these: To send individual images from your website to Amazon S3, you can use an SDK to add functionality to your website code. There are SDKs available for many languages including Java, .Net, PHP, Node.js, Ruby and Python. Once the images are on Amazon S3, you can refer to them in your image tags. This means you could serve HTML from your existing web app, but include pictures from S3. You can also add security so that the pictures are private, but use a Pre-Signed URL to display the pictures with a special URL for a limited time period. Read the Amazon S3 Developer Guide for information about the capabilities of S3 and how to store and retrieve data. The speed of data transfer (for backup or retrieval) would be limited by your own Internet bandwidth. Use an online data transfer calculator to estimate the time it would take to copy your data.
563e30c861a80130652672cb	X	thanks for your advice
563e30c861a80130652672cc	X	I wanted to ask if there is a way to backed up an Amazon S3 Bucket on Standard Storage to an Amazon S3 RRS Redundant Storage Bucket in a different account directly without using a Server/Custom Script in between. I know the storage has high redundancy but a second maintained copy would be an advantage and not have to use a server nightly etc would also be an advantage. thx
563e30c861a80130652672cd	X	If you want to have your files under a different user account, I think the only way is to write a custom script that uses the S3 API (for example in Python with boto). However if you are looking for long-term durable storage, I would suggest to have a look at Amazon Glacier
563e30c961a80130652672ce	X	Yes, Glacier is good option for chipper backup. But for that you have to use such tool which provide you enough description about the uploaded archive file because Glacier does not upload file as file name, it keep file in different alphanumeric name generated by it. 2. It will not replace your changed file by new one, it will upload new with old one. So If you want to keep your data for long time and do not want to retrieve repeatedly then use it. Amazon S3 STANDARD STORAGE KEEP YOUR DATA SAFE BUT EVEN YOU WANT TO HAVE BACKUP THEN YOU CAN USE ANY TOOL WHICH SUPPORTING COPY BETWEEN TWO BUCKET. AND THEN YOU CAN APPLY RRS STORAGE TYPE ON ALL THE OBJECTS OF THE BUCKET. I am one of the developer of Bucket Explorer S3 Tool. You can use Bucket Explorer for copying your data between two difference bucket either in same or different aws account. and also provide batch update metadata option to update RRS setting on the all existing files. Thanks
563e30c961a80130652672cf	X	The limit on static file sizes is 10MB, and has been for quite some time.
563e30c961a80130652672d0	X	Thanks, I've updated the max file size (up from 1MB to 10MB since I wrote this post), and the free quota for bandwidth (down from 10GB to 1GB). For future readers - check Google's pages to confirm these numbers, they will change from time to time.
563e30c961a80130652672d1	X	More more work with amazon? I would think the opposite. If you use Amazon EC2, you can pick what ever platform that suits you! GAE is the one that forces you to do things exactly the way they want you to. I use both. Amazon for the more important stuff (where I need the freedom to do what I want) and GAE for the less critical apps (cause its basically free!)
563e30c961a80130652672d2	X	But freedom means more work. GAE presents it to you on a plate while Amazon requires you to set it up.
563e30c961a80130652672d3	X	What do you see as the advantages and disadvantages of Amazon Web Services S3 compared with Google Application Engine? The cost per gigabyte for the two is, at the time I ask, roughly similar; I have not seen any widespread complaints about the quality of service; so I think the decision of which one to use may depend on the API (of all things). Google's API breaks your content into what they call static content, such as your CSS files, favicons, images, etc and non-static dynamically-generated HTTP responses. Requests for static stuff will be served to whoever requests it until your bandwidth limit is reached; non-static requests will be fulfilled until your bandwidth or CPU limit is reached. With respect to your non-static requests, you can provide any logic you are able to express in Python, so you can be choosy about who you serve. Amazon's API treats all your content as blobs in a bucket, and provides an access protocol that lets you distinguish between a variety of fulfillable requests ranging from world-readable to owner-only. If you want to something that's not in the kit, though, I don't know what you do beyond being thoughtful about distributing your URIs. What differences do you see between the two? Are there other cloud storage services you like? Zetta had a press release today, but they're looking for a minimum of ten terabytes on the beta application, and none of my clients are there (yet); and Joyent will probably do something in the near future.
563e30c961a80130652672d4	X	GAE has a limit of 10MB each on static files uploaded through appcfg.py (look right at the bottom of http://code.google.com/appengine/docs/python/tools/uploadinganapp.html). Obviously you can write code to slice large files into bits and reassemble at download time, but it suggests to me that Google doesn't expect App Engine to be used just as a simple CDN, and that if you want to use it as one you'll have to do some work. S3 does the job out of the box, all you have to do is grab a third-party interface app. If you want to do something non-standard with file access on S3, then probably Amazon expects you to spring for a server instance on EC2. Once this is done, you have much more flexibility than GAE's environment, but you pay more (in cash and probably in maintenance). The plus point for GAE is that it has "cheap" on its side for small apps (up to 1GB storage, 1GB bandwidth and 1.3 million hits a day are free: http://code.google.com/appengine/docs/quotas.html). Depending on your use, this might be significant, or it might be irrelevant on the scale of your total bandwidth costs. Coincidentally, I have just this last couple of days looked at GAE for the first time. I took an old Perl CGI script and turned it into a GAE app, which is up and running. About 10 hours total, including reading the GAE introductory docs and remembering how Python is supposed to work enough to write a couple of hundred lines. I'd speculate that's more effort than loading a bunch of files onto S3, but less effort than maintaining EC2 server(s). However, I haven't used Amazon. [Edited to add: this sounds like the advantages are all with Amazon for commercial purposes. This may well be true, but then GAE is not yet mature and presumably will get better from here fairly rapidly. They only let people start paying in December or so, before that it was free-quota-only except by special arrangement with Google. While Google sometimes takes flack for its claims of "perpetual beta", I think GAE genuinely is still starting up. If your app is a good fit for the BigTable data paradigm, then it might scale better on GAE than EC2. For storage I assume that S3 is already good enough for all reasonable purposes, and Google's clever architecture gives GAE no advantages to compensate when all you're doing is serving files.] * Except that Google has just offered me a preview of GAE's Java support. ** Just noticed that you can set up chron jobs, but they're limited by the same rules as any other request (30 second runtime, can't modify files, etc).
563e30c961a80130652672d5	X	The way I see it is the Google App Engine basically provides a sandbox for you to deploy your app as long as it is written with their requirements (Python etc). Amazon gives you a virtual machine with a lot more flexibility in what can be done but probably more work on your side needed. MS new Azure seems to be going down the GAE route, but replace Python with .NET.
563e30c961a80130652672d6	X	Hi, thanks for the reply , i tried this and i am getting error like ERROR: Invalid command: u'modify'
563e30c961a80130652672d7	X	Files giving access denied after this change...
563e30ca61a80130652672d8	X	@Rajaraman u know anything about it?? after this step its showing access denied
563e30ca61a80130652672d9	X	This works.. tested with s3cmd version 1.5.0-rc1 BUT!: this changes the object's Content-type header to "binary/octet-stream"! Tested with one PNG -file.
563e30ca61a80130652672da	X	I experienced the same problem as @Hardy. It also removed public readability from all my files. Watch out.
563e30ca61a80130652672db	X	This works if you are uploading to AWS from a local path on your computer and want everything to keep proper cache-control
563e30ca61a80130652672dc	X	That works to set cache headers at time of initial sync but does not update existing files. You can force an update like so find . -type f -exec touch '{}' \;; aws s3 sync /path s3://yourbucket/ --recursive --cache-control max-age=604800
563e30cb61a80130652672dd	X	I have moved 20000 files to AWS S3 by s3cmd command. Now i want to add cache-control for all images (.jpg) These files are located in ( s3://bucket-name/images/ ). How can i add cache-control for all images by s3cmd or is there any other way to add header ? Thanks
563e30cb61a80130652672de	X	Please try the current upstream master branch (https://github.com/s3tools/s3cmd), as it now has a modify command, used as follows:
563e30cb61a80130652672df	X	Also with the AWS's own client:
563e30cb61a80130652672e0	X	Every Metadata setting contains a Key-Value pair. Cache control metadata key is “Cache-Control” and Value is “max-age=<time for which you want your object to be accessed from cache in seconds>” You can set Cache Control Custom Header for Amazon S3 Objects by sending HTTP PUT Request to Amazon S3 Server with appropriate headers in two ways: Set Cache Control Metadata using Amazon S3 REST API PUT Object Request - If you are a programmer, you can write your own software program to use Amazon S3 REST or SOAP APIs to set Custom Headers with PUT Object Request. This website only refers to Amazon S3 REST APIs, please refer to AWS documentation website for details on how to use SOAP APIs. Set Cache Control Metadata using Bucket Explorer User Interface - If you like to set custom HTTP Headers like Cache Control using mouse clicks instead of writing a software program, you can use Bucket Explorer's user interface for that. With this Custom HTTP Header, you can specify the caching behavior that must be followed with the request/response chain and to prevent caches from interfering with the request or response. for more information please check How to Set Cache Control Header for Amazon S3 Object?`
563e30cb61a80130652672e1	X	Just upgrade the s3cmd to version 1.5.1 and the issue will resolve.
563e30cb61a80130652672e2	X	(Since the OP asked for any other way) You can also do it via aws-cli, e.g. (v: aws-cli/1.8.8 Python/2.7.2 Darwin/12.5.0): Although please note that you will rewrite any existing object.
563e30cb61a80130652672e3	X	Well, I created an API to manage for our websites some attachments uploads and store into Amazon S3 buckets The scenario : Once visitor / user in the form and wants to submit it with attachment, once the file is selected then button clicked an Ajax request fire to the micro service API so it can store the file into S3 do some processing then return the direct link or identifier. The question is : how can we authenticate the user using for example a short live token or something like that without being hijacked, mis-usage of the token.. In Javascript everything is visible to the visitor, and we try to not integrate any heavy process in the backend
563e30cc61a80130652672e4	X	If I got your question straight, you have a web interface in which files are uploaded to an S3 bucket and you need to make sure that in a certain back end API (such as REST) all file upload commands will have authentication and authorization. The answer is highly dependent on your architecture but generally speaking, all Javascript calls are nothing but HTTP calls. So you need HTTP authentication/authorization. In general, the most straightforward method for REST over HTTP is the basic authentication, in which the client sends a credential in every single request. This may sound odd at first but it is quite standard since HTTP is supposed to be stateless. So the short answer, at least for the scenario I just described, would be to ask the user to provide the credentials that Javascript will keep in the client side, then send basic authentication which the REST interface can understand. The server-side processes will then get such information and decide whether a certain file can be written in a certain S3 bucket.
563e30cc61a80130652672e5	X	I am working on Amazon Web Service (EC2, S3) to set up an instances given the following detail on the account. (I don't have administrative rights to the Amazon account through the web browser) Do anyone know how can I check the total usage cost spent through command line interface? I wouldn't want to give the owner a surprise of how much Amazon have charged him at the end of the month. P.S.: To date of writing Amazon does not provide account charges information through the command line or API (According to @Eric Hammond). If Amazon does in the later date, please give me a head up. Thanks!
563e30cc61a80130652672e6	X	Amazon AWS does not currently provide account charges information through the command line or API. Accrued account charges can be viewed through a login on the AWS web site: http://aws-portal.amazon.com/gp/aws/developer/account/index.html?ie=UTF8&action=activity-summary
563e30cc61a80130652672e7	X	Re: your shoestring budget: turn on detailed billing and set up cloudwatch monitoring for your billing -- AWS obviously is in business to make money, but they don't want or need to do it by sticking people with unexpected charges, so they have made it pretty easy to avoid surprises. You can view graphs of your approximate charges-to-date and even set alarms if your accrued charges exceed thresholds you define. Data transfer has its own metrics.
563e30cc61a80130652672e8	X	Actually, the US Standard region is its own region: us-east-1. (source: github.com/aws/aws-sdk-js/issues/276#issuecomment-43145369)
563e30cc61a80130652672e9	X	That's correct, but what I was trying to say is that us-east-1 (S3 "standard") uses data center facilities in both Virginia and Oregon. It's a really weird duck in the AWS service family & the only service I know of that lets me spin up resources in two regions. I can even specify just the Virginia region with s3-external-1.amazonaws.com (Northern Virginia only). docs.aws.amazon.com/general/latest/gr/rande.html (browse down to S3)
563e30cd61a80130652672ea	X	I am planning on running a script located on an EC2 instance in us-east-1d. The script basically pulls in images from a few different places and will throw them into an S3 bucket in the US-Standard Region. Since there is no way to upload directly into an s3 bucket (by sending an API request that causes S3 to fetch a file from a remote URL as written about here and I don't think this has changed) I would like to make sure that the each image I save as temp file on my ec2 will not result in additional bandwidth charges when written to S3 (ie. leaves the Amazon data center). Will a us-east-1d EC2 instance uploading to a US-Standard S3 bucket will be communicating within the same AWS region? Any insight into this would be greatly appreciated as it will be terabytes of data and I'm on a shoestring bucket I'd like to know before proceeding.
563e30cd61a80130652672eb	X	"US Standard" means "us-east-1". According to S3 Pricing FAQ There is no Data Transfer charge for data transferred between Amazon EC2 and Amazon S3 within the same Region or for data transferred between the Amazon EC2 Northern Virginia Region and the Amazon S3 US Standard Region. This will mean that if your instance is in any of the us-east-1 AZs and your bucket is in the US Standard region, any movement of data between the 2 should cost nothing. Also, depending on your use case, you may want to look at the new AWS SDK for JavaScript in the Browser as it may offer the direct to S3 uploads you're looking for.
563e30cd61a80130652672ec	X	US standard includes us-east and part of us-west (oregon). It's a legacy construct that only applies for S3
563e30cd61a80130652672ed	X	My suggestion might be a bit off topic but you also have the option of base64 encoded images. Though not very efficient but they have served me well. davidbcalhoun.com/2011/…
563e30cd61a80130652672ee	X	which servlet container you are using?
563e30cd61a80130652672ef	X	@AnupamSaini im very new to server programming so simply using restkit seemed to be the easiest solution.
563e30cd61a80130652672f0	X	@RameshPVK apache tomcat i believe
563e30cd61a80130652672f1	X	@RameshPVK the instance image on my ec2 says: AMI: ElasticBeanstalk-Tomcat6-64bit-201202071737 (ami-d5ec3cbc)
563e30cd61a80130652672f2	X	Thanks for your reply, the apache commons fileupload says the specifies the following: if an HTTP request is submitted using the POST method, and with a content type of "multipart/form-data", then FileUpload can parse that request, and make the results available in a manner easily used by the caller. I my request compliant with this?
563e30cd61a80130652672f3	X	@LuisOscar: Try and you shall find the answer.
563e30ce61a80130652672f4	X	@LuisOscar: Your request is indeed a POST. From what I gather from RestKit's RKParams reference documentation, it should be a multi-part message.
563e30ce61a80130652672f5	X	thanks, im trying to install the commons fileupload to java so i can test it ill get back to you in a bit.
563e30ce61a80130652672f6	X	Ok i managed to import the commons but im getting the following warnings: List is a raw type. References to generic type List<E> should be parameterized. Iterator is a raw type. References to generic type Iterator<E> should be parameterized.
563e30ce61a80130652672f7	X	It's the way that works without any additional library +1.
563e30ce61a80130652672f8	X	It seems i cant use this approach because im running tomcat 6 and according to this site: tomcat.apache.org/whichversion.html I only have 2.x serlvets
563e30ce61a80130652672f9	X	then you should use commons apache file upload
563e30ce61a80130652672fa	X	Ok thanks for your help
563e30ce61a80130652672fb	X	@Lion: it's also the way that works only on recent containers :) Not everybody is that lucky, whereas the lib-way is relatively easy and backward- and forward-compatible. If you have a Servlet 3+ capable container, then I'd definitely go for that then.
563e30ce61a80130652672fc	X	I am trying to send a picture to my java servlet (hosted on amazon ec2) to later transfer it to amazon s3 and wonder how to retrieve the Image from the post request. The request is sent through iOS RestKit API like this (pic.imageData is a NSData type): This is how I parse the other 2 parameters on the Java servlet: The question is: how do I retrieve and parse the image on my server?
563e30ce61a80130652672fd	X	Something along the lines of this, using Apache Commons FileUpload: Refer to the FileItem API reference doc to determine what to do next.
563e30ce61a80130652672fe	X	Servlet 3.0 has support for reading multipart data. MutlipartConfig support in Servlet 3.0 If a servelt is annotated using @MutlipartConfig annotation, the container is responsible for making the Multipart parts available through References:
563e30cf61a80130652672ff	X	So you want someone to read the referenced link and process the flow of the code and then answer your very general question? Down vote!
563e30cf61a8013065267300	X	I apologize - I thought it would help eliminate the game of fifty questions since my code is almost verbatim... I'll update my question to include the code in my controller
563e30cf61a8013065267301	X	thank you for the feedback - this is what I discovered yesterday but unfortunately won't work for my scenario. My user create screen sends a nested JSON response to the API and I was hoping I could somehow add the file attachment to it. It looks like as of right now I'll just have to create the user, return the id and then add attachments with the correspondind user id. It's not the end of the world, I just thought it would be nice to handle it all in one call
563e30cf61a8013065267302	X	I followed this guide http://jaliyaudagedara.blogspot.com/2014/08/how-to-directly-upload-files-to-amazon.html and I was able to successfully upload files to Amazon S3 in my Web API Project. Happy with the results I decided to add the remaining logic which was JSON from the client. The screen is a create user screen that also has a file uploader. I'd like to send all the user data and uploaded file at once instead of using multiple calls. Is this possible? If so how should I should tweak the referenced example so it can accept both JSON and the uploaded file?
563e30cf61a8013065267303	X	Yes, it is possible to capture form data on the POST as well as the file, though not as JSON, like other API calls. Take a look at the form post in Fiddler. You should see something like this on the text view tab: In this example, my form has a "RecordTypeId", "RecordId" and "Tags" field on the form. So you can read that data from the "provider.FormData", just like you did with the username. So, each piece of the data that would have been in your JSON object, would show up in the form data.
563e30d061a8013065267304	X	Why are you doing this? Why not upload the file directly to S3?
563e30d061a8013065267305	X	@Nick: one reason to do this is to have GAE resize the photo.
563e30d061a8013065267306	X	@Nick: I´m using S3 and GoogleStorage for permanent storage, the users are uploading to the blobstore first and deciding later where there data has to be saved.
563e30d061a8013065267307	X	@Michael This seems unnecessarily kludgy - and it'll also limit you to 1MB, since that's all you can send with URLFetch. It's possible to do form-based uploads to S3; this would be a far neater approach.
563e30d061a8013065267308	X	@Nick: This App should be a kind of storage management solution for different private and public cloud services (Amazon S3, Google Storage, Walrus from Eucalyptus, etc.) The allocation to different services should be done by the AppEngine to save users time and bandwith. Using form-based uploads will triple users bandwith needs. I will try Calvins approach using the BlobReader class, perhaps I can realize some "dirty" workaraound.
563e30d061a8013065267309	X	Be careful not to timeout uploading really large files from GAE to S3.
563e30d061a801306526730a	X	@Amir: How this should work? Calling blob_reader = blobstore.BlobReader(blob_key, buffer_size=1048576) in a loop and appending to a S3 upload stream? Boto doesn´t support that, if you take a look at botos upload methods like the set_contents_from_string method - there is no possibility to send small chunks (like 1MB) one after another. So the BlobReader class will not help in that case, or did I overlook something?
563e30d061a801306526730b	X	I've never used boto, but you should be able to read the entire blob into a buffer object and encode and send that (perhaps using set_contents_from_file?). You can pretend a buffer is a file using StringIO.
563e30d061a801306526730c	X	I'm uploading data to the blobstore. There should it stay only temporary and be uploaded from within my AppEngine app to Amazon S3. As it seems I can only get the data through a BlobDonwloadHandler as described at the Blobstore API: http://code.google.com/intl/de-DE/appengine/docs/python/blobstore/overview.html#Serving_a_Blob So I tried to fetch that blob specific download URL from within my application (remember my question yesterday). Fetching internal URLs from within the AppEngine (not the development server) is working, even it´s bad style - I know. But getting the blob is not working. My code looks like: And I'm getting DownloadError: ApplicationError: 2 Raised by: File "/base/python_runtime/python_lib/versions/1/google/appengine/api/urlfetch.py", line 332, in _get_fetch_result raise DownloadError(str(err)) Even after searching I really don't know what to do. All tutorials I've seen are focusing only on serving through a URL back to the user. But I want to retrieve the blob from the Blobstore and send it to a S3 bucket. Anyone any idea how I could realize that or is that even not possible? Thanks in advance.
563e30d061a801306526730d	X	You can use a BlobReader to access that Blob and send that to S3. http://code.google.com/intl/de-DE/appengine/docs/python/blobstore/blobreaderclass.html
563e30d161a801306526730e	X	+1 totally agree on both points.
563e30d161a801306526730f	X	Thanks for the link. I am aware of the second issue, but my client is worried about somebody guessing the URL(don't ask me how..).
563e30d161a8013065267310	X	On the bright side, they've latched onto the side of things you can actually control a bit. :-)
563e30d161a8013065267311	X	+1. Thanks for the answer.
563e30d161a8013065267312	X	I have several PDF files stored in Amazon S3. Each file is associated with a user and only the file owner can access the file. I have enforced this in my download page. But the actual PDF link points to Amazon S3 url, which is accessible to anybody. How do I enforce the access-control rules for this url?(without making my server a proxy for all the PDF download links)
563e30d161a8013065267313	X	I would suggest using Amazon S3's authenticated REST URLs with an expiration date. They allow temporary, expiring access to a non-public S3 object. That said, if they're going to share the URL, what's stopping them from sharing the file itself?
563e30d161a8013065267314	X	Each file on your S3 account can have special access rights (ACL). You should set all your PDFs ACL to private. Then nobody will be able to access them. S3 has an API which allows you to "temporarily" grant read-access. For example Amazon's S3 PHP Library has a getAuthenticatedURL (string $bucket, string $uri, integer $lifetime, [boolean $hostBucket = false], [boolean $https = false]) function. This will enable you to allow access to a PDF for a defined amount of time (like 5 minutes) - which is more than enough if you immediatly redirect the user to S3.
563e30d261a8013065267315	X	I use Node.js to host a web server. The server will communicate with different third party storage providers, such as Box, dropbox, googledrive, amazon s3, skydrive, ftp server, etc. The server will download files from or upload files to the storage providers. I'm using the OAuth module to implement the authorization process and get the auth_token and auth_secret. OAuth module unifies the authorization interfaces for different servers. OAuth module encapsulates the APIs of the oauth servers and provides a uniform and friendly interfaces. With OAuth module, I don't need to call the API of the oauth servers in my code. It is pretty good and saves lot of my work. I encounter some difficulties when download/upload files from/to the different storage providers. I must write the different implementations for each storage server. Such as calling box APi to upload file to box, calling s3 API to upload file to s3, etc. I wonder if there is some node module which encapsulates all the APIs for the storage providers, which works like the OAuth module for the authorization APIs. The pseudo code below expresses how I expect this kind of module works. I suppose it's name is 'storage'. Please let me know which node module can meet my requirement. Thanks, Jeffrey
563e30d261a8013065267316	X	You should check out Temboo - it does exactly what you're looking. Temboo normalizes APIs so that they all look and and feel the same. Once you know how to talk to a single storage API via Temboo you know to talk them to all. Temboo has a Node.js SDK, and you can find out more about the storage APIs that Temboo supports here. (Full disclosure: I work at Temboo)
563e30d261a8013065267317	X	I've created a s3 server which contain a large number of images. I'm now trying to create a bucket policy, which fits my needs. First of all i want everybody to have read permission, so they can see the images. However i also want to give a specific website the permission to upload and delete images. this website is not stored on a amazon server? how can i achieve this? so far i've created an bucket policy which enables everybody to see the images
563e30d261a8013065267318	X	You can delegate access to your bucket. To do this, the other server will need AWS credentials. If the other server were an EC2 instance that you owned then you could do this easily by launching it with an IAM role. If the other server were an EC2 instance that someone else owned, then you could delegate access to them by allowing them to assume an appropriate IAM role in your account. But for a non-EC2 server, as seems to be the case here, you will have to provide AWS credentials in some other fashion. One way to do this is by adding an IAM user with a policy allowing s3:PutObject and s3:DeleteObject on resource "arn:aws:s3:::examplebucket/*", and then give the other server those credentials. A better way would be to create an IAM role that has the same policy and then have the other server assume that role. The upside is that the credentials must be rotated periodically so if they are leaked then the window of exposure is smaller. To assume a role, however, the other server will still need to authenticate so will need some base IAM user credentials (unless you have some way to get credentials via identity federation). You could add a base IAM user who has permissions to assume the aforementioned role (but has no other permissions) and supply the base IAM user credentials to the other server. When using AssumeRole in this fashion you should require an external ID. You may also be able to restrict the entity assuming this role to the specific IP address(es) of the other server using a policy condition (not 100% sure if this is possible).
563e30d261a8013065267319	X	The Bucket Policy will work nicely to give everybody read-only access. To give specific permissions to an application: See also: Amazon S3 Developer Guide
563e30d261a801306526731a	X	Looks like an Answer! I should have read back to May on your Blog (Designing RavenFS (ayende.com/blog/4828/designing-ravenfs ) and RavenFS & Rsync (ayende.com/blog/4829/ravenfs-rsync )). I've posted a bit of background here (solutionevangelist.com/post/13313 ). The only issue is I'd like to use it for a few personal non-commercial projects too which might be a bit price sensitive. However, this may be solvable via a developer license, and I'm sure we'd be able to find paying clients/projects for this. Can you tell me more?
563e30d261a801306526731b	X	I am looking for an API that performs functionality roughly analogous to Rackspace Cloud Files / OpenStack Swift, Microsoft Azure Blob Storage, or Amazon S3 that can be run on a Windows Server. I am not speaking of all the add-ons including replication, etc, but an API that enables a similar RESTful API for the storage/serving (including Anonymous). Some examples of functionality I like, and would be missing if I rolled my own right now, are: Options like MongoDB's GridFS are getting close, but wouldn't quite cut it. RavenDB's "Attachments" functionality is pretty close, I understand it only supports up to 2Gb via the ESENT storage engine Just to clarify, I'm not exactly sure what form this would take. I'm not looking for a pre-built product (which I don't see exists), but perhaps a stub of a project, an open source project planning to provide this functionality, people who might have developed their own similar solution in C#, etc.
563e30d361a801306526731c	X	We have RavenFS that handles that scenario, I think. It is a commercial offering, though.
563e30d361a801306526731d	X	you can do this with a condition block, but it's fairly complicated. Instead, put your private files in a second bucket.
563e30d361a801306526731e	X	Can you please answer my questions here: stackoverflow.com/questions/29749203/…
563e30d361a801306526731f	X	I am using Amazon S3 to upload files into different folders. All folders and files are public and can be seen by anyone. I created a private folder, where i want to put private images so that only i can see them. I already created a bucket policy rule that will deny the access to that folder. But how can i see the files ? Is there a special link like this https://s3.amazonaws.com/bucket/private_folder/file.jpg?secret_key=123 that will let me and someone who know`s that secret key to see the files ? Is there any way of uploading private files that can be seen by using a secret_key, url or something like that ?
563e30d361a8013065267320	X	By default, all objects in Amazon S3 are private. Objects can then be made "public" by adding permissions, via one of: As long as one of these methods grants access, the person will be able to access the object. It is also possible to assign Deny permissions that override Allow permissions. When an object is being accessed via an un-authenticated URL (eg s3.amazonaws.com/bucket-name/object-key), the above rules determine access. However, even "private" files can be accessed if you authenticate against the service, such as calling an S3 API with your user credentials or using a pre-signed URL. To see how this works, click a private file in the Amazon S3 Management Console, then choose Open from the Actions menu. The object will be opened. This is done by providing the browser with a pre-signed URL that includes a cryptographically-sized URL and a period of validity. The URL will work to Get the private file only until a defined time. So, to answer your question, you can still access private files via: Just be careful that you don't define DENY rules that override even your ability to access files. It's easier to simply ALLOW the directories you'd like to be public. See: Query String Request Authentication Alternative
563e30d361a8013065267321	X	Is there a way to set an expiry date in Amazon Glacier? I want to copy in weekly backup files, but I dont want to hang on to more than 1 years worth. Can the files be set to "expire" after one year, or is this something I will have to do manually?
563e30d461a8013065267322	X	While not available natively within Amazon Glacier, AWS has recently enabled Archiving Amazon S3 Data to Amazon Glacier, which makes working with Glacier much easier in the first place already: [...] Amazon S3 was designed for rapid retrieval. Glacier, in contrast, trades off retrieval time for cost, providing storage for as little at $0.01 per Gigabyte per month while retrieving data within three to five hours. How would you like to have the best of both worlds? How about rapid retrieval of fresh data stored in S3, with automatic, policy-driven archiving to lower cost Glacier storage as your data ages, along with easy, API-driven or console-powered retrieval? [emphasis mine] [...] You can now use Amazon Glacier as a storage option for Amazon S3. This is enabled by facilitating Amazon S3 Object Lifecycle Management, which not only drives the mentioned Object Archival (Transition Objects to the Glacier Storage Class) but also includes optional Object Expiration, which allows you to achieve what you want as outlined in section Before You Decide to Expire Objects within Lifecycle Configuration Rules: So at the small price of having your objects stored in S3 for a short time (which actually eases working with Glacier a lot due to removing the need to manage archives/inventories) you gain the benefit of optional automatic expiration.
563e30d461a8013065267323	X	You can do this in the AWS Command Line Interface. http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html
563e30d461a8013065267324	X	+1 @Ryan. I had assumed it was a S3 certificate change. The bug was introduced in the SDK in version 1.3.21.
563e30d461a8013065267325	X	To be fair, if you request https://foo.example.com.s3.amazonaws.com/... in your URL, the *.s3.amazonaws.com doesn't cover that host name: the wildcard doesn't propagate across the dots according to the specs ("*.a.com matches foo.a.com but not bar.foo.a.com"). What Amazon could do is get a certificate for *.*.*.s3.amazonaws.com.
563e30d461a8013065267326	X	Agreed @Bruno. What the question is talking however is using their Java API. Basically I can't do anything using their API with a bucket that has a period in its name.
563e30d461a8013065267327	X	Just reading RFC 6125 (best practice spec, but not really implemented yet). I'm not sure how to interpret whether *.*.*.something would be allowed with rule 1. The right thing (if you really need a custom handler) would indeed be to write your own verifier that expects the right pattern. The problem with the ALLOW_ALL_HOSTNAME_VERIFIER is that it allows MITM attacks.
563e30d461a8013065267328	X	Amazon has a way to address the buckets without affecting the hostname. That's how they are going to fix it. I was looking for a short/mid term solution to issue.
563e30d461a8013065267329	X	for reference the 1.3.22 does not fix this for non-US buckets. Path-style addressing requires to change the endpoint to the region specific one using #setEndpoint so a little more work is needed setting the endpoint for the s3client instance in use
563e30d461a801306526732a	X	Amazon "upgraded" the SSL security in its AWS Java SDK in the 1.3.21 version. This broke access any S3 buckets that have periods in their name when using Amazon's AWS Java API. I'm using version 1.3.21.1 which is current up to Oct/5/2012. I've provided some solutions in my answer below but I'm looking for additional work arounds to this issue. If you are getting this error, you will see something like the following message in your exceptions/logs. In this example, the bucket name is foo.example.com. You can see documentation of this problem on the AWS S3 discussion forum: https://forums.aws.amazon.com/thread.jspa?messageID=387508&#387508 Amazon's response to the problem is the following. We should be able to fix this by using the older path style method of bucket addressing (instead of the newer virtual host style addressing) for buckets with this naming pattern. We'll get started on the fix and ensure that our internal integration tests have test cases for buckets names containing periods. Any workaround or other solutions? Thanks for any feedback.
563e30d461a801306526732b	X	Turns out that Amazon "upgraded" the SSL security on S3 in late September 2012. This broke access any S3 buckets that have periods in their name when using Amazon's AWS Java API. This is inaccurate. S3's SSL wildcard matching has been the same as when S3 launched back in 2006. What's more likely is that the AWS Java SDK team enabled stricter validation of SSL certificates (good), but ended up breaking bucket names that have been running afoul of S3's SSL cert (bad). The right answer is that you need to use path-style addressing instead of DNS-style addressing. That is the only secure way of working around the issue with the wildcard matching on the SSL certificate. Disabling the verification opens you up to Man-In-The-Middle attacks. What I don't presently know is if the Java SDK provides this as a configurable option. If so, that's your answer. Otherwise, it sounds like the Java SDK team said "we'll add this feature, and then add integration tests to make sure it all works."
563e30d461a801306526732c	X	Edit: So after work on 10/5/2012, Amazon released version 1.3.22 which resolves this issue. I've verified that our code now works. To quote from their release notes: Buckets whose name contains periods can now be correctly addressed again over HTTPS. There are a couple of solutions that I can see, aside from waiting till Amazon releases a new API. Obviously you could roll back to 1.3.20 version of the AWS Java SDK. Unfortunately I needed some of the features in 1.3.21. You can replace the org.apache.http.conn.ssl.StrictHostnameVerifier in the classpath. This is a hack however which will remove all SSL checking for Apache http connections I think. Here's the code that worked for me: http://pastebin.com/bvFELdJE I ended up downloading and building my own package from the AWS source jar. I applied the following approximate patch to the HttpClientFactory source. The right fix is to change from domain-name bucket handling to path based handling. Btw, the following seems like it might work but it does not. The AWS client specifically requests the STRICT verifier and does not use the default one:
563e30d561a801306526732d	X	Are there any other products much similar to Amazon S3's offerings
563e30d561a801306526732e	X	I have decided to write an abstraction layer myself, so that it will be easier for me to port this to GAE later on.
563e30d561a801306526732f	X	Google App Engine provides a image API for storing / retrieving images. We are currently not in a position to deploy our application on top of App Engine because of limitations in the java frameworks (jboss seam 2.2.0) we are using to build our j2ee application. We would eventually want to deploy our production application on top of Google App Engine, but what are the short term options (java based open source products) which provides comparable functionality to Google App Engine's Image API and will have an easier migration path at a later point in time.
563e30d561a8013065267330	X	I know it's not a java-based open source product but if you are talking about the Blobstore API (just for storing/retrieving images), I'd recommend replacing it with Amazon S3 : http://aws.amazon.com/s3/ It doesn't have the features of the Image Manipulation API from Google App Engine though.
563e30d561a8013065267331	X	If you're looking good image storage try Bildero.
563e30d561a8013065267332	X	I am trying to create a signed URL for a PUT request into an Amazon S3 bucket. I get this error: The request signature we calculated does not match the signature you provided. Check your key and signing method. Now I've tried everything under the sun to fix this, like different solutions to similar questions from others posted here on SO. I have made sure that the that Amazon returns in the error matches the string that the script signs. Like when I get this as response: It looked like Amazon did not expect the amz headers and the mimetype, so I left that out. But still to no avail. Here's the code I'm using, courtesy and slightly adapted of: http://danielgallo.co.uk/dev/extjs-amazons3/docs/#!/api/AmazonS3-method-uploadFile Here's what I can exclude from being a solution as I tried this: Any help is greatly appreciated, thanks!
563e30d561a8013065267333	X	Can't you just query whois.ripe.net? Or am I on the wrong page here?
563e30d561a8013065267334	X	I was looking for a Whois Api, but most of them charge heavy price and not reliable enough. We can code in Python or Php. We need to make a Whois lookup service, to integrate with our site. What AWS Resource we need for this? We need at least 5k lookups per day. AWS provides: S3 , elastic, and others. We are confused. As Amazon provides free tire. Does it allow who is lookup? As google app engine never allowed this.
563e30d561a8013065267335	X	The Amazon service you want to use is the server service: EC2. You get full access to a server and, of course, you can performs socket connections on port 43 (the one required by the Whois protocol).
563e30d561a8013065267336	X	Has nothing to do with nodejs, but in S3, you cannot delete all files at one time. You have to delete one file/object at a time, and so you have to have a loop to delete all files/objects.
563e30d561a8013065267337	X	@rsmoorthy: This used to be the case indeed, but has been mostly remedied via Amazon S3 - Multi-Object Delete as of December 2011 - it's still limited to 1000 objects at a time, but a significant improvement regardless of course.
563e30d661a8013065267338	X	@steffenOpel - Thanks! I missed that announcement. Yes, that will speed up the deletions. Last year, I had to delete more than 10M objects and it took days!
563e30d661a8013065267339	X	Recommending only you own stuff 8 times on you first day here is not totally appreciated. See this FAQ on promotion.
563e30d661a801306526733a	X	Sorry about that, I'll go read the FAQ a bit more. However, knox is a library that is no longer maintained and therefore people get caught out. AwsSum is maintained, I try to help people and AWS is my specialty so I'd hoped StackOverflow would like that knowledge. I apologise and I'll stop answering questions in this manner.
563e30d661a801306526733b	X	I'm not a moderator, just a user who noticed your posts in the sites Review feature. A new user who recommends a single product in several posts on his first day, will make the spam indicator lights start to warm up. The fact that you stated up front that it is your project was good. And that your answers are still here shows that the spam filter wasn't triggered. Just wanted you to stay clear of that.
563e30d661a801306526733c	X	I am using knox npm module to upload files to a bucket on S3. It works great. What I am not sure how to do is how to remove all the files from a bucket in one call instead of deleting one file at the time? any idea would help...
563e30d661a801306526733d	X	This has not been possible in the past, however, Amazon has finally introduced Amazon S3 - Multi-Object Delete in December 2011: Amazon S3's new Multi-Object Delete gives you the ability to delete up to 1000 objects from an S3 bucket with a single request. Obviously client libraries like knox must add dedicated support for this API now in turn, and a respective issue does indeed exist already in knox' issue tracker (still pending as of today), see Multi-Object Delete. Accordingly you should monitor this issue and/or participate in the implementation :)
563e30d661a801306526733e	X	My AwsSum library can already do multi object delete. The operation in the library is called 'DeleteMultipleObjects': You can install AwsSum via npm doing: $ npm install awssum There is an example here: Hope it goes well and give me a shout if you need any assistance. :)
563e30d661a801306526733f	X	Hi Patashu, Thanks for your response. I had gone through this link earlier. The code which they have provided wont run on my localhost. The explanation is not clear. As they have not mentioned where all we need to make changes in order to run smoothly.
563e30d661a8013065267340	X	We want to integrate YouTube API into our website. We have website which gets photos and videos from amazon s3 storage. We would like to add feature which will allow user to upload video to his YouTube account from our website. I spent whole day searching for it, but ended with nothing. Any suggestion or help highly appreciated. Thanks in advance Vinay Kulkarni
563e30d661a8013065267341	X	Speaking as someone who has not tried it before, I would start here: https://developers.google.com/youtube/2.0/developers_guide_protocol "In addition, the API lets your application upload videos to YouTube or update existing videos."
563e30d661a8013065267342	X	What are some Cloud storage Services that can download files from web directly. For ex- I want to download a file -> www.example.com/god.avi Now what are some cloud storage services that will allow me to directly upload the file to my account. Google Drive and Dropbox are cloud services but they dont have this facility.
563e30d661a8013065267343	X	I assume since you're asking this question on Stack Overflow that you're asking this in the context of programming. In that case, Dropbox has the Saver, which lets a user download a file directly to Dropbox. (Dropbox transfers the file from the URL server-side.)
563e30d661a8013065267344	X	You can download/upload files pragmatically to Dropbox and also to Google Drive. They have API to do it. However, if you are really looking for really complete service where you can scale up your data storage until you want, you really want to take a look at Amazon S3. In fact, as far as I know, Dropbox works on the top of Amazon S3 to provide their services. If you would like to have an idea about how to upload/download a file to Amazon S3, you can take a look to this application example. If you want the same thing on Dropbox or Google Drive, there are a lot of examples on Internet. However, on these two providers you need a token to upload files, what I don't like. I prefer the way in which it works for Amazon S3 (just for programming purposes - GUI is better on GD or Dropbox).
563e30d761a8013065267345	X	Is your goal to not use nodejs during development? Or is it enough to not have a requirement for node in production?
563e30d761a8013065267346	X	My goal is to put all html, js and css etc on amazon s3 where they can only host static web pages. So I guess I could use node in development.
563e30d761a8013065267347	X	Yep I'd suggest that, employ node/webpack as your build pipeline then just copy the static files up to S3
563e30d761a8013065267348	X	Thank you! So all webpack features would still work right? Like lazy loading (code splitting)?
563e30d761a8013065267349	X	Sure. Have a look here and here for a detailed code base. The important parts are require.ensure (CommonJs) or require (AMD).
563e30d761a801306526734a	X	Thank you very much!
563e30d761a801306526734b	X	I am trying to build a web app where I want to store all html, js and css files on amazon s3, and communicate with a restful server through api. I am trying to achieve lazy loading and maybe routing with react router. It seems that webpack has this feature code splitting that would work similarly as lazy loading. However, all of the tutorial and examples I found involves webpack-dev-server, which is a small node express server. Is there anyway I could generate bundle at build time and upload everything to amazon s3 and achieve something similar to Angular's ocLazyLoading?
563e30d761a801306526734c	X	It's definitely possible to create a static bundle js file, which you can use in your production code that does not include webpack-dev-server. See this example as a reference (note: I am the owner of this repo). webpack.prod.config.js does create a production ready bundle file using webpack via node.js which itself does not require node.js anymore. Because of that you can simply serve it as a simple static file (which is done in the live example). The key difference is how the entry points are written in the dev- and production environments. For development webpack-dev-server is being used In the production environment you skip the webpack-dev-server and the hot reloading part If you want to split your code into more than one bundle, you might want to have a look at how to define multiple entry points and link the files accordingly.
563e30d761a801306526734d	X	Hi! I am having problems with building libs3, can you help me?
563e30d761a801306526734e	X	Thanks! As a novice programmer myself, I'm curious: how did you know that -ls3 was the right flag?
563e30d861a801306526734f	X	In GCC, libraries are added to the linker command line (when invoked via the gcc/g++ driver) via the -l flag. The syntax is -lname, where name is the part after lib and before the .so/.a. I guessed on the library file being libs3.so
563e30d861a8013065267350	X	actually it will be -ls3
563e30d861a8013065267351	X	I'm trying to install a library that I can use to access Amazon's S3 service (I just need to be able to upload files there). The code needs to be in C++ because it's going to be bundled as part of an application I'm working on. I'm trying to work with Bryan Ischo's library located here: http://libs3.ischo.com.s3.amazonaws.com/index.html I'm having some installation issues though. I changed GNUMakefile.macosx to GNUMakefile and then ran "sudo make install", as I'm developing on a Mac. Then I made a test .cpp file. All I want to do is to be able to initialize the library, since this is what his API says to do. However, I get back this error: I'd like some help either fixing my installation of libs3 or getting a few tips on accessing S3 through C++. Thanks!
563e30d861a8013065267352	X	Your test application is not linking with libs3. You will need to add it to the linker flags, such as -ls3 (if the library is libs3.so/a)
563e30d861a8013065267353	X	I faced a similar problem with executing a C file using Byan Ischo's library on mac and I had to add some more parameters before I was able to successfully run my test file. How to compile libs3 on mac? How to compile test.c? How to execute a.out?
563e30d861a8013065267354	X	I have XSSFWorkbook object which I want to save to a particular location (using Amazon S3). For saving I am using some API which takes File object parameter. I know that XSSFWorkbook exposes write method to write to a path. Is there any way I can get file object from XSSFWorkbook.
563e30d861a8013065267355	X	I'm developing an api, using Loopback.js and the loopback-component-storage. This component works great if one sends a file with the request, however, I'm generating some images in my server. Each image is a buffer object. How can I upload this buffers to Amazon S3? This is what I have so far: what can I do now with this stream? Thanks!
563e30d861a8013065267356	X	Thanks for the reply! Any chance you know if I can set the response Access-Control-Allow-Origin header on Amazon S3?
563e30d861a8013065267357	X	@Kevin I've been researching this. Amazon S3 does not support setting this header on buckets or objects. For more information, check out this thread. Amazon seems to have zero interest in supporting it in the future, according to their (lack of) response.
563e30d961a8013065267358	X	@Dan Thanks. I was actually a 'poster' on that thread and have given up hope.
563e30d961a8013065267359	X	CORS is now available on S3 (3.5 years after being requested!)
563e30d961a801306526735a	X	No, you don't. Amazon supports HMAC SHA-2 encoding of the secret key in combination with an expiration token. They allow direct uploads through regular posts (doc.s3.amazonaws.com/proposals/post.html) I just need it to be AJAX.
563e30d961a801306526735b	X	Awesome. Didn't know about that!
563e30d961a801306526735c	X	Yeah, still not sure how to fix my bugs Ariejan. Anyone else know? See post above!
563e30d961a801306526735d	X	I am looking for an HTML 5 AJAX library / framework for users to upload files directly to Amazon S3. The goal is to avoid uploading attachments to the web server (as the web server blocks when it transfer them to Amazon). My understanding is that this should be possible using XDomainRequest, but I can't figure out how. I am running ruby-on-rails and wanted to assign the uploaded file a temporary name (using a UUID) that will be posted back to the web server so the file can later be renamed and integrated with paperclip. Any ideas? Is this something jQuery can handle? Flash isn't an option for this project. Thanks! Edit: I managed to get a basic post working but am still have issues. I'm not exactly sure what headers are required, or how to encode the Amazon required parameters in the request (can I put them in the request header?). Here is my progress thus far: Edit: After further updates, I've managed to get the following error: XMLHttpRequest cannot load http://bucket.s3.amazonaws.com/. Origin http://local.app is not allowed by Access-Control-Allow-Origin. I've uploaded a crossdomain.xml file that allows access from the wildcard (*) domain. Not sure how to continue... Edit: After having done more investigation, I'm starting to think that the JavaScript POST might not be possible to S3. Will I be required to post to an EC2 instance before doing a transfer? I might be able to secure a micro instance, but I'd prefer to go direct to S3 if possible! Thanks! Edit: I posted the question on the Amazon Forums and haven't received any feedback. For cross references the other post can be found here: https://forums.aws.amazon.com/message.jspa?messageID=206650#206650.
563e30d961a801306526735e	X	You need to make the other side issue an Access-Control-Allow-Origin header. In you case the other side is Amazon S3 server. Unless they mention your domain in that header you won't be able to make any cross-site requests to them. Amazon S3 now supports Cross Origin Resource Sharing and you can configure any of your S3 buckets for cross-domain access by adding one or more CORS rules to your bucket. Each rule can specify a domain that should have access to your bucket and a set of HTTP verbs you wish to allow.
563e30d961a801306526735f	X	Today Amazon announces complete support for Cross-Origin Resource Sharing (CORS) in Amazon S3. You can now easily build web applications that use JavaScript and HTML5 to interact with resources in Amazon S3, enabling you to implement HTML5 drag and drop uploads to Amazon S3, show upload progress, or update content.
563e30d961a8013065267360	X	This would mean that you have to expose your S3 credentials in JavaScript. You don't want that. The best solution is to use Paperclip. Yes, you must upload to your server first, but at least its secure. Nevermind that, check the comments.
563e30d961a8013065267361	X	S3 can host html pages with jquery no problem. The bucket becomes the server URL. If you use a tool like S3 Bucket Explorer you can get a URL in one click for any HTML page in a bucket. Then you can simply use the PUT command in an XMLHttpRequest to upload files. This is essentially how the JQuery-HTML5-Upload plugin works for Amazon S3 (See their Issue #12). In fact you can experiment with the Amazon S3 REST API syntax by just plugging it into variables and then using that in conjunction with the XMLHttpRequest open() method. Peace in the multiverse.
563e30d961a8013065267362	X	Amazon has finally added support for Cross Origin Resource Sharing (CORS) which allows this: http://aws.typepad.com/aws/2012/08/amazon-s3-cross-origin-resource-sharing.html
563e30d961a8013065267363	X	Have not tried myself, but they seem to have this working in the jquery-html5-upload plugin http://code.google.com/p/jquery-html5-upload/issues/detail?id=12
563e30d961a8013065267364	X	There is a good rails gem called s3_drect_upload which does exactly what you want out of the box. It's an html5/javascript uploader with support for renaming files.
563e30d961a8013065267365	X	That was the problem, Ray. The bucket properties for "static website hosting" specify the endpoint as upload.roughdrag.com.s3-website-us-east-1.amazonaws.com but as soon as I changed it to upload.roughdrag.com.s3.amazonaws.com this started working for me. Thank you very much.
563e30d961a8013065267366	X	I'm having a very similar situation to the original poster. Can you share your DNS settings for me? And, are you using Amazon for the DNS? I'm just mapping uploads.mydomain.com to my bucket's endpoint BUCKETNAME.s3-website-us-east-1.amazonaws.com.
563e30da61a8013065267367	X	I have been banging my head against the wall on this and am entirely stumped. I am trying to use FineUploader to upload files directly to my Amazon S3 bucket. I have essentially copied the code from the fineuploader.com web page (Upload Files Directly to Amazon S3) and the server-side PHP. When I attempt to upload a file I see the post to the signature endpoint seems to work successfully but when it attempts to upload to S3 I get a 405 "Method Not Allowed" error. HTML PHP Signature Endpoint - uploadHandler.php Amazon S3 CORS Configuration IAM Group Security Policy uploader.js was captured from http://fineuploader.com/source/all.fineuploader-3.9.1.min.js Console response The software looks amazing but I just can't get past this. Any help is appreciated.
563e30da61a8013065267368	X	I'm guessing this is a DNS issue created when you mapped your custom domain name to your S3 bucket. After resolving upload.roughdrag.com, it looks like you have mapped this CNAME to "upload.roughdrag.com.s3-website-us-east-1.amazonaws.com". Try mapping that CNAME to "upload.roughdrag.com.s3.amazaonaws.com" instead. Update 1: If you are still seeing issues after this change, I would post in the AWS S3 forums. Hopefully an employee will answer. There may be an issue with your bucket/CNAME that I cannot see from my end. It looks like a POST request to upload.roughdrag.com.s3.amazonaws.com goes though, but there are issues sending a POST request to upload.roughdrag.com. I verified this with Postman. Update 2: With your latest CNAME change, it looks like POST requests are being accepted by S3.
563e30da61a8013065267369	X	I had the same issue. Fixed it by adding the region to the function below: Find the region of your bucket by clicking on properties. Then Static website hosting and only take the details from the endpoint from ap-xxxxx-x
563e30da61a801306526736a	X	Is it possible to use objectstore (swift) provided by openstack via opscenter as a backup-location? In version 5.1.1. backups to Amazon S3 are supported. But how do I configure object-store from another provider? I only found the link below: http://www.datastax.com/documentation/opscenter/5.1/api/docs/backups.html#method-update-a-destination
563e30da61a801306526736b	X	You can use a custom pre-post backup script if you want to back up cassandra to a location that is not on your local file system or on Amazon S3: http://www.datastax.com/documentation/opscenter/5.1/opsc/online_help/services/opscSchedulingBackup_t.html In essence this is a hook for you to run a program before or after the OpsCenter backup. You can use that to push out to Swift. Here is an example of what a backup script may look like (this one is to back up to S3 which is now obsolete by the S3 functionality in OpsCenter 5.1, but you can use it as an example for your custom Swift script). https://gist.github.com/phact/7500b6cc9fb6f963c849
563e30db61a801306526736c	X	Look at GitHub Pages.
563e30db61a801306526736d	X	Yup, realized this a few hours ago. Thanks!
563e30db61a801306526736e	X	I used up my free year of Amazon S3. Is there somewhere I can host an mp3 file (with url: "http://.mp3") Also, where can I host a directory full of html, js, and css files? These assets will respond to a Rails API.
563e30db61a801306526736f	X	Yup. This is very confusing. And I found that not much documentation is available for boto too.
563e30db61a8013065267370	X	I am trying to use boto api to upload photos to Amazon S3. I can successfully upload photos there if I haven't specified the Canned ACL. But if I specified ACL as follow. I got the following error. Error as follow. I tried for a long time but still cannot get any hints. Anyone knows why? Thanks!
563e30db61a8013065267371	X	The upload_part_from_file method should not have a policy parameter. This is a bug in boto. To assign a policy to a multipart file, you specify the canned policy as the policy parameter on the initiate_multipart_upload call and then upload the parts and complete the upload. Don't try to pass the policy when uploading the individual parts. We should create an issue on github for boto to remove the policy parameter. It's confusing and doesn't work.
563e30db61a8013065267372	X	Can you capture your actual http request headers?
563e30db61a8013065267373	X	I can reproduce this. Investigating ....
563e30db61a8013065267374	X	@SébastienStormacq . Thanks :)
563e30db61a8013065267375	X	It might be a bug. Waiting for confirmation
563e30db61a8013065267376	X	If-Modified-Since is currently formatted as yyyy-MM-dd'T'HH:mm:ss'Z' where it should be EEE, dd MMM yyyy HH:mm:ss z. S3 uses yyyy-MM-dd'T'HH:mm:ss'Z' as the date format, but If-Modified-Since is a part of HTTP specifications and needs to be formatted differently. We are working on a fix, and the next update should address this bug. Thanks.
563e30db61a8013065267377	X	I have tested this and can confirm that it now works.
563e30dc61a8013065267378	X	Building for iOS 7+ , Building on Xcode 6.1 , Using Amazon SDK AWSiOSSDKv2 2.0.12 , Testing on iPhone 5s and iPad 2 I am downloading images from my Amazon S3 bucket with the Amazon SDK for iOS. The downloading is working fine but I want to use the ifModifiedSince property to retrieve only images that have been modified since a certain date (see http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSS3GetObjectRequest.html#//api/name/ifModifiedSince ) However, this is not working. Even when I specify a ifModifiedSince date that is LATER THAN the modified date of the file on S3, the file is returned. According to the Amazon documentation: ifModifiedSince - Return the object only if it has been modified since the specified time, otherwise return a 304 (not modified). So I am not sure if I am doing something wrong or Amazon has a bug in the SDK. Here is my code:
563e30dc61a8013065267379	X	We've released the AWS Mobile SDK for iOS 2.0.14. It should address the time formatting issue.
563e30dc61a801306526737a	X	Thanks Rico for laying out different options. Looks like some of them do cost. So I am going to stick with the current solution until the costs get prohibitively high at which point 3rd party moving apis might make sense. Also yah it is ironic that Dropbox using Amazon S3 as well. Thanks again!
563e30dc61a801306526737b	X	We are looking to support export of photos from our S3 location to users' Dropbox. Currently I am using the code like below: The above method costs twice the bandwidth. Is there anyway I can transfer the images directly from S3 to Dropbox without copying them locally? Thanks in advance!
563e30dc61a801306526737c	X	How about trying something like mover and use their APIs? https://mover.io/ http://support.mover.io/knowledgebase/articles/214572-how-to-transfer-or-backup-your-amazon-s3-buckets-t Or you can also try SME Storage (Storage Made Easy) http://storagemadeeasy.com/ It's kinda ironic that DropBox uses Amazon S3 to stores all its files. Or you can also write your own streamer in Ruby and run it in an Amazon instance it will be much faster since all the data would be within Amazon. How do I HTTP post stream data from memory in Ruby?
563e30dc61a801306526737d	X	The article might have been written before Facebook added the feature. Canvas iframes are now always loaded via form POST (for extra security or something), and it can't be turned off any more. Looks like a dead end
563e30dc61a801306526737e	X	These days a free SSL cert can be had in less than 24 hours for most domains. Using S3 because you don't have a cert seems silly to me.
563e30dc61a801306526737f	X	I was involved in a fanpage project using the JS API so we decided to host the site on an Amazon S3 bucket as a) it's static content and b) Amazon have an SSL certificate required by Facebook apps since Oct 2011. But it turns out that instead of going a HTTP GET, Facebook is requesting the fanpage via an HTTP POST (an additional security check? why don't they just to an HTTP HEADERS?). Amazon wisely sends back the following: ...as it figures that Facebook is trying to upload via the POST The irony is that Facebook actually recommend using S3 for those who don't have an SSL certificate on http://www.facebook.com/note.php?note_id=10150223945345844 Bottom line: Has anyone managed to host a fanpage on an S3 bucket post October 2011? Is there a bucket policy that can help with this?
563e30dc61a8013065267380	X	An option would be to use cloudfront to point to an EC2 instance. This will happily accept Post requests. Just make sure you set a very long cache TTL on your response headers to ensure the instance doesn't keep getting hit by requests. You can still host your images etc in s3. The EC2 instance will just be in charge of translating the post request.
563e30dc61a8013065267381	X	I have some doubts about accessing and processing data in cloud storage services. 1.Is there any common API that i can use to write applications for the mainstream cloud storage providers(Amazon s3,Google cloud storage,Windows Azure..Please point out providers whom i missed in this list. I am concentrating on Enterprise domain only, not personal storage) 2.Now the sdk part. If i want to write an application(let us say a j2ee application) that process data in iCloud, does iCloud sdk provides me that kind of flexibility? 3.Will the cloud storage service provide me transaction support?I mean support for ACID properties.Or i have to take the responsibility for it?
563e30dc61a8013065267382	X	Here is how to use the jClouds storage context:
563e30dd61a8013065267383	X	The basis of any Cloud service provider is to use REST interface. You can use any language to wrap REST request as long as the language supports networking and PKI security infrastructure. All modern language have such functionality. Any SDK in most cases is just a wrapper to these REST interface so you can very easily write the coder and sync or async way and get what you are looking. SDK just expedite the work to manifold, comparative to using REST directly. That does not mean you can not use REST direclty, it is just an SDK is there to help you to connect to specific cloud service. How SDK are implemented and what kind of functionality it provides, varies from providder to provider and the services they have. You can not use the Windows Azure SDK for Amazon Cloud because the internal service connection endpoints and underneath interfaces are encapsulated within the SDK itself. It does not mean an SDK can not be created to connect all of the cloud service provide however each cloud service provide individualize the SDK for their cloud service. Cloud services provide infrastructure and platform your users to deploy their application and about ACID, you will have to take care of application level work, however ACID could be a totally separate and lengthy discussion..
563e30dd61a8013065267384	X	Regarding your 3 questions:
563e30dd61a8013065267385	X	For question 1. If you're asking if all the services you list share a common API, no. They all use REST which means you can use JSON with any of them, but the call will have different names and possibly syntax.
563e30dd61a8013065267386	X	You ca use iClous only through Apple's SDK for iOs and Mac OS X. I am almost sure you cannot access it from a j2ee application.
563e30dd61a8013065267387	X	try disposing of your result, and be sure to handle IOExceptions
563e30dd61a8013065267388	X	I've not used these bits before, but since you reference filedata you seem to be using the FileStreamProvider. using the memoryStreamProvider would mean the files are only stored in memory & not written to disk. Not sure if your S3helper supports that though
563e30dd61a8013065267389	X	FileInfo creates a lock on the file even if you dont actually do anything with it. I'd suggest copy the filepath to a string variable and use that instead.
563e30dd61a801306526738a	X	I was wondering if that was the issue when I came back around and read this today. Anyone know if there is a way around having these files put on the servers hard drive in the first place?
563e30dd61a801306526738b	X	If the files are small (1MB or less) and the traffic on your site is medium - low you could store the file in a memory buffer (either byte[] or MemoryStream ), I'm not familiar with S3 uploads, maybe it has an option to upload a stream, then you can grab the stream as it comes from the upload and push it over.
563e30dd61a801306526738c	X	The S3Helper was locking the file. Duh. I also didn't know FileInfo had Delete on it. How handy... Thanks!
563e30dd61a801306526738d	X	I am trying to upload files to S3 after a user uploads a file to my API. I obviously don't want them to live on MY server, in fact I'd prefer they never exist on the server at all. My problem is that the files appear to remain in use for the lifetime of server app! Here is the code: How, when or where do I delete these files? OR is there a way to keep the files from being written to my server's disk in the first place?
563e30dd61a801306526738e	X	The similar question shows that this approach should work in general. The only visible differences are:
563e30de61a801306526738f	X	Not sure but s3helper.upload might be uploading on background thread. Try this Just looked at s3 docs and yes it uses not just a background thread but multiple! Uploads the specified file. The object key is derived from the file's name. Multiple threads are used to read the file and perform multiple uploads in parallel. For large uploads, the file will be divided and uploaded in parts using Amazon S3's multipart API. The parts will be reassembled as one object in Amazon S3.
563e30de61a8013065267390	X	I am looking for online RESTful Web Service Demo and not good examples of RESTful API. Are the following links the only available demos? http://mooshup.com/mashup.jsp?author=keith&mashup=RESTDemo (REST and WSDL 2.0 Discussion continues) http://www.thomas-bayer.com/rest-demo.htm
563e30de61a8013065267391	X	You might want to check out the Amazon S3 REST API. As an example directly from the S3 documentation, the following would be a request to delete the puppy.jpg file from the mybucket bucket: You would have to create an AWS account if you do not have one. It will not only serve as an online demo, but will also be a very practical exercise.
563e30de61a8013065267392	X	Here is an experimental one that provides access to the MSDN documentation. I'm not sure how much work has been done on in the last couple of years, but it is a good example of using XHTML as an API format.
563e30de61a8013065267393	X	You may take a look at an implementation of the RESTful AtomPub protocol by WordPress.com. You have to have an account to do something useful. The AtomPub service document is located at:
563e30de61a8013065267394	X	I am working on a show case that will also include a demo implementation within a couple of weeks. It is still growing but you can start reading and perhaps check back in a week or so: http://www.nordsc.com/blog/?cat=13 (read bottom to top) Jan
563e30de61a8013065267395	X	You might want to take a look here: http://www.thomas-bayer.com/sqlrest/ found it on this SO question
563e30de61a8013065267396	X	Sorry to destroy your last shred of hope, but no, there's not a documented way to do this... but also, given that rfc-7233 indicates that Range: support is optional in servers and proxies, and that the concept of a "partial" img is not necessarily something browsers might universally understand as a sane concept... what are you trying to accomplish with only a subset of an image's data?
563e30de61a8013065267397	X	Thanks for your help--I'm hosting deep-zoom image pyramids, which typically contain thousands of tiny image files. Uploading them all to S3 is slow and costly (cost per PUT). I'm trying to glob the images together into one file, tracking the byte ranges for the individual chunks. I can then fetch a chunk via an AJAX req and set the img data accordingly (base64 enc, unfortunately). I have a working implementation, but caching seems to be an issue.
563e30de61a8013065267398	X	S3 PUT requests are $5 for 1 million. Still, it's an interesting approach you're taking.
563e30de61a8013065267399	X	I'm familiar with the Range HTTP header; however, the interface I'm using to query S3 (an img element's .src property) doesn't allow me to specify HTTP headers. Is there a way for me to specify my desired range via a parameter in the query string? It doesn't seem like there is, but I'm just holding out a shred of hope before I roll my own solution with ajax requests.
563e30de61a801306526739a	X	Amazon S3 supports Range GET requests, as do some HTTP servers, for example, Apache and IIS. How CloudFront Processes Partial Requests for an Object (Range GETs) I tried to get my S3 object via cURL: and AWS SDK for JavaScript: These two methods work fine for me. AWS SDK for JavaScript API Reference (getObject)
563e30df61a801306526739b	X	Thanks for the response. Do you think I need to use BeginUpload in asp.net app or I can just use the normal Upload method?
563e30df61a801306526739c	X	You're welcome. It really depends on whether you want the upload to be synchronous or asynchronous.
563e30df61a801306526739d	X	If I am doing asynchronously how the status will be shown in ASP.Net app? Is there any example available?
563e30df61a801306526739e	X	Please have a look into this article from MSDN on sync vs. async.
563e30df61a801306526739f	X	TransferUtility.Upload is 4 times faster than PutObject when I uploaded a 110MB file, TransferUtility.Upload is the way to go
563e30df61a80130652673a0	X	I know there are two methods available to upload files in AWS S3 (i.e. PutObject and TransferUtility.Upload). Can someone please explain which one to use? FYI, I have files ranging from 1kb to 250MB. Thanks in advance.
563e30df61a80130652673a1	X	Based in Amazon docs, I would stick with TransferUtility.Upload: Provides a high level utility for managing transfers to and from Amazon S3. TransferUtility provides a simple API for uploading content to and downloading content from Amazon S3. It makes extensive use of Amazon S3 multipart uploads to achieve enhanced throughput, performance, and reliability. When uploading large files by specifying file paths instead of a stream, TransferUtility uses multiple threads to upload multiple parts of a single upload at once. When dealing with large content sizes and high bandwidth, this can increase throughput significantly. But please be aware of possible concurrency issues and the recommendation about using BeginUpload (the asynchronous version), like in this related post
563e30df61a80130652673a2	X	Mostly we people try to upload large files on S3 that take too much time to upload,at that situations we need to have progress information such as the total number of bytes transferred and remaining amount of data to transfer. Following link has detailed way that helps us to create event that occurs periodically and returns multipart upload progress information to us. https://docs.aws.amazon.com/AmazonS3/latest/dev/HLTrackProgressMPUDotNet.html
563e30df61a80130652673a3	X	Great thanks. Now I suppose I need to figure out some regex to separate that key.name in to the folders so I can build some organisation in to bucket browsing rather than just a huge list of images.
563e30df61a80130652673a4	X	well, if remember correctly you can just use the bucket.list(s3_path) function if you like to organize your files in a "unix path like" manner, because it lists all keys having this start string in common.
563e30df61a80130652673a5	X	readthedocs.org/docs/boto/en/latest/ref/s3.html#boto-s3-bucket look at the "list()" function's parameters (i.e. prefix, etc..)
563e30df61a80130652673a6	X	docs.amazonwebservices.com/AmazonS3/latest/API/…
563e30df61a80130652673a7	X	Thank You. Boto is turning out to be great. Now I just need to keep reading the Djangobook to figure out how to store this info.
563e30df61a80130652673a8	X	I've been using Django for a couple of days & setup a basic blog from a tutorial with django comments. I've got a totally separate python script that generates screenshots and uploads them to Amazon S3, now I'd like my django app to display all the images in the bucket and use a comment system on the images. Preferably I'd do this by just storing the URLs in my sqlite db, which I've got hard-coded currently to display all images in the db and has comments enabled on these. My model: (Does this need a foreign key to the django comments or is that just part of the Django Magic?!) My bucket structure: https://s3-eu-west-1.amazonaws.com/bucket/revision/process/images.png Almost all the tutorials and packages I'm finding are based on upload/download rather than a simple for keys in bucket type approach that I want. One of my problems is understanding how I can integrate my Boto functions with Django if I'm using Base.html. In an earlier tutorial I had an index page which had a view and could call functions from there. But base doesn't need that so I'm starting to get a little lost.
563e30df61a80130652673a9	X	haven't looked up if boto api changed, but this is how it worked last time i looked Update: Amazon s3 is a key value store, where key is a string. So nothing prevents you from putting in keys like: now bucket.list(prefix="/folder/images/") would yield the latter three. Look here for further details:
563e30e061a80130652673aa	X	This is my code to store result from s3 to mysql by boto, django.
563e30e061a80130652673ab	X	Apparently, there is one. Will be released with the next version of S3fm. Stay tuned :)
563e30e061a80130652673ac	X	http://www.s3fm.com/ is really useful for uploading/viewing files for personal viewing. I was wondering if there was something good for social networks (using Amazon s3 or something similar), where it will only show the subset of files uploaded by a specific user (for example, limit access to the user's specific bucket). Can s3fm be adapted to this solution? or is there something else out there?
563e30e061a80130652673ad	X	Chris, thanks for bringing this up. Next version of S3fm will allow just that: sharing files and "folders" with your friends and colleagues using your own S3 account. A bucket owner will be able to use his or her credentials to create new (non-AWS) "accounts" and assign different permissions for each user. Then s/he will be able to select files to share or "folders" for uploads for each of those users. A secure authentication method has been developed on top of regular Amazon S3 API so no 3rd party service will be required for that purpose. In fact, your newly created account credentials are not even accessible to anyone but you and you users . On the flip side, if you loose them - they are gone, we wont be able to restore them. :) This version was expected this coming Fri (Aug 9, 2009), but apparently will be delayed another week or so. Happy to help, feel free to follow up with questions or ideas, Alex
563e30e061a80130652673ae	X	I believe you would need to build your own system to do this. What you use doesn't really matter, you could use S3, Azure, etc as your base storage "cloud."
563e30e061a80130652673af	X	There is no method of authentication on S3, it only serves files publicly. You can of course obfuscate the file names by naming them with hashes. But still only a fileserver. Maybe roll your own system? Then make it public so I can use it... it would be awesome!
563e30e061a80130652673b0	X	I have updated my S3 class - Link: http://amazon-s3-php-class.googlecode.com/files/s3-php5-curl_0.4.0.tar.gz
563e30e061a80130652673b1	X	Thanks for you support, its done.
563e30e061a80130652673b2	X	Yep Grantee: Frank Open/Download: Yes View Permissions:Yes Edit Permissions:Yes
563e30e061a80130652673b3	X	Then tell me what else
563e30e061a80130652673b4	X	Am using below third party API in my project development http://undesigned.org.za/2007/10/22/amazon-s3-php-class I have done all task like upload, delete, bucket-list, object-list with this API, but one of major task to create object download link form a bucket is hazy. Official Amazon API has : get_object_url ( $bucket, $filename, $preauth, $opt ) to get any object's URL, but with above API is lack of method and documentation. Its shows following error with this code when i click on download link: Code Error If someone has any idea or experience with this API then suggest me. Thanks
563e30e061a80130652673b5	X	So you are using : http://undesigned.org.za/2007/10/22/amazon-s3-php-class S3 class, and want to create download link of an object from bucket You should try this: This download link will expire after 1hr(3600sec.), you can extend expire time by the last parameter to increment it.
563e30e161a80130652673b6	X	Does all files have all user read permission (public accessible) ? if you want to access that file without authenticated way then you have to give read permission to that file. else you can also create signed url.
563e30e161a80130652673b7	X	I m getting permission denied. And more specifically, I need to know abt the rest based S3 API using federated user
563e30e161a80130652673b8	X	It was improper token usages. COrrected it with the help of the docs.
563e30e161a80130652673b9	X	How can I upload file directly to S3 using curl My current flow is: I am yet to identify how the step 3 need to be implemented. I have given CreateObject permission the federated user. But the upload is failing. I am trying to make the service run independent of the logic I am running behind and without knowledge Any help will be appreciated. Looks like a silly mistake i am doing here.
563e30e161a80130652673ba	X	What kind of error do you receive (take a look at downloaded file) ? Most likely you incorrectly sign outgoing request. Please take a look at this document: Signing and Authenticating REST Requests. I would also recommend you to carefully read Amazon S3 REST API to understand how you should operate with S3 buckets / objects. Hope it helps you !
563e30e161a80130652673bb	X	Why the @ before the <script> ?
563e30e161a80130652673bc	X	yes , I assume it with razor engine,
563e30e161a80130652673bd	X	Still, you don't need it.
563e30e161a80130652673be	X	Minus the @ symbol before Script, and this is the correct answer. Thanks
563e30e161a80130652673bf	X	you need the @ sign when using traditional <script> tags, just tested
563e30e161a80130652673c0	X	Thanks I have also tried that already.
563e30e161a80130652673c1	X	@Doomsknight. And I hope you won't use it.
563e30e261a80130652673c2	X	Well in that case, check the value of ViewBag.CC server side, you might not be setting it in some case.
563e30e261a80130652673c3	X	Im not sure wht showWarning does. (didnt do anything for me) but the rest did contain the correct value. So I saw showWarning('2'); in my code.
563e30e261a80130652673c4	X	I was using this code, it calls just alert and it works fine. you can change alert('@ViewBag.Message'); it will work.
563e30e261a80130652673c5	X	I think you have an additional bracket? Ive tried similar to this but with ".
563e30e261a80130652673c6	X	@Doomsknight. ' or " is the same thing. Well Did it work?
563e30e261a80130652673c7	X	On run time i see var c = $('#' + '2').val();
563e30e261a80130652673c8	X	@Doomsknight. O.k. So it's working... :) You have 2 in the ViewBag.CC
563e30e261a80130652673c9	X	Seeing the 2 now, Ive gone back to var c = '@ViewBag.CC'; seems to be working now :/ Maybe running a few methods together were breaking it all. Thanks
563e30e261a80130652673ca	X	My attempted methods. Looking at the JS via browser, the @ViewBag.CC is just blank... (missing)
563e30e261a80130652673cb	X	if you are using razor engine template then do the following in your view write : UPDATE: A more appropriate approach is to define a set of configuration on the master layout for example, base url, facebook API Key, Amazon S3 base URL, etc ...``` And you can use it in your client side code as follow:
563e30e261a80130652673cc	X	try: var cc = @Html.Raw(Json.Encode(ViewBag.CC)
563e30e261a80130652673cd	X	You can use ViewBag.PropertyName in javascript like this.
563e30e261a80130652673ce	X	ViewBag is server side code. Javascript is client side code. You can't really connect them. You can do something like this: But it will get parsed on the server, so you didn't really connect them.
563e30e261a80130652673cf	X	I will give you a hint though your issue is in your headers
563e30e261a80130652673d0	X	I am offering a bounty because I am really close to solving the issue I think. Still getting a 403 error. I used the following example code to get to where I am at currently. aws.amazon.com/code/1092
563e30e361a80130652673d1	X	I'm having the same issue. I tried using the lib from the amazon example, Eugeny89's code and the elctech library. None of those worked, I always get the 403 response back from s3. I'm using the same policy together with uploadify and that's working fine, so I don't think the policy is the issue. I do get a warning before the upload though: Warning: Domain mybucket.s3.amazonaws.com does not specify a meta-policy. Applying default meta-policy 'master-only'.
563e30e361a80130652673d2	X	... and sure enough it was the policy the problem. Copying the policy from the elctech library over made it work.
563e30e361a80130652673d3	X	Bastien - do you have a working example that you could share ?
563e30e361a80130652673d4	X	interesting ... I will try give this a try and let you know.
563e30e361a80130652673d5	X	you seem to be missing: s3options.AWSAccessKeyId = Settings.accessKey; and you're not setting the uploadURL.url. But even with those changes I can't get it to work...
563e30e361a80130652673d6	X	there are quite a few ways to upload to S3 from flash. I am trying to implement code from the following example on Amazon's site http://aws.amazon.com/code/1092?_encoding=UTF8&jiveRedirect=1 Some posts that I ran across indicated that a lot of the fields from Amazon are now Requiered and unless you fill them in you will get this dreaded 403 error. I have tried several things and I am hoping there will be a solution soon. I used the following libs from here http://code.google.com/p/as3awss3lib/. Here is my class that handles all the uploading
563e30e361a80130652673d7	X	I'd worked with s3 and I didn't used special libs for that (maybe except generating policy and signature). Try something like the following:
563e30e361a80130652673d8	X	I managed to get my code to work, the difficulty is to get the right policy. I'm using elctech's library to generate the s3 upload call. I get the amazon s3 certificate from our main Rails application via our API, here's what the policy generation looks like:
563e30e361a80130652673d9	X	A quick search for "s3cmd iam role" would have shown you that you can use IAM roles with s3cmd. Also, you can use the aws cli tool to access S3 via IAM roles.
563e30e361a80130652673da	X	"with IAM users and not using access keys" doesn't entirely make sense... IAM users are identified by... their access keys. If "IT policy" is that you can't have access keys, then the question for IT is whether you are authorized to access S3 and how that should be done. (One assumes the answer would be one of: "use the instance's IAM Role" or "use the following key and secret" or "we already said you can't do that.")
563e30e361a80130652673db	X	Can you connect to S3 via s3cmd or mount S3 to and ec2 instance with IAM users and not using access keys? All the tutorials I see say to use access keys but what if you can't create your own access keys (IT policy).
563e30e361a80130652673dc	X	There are two ways to access data in Amazon S3: Via an API, or via URLs. Via an API When accessing Amazon S3 via API (which includes code using an AWS SDK and also the AWS Command-Line Interface (CLI)), user credentials must be provided in the form of an Access Key and a Secret Key. The aws and s3cmd utilities, and also software that mounts Amazon S3 as a drive, require access to the API and therefore require credentials. If you have been given a login to an AWS account, you should be able to ask your administrators to also create credentials that are associated with your User. These credentials will have exactly the same permissions as your normal login (via Username/password), so it's strange that they would be disallowing it. They can be very useful for automating AWS activities, such as starting/stopping Amazon EC2 instances. Via URLs Objects stored in Amazon S3 can also be made available via a URL that points directly to the data, eg s3.amazonaws.com/bucket-name/object.txt To provide public access to these objects without requiring credentials, either add permission to each object or create a Bucket Policy that grants access to content within the bucket. This access method can be used to retrieve individual objects, but is not sufficient to mount Amazon S3 as a drive.
563e30e361a80130652673dd	X	I'm trying to figure out how to upload data to an Amazon S3 bucket via a RESTful API that I'm writing in Node.js/Restify. I think I've got the basic concepts all working, but when I go to connect to the body of my POST request, that's when things go awry. When I set up my callback function to simply pass a string to S3, it works just fine and the file is created in the appropriate S3 bucket: Obviously, I need to eventually stream/pipe my request from the body of the request. I assumed all that I needed to do would be to simply switch the body of the params to the request stream: But that ends up causing the following log message to be displayed: "Error uploading data: [TypeError: path must be a string]" which gives me very little indication of what I need to do to fix the error. Ultimately, I want to be able to pipe the result since the data being sent could be quite large (I'm not sure if the previous examples are causing the body to be stored in memory), so I thought that something like this might work: Since I've done something similar in a GET function that works just fine:(s3.client.getObject(params).createReadStream().pipe(res);). But that also did not work. I'm at a bit of a loss at this point so any guidance would be greatly appreciated!
563e30e361a80130652673de	X	So, I finally discovered the answer after posting on the AWS Developer Forums. It turns out that the Content-Length header was missing from my S3 requests. Loren@AWS summed it up very well: In order to upload any object to S3, you need to provide a Content-Length. Typically, the SDK can infer the contents from Buffer and String data (or any object with a .length property), and we have special detections for file streams to get file length. Unfortunately, there's no way the SDK can figure out the length of an arbitrary stream, so if you pass something like an HTTP stream, you will need to manually provide the content length yourself. The suggested solution was to simply pass the content length from the headers of the http.IncomingMessage object: If anyone is interested in reading the entire thread, you can access it here.
563e30e461a80130652673df	X	Thank you, but the second link is for google drive, and I need to use Google Cloud Storage
563e30e461a80130652673e0	X	I'm developing an application that lets my users upload files, and I've made it works with "local" disk using filesystem feature, but now I want to migrate and use google Google Cloud Storage for it. It has been a lot difficult to find some useful information. In the docs there is an "example" for work with dropbox, but it's not complete. It doesn't shows how to config drivers and disk, so it isn't clear enough for me. I would like to know what to do, since I have no idea from now. I've just used this tutorial http://www.codetutorial.io/laravel-5-file-upload-storage-download/ and it's working for local, so I really need to migrate to google cloud storage, and only that. I'm using openshift and I feel comfortable about it. Could you please help me to know what should I configure filesystem to be used as I need? Thank you
563e30e461a80130652673e1	X	I've find a solution. It seems that Google Cloud Storage uses the same api than Amazon S3, so I could use it as I would use amazon, the same driver. The only thing I needed to change was when I config disks in laravel, using this code in config/filesystems when adding a disk for google:
563e30e461a80130652673e2	X	I didn't find solutions\adapters like here This is what i found but it's for google drive.
563e30e461a80130652673e3	X	The files are custom binary files. I added the crossdomain file to the question.
563e30e461a80130652673e4	X	What about the policy log -- did it provide any additional information?
563e30e461a80130652673e5	X	Also, when you try and load myowndomain.com/filename in your browser, outside of Flash ... do you get a certificate error or is the certificate valid and the request succeeds?
563e30e461a80130652673e6	X	Your crossdomain.xml looks OK on the surface. Have you witnessed your browser make a successful request for your crossdomain.xml? Alternatively, are you explicitly loading the policy file in advance, from its https location? A tool like ieHttpHeaders (for IE) or HttpFox (for Firefox) to inspect is often helpful.
563e30e461a80130652673e7	X	And don't forget to look at the policy log!
563e30e461a80130652673e8	X	I am trying to access files in Amazon S3 bucket with SSL with ActionScript3. When I use this format... I get security sandbox error. "Error #2048: Security sandbox violation: " When I switch to this format... It works like a charm (until I try it on a browser other than Firefox). It generates a certificate error (host name mismatch) for the other browsers. Once I add exceptions it works fine. But that's not practical. Third option which would be the ideal version... ... generates the same security violation for all browsers. Needless to say, the domain is mapped to the bucket. The bucket has its own crossdomain.xml. The files are custom binary files. I went thru the security white paper and new rules for Flash Player 10. No luck so far. Any ideas?   Ok it gets more interesting, and I suspect this is causing the problem. While sanitizing the name of my bucket, I oversimplified. My bucket name has a dot in it and appearently it is not a good thing. http://faindu.wordpress.com/2008/12/18/amazon-s3-flash-crossdomainxml-ie7-certifacte-error/ So I would appreciate it, if there is an alternative to that.
563e30e461a80130652673e9	X	This is due to browser restrictions. Also, if you trying to access S3 from AS3 then you'll probably fine the AS3 API quite useful though this too runs into browser restrictions: This is an AS3 library for accessing Amazon's S3 service. It only works in Apollo because of restrictions in the browser player.
563e30e461a80130652673ea	X	During your troubleshooting, did you enable the Flash Player's policy file logging feature? You can get more specific information behind the sandbox violation error. Read the following to learn how to set up policy file logging: Policy file changes in Flash Player 9 and Flash Player 10 Personally, I suspect you should be able to get your third option to work, at least, since you'd be able to host a crossdomain.xml at the root location of https://www.myowndomain.com/crossdomain.xml -- but let's see what you have in your crossdomain.xml. I suggest you post a copy here, sanitized if necessary. And, tell us, what kind of files are you trying to load in the player?
563e30e461a80130652673eb	X	thanks for the suggestion...I like to use the php sdk but not sure how to, are you saying that I don't need to include the factory method for declare my IAM credentials? Is it possible to make a variable for the bucket url and use it like: my_s3_bucket='http://link/to/my/s3 and then $thisFu['original_img']='my_s3_bucket/uploads/fufu/'.$_POST['cat'].'/original_'‌​.uniqid('fu_').'.jpg'; ?
563e30e461a80130652673ec	X	Sorry for delay but I don't receive notification about new update. Wow, thank you for your great and detailed explanation, that seems so easy...instead days ago I tried with s3fs and now my upload folder is hidden and not accessible in any way (you can read my 4th update above). I wish to try this method but until I have the problem with my upload folder I cannot even try.
563e30e461a80130652673ed	X	I think if I just make edit of post then notification doesn't come...I was glad that info was usefull for you :) about you problem at update 4: I'm little bit lost what exactly your problem? Disable s3fs? Why you can't just come to source folder and make: "sudo make uninstall"? btw I see you mentioned some permissions issue, did you make commands from root or sudo?
563e30e561a80130652673ee	X	did you reboot after uninstall?
563e30e561a80130652673ef	X	apache knows nothing about s3fs, reboot your instance
563e30e561a80130652673f0	X	Hi, after the connection, how could I automatically upload to the bucket pictures that users upload? My code is like this: $this['original_img'] = '/uploads/img/'.$_POST['cat'].'/original_'.uniqid('my_').'.jpg'; Thanks
563e30e561a80130652673f1	X	@Simone Have you seen the "uploading a file" section on that page ?
563e30e561a80130652673f2	X	@Simone What do you mean by constant to use for the url ? You can specify options to the function to make a file public or private.
563e30e561a80130652673f3	X	As I wrote, I'm a beginner, I don't know what's the correct path to upload images into s3...what should I code before /upload/img/ ?
563e30e561a80130652673f4	X	Hi, I changed my question, could you please change your vote? Thanks
563e30e561a80130652673f5	X	Finally someone! Thanks for reply, however I cannot install anything like that, indeed I'm on Amazon EC2...is it that difficult manually? I thought was enough take all folders and place in root :)
563e30e561a80130652673f6	X	In the first link for installing the SDK, go to the Install from zip instructions and instead of the line use Aws\S3\S3Client; use require '/path/to/aws-autoloader.php';
563e30e561a80130652673f7	X	That is clear but where I supposed to place into my project? I have my own framework, do I place this inside root folder? or admin folder perhaps? What's the difference if any
563e30e561a80130652673f8	X	Place it whereever you would like inside your directory with the PHP code you use!
563e30e561a80130652673f9	X	:D ok that's great...I asked only for security reasons
563e30e561a80130652673fa	X	Thanks, so basically using this way I cannot use Cloudfront as CDN? I activated Cloudfront and for origin I have placed my bucket
563e30e561a80130652673fb	X	If the only change you make is using s3fs then yes, You can write the image to s3 with s3fs and then use the Cloudfront links with the API. But would need code changes
563e30e561a80130652673fc	X	I updated my question, could you please check it?
563e30e561a80130652673fd	X	are there files in the dir already ? try mv /uploads/fufu /uploads/fufu.old; mkdir /uploads/fufu; then try the s3fs command again
563e30e561a80130652673fe	X	the non empty command probably didnt help. try unmounting the s3fs (fusermount -u /uploads/fufu) confirm its no longer mounted (doesnt appear in output of df), make sure nothing exists in folder. (ls -la) is there is nothing there do the s3fs command again. Everything should work after that. Ived edited my answer. with an empty dir you shouldnt need the non empty option though
563e30e561a80130652673ff	X	I'm on an EC2 instance and I wish to connect my PHP website with my Amazon S3 bucket, I already saw the API for PHP here: http://aws.amazon.com/sdkforphp/ but it's not clear. This is the code line I need to edit in my controller: I need to connect to Amazon S3 and be able to change the code like this: I already configured an IAM user for the purpose but I don't know all the steps needed to accomplished the job. How could I connect and interact with Amazon S3 to upload and retrieve public images? UPDATE I decided to try using the s3fs as suggested, so I installed it as described here (my OS is Ubuntu 14.04) I run from console: Everything was properly installed but what's next? Where should I declare credentials and how could I use this integration in my project? 2nd UPDATE I created a file called .passwd-s3fs with a single code line with my IAM credentials accessKeyId:secretAccessKey. I place it into my home/ubuntu directory and give it a 600 permission with chmod 600 ~/.passwd-s3fs Next from console I run /usr/bin/s3fs My_S3bucket /uploads/fufu Inside the /uploads/fufu there are all my bucket folders now. However when I try this command: I get this error message: 3rd UPDATE As suggested I run this fusermount -u /uploads/fufu, after that I checked the fufu folder and is empty as expected. After that I tried again this command (with one more -o): and got this error message: Any other suggestion? 4th UPDATE 18/04/15 Under suggestion from console I run sudo usermod -a -G fuse ubuntu and sudo vim /etc/fuse.conf where I uncommented mount_max = 1000 and user_allow_other Than I run s3fs -o nonempty -o allow_other My_S3bucket /uploads/fufu At first sight no errors, so I thought everythings fine but it's exactly the opposite. I'm a bit frustrated now, because I don't know what happened but my folder /uploads/fufu is hidden and using ls -Al I see only this I cannot sudo rm -r or -rf or mv -r it says that /uploads/fufu is a directory I tried to reboot exit and mount -a, but nothing. I tried to unmount using fusermount and the error message is fusermount: entry for /uploads/fufu not found in /etc/mtab But I tried sudo vim /etc/mtab and I found this line: s3fs /uploads/fufu fuse.s3fs rw,nosuid,nodev,allow_other 0 0 Could someone tell me how can I unmount and finally remove this folder /uploads/fufu ?
563e30e561a8013065267400	X	Despite to "S3fs is very reliable in recent builds", I can share my own experience with s3fs and info that we moved write operation from direct s3fs mounted folder access to aws console(SDK api possible way also) after periodic randomly system crashes . Possible that you won't have any problem with small size files like images, but it certainly made the problem while we tried to write mp4 files. So last message at log before system crash was: and it was rare randomly cases, but that made system unstable. So we decided still to keep s3fs mounted, but use it only for read access Below I show how to mount s3fs with AIM credentials without password file Also you will need to create IAM role that assigned to the instance with attached policy: In you case, seems it is reasonable to use php sdk (other answer has usage example already), but you also can write images to s3 with aws console: If you will have IAM role created and assigned to your instance you won't need to provide any additional credentials Update - answer to your question: Yes if you will have IAM assigned to ec2 instance, then at code you just need to create the client as: option 2: If you do not need file local storage stage , but will put direclt from upload form: And to get s3 link if you will have public access Or generate signed url to private content:
563e30e561a8013065267401	X	I agree the documentation at that link is a bit hard to dig and leaves a lot of dots to be connected. However, I found something a lot better here: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/service-s3.html It has sample code and instructions for almost all the S3 operations.
563e30e661a8013065267402	X	To give you a little more clarity, since you are a beginner: download the AWS SDK via this Installation Guide Then set up your AWS account client on your PHP webserver using this bit If you would like more information on how to use AWS credentials files, head here. Then to upload a file that you have on your own PHP server: If you are interested in learning how to upload images to a php file, I would recommend looking at this W3 schools tutorial. This tutorial can help you get off the ground for saving the file locally on your server in a temporary directory before it gets uploaded to your S3 bucket.
563e30e661a8013065267403	X	A much easier and transparent to your application setup is simply to mount the s3 partition with s3fs https://github.com/s3fs-fuse/s3fs-fuse (Use option allow_other) Your s3fs then behaves like a normal folder would just move the file to that folder. S3fs then uploads S3fs is very reliable in recent builds You can read images this way also, But loose any effect that the AWS CDN has, though last time i tried it it wasn't a huge difference you need a s3fspassword file in the format accessKeyId:secretAccessKey it can be in either of these places The file needs 600 permissions https://github.com/s3fs-fuse/s3fs-fuse/wiki/Fuse-Over-Amazon Has some info. When that is complete the command is s3fs bucket_name mnt_dir you can find the keys here https://console.aws.amazon.com/iam/home?#security_credential from the example about i would assume your mnt_dir would be /uploads/fufu so s3fs bucket /uploads/fufu to your second problem is wrong, you need to specify -o again the user you are mounting as needs to be in the fuse group or sudo addgroup your_user fuse
563e30e661a8013065267404	X	Thanks! this looks promising.. any idea if i begin using Cloudfront after S3?
563e30e661a8013065267405	X	Don't want to speak to that as I've never used Cloudfront. Sorry!
563e30e661a8013065267406	X	I would like to allow users of my website see a special "Image/Video/HTML" but only if they login through facebook connect on my Rails site... I would like to use Amazon S3 to store the media.. My question is how do I give limited access to my users only if they are logged in? Once they have the URL of the "Image/Video/HTML" I would not like them to be able to access it unless they are logged in with facebook on my site.. Also, will I be able to continue this kind of private site if I want to use Amazon Cloudfront? This is a starting point for whitelisting only my domain... but I want to make sure the user is logged in through fb connect in order to serve them the appropriate resource... https://gist.github.com/3716433
563e30e661a8013065267407	X	I think this depends on how you're including S3 assets in your app. Here are instructions if you're using paperclip: https://github.com/thoughtbot/paperclip/wiki/Restricting-Access-to-Objects-Stored-on-Amazon-S3 Here's another question that deals with this issue using the aws-sdk gem: How to store data in S3 and allow user access in a secure way with rails API / iOS client?
563e30e661a8013065267408	X	What happens if you reverse the order of these two statements? Does $tbfile get uploaded and $file not? Have you verified that $tbfile and $tbdirectoryandfilename contain what you think they do, and the file you're trying to upload exists?
563e30e661a8013065267409	X	I flip them around and it's now working. How odd. Thanks for the suggestion.
563e30e661a801306526740a	X	I am executing this function which uploads a file to my Amazon S3 bucket for two separate files so I have the line of code in there twice but looking at two different files: But only the first line will ever run. Where am I going wrong? The S3 PHP class documentation and code is at http://undesigned.org.za/2007/10/22/amazon-s3-php-class
563e30e761a801306526740b	X	I've never used any Amazon API, but after a quick look from their documentation, the putObjectFile() method is a legacy method, and may no longer be properly supported. Instead, you should consider using the inputFile() method. See http://undesigned.org.za/2007/10/22/amazon-s3-php-class/documentation#inputFile
563e30e761a801306526740c	X	ok so Picture.objects.get(self.get_object()).file.delete() it's correct?
563e30e761a801306526740d	X	I imagine that you are using class based views. If so, then self.get_object() returns an object (Picture) in question. So to delete the file, then you can do self.get_object().file.delete().
563e30e761a801306526740e	X	ok sry but I want to delete in database reference and in the folder
563e30e761a801306526740f	X	What do you mean by that? Do you want to remove the Picture instance which will also delete the file associated with that picture?
563e30e761a8013065267410	X	i want to delete the file ( HDD file) and the object in the database
563e30e761a8013065267411	X	I would like to delete file associated to a file-field But it doesn't work. Can you fix it please?
563e30e761a8013065267412	X	You can remove file objects using the FileField api: That will use storage API to remove the file. The advantage of that is that this approach works even if you want to switch your storage to different system such as Amazon S3.
563e30e761a8013065267413	X	My team wants to store the blobs for our BOSH release in a remote blobstore. However we have an internal CEPH / Rados store that we want to use. I know that Rados has S3 compatible interfaces so I was wondering how I could enable this as the final blobstore. I know that typically I'd only need to give the access key, secrete key, and bucket to BOSH. But now I also have an IP host (and probably in the future a url) that specifies where the bucket exists. Is there currently a way to set this up?
563e30e761a8013065267414	X	As an example on how BOSH can interact with a CEPH / Rados store: Storing packages in an internal CEPH / Rados store This requires configuring the release to know about the intended blobstore. Keep in mind that when you run bosh upload blobs the command is parsed by the BOSH CLI rather than a BOSH or MicroBOSH VM if you have happened to target one. Assume that you have a store at address IP_ADDRESS:PORT with an existent bucket named BUCKET and keys ACCESS_KEY and SECRET_ACCESS_KEY. The config/final.yml file should look like: But the config/private.yml file should look like: Depending on how the CEPH store is set up, it may be necessary to turn off ssl verification as well which would include adding ssl_verify_peer: false under the s3_port option (i.e. the nesting would be blobstore -> s3 -> ssl_verify_peer. At this point calling bosh upload blobs will work as expected. Telling a BOSH VM about the CEPH / Rados store The deployment manifest file needs to have the items in the private.yml and the bucket name in the blobstore properties section. EDIT Aug 20, 2015 According to the CEPH website http://ceph.com/docs/master/radosgw/s3/#api there is currently no support for Bucket policies. Therefore the fact that a bucket is readable will not be inherited by a bucket's objects. This means that if a CEPH store is used in place of the official Amazon S3 for storage, the config/private.yml file is mandatory.
563e30e861a8013065267415	X	I use Carrierwave to upload images straight to Amazon S3. I have a JSON API for the upload where I encode the image as Base64 in the client and send it. I followed this tutorial blog post to do it. Image upload fails with this message on the console: I see that the value it is trying to insert into the DB for image column is not a string by an object. When I upload via the browser(without using API), the value inserted in image column is just the file name. Where am I going wrong here ? Here is my controller: Here is my model. I have mounted ImageUploader on image: The JSON I am sending is: The above JSON was sent from a hand crafted test script:
563e30e861a8013065267416	X	I finally got the photo upload via API to work. In the create() function instead of I changed it to Here is the complete code:
563e30e861a8013065267417	X	How exactly do you want to authenticate? Are you going to send a key with each request?
563e30e961a8013065267418	X	I'm trying to find information on securing a HTTP REST API in a Symfony project, but all I can find is information about using sfGuardPlugin. From what I can see, this plugin isn't very useful for web services. It tries to have user profile models (which aren't always that simple) and have "sign in" and "sign out" pages, which obviously are pointless for a stateless REST API. It does a lot more than I'll ever have need for and I what to keep it simple. I want to know where to implement my own authorisation method (loosely based on Amazon S3's approach). I know how I want the authorisation method to actually work, I just don't know where I can put code in my Symfony app so that it runs before every request is processed, and lets approved requests continue but unsuccessful requests return a 403. Any ideas? I can't imagine this is hard, I just don't know where to start looking.
563e30e961a8013065267419	X	There is a plugin for RESTful authentication -> http://www.symfony-project.org/plugins/sfRestfulAuthenticationPlugin Not used it though .... How where you planning to authenticate users ? The jobeet tutorial uses tokens ... http://www.symfony-project.org/jobeet/1_4/Doctrine/en/15
563e30e961a801306526741a	X	I ended up finding what I was looking for by digging into the code for sfHttpAuthPlugin. What I was looking for was a "Filter". Some details and an example is described in the Askeet sample project.
563e30e961a801306526741b	X	Stick a HTTP basicAuth script in your <appname>_dev.php (Symfony 1.4 =<) between the project configuration "require" and the configuration instance creation. Test it on your dev. If it works, put the code in your index.php (the live equivalent of <appname>_dev.php) and push it live. Quick and dirty but it works. You may want to protect that username/password in the script though. e.g.
563e30ea61a801306526741c	X	Instead: Consider lambda.
563e30eb61a801306526741d	X	In short: Although it is legal to do so, it usually doesn't make sense to use the comma operator in the condition part of an if or while statement (EDIT: Although the latter might sometimes be helpful as user5534870 explains in his answer). A more elaborate explanation: Aside from its syntactic function (e.g. separating elements in initializer lists, variable declarations or function calls/declarations), in C and C++, the , can also be a normal operator just like e.g. +, and so it can be used everywhere, where an expression is allowed (in C++ you can even overload it). The difference to most other operators is that - although both sides get evaluated - it doesn't combine the outputs of the left and right expressions in any way, but just returns the right one. It was introduced, because someone (probably Dennis Ritchie) decided for some reason that C required a syntax to write two (or more) unrelated expressions at a position, where you ordinarily only could write a single expression. Now, the condition of an if statement is (among others) such a place and consequently, you can also use the , operator there - whether it makes sense to do so or not is an entirely different question! In particular - and different from e.g. function calls or variable declarations - the comma has no special meaning there, so it does, what it always does: It evaluates the expressions to the left and right, but only returns the result of the right one, which is then used by the if statement. The only two points I can think of right now, where using the (non-overloaded) ,-operator makes sense are: If you want to increment multiple iterators in the head of a for loop: If you want to evaluate more than one expression in a C++11 constexpr function. To repeat this once more: Using the comma operator in an if or while statement - in the way you showed it in your example - isn't something sensible to do. It is just another example where the language syntaxes of C and C++ allow you to write code, that doesn't behave the way that one - on first glance - would expect it to. There are many more....
563e30eb61a801306526741e	X	My server is hosting an application which preloads >100 thumbnail images on each page load. The thumbnail images do not change often. I am trying to make consecutive thumbnail loads faster by using Cache-Control: public,max-age=MANY_SECONDS where MANY_SECONDS is up to a year. The thumbnails are requested via a Flask endpoint that looks like this: I set the Cache-Control header to public,max-age=MANY_SECONDS for all the *-thumb.png keys, still Firefox fires a request against /api/thumbnail/... and gets the 301, then a 304 from Amazon S3. I'm under the impression that 301 responses are cached seemingly for an eternity, and the Cache-Control headers for the thumbnail files served from Amazon S3 should allow Firefox to cache the thumbnail files locally for up to a year. All these thumbs × 2 requests are really an overhead. I want them cached for an eternity.
563e30eb61a801306526741f	X	My solution was to use a HTML5 manifest file.
563e30eb61a8013065267420	X	This part of the S3 dev guide hints that it is possible, but doesn't explain where to get the components of the URL docs.amazonwebservices.com/AmazonS3/2006-03-01/dev/…
563e30eb61a8013065267421	X	Thanks for your help! I was hoping to do it without writing a little app for it. I ended up finding an online one, but in the end, decided to use CloudFTP as an FTP endpoint to my S3 bucket. See: stackoverflow.com/questions/1855109/amazon-s3-ftp-interface
563e30eb61a8013065267422	X	I am attempting to use an S3 bucket as a deployment location for an internal, auto-updating application's files. It would be the location where the new version's files are dumped for the application to puck up on an update. Since this is an internal application, I was hoping to have the URL be private, but to be able to access it using only a URL. I was hoping to look into using third party auto updating software, which means I can't use the Amazon API to access it. Does anyone know a way to get a URL to a private bucket on S3?
563e30ec61a8013065267423	X	You probably want to use one of the available AWS Software Development Kits (SDKs), which all implement the respective methods to generate these URLs by means of the GetPreSignedURL() method (e.g. Java: generatePresignedUrl(), C#: GetPreSignedURL()): The GetPreSignedURL operations creates a signed http request. Query string authentication is useful for giving HTTP or browser access to resources that would normally require authentication. When using query string authentication, you create a query, specify an expiration time for the query, sign it with your signature, place the data in an HTTP request, and distribute the request to a user or embed the request in a web page. A PreSigned URL can be generated for GET, PUT and HEAD operations on your bucket, keys, and versions. There are a couple of related questions already and e.g. Why is my S3 pre-signed request invalid when I set a response header override that contains a “+”? contains a working sample in C# (aside from the content type issue Ragesh is experiencing of course). Good luck!
563e30ec61a8013065267424	X	Thanks! That looks really cool. How does it scale when having a lot of traffic and users? Are there fees involved?
563e30ec61a8013065267425	X	Sorry, I see there billing service. I was wondering if the user has to log in with there service or something?
563e30ec61a8013065267426	X	It's free until 1 000 000 request per month, you can create free account easily.
563e30ec61a8013065267427	X	Does the user have to log in with a Google account?
563e30ec61a8013065267428	X	Nope. You need to make a google apps account yourself (you as a developer) and setup the cloud service with the servlet and code which will handle the requests sent by the mobile devices. The users themselves do not have to log in with any accounts unless you make use of google's authentication tokens for part of the information exchange.
563e30ec61a8013065267429	X	Out of your point of view, what do you believe is the better choice to go with: Google App Engine or parse.com?
563e30ec61a801306526742a	X	I have never used parse.com, but you can get more information about Google App Engine's billing here: developers.google.com/appengine/docs/billing I personally liked the flexibility and detail that Google App Engine would let you go into and I would pick it again over another alternative. It has got great tools for managing your application. I have looked at parse.com website briefly and it seems that they have APIs which might let you develop faster, but they focus in cloud storage alone, while Google App Engine gives you more freedom in implementing logic along with that storage.
563e30ec61a801306526742b	X	I was wondering if there are any free options to store and retrieve data from the cloud with Android.
563e30ec61a801306526742c	X	I use parse.com for object cloud storage, and it's really simple to use, see http://www.parse.com
563e30ed61a801306526742d	X	You can use Google App Engine. It is free up to a certain limit. After that threshold you have to pay if you want additional storage and/or traffic.
563e30ed61a801306526742e	X	if you are trying to only store and retrieve data from cloud, you could use Google App engine which is pretty easy to use. It has APIs in Java and python If you want more control over how you store the data, you should create an account with Amazon Web services and try using S3. Overall, Amazon S3 will also turn out to be cheaper than Google App Engine.
563e30ed61a801306526742f	X	If I am not missing something this appears to be a duplicate of/ highly related to stackoverflow.com/questions/13074791/…
563e30ed61a8013065267430	X	I have seen that code snippet as well an do not understand it either. I do not see where the file is being obtained from. It is not as cut and dry as the the MVC Request.Files[0] so because of that I am lost as to how I could implement even that snippet to my current project.
563e30ed61a8013065267431	X	I am using an third party uploader called Plupload and I wish to pass a selected image to a WebAPI method but I just cant seem to figure out how. on the front end I Post a file object on the back end I receive the file object but I don't know what to do with it... there is a MVC code snippet out there that I tried to use but apparently Web API does not support Request.Files[0] so I am lost. this is the snippet my method looks like this..its in flux right now. to sum it up. I am passing the file object to my method and trying to push it up to amazon s3 the stated MVC method work as it would seem but the same does not apply for Web API. How might I do this?
563e30ee61a8013065267432	X	Thanks - this is sort of what I suspected, but I'd love to find a solution that requires less server maintenance. So I suppose using an API such as Truevault, my main application would need to run on a local server
563e30ee61a8013065267433	X	@DaveTsay Even if you use Truevault to store the more sensitive data, you will likely need to have a database running locally for other parts of your application.
563e30ee61a8013065267434	X	@DaveTsay, check out www.atlashealth.com, specifically the Managed Cloud offering (I'm on the management team and founder). You'll get a fully managed AWS environment which we set up and maintain, so that all you have to worry about are deployments. The cost of the service is $199 per month (plus AWS usage fees, excluding the $2 dedicated fee surcharge, which we cover). We sign BAA's too.
563e30ee61a8013065267435	X	We also offer an API for storage and other data needs but it's better suited to specific use cases, such as mobile apps. In the case of a Web application, PHI will likely flow through the entire stack. Therefore, even if you're passing off the data to an API, the service handling the post-back (e.g. Heroku) would be considered a HIPAA subcontractor per the recent "omnibus" rule and would need to sign a BAA.
563e30ee61a8013065267436	X	are you still in touch with the founders?
563e30ee61a8013065267437	X	i am. why do you ask?
563e30ee61a8013065267438	X	we're interested in using their service. can you hook me up with a phone call or demo?
563e30ee61a8013065267439	X	contact me and i'll forward along. you can find me on linkedin
563e30ef61a801306526743a	X	There's a recent startup out of YC which seems interesting called Truevault.com, which allows you to store JSON documents in their database via an API and is HIPAA compliant. I am working on a healthcare app, and am wondering which is a better strategy in terms of HIPAA compliance: 1) Heroku + Truevault - easier deployment initially but Heroku won't sign Business Associate Agreement, so not sure if this is truly HIPAA complicant even if I don't store PHI on the heroku server or temporarily store it there. 2) Run everything on Amazon EC2 - Amazon will sign BAA so no issue here, but will have to do server maintenance myself (rather not) 3) Heroku + Amazon S3 database - run server on Heroku but store everything on S3, Amazon to sign BAA Anyone with experience what would be most compliant yet practical? Thanks in advance.
563e30ef61a801306526743b	X	Without knowing specifics about how your application works, its likely that you will have to run all of your application on EC2 and other amazon web services. Heroku nodes are basically EC2 instances with a bit of automation on top, turning it more of a platform than infrastructure. However, if you are working in a field that requires legal compliance on how your data is handled, not having full control may be a bad thing. You can do much of the automation heroku does with tools like Chef and Puppet. Also, if you do use EC2, make sure your infrastructure is configured in VPC is the way to go. Ads a bit of extra work, but gives you more control over network access to different instances. S3 is not really a database, its an object store. Its basically a key/value store with keys that look like file paths. And it can store some very very large values.
563e30ef61a801306526743c	X	Aptible are working on a platform to do exactly that, i.e. automating HIPAA compliance where possible, training you on the stuff you need to do yourself, and letting you build systems on standard databases and ecosystems. They're in private beta at the moment. Disclaimer: I'm not associated with them, but I did meet the founders today and they're an approachable, clever bunch.
563e30ef61a801306526743d	X	So basically your iPhone developer is wrong...
563e30ef61a801306526743e	X	but remember that this adds crypto code to your application and may result in problems during the approval process
563e30ef61a801306526743f	X	This may be a simple question... To convert the *out would you use something like [[NSString alloc] initWithData:out encoding:NSUTF8StringEncoding];?
563e30ef61a8013065267440	X	just like that. you can see github.com/soundcloud/cocoa-api-wrapper/blob/… for a real life example.
563e30ef61a8013065267441	X	SHA-1 is not the same thing as HMAC-SHA-1.
563e30ef61a8013065267442	X	Implicit conversion loses integer precision (NSUInteger to CC_LONG)
563e30f061a8013065267443	X	Hi! And did you work with libs3?
563e30f061a8013065267444	X	For all operation with Amazon services(S3, EC2, SimpleDB) You need to sign all resquest with HMAC-SHA-1 Signature(http://en.wikipedia.org/wiki/HMAC , http://docs.amazonwebservices.com/AWSFWS/latest/DeveloperGuide/index.html?SummaryOfAuthentication.html). I'm working under asp.net backend and there is no problems. Problem is in the iPhone application. iPhone developer says that there is no way to use HMAC-SHA-1 encoding, and he have no rigths to implement his own algorithm. As programmer I cannot understand why there can be a problem. So I want too know is iPhone developer right? I've never coded for iPhone, so I don't even where to search such an information.
563e30f061a8013065267445	X	CommonCrypto will do it. But if you want code, I have some here: http://oauth.googlecode.com/svn/code/obj-c/OAuthConsumer/Crypto/ Which I wrote for use in the Cocoa OAuth implementation: http://code.google.com/p/oauthconsumer/wiki/UsingOAuthConsumer
563e30f061a8013065267446	X	CommonCrypto does the trick. then later
563e30f061a8013065267447	X	This article demonstrates a little function that will generate an SHA-1 hash digest that will match what the php sha1() function will generate if you give it the same input:
563e30f061a8013065267448	X	A bit of googling and I found this document. Exporting of SHA1 is subject to (United Statese)Federal Government export controls and exporters are advised to contact the Department of Commerce, Bureau of Export Administration for more information. I also found this: People's Republic of China and the former Soviet Block can import SHA as long as it's intended for civil end-user applications rather than for military purpose. The following countries are prohibited from importing SHA: Cuba, Iran, Iraq, Libya, North Korea, Serbia, Syria, and Sudan. Please note that this list of embargo countries changes over time. (Not a direct answer to your question, but certainly pertinent.)
563e30f061a8013065267449	X	Not for iPhone in particular, but the library libs3 provides a C API for accessing Amazon's S3 services. It, or the FUSE s3fs component, may be good sources for extracting the routines needed to communicate with Amazon's Web Services. As Objective-C is still C at its core, these routines should work just fine on the iPhone. I know at least one developer who is using something similar within their iPhone application to communicate with S3 buckets.
563e30f061a801306526744a	X	I think the CommonCrypto library will do what you want. Look at this file: /Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS2.2.sdk/usr/include/CommonCrypto/CommonHMAC.h
563e30f061a801306526744b	X	I don't know if this is the case anymore, but there used to be restrictions on encryption algorithms and your right to distribute them to certain countries were restricted. If this is still the case it could be that Apple don't want/can't restrict certain applications from being downloaded in these countries.
563e30f161a801306526744c	X	Can you provide more detail about specifically which error you are receiving?
563e30f161a801306526744d	X	I have set up the DNS CNAME but javascript still can't access iframe's height. What can I do?
563e30f161a801306526744e	X	I store my newsletters .html files in S3 and I have created a function that pulls the newsletters and puts them into iFrames. My problem is I can't set an auto height for the iFrames because I can't read the content. I have a plugin that works if the files are on the same domain. Can I set some headers to allow access from my sub-domain?
563e30f161a801306526744f	X	S3 has a feature called "Virtual Hosting". Virtual Hosting, in general, is the practice of serving multiple web sites from a single web server. One way to differentiate sites is by using the apparent host name of the request instead of just the path name part of the URI. An ordinary Amazon S3 REST request specifies a bucket using the first slash delimited component of the Request-URI path. Alternatively, using Amazon S3 Virtual Hosting, you can address a bucket in a REST API call using the HTTP Host header. In practice, Amazon S3 interprets Host as meaning that most buckets are automatically accessible (for limited types of requests) at http://bucketname.s3.amazonaws.com. Furthermore, by naming your bucket after your registered domain name and by making that name a DNS alias for Amazon S3, you can completely customize the URL of your Amazon S3 resources, for example: http://my.bucketname.com/ Try mapping your S3 bucket to your domain. See here for an example. Accessing S3 Buckets via Virtual Host URLs S3 provides two ways to access your content. One way uses s3.amazonaws.com host name URLs, such as this: http://s3.amazonaws.com/mybucket.mydomain.com/myObjectKey The other way to access your S3 content uses a virtual host name in the URL: http://mybucket.mydomain.com.s3.amazonaws.com/myObjectKey Both of these URLs map to the same object in S3. You can make the virtual host name URL shorter by setting up a DNS CNAME that maps mybucket.mydomain.com to mybucket.mydomain.com.s3.amazonaws.com. With this DNS CNAME alias in place, the above URL can also be written as follows: http://mybucket.mydomain.com/myObjectKey This shorter virtual host name URL works only if you setup the DNS CNAME alias for the bucket.
563e30f161a8013065267450	X	You can use this link or you can upload via s3 commandline tool aws.amazon.com/cli
563e30f161a8013065267451	X	I am working on a web application using angular js (for UI) and java (@back end) in which i am creating a user interface through which a user can manipulate data in S3 buckets. I need to upload data to buckets which are of size about 500 mb. I am currently being able to send 5mb chunks of data to servlets but i am not able to combine those chunked objects to upload my original data to S3. Is there any alternative way for achieving this?
563e30f161a8013065267452	X	Take a look at https://github.com/minio/minio-java. It has minimal set of abstracted API's implementing most commonly used S3 calls. Here is an example of streaming upload. putObject() here is a fully managed single function call for file sizes over 5MB it automatically does multipart internally and once complete Amazon S3 will present it as a single object. You can resume a failed upload as well and it will start from where its left off by verifying previously upload parts. With this putObject() you can upload upto 5TB single large object (5TB is maximum single object size on Amazon S3)
563e30f161a8013065267453	X	TY, wow, that is pretty cool.
563e30f161a8013065267454	X	Thanks, exactly what I was looking for. I would give you more votes if I could :)
563e30f161a8013065267455	X	Beautiful! Just Beautiful!
563e30f161a8013065267456	X	I am pretty confused and lost in all the documentation. What I want to do is store a number of objects online (less than 20 Mb). The app that I am developing should be able to retrieve those objects ( preferable JSON but XML would do) and perhaps upload other such objects. From what I see, the Google Cloud storage is not free. Is there anything that does what I need and is also free? If not, can you point me to the right documentation to achieve what I want? I have been looking into the topic for a couple of days and I am still lost between buckets, the Cloud JSON API, Cloud Storage, all these being new to me. Any help would be much appreciated!
563e30f261a8013065267457	X	If you have just 20 MB of data then you may use Parse for storing data upto 1Gb for free. Also see https://parse.com/products/data. I heard from those who used it, that the API is also straight forward.
563e30f261a8013065267458	X	Not sure if this is exactly what you are looking fore, but dropbox provides a great android api: http://www.dropbox.com You can upload/download/modify files pretty easily. However, this would not be the best solution onless you want to give user's the ability to sign into their own dropbox and save/modify files on a per-user basis. It seems like you might be looking for something more like Amazon s3 storage: http://aws.amazon.com/s3/ . It is a very popular cloud storage provider, and you get 5 gigs free (I think), plus it should be simple to integrate with android. see here amazon S3 and android and here: http://aws.amazon.com/pt/sdkforandroid/
563e30f361a8013065267459	X	What's your question then? "Explain everything I mentioned in the previous four paragraphs?"
563e30f361a801306526745a	X	these are 2 task: uploading form mac to server. upload from server to s3. both have been answered.
563e30f361a801306526745b	X	that's what I was looking for! thanks!
563e30f361a801306526745c	X	I have asked this question before, but it was deleted due too little information. I'll try to be more concrete this time. I have an Objective-C mac application, which should allow users to upload files to S3-storage. The s3 storage is mine, the users don't have an Amazon account. Until now, the files were uploaded directly to the amazon servers. After thinking some more about it, it wasn't really a great concept, regarding security and flexibility. I want to add a server in between. The user should authenticate with my server, the server would open a session if the authentication was successful, and the file-sharing could begin. Now my question. I want to upload the files to S3. One option would be to make a POST-request and wait until the server would receive the file. Problems here are, that there would be a delay, when the file is being uploaded from my server to the S3 servers, and it would double the uploading time. Best would be, if I could validate the request, and then redirecting it, so the client uploads it directly to the s3-storage. Not sure if this is possible somehow. Uploading directly to S3 doesn't seem to be very smart. After looking into other apps like Droplr and Dropmark, it looks like they don't do this. Btw. I did this using Little Snitch. They have their api on their own web-server, and that's it. Could someone clear things up for me? EDIT How should I transmit my files to S3? Is there a way to "forward" it, or do I have to upload it to my server and then upload it from there to S3? Like I said, other apps can do this efficiently and without the need of communicating with S3 directly.
563e30f361a801306526745d	X	If authentication is managed on your server, but you don't want two uploads, then look into AWS/IAM temporary security credentials. http://aws.amazon.com/releasenotes/Java/5316957573949696 In short:
563e30f361a801306526745e	X	you should provide what you have achieved till now so we can give the correct suggestions.
563e30f361a801306526745f	X	Apologies if I didn't seem specific enough... it was more that what I had done already made it so all the images locally cached perfectly, but those elsewhere didn't. Someone below pointed out the obvious point I had missed where I cannot control the cache of another site, so I had to do it within S3 itself.
563e30f361a8013065267460	X	Thanks a lot. I know it sounds stupid, but I hadn't thought that I'd have to cache it on the S3 server. Sounds really obvious now!!
563e30f361a8013065267461	X	The author of the question stated that they're using IIS which uses primarily Web.config file for configuration and probably not .htaccess file (unless using something like Helicon Ape).
563e30f361a8013065267462	X	Thanks for the reply, but as Tom said, this project is in the Microsoft world.
563e30f361a8013065267463	X	I've been able to correctly (I think) enable caching on IIS. The only problem now is that when I run Google's PageSpeed Insights it still says Setting an expiry date or a maximum age in the HTTP headers for static resources instructs the browser to load previously downloaded resources from local disk rather than over the network. But all of the suggestions are external images. I am using Amazon's S3 to externally host images (linking to direct URLs, as < img src="http://s3.amazon.com......."/>. Is there a way I can "leverage browser caching" for these external images? Thanks in advance. Andy
563e30f361a8013065267464	X	Yes, with Amazon S3 you can still set the Expires header of the objects stored in the bucket. You will have to set this header when storing the object so there are two ways: If you use the API you can do something like http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html For the second case maybe this link will help: http://www.newvem.com/how-to-add-caching-headers-to-your-objects-using-amazon-s3/ Hope this helps.
563e30f461a8013065267465	X	Have you tried: in your .htaccess file, hope this helps.
563e30f461a8013065267466	X	It should be noted that the EC2 AMI tools are only for dealing with the older style instance-store (S3 based) AMIs. The tools for the newer, more popular EBS boot AMIs are in the EC2 API tools (ec2-register, ec2-deregister, ec2-create-image). The documentation from Amazon is not clear on this distiction.
563e30f461a8013065267467	X	@EricHammond.. Thanks for pointing that out!
563e30f461a8013065267468	X	The developer tools page here list two set of tools for Amazon EC2 What are the differences between the two set of tools?
563e30f461a8013065267469	X	The API tools serve as the client interface to the Amazon EC2 web service. Use these tools to register and launch instances, manipulate security groups, and more The Amazon EC2 AMI Tools are command-line utilities to help bundle an Amazon Machine Image (AMI), create an AMI from an existing machine or installed volume, and upload a bundled AMI to Amazon S3. From the definitions shown at Developer Tools. So, if you want to manage instances, use API tools; if you want to build and upload an AMI (Amazon Machine Image), use AMI tools.
563e30f461a801306526746a	X	Here are the lines from Amazon Documentation: The Amazon EC2 command line interface tools (also called the CLI tools) wrap the Amazon EC2 API actions. These tools are written in Java and include shell scripts for both Windows and Linux/UNIX/Mac OSX. Note Alternatively, you can use the AWS Command Line Interface (AWS CLI), which provides commands for a broad set of AWS products, including Amazon EC2. To get started with the AWS CLI, see the AWS Command Line Interface User Guide. For more information about the AWS CLI commands for Amazon EC2, see ec2 in the AWS Command Line Interface Reference. Before you can use the Amazon EC2 CLI tools, you need to download them and configure them to use your AWS account. You can set up the tools on your own computer or on an Amazon EC2 instance. http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/set-up-ec2-cli-windows.html
563e30f561a801306526746b	X	I feel that this question is too broad as it is presently worded. You can do your own research to find out the pros and cons of both of these easily enough yourself. If you have a specific question, that good for StackOverflow.
563e30f561a801306526746c	X	The default bucket in Cloud Storage is also free up to 5GBs. And above that, as of now, they both cost the same: 0.026$ / GB / Month.
563e30f561a801306526746d	X	reads and writes to the BlobStore cost after a point, CloudStore they don't you just pay for storage over the 5GB limit. It is pretty clear that Google wants to deprecate the Blobstore for the CloudStore and they are making the CloudStore the cheaper option to accomplish this. as well as removing that ridiclous upload url call back requirement for the Blobstore. GCS is plain old PUT/POST to a Servlet or writing to a stream from the server side.
563e30f561a801306526746e	X	Actually Blobstore is better when it comes to images , because can easily be served resized images to many apps and can be served only one resized image at a time from Cloud Storage. ( cloud.google.com/appengine/docs/java/images/… ) Also the fact that when using Blobstore the request is not going through the server but the image is served directly to the client app. These two reasons are making Blobstore better for me. (also Blobstore now is without size limit)
563e30f561a801306526746f	X	Given that they have "extracted" the App Engine Datastore into a self-contained Cloud Service, it seems clear they are moving towards an AWS-like model of discrete services you can use separately or together. Google Cloud Storage already fills that role and is actively promoted; whereas Blobstore is seemingly redundant.
563e30f561a8013065267470	X	Blobstore file size limits were completely removed August 2011 code.google.com/p/googleappengine/issues/detail?id=2560
563e30f561a8013065267471	X	We have our application hosted on Google app engine for Java and we have a requirement where we want to store the blood donor appreciation certificates (html files) somewhere. So, we can use either Google blob store or Google cloud storage. I know both of these solutions are feasible to do using gae for Java However, the problem for us is to decide which one to use. What are the pros and cons of these two approaches? We are a non profit and cannot pay a lot.
563e30f561a8013065267472	X	Better use the Blobstore. Is has free 5 GB space (as of March 2012). The cloud storage is paid service. The App Engine blobstore is like Amazon S3, but less flexible. It has HTTP-based API and Java / Python APIs (see http://code.google.com/appengine/docs/java/blobstore/overview.html).
563e30f561a8013065267473	X	If you're starting a new project, I would go with Cloud Storage. It seems that Google is pushing their Cloud Storage platform harder than their blobstore platform. For example, currently programmatically writing files to the blobstore is deprecated but is supported by Cloud Storage. I can't read the future, but I would bet that Google will be deprecating more and more of the blobstore API in favor of the Cloud Storage API, which will lead to headaches down the road.
563e30f561a8013065267474	X	First, I'd say that if your HTML file(s) are small (or could be small via gzip compression), then just store it as a BlobProperty in the datastore and add meta-data properties so you can retrieve it appropriately later. If this is not an option, then perhaps consider the future growth of your application. The two big things Cloud Storage has over the Blobstore are 1) accessibility by third-party and 2) no file size restrictions. If, however, you KNOW that these two things will never need to be addressed for your application, then just stick with Blobstore.
563e30f661a8013065267475	X	The only problem I see with this is that I'd need some way to check if that directory already existed before making it, otherwise it'd overwrite the directory.
563e30f661a8013065267476	X	You can check with ftp_nlist, but I would just try to change into the directory and create it if that fails.
563e30f661a8013065267477	X	I am launching a web application soon that will be serving a fair amount of images so I'd like to have a main web server and a static content server and possibly a separate database server later on. I'd like the user to: The problem is I don't know how to have the user instantly upload an image to a separate server. I thought about using amazon s3, but you can't edit filenames before posting them. (through POST, I'd rather not use the REST api) I could also use php's ftp function to upload to a separate server, but I'd like to dynamically create folders based on the properties of the image (so I don't have all the images in one big folder obviously), but I don't know how this would work if I used ftp... Or I could save them locally and use a CDN, I'm not too familiar with CDN's so I don't know if using them this way would be appropriate or cost-effective. What are my options here? I'd like the images to be available instantly (no cron jobs/queues) Thanks.
563e30f661a8013065267478	X	You can create directories over FTP with PHP, so that should not be a showstopper.
563e30f661a8013065267479	X	I thought about using amazon s3, but you can't edit filenames before posting them. (through POST, I'd rather not use the REST api) If you let your PHP server do the uploading to S3 via POST, you can name the files whatever you want. You should do that anyway, letting your users upload to S3 directly, without your PHP code inbetween, sounds like bad for security to me.
563e30f661a801306526747a	X	How do you iterate over all albums?
563e30f661a801306526747b	X	I am somewhat desperate posting images via the graph api to a user's page. What I want to achieve is: I've got an image in an amazon S3 bucket. I'm using the /userid/photos endpoint to post the image url and the message property like: Now, the API creates an album called: "AppName Photos". When I post multiple time to the same album, it suddenly arranges all images in a view, but I want them to be seperate posts each by each. So, what I want to achieve is: Post an image to a user's page without putting it into an album, as if the user uploaded the image hisself. www.klout.com is able to do this. When I try to share & upload a custom image, the image is being stored in the "Chronic photos". How can I store the image there and not in an app-dependent album?
563e30f661a801306526747c	X	Got it. Simply iterate over all albums (you need user_photo access) and post to that specific album of type "wall". It is not the most beautiful solution I've ever seen but at least it works and prevents grouping nicely :)
563e30f661a801306526747d	X	Is there anybody here?
563e30f661a801306526747e	X	I'm working on a Rails3 project with video encoding through Zencoder, assets are stored on Amazon S3. I'm creating 5 thumbnails for each video encoded, and each PNG (same ratio as the video: 620x465) weight about 600 Ko. Is it possible to preserve ratio but optimize the weight ? I can set the format to JPG but I can't see any other option in Zencoder API: https://app.zencoder.com/docs/api/encoding/thumbnails Thx !
563e30f661a801306526747f	X	Here's the Zencoder support answer: This is something that we're hoping to add in the future, though I don't know of a definite timeline right now. Keep an eye on our blog and newsletter and we'll announce it when the option is available. I hope they will soon :-)
563e30f861a8013065267480	X	This error shouldnt be happening if you followed step #7. Are you sure it finished successfully?
563e30f861a8013065267481	X	I kind of messed up and installed rails before number #7, and then went back to install #7 (and I tried install rails command again, but it didn't really do anything)
563e30f861a8013065267482	X	Finish #7 and then gem install aws-sdk should work. Does rails not work either? I dont know why you need aws-sdk so I can't answer that question. You certainly dont NEED it for running Rails on EC2.
563e30f861a8013065267483	X	[I just uninstalled it, and reinstalling rails... ]
563e30f861a8013065267484	X	I'm not sure if rails works... i'm totally new at this. I want to get to "hello world" but i'm dying here... I'm not really sure why I need aws-sdk either...
563e30f861a8013065267485	X	I've recently followed this guide for installing rails on my EC2 server. What I'm trying to figure out now is how do I install the rails aws-sdk (do I even need to?). When I run "gem install aws-sdk" I get the following error: gem install aws-sdk I'm not really sure what to do from here....
563e30f861a8013065267486	X	You don't need the aws-sdk gem unless you are making API calls directly to AWS from your Rails application. This can be common if you want to upload files to Amazon S3, connect to Amazon DynamoDB, or send emails with Amazon Simple Email Service. If you are going to start using the aws-sdk, I would strongly recommend using the version 2 SDK. It has a single dependency on a pure Ruby gem, so you would not be experiencing these gem install failures.
563e30f861a8013065267487	X	I'm using Amazon's simple storage service (S3). I noticed that others like Trello were able to configure sub-domain for their S3 links. In the following link they have trello-attachments as sub-domain. https://trello-attachments.s3.amazonaws.com/.../.../..../file.png Where can I configure this?
563e30f861a8013065267488	X	You don't have to configure it. All buckets work that way if there are no dots in the bucket name and it's otherwise a hostname made up of valid characters. If your bucket isn't in the "US-Standard" region, you may have to use the correct endpoint instead of ".s3.amazonaws.com" to avoid a redirect (or to make it work at all). An ordinary Amazon S3 REST request specifies a bucket by using the first slash-delimited component of the Request-URI path. Alternatively, you can use Amazon S3 virtual hosting to address a bucket in a REST API call by using the HTTP Host header. In practice, Amazon S3 interprets Host as meaning that most buckets are automatically accessible (for limited types of requests) at http://bucketname.s3.amazonaws.com. — http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html
563e30f961a8013065267489	X	I am afraid the total operation could not be done within 60 seconds... Is it possible to use other libraries to handle this like urllib2?
563e30f961a801306526748a	X	No, all HTTP communications in app engine are done via GAE provided libraries, even if you use some other library it's all subject to the 60 second timeout limit.
563e30f961a801306526748b	X	when you use urllib2 etc in GAE you are really using Google versions btw.
563e30f961a801306526748c	X	OK. I guess maybe I should separate the file uploading step.
563e30f961a801306526748d	X	I had a create a REST API using bottle framework, which receives calls from GAE. Once this REST API is invoked, it did some calculations and sent outputs as a zip file to AMAZON S3 server and return the link to GAE. Everything works fine expect the timeout issue. I tried to adjust the deadline of urlfetch to 60 seconds, which did not solve the problem. I appreciate any suggestions. GAE side: Broser error info.: REST server: Errors from the REST server:
563e30f961a801306526748e	X	The deadline is a maximum value, once reached it'll fail. And it's failing with Deadline exceeded while waiting for HTTP response So you should try to catch that exception and try again. If the entire operation can't be done in under 60 seconds then there is nothing else to be done, it's a hard limit in GAE that HTTP requests can't exceed 60 seconds.
563e30f961a801306526748f	X	This question might be helpful: stackoverflow.com/questions/701545/…
563e30f961a8013065267490	X	That link is dead, by the way.
563e30f961a8013065267491	X	Sorry: developer.amazonwebservices.com/connect/…
563e30f961a8013065267492	X	If you're lazy like me, Newvem basically does this on your behalf and aggregates/tracks the results on a per-bucket level across your S3 account.
563e30f961a8013065267493	X	@rcoup This newvem.com link is broken.
563e30f961a8013065267494	X	doesn't work if files are in sub folders.
563e30fa61a8013065267495	X	It counted files in subfolders for me...
563e30fa61a8013065267496	X	The -r in the command is for --recursive, so it should work for sub-folders as well.
563e30fa61a8013065267497	X	Why did you resurrect a 5-year-old question to post a poorly formatted copy of an existing answer?
563e30fa61a8013065267498	X	The previous answer piped the output into a txt file unnecessarily.
563e30fa61a8013065267499	X	IMO this should be a comment on that answer, then. This is a really trivial difference.
563e30fa61a801306526749a	X	Seems like a worthy answer- especially since the selected answer for this question starts with 'There is no way...' and @mjsa has provided a one-line answer.
563e30fa61a801306526749b	X	For some reason, the ruby libs (right_aws/appoxy_aws) won't list more than the the first 1000 objects in a bucket. Are there others that will list all of the objects?
563e30fa61a801306526749c	X	When you request the list, they provide a "NextToken" field, which you can use to send the request again with the token, and it will list more.
563e30fa61a801306526749d	X	Unless I'm missing something, it seems that none of the APIs I've looked at will tell you how many objects are in an S3 bucket / folder(prefix). Is there any way to get a count?
563e30fa61a801306526749e	X	There is no way, unless you list them all in batches of 1000 (which can be slow and suck bandwidth - amazon seems to never compress the XML responses), or log into your account on S3, and go Account - Usage. It seems the billing dept knows exactly how many objects you have stored! Simply downloading the list of all your objects will actually take some time and cost some money if you have 50 million objects stored. Also see this thread about StorageObjectCount - which is in the usage data. An S3 API to get at least the basics, even if it was hours old, would be great.
563e30fa61a801306526749f	X	If you use the s3cmd command-line tool, you can get a recursive listing of a particular bucket, outputting it to a text file. Then in linux you can run a wc -l on the file to count the lines (1 line per object).
563e30fa61a80130652674a0	X	Go to AWS Billing, then reports, then AWS Usage reports. Select Amazon Simple Storage Service, then Operation StandardStorage. Then you can download a CSV file that includes a UsageType of StorageObjectCount that lists the item count for each bucket.
563e30fa61a80130652674a1	X	In s3cmd, simply run the following command (on a Ubuntu system): s3cmd ls -r s3://mybucket | wc -l
563e30fb61a80130652674a2	X	None of the APIs will give you a count because there really isn't any Amazon specific API to do that. You have to just run a list-contents and count the number of results that are returned.
563e30fb61a80130652674a3	X	Old thread, but still relevant as I was looking for the answer until I just figured this out. I wanted a file count using a GUI-based tool (i.e. no code). I happen to already use a tool called 3Hub for drag & drop transfers to and from S3. I wanted to know how many files I had in a particular bucket (I don't think billing breaks it down by buckets). I had 20521 files in the bucket and did the file count in less than a minute.
563e30fb61a80130652674a4	X	The api will return the list in increments of 1000. Check the IsTruncated property to see if there are still more. If there are, you need to make another call and pass the last key that you got as the Marker property on the next call. You would then continue to loop like this until IsTruncated is false. See this Amazon doc for more info: Iterating Through Multi-Page Results
563e30fb61a80130652674a5	X	3Hub is discontinued. There's a better solution, you can use Transmit (Mac only), then you just connect to your bucket and choose Show Item Count from the View menu.
563e30fb61a80130652674a6	X	I used the python script from scalablelogic.com (adding in the count logging). Worked great.
563e30fb61a80130652674a7	X	Using new aws CLI, it's possible now.
563e30fb61a80130652674a8	X	There is an easy solution with the S3 API now (available in the AWS cli): or for a specific folder:
563e30fb61a80130652674a9	X	The problem with using OFFSET is that it requires the server to iterate over the entire result set before the "LIMIT", so it breaks down for very large data sets. I've actually used an API that did this and we had major problems getting the data after about 100,000 results.
563e30fb61a80130652674aa	X	Following up from my previous question: Using "Cursors" for paging in PostgreSQL What is a good way to provide an API client with 1,000,000 database results? We are currently using PostgreSQL. A few suggested methods: What haven't I thought of that is stupidly simple and way better than any of these options?
563e30fb61a80130652674ab	X	The table has a primary key. Make use of it. Instead of LIMIT and OFFSET, do your paging with a filter on the primary key. You hinted at this with your comment: Paging using random numbers ( Add "GREATER THAN ORDER BY " to each query ) but there's nothing random about how you should do it. Allow the client to specify both parameters, the last ID it saw and the number of records to fetch. Your API will have to either have a placeholder, extra parameter, or alternate call for "fetch the first n IDs" where it omits the WHERE clause from the query, but that's trivial. This approach will use a fairly efficient index scan to get the records in order, generally avoiding a sort or the need to iterate through all the skipped records. The client can decide how many rows it wants at once. This approach differs from the LIMIT and OFFSET approach in one key way: concurrent modification. If you INSERT into the table with a key lower than a key some client has already seen, this approach will not change its results at all, whereas the OFFSET approach will repeat a row. Similarly, if you DELETE a row with a lower-than-already-seen ID the results of this approach will not change, whereas OFFSET will skip an unseen row. There is no difference for append-only tables with generated keys, though. If you know in advance that the client will want the whole result set, the most efficient thing to do is just send them the whole result set with none of this paging business. That's where I would use a cursor. Read the rows from the DB and send them to the client as fast as the client will accept them. This API would need to set limits on how slow the client was allowed to be to avoid excessive backend load; for a slow client I'd probably switch to paging (as described above) or spool the whole cursor result out to a temporary file and close the DB connection. Important caveats:
563e30fb61a80130652674ac	X	Have the API accept an offset to start from and the number of records to return. This is a sort of paging where the client can determine how many records to return in one page request. The API should also return the total number of records possible for the query so the client knows how many "pages", or optionally it can derive when it has retrieved the last records when the number of records returned is zero or less than the number of records requested. You can control this in your PostgresSQL query by using the OFFSET clause (which record to start retrieving at) and the LIMIT clause (number of records to return) in your SELECT statement.
563e30fb61a80130652674ad	X	I'm writing this application that uses some information from a website and I'm using PhantomJs to extract this information. Now I want the user to be able to run my application without the need of PhantomJs in their system. That way it'll be more like a service call. I have followed the following guide: http://ariya.ofilabs.com/2012/07/cloud-phantomjs-with-ironworker.html To get PhantomJs working and getting the information that I need for some site, now I can queue a worker and get the result in the log using Iron.io's web interface. I would like to know if there is a way to get the result of the execution programmatically. I have taken a look at the API, but I need to authenticate and I also need to provide a different task ID ( which I don't know how to get ).
563e30fc61a80130652674ae	X	Queue task => obtain task_id as result of operation. But there are plenty of different ways to get result: read task log via api (you need project_id, token, task_id), Store data to Amazon S3, push information to some kind of queue, touch own api, send info to webhook, write information to database etc
563e30fc61a80130652674af	X	Thanks. I will evaluate this option against proxying a request to s3 (and thus avoiding the latency from the redirect, which I suspect won't be acceptable). Other options always welcome!
563e30fc61a80130652674b0	X	So, I'm writing a web application in node.js where users can upload photos, and they can specify some access control settings on every photo (public, private, friends only). I then check the users' session key on every request to ensure that they have access. If they do, I send them the file by opening it using fs and piping it to the response object. However, when I benchmark this with apachebench, I get around 1500 requests per second. If I remove all the database stuff, it doesn't get much faster. By comparison, Nginx serves 17000 requests per second on the same photo. Obviously this order-of-magnitude difference is going to be a huge cost problem if my service takes off. Is there a better way to control access while preserving static-like performance, apart from making them all public? Edit: realistically, the file is going to be hosted on S3, not in the filesystem. So node will be acting less as a static fileserver and more as an http proxy, which I suspect it will be much better at.
563e30fc61a80130652674b1	X	Use an S3 signed URL. A signed URL is a temporary URL for private files that you can send to a single user that references an S3 object. You can also put an expiration time on a signed URL so it doesn't stick around forever. So the flow would look like this: Here's a related blog post: Amazon S3 Signed URLs with NodeJS.
563e30fc61a80130652674b2	X	It's deprecated because it has no use case and is pending removal in future releases. Would you please tell me what you need it for?
563e30fc61a80130652674b3	X	Yes, the code uses it to get the amazon server date and time and uses that combination for a sync service whenever the app comes online.
563e30fc61a80130652674b4	X	My question is if they are deprecating a method, there should be some replacement for that right?? What is it in this case?? Or you are saying like they have decided the method is useless and about to remove it in future release??
563e30fc61a80130652674b5	X	Thanks Yangfan. You really explained it well. As you have said, I am now using the formatRFC822Date method from Dateutils in AWS SDK to get the date and time from the server. Thank you again.
563e30fc61a80130652674b6	X	I have been using Android AWS SDK 1.4.6 version to connect with S3object of Amazon web service. I have updated my SDK to 2.2.1 last day and found gethttprequest() method to be deprecated. This method is in com.amazonaws.services.s3.model.S3ObjectInputStream package currently. However I am not able to find a replacement for that method in documentation too. Do anyone know, which method to use as a replacement for gethttprequest()? or any useful links could do too. Thanks in advance.
563e30fc61a80130652674b7	X	There are two reasons that lead to the deprecation: The server date or time from the response isn't part of the Amazon S3 APIs. It should be correct, but I suggest you not rely on it. There are many ways to get the date or time, say NPT time server or from system.
563e30fc61a80130652674b8	X	I am using fineUploader in one of my project on MEAN.IO , I am able to configure fineUploader with angular so as to make a request on server to upload file to S3 bucket, which is working fine . What my requirement is I want to send the bucket owner a zip download link of all the files that uploaded in a particular session on S3. So that bucket owner can just download the zip of those file from the mail only . I go through fineUploader document but could not find anything specific . I also google the solution but don't get any idea on how to approach for this . Any suggestion or link to read will be very helpful
563e30fc61a80130652674b9	X	Here's how you would do this: This is a server-side process, Fine Uploader is out of the picture.
563e30fd61a80130652674ba	X	Instead of creating separate files, you can "prepare" a single file by using ftruncate() with the desired size and then write each section inside using fseek() followed by stream_copy_to_stream() :)
563e30fd61a80130652674bb	X	Oh, interesting point! Didn't think about that one.
563e30fd61a80130652674bc	X	Hope that gives you enough ideas to venture into this yourself ;-)
563e30fd61a80130652674bd	X	What would be the best/most efficient way to store the file parts on the backend if I am to write this myself (I'm still thinking there's got to be a library for this, but I'm prepared to do the dirty work if there isn't). Better to use a database or raw files, for example? I did read the client spec from Amazon so I understand how it works, I'm more interested in a library or else what would be the best way to implement the server.
563e30fd61a80130652674be	X	Once the data is transferred you will recombine them and store as a single file as opposed to multiple parts. In terms managing the files transferred to the backend and storing them in either a db or a filesystem would require more context about the data and the requirements. In terms of libraries I have not come across anything specific so far.
563e30fd61a80130652674bf	X	You can take a look at the multipart implementation of boto github.com/boto/boto/blob/develop/boto/s3/multipart.py
563e30fd61a80130652674c0	X	Thanks for your help!
563e30fd61a80130652674c1	X	Amazon S3 has a very nice feature that allows the upload of files in parts for larger files. This would be very useful to me if I was using S3, but I am not. Here's my problem: I am going to have Android phones uploading reasonably large files (~50MB each of binary data) on a semi-regular basis. Because these phones are using the mobile network to do this, and the coverage is spotty in some of the places where they're being used, strong signal cannot be guaranteed. Therefore, doing a simple PUT with 40MB of data in the content body will not work very well. I need to split up the data somehow (probably into 10MB chunks) and upload them whenever the signal will allow it. Once all of the chunks have been uploaded, they need to be merged into a single file. I have a basic understanding of how the client needs to behave to support this through reading Amazon's S3 Client APIs, but have no idea what the server is doing to allow this. I'm willing to write the server in Python or PHP. Are there any libraries out there for either language to allow this sort of thing? I couldn't find anything after about one hour of searching. Basically, I'm looking for anything that can help point me in the right direction. Information on this and what protocols and headers to use to make this as RESTful as possible would be fantastic. Thanks!
563e30fd61a80130652674c2	X	From the REST API documentation for multi-part upload it seems that Amazon expects the client to break the large file into smaller multiple parts and upload them individually. Prior to uploading you need to obtain an upload id and on every upload you include the upload id and the a part number for the portion of the file being uploaded. The way you may have to go about structuring is to create a client which can split a huge file into multiple parts and upload them in parallel using the above specified convention.
563e30fd61a80130652674c3	X	I'm having the same thoughts currently. Do you have any recommendations, two years later?
563e30fd61a80130652674c4	X	@uval I got a chance to talk to top AWS guys (including Werner Vogels) about this. And the answer is that the Android SDK was something they're not recommending themselves. It is not suited for serious work!
563e30fd61a80130652674c5	X	So I understand that you eventually got convinced to drop the idea. It could have been a great advantage indeed. Thanks!! {And thanks to StackOverflow for making this kind of interaction possible!}
563e30fd61a80130652674c6	X	The Amazon Access Key here would be like an access login for the app, and since there is no server/web service running and all business logic happens through the app, the bullet points 1 & 3 does not have relevance here. As for threats, I meant by hackers.
563e30fd61a80130652674c7	X	Amazon Cloud Services (AWS) has provided the ready to use Library to make calls to SDB, S3, SNS etc right from your Android app. This makes it really easy for a mobile developer who is not familiar with web services and web applications to create a completely scalable cloud based app. We give the Amazon Access Credentials in these API calls to connect to our cloud Account; My question is:
563e30fd61a80130652674c8	X	I talked to the Amazon Advocate for our region and he told that Amazon client library is not designed for such a purpose. Obviously, I was not very convinced. I think an entire client library to Amazon communication (bypassing the need for a webserver) could be a great advantage for Mobile devs.
563e30fe61a80130652674c9	X	Re: Would hard coding the Amazon Access Credentials inside the code (as a field Constant etc) make it vulnerable to extraction? Via decompiling etc.? Yes, by looking for strings and patterns in the binary. Also decompiling, but that'd often not be necessary. The first question is, what sort of threats are you trying to protect against? Governments? Paid hackers? Or you just want to make it not easy to gain access other than via the app?
563e30fe61a80130652674ca	X	Thak you for your detailed response. This code work for me, when I changing x.Size == 0 on x.Size == 1. But it does not recognize all folder, only a small part of them. I can't understand why that's happening. I'm exactly that my cycle goes over and over again until it reaches the end. But I can't get all of the folders. What can be the reason?
563e30fe61a80130652674cb	X	Maybe related to you changing Size to 1? 'Folders' are presented in this list call as S3 objects with no contents: they should be Size 0. You could simply remove size from the predicate if you're finding the objects you need without it.
563e30fe61a80130652674cc	X	If you're still having trouble after that, maybe make sure your ListObjectsRequest isn't limiting your request too much. For example, if you specified a prefix... test your code without it to make sure it isn't limiting your request too much. If that doesn't work, feel free to ask a new question (this one is very clear as-is, we don't want to clutter it up) with an example of your bucket's structure and the request you're trying to make.
563e30fe61a80130652674cd	X	If I tried to delete size comparison, I have the same result when I write x.Size == 1. Can you look my code with my results? codeshare.io/4eBiJ These are how it must look: snag.gy/HbLhK.jpg
563e30fe61a80130652674ce	X	request.Delimiter is unnecessary and may be causing trouble, try it without that. It is interesting that BF/Music/ seems to be getting skipped outright; could you do a sanity check and make sure it exists as you expect in the S3 bucket? As for the subdirectories, if you don't want them you can simply filter your list further after retrieval via regex or matching a single slash, etc.
563e30fe61a80130652674cf	X	All that I found, it's this method GET Bucket But I can't understand how can I get only a list of folders in the current folder. Which prefix and delimiter I need to using? Is that possible at all?
563e30fe61a80130652674d0	X	For the sake of example, assume I have a bucket in the USEast1 region called MyBucketName, with the following keys: Working with folders can be confusing because S3 does not natively support a hierarchy structure -- rather, these are simply keys like any other S3 object. Folders are simply an abstraction available in the S3 web console to make it easier to navigate a bucket. So when we're working programatically, we want to find keys matching the dimensions of a 'folder' (delimiter '/', size = 0) because they will likely be 'folders' as presented to us by the S3 console. Note for both examples: I'm using the AWSSDK.S3 version 3.1 NuGet package. Example 1: All folders in a bucket This code is modified from this basic example in the S3 documentation to list all keys in a bucket. The example below will identify all keys that end with the delimiter character /, and are also empty. Expected output to console: Example 2: Folders matching a specified prefix You could further limit this to only retrieve folders matching a specified Prefix by setting the Prefix property on ListObjectsRequest. When applied to Example 1, we would expect the following output: Further reading:
563e30fe61a80130652674d1	X	Using prefix of the/path/to/read/ (note that there is no leading slash, but there is a trailing slash), and delimiter of /, you'll find all the folders within that folder inside <CommonPrefixes>. CommonPrefixes A response can contain CommonPrefixes only if you specify a delimiter. When you do, CommonPrefixes contains all (if there are any) keys between Prefix and the next occurrence of the string specified by delimiter. In effect, CommonPrefixes lists keys that act like subdirectories in the directory specified by Prefix. For example, if prefix is notes/ and delimiter is a slash (/), in notes/summer/july, the common prefix is notes/summer/. All of the keys rolled up in a common prefix count as a single return when calculating the number of returns. See MaxKeys. http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e30fe61a80130652674d2	X	Alternatively another simpler approach is using https://github.com/minio/minio-dotnet Minio .Net implements minimal API's to work with Amazon S3 and other compatible storage solutions. Following example shows how you can filter out only directories. Here the CommonPrefix is abstracted as a folder through the ListObjects() API.
563e30fe61a80130652674d3	X	I'm having the same problem! I'm using the same set of resources (the blog and model code), and having the exact same result. About half the time, the request gives ERR_CONNECTION_RESET with no other information.
563e30fe61a80130652674d4	X	Thanks Joan. I have an active question in with AWS, but it requires me to replicate my browser code as a stand-alone process for them to debug -- which requires some development.
563e30fe61a80130652674d5	X	Let me know if you want help putting something together for AWS. The sooner I get this straightened out, the better. I'd be happy to work with you on github (or alternative), if that would be useful.
563e30fe61a80130652674d6	X	Just wanted to add a +1. I'm experiencing the same issue, having largely followed this guide for downloading from a Angular/coffeescript front end for my Django app.
563e30ff61a80130652674d7	X	Update added -- no errors when using an EC2 instance as a client.
563e30ff61a80130652674d8	X	I am PUTting files to S3 via ajax requests and about 50% of the time I get ERR_CONNECTION_RESET errors. I know the requests are signed correctly -- any ideas what may be causing this? Again, this is an intermittent problem that I see from multiple locations and machines. Here is the relevant coffeescript code I am using to PUT my files to S3. It is derived from Micah Roberson's and Rok Krulec's work at http://micahroberson.com/upload-files-directly-to-s3-w-backbone-on-heroku/ and http://codeartists.com/post/36892733572/how-to-directly-upload-files-to-amazon-s3-from-your. Update I've been getting very attentive support from Amazon on this issue. Per their suggestion, I created an EC2 Windows instance, loaded the Chrome browser on it, and attempted to upload 5 files 10 times with my code. I did not see the error once. I did see some SignatureDoesNotMatch errors occasionally, but not a single ERR_CONNECTION_RESET error. I am still seeing ERR_CONNECTION_RESET errors though on every non-EC2 client/network location I use. Update Still no solution here. I have moved from using a self-rolled signing algorithm to one provided by boto. No impact on the ERR_CONNECTION_RESET issue though.
563e30ff61a80130652674d9	X	I finally gave up on getting this to work. Instead, I am now using Fine Uploader to provide this functionality.
563e30ff61a80130652674da	X	I suppose this problem has no decision. Try a POST request: https://aws.amazon.com/articles/1434 Use FormData to append fields, if you prefer AJAX sending To sign a request in nodejs: Amazon S3 POST api, and signing a policy with NodeJS
563e30ff61a80130652674db	X	Isn't it unRESTful that in this case the Location for PUT will not be valid anymore afterwards? Normally if you PUT something, you'd expect to be able to GET it after.
563e30ff61a80130652674dc	X	I think there's no such expectation. At any point in time (also right after the PUT) could another client issue DELETE on the particular resource. The "PUT, GET" sequence you describe does not happen within a transaction or so.
563e30ff61a80130652674dd	X	To be more precise, I think it could be said that there's an "expectation" that right after a PUT, a GET would return the entity that has been PUT there, yet there's no requirement for this to be the case. If the expectation of the client isn't met, this indicates that another client has been fussing with the resource.
563e30ff61a80130652674de	X	Please note that it might take a while before I try this implementation. REST (pun intended) assured that I will get back to accept the answer if it turns out satisfactory.
563e30ff61a80130652674df	X	How about upvoting it at as useful? It's useful, right?
563e30ff61a80130652674e0	X	I'm creating a REST API for a backup service, which in principle is quite simple: Now I'd like to greatly reduce traffic to my server by delegating up- and downloads to a service like Amazon S3. Redirecting downloads is not a problem, since I could just perform a regular redirect (301 or 307?) to some generated expiring URL. But what about the uploads? I hope to have something like this: The point is that it all needs to be as transparent as possible to the users.
563e30ff61a80130652674e1	X	I don't think the initial POST should include the entire entity. Rather, the POST must explicitly be a request for an "upload bucket" resource to be created. You'd then simply respond to the POST request with 201 Created, with the Location header pointing to the new resource where the file should be uploaded. If the upload bucket chosen must depend on specifics of the file (file size, type), then I'd let the client submit metadata in the POST body.
563e30ff61a80130652674e2	X	Please recheck the faq. This is much too broad (and subjective in parts) a question for this site.
563e310061a80130652674e3	X	Hey Mat, I know this would come :( But I was hoping someone would send a link to a book/tutorial which covers all of it. I wasn't expecting such a answer though.
563e310061a80130652674e4	X	Thx Laykes! Awesome. This helps me a lot.
563e310061a80130652674e5	X	Would you allow users uploading straight to the bucket or would you always go through a server?
563e310061a80130652674e6	X	Can you recommend a book/tutorial for cloud programming with php zend and amazon?
563e310061a80130652674e7	X	I go through a server. Uploading directly to a server, while easier, means that you don't get the opportunity to get a lot of information about the files as it is transfered. You can get image dimensions, meta information, mime types etc, which you then use for your Content-Type in Amazon S3. However, what I do, is I let them upload the file to my server, and then I add a job to my Zend_Queue server. I then run a daemon which constantly uploads files to S3. That way the user only needs to upload to my server. I store the file outside of DocRoot, and I rename it to an md5hash for security.
563e310061a80130652674e8	X	No, there aren't any books. I started off using Zend_Service_Amazon_S3 component, but then I switched to the actual Amazon S3 SDK/PHP API, (you can find the class by Googling). The component maintainer is doing a very good job with the Zend_*_Amazon_S3 library, but I found it missed out on several key functions which were essential to my use immediately, so I switched while I still found it easy to do so. There are no books available, at all. Just the Amazon API docs, which are actually pretty good. I've only found one mistake which once reported they updated immediately.
563e310061a80130652674e9	X	I have never programmed any cloud application so I'm basically researching to get started with it. I'm developing in Zend Framework and want to use the cloud to store media assets. The whole project should be scaleable for the cloud. Thinking about this, more and more questions are coming up in my mind: What I want to do: I would be very thankful for hints on how to tackle this project ;-]
563e310061a80130652674ea	X	I would recommend Amazon S3, it is also what I have been developing on top of. I will also answer your question from an AWS S3 perspective. How do I handle access rights to the assets? (Public should only be able to access them if the according article is released. ) When files are uploaded to Amazon S3, you can choose an access policy. You can also set an access policy to every file in the entire "bucket". A bucket is a unique name used to refer to your "cloud" based storage repository. Each file in the bucket is accessed by a single key. For example, you upload a file called myAwesomeImage.jpg. Now when you transfer that file to S3, you get to choose several options for that file. So you can choose to put your awesome image, in a "fake directory" called some/path/to/file. So you would create a "key" for this Object to be stored under the "key" "some/path/to/file/myAwesomeImage.jpg". Your buckets can store billions of Objects, and you can chose how you want to store them, you can choose to use the forward slash to create a folder, but it doesn't actually create a folder, it is just a useful mechanism that you can them employ in your application to signifify depth and organisation in your files. Now, ACL So when you upload your Object, you can pick several default access policies, or you can create your own. For example, if you upload an Object as ACL_PUBLIC, that means that anyone can access it. However, if you upload it as ACL_PRIVATE that means that it is a private and only the owner of the file can access it. a database to assign them to an article?) So you have a few options here. You can either cache everything to store a local state of your Bucket, or you can constantly check with the Amazon S3 API to find out what files you have. You will know which you need based on your application. Take a situation that I have... A image is uploaded to our companies file manager, and then three thumbnails are automatically generated, and then watermarks are also applied. This means that each image, could generate at least 3 images, and up to hundreds (depending on how many different watermarks we need to apply). In our situation, I uploaded 20k images to S3 last week and then imported it into our File Manager. I have to store a local representation of waht we have in S3 because otherwise it takes too long to search and query the repository. I also am not interested in which watermark files and thumbnails we have, but I do need to make sure that they are generated. Storing them locally means you can do all of this. This is the schema for my files table. (but I also have another files_dimensions) table which stores all of my dimensions too. How do I refer to them in zend framework? (Does it make sense to use a cdn like cloudfront? How to create urls?) You would create a View Helper, and then have something like $view->createUrl( $file ) where $file contains everything that it needs to construct your URL. So you would have your Object path and it's key. Not really. Zend_Cloud is not fully developed yet. The idea with Zend_Cloud is that it will be interchangeable with any Cloud storage adapter, but it isn't ready. I create different sizes of all of my Objects. I then append it like /123123123/large.jpg /123123123/medium.jpg http://i.stack.imgur.com/AkT0B.jpg 
563e310061a80130652674eb	X	any luck on this?
563e310061a80130652674ec	X	No, I am using backend PHP to do this.
563e310061a80130652674ed	X	i think it's possible, just need to use multipart/form-data in the body of request. see here: developers.facebook.com/blog/post/498
563e310061a80130652674ee	X	Since the Facebook JavaScript SDK documentation talks about an URL and not a data-URL, I would say the answer is: No. But how did you managed to do this by PHP SDK?
563e310061a80130652674ef	X	I converted the base64 data-URL string to binary data and created an image file on the server and uploaded the same.
563e310061a80130652674f0	X	The following code snippet is used to upload photo using facebook Javascript SDK. Can I use the imgURL to be base64 image URL string which is generated from Canvas? Something like "data:image/png;base64,iVBORw0KGgoAAAANSUh..."
563e310061a80130652674f1	X	My solution was to upload the base64 data URL to imgur or Amazon S3, then post the returned URL to facebook: http://alipman88.github.io/debt/about/index.html Alternative solution using only JavaScript (haven't tested personally): Upload Base64 Image Facebook Graph API
563e310161a80130652674f2	X	Dropbox has a Java API and is free, have you tried it? dropbox.com/developers/core/start/java
563e310161a80130652674f3	X	My java app generates some .png image which i want to store to any cloud storage through java API. I dont want any paid service if possible in free. Not getting a good answer for last 4 hours. Any suggestion?
563e310161a80130652674f4	X	Google Cloud Storage doesn't have a free tier, although for only a handful of PNG images you will likely be spending less than $1 per month. Google Drive, however, is free for the first 15 GB or so (it's shared with your GMail account, your Google+ photos, etc). Google Drive also provides a Java API: https://developers.google.com/drive/quickstart-java Dropbox and S3 also both have APIs and various quantities/time of free storage. If you're just looking to host images, and you're not trying to make money, and you don't need to host that many, you could look into image-specific services. Imgur.com, for example, provides an API.
563e310161a80130652674f5	X	Dropbox and Google Drive could meet your needs. You can download/upload files pragmatically to Dropbox and also to Google Drive. They have API to do it. However, if you are really looking for really complete service where you can scale up your data storage until you want, you really want to take a look at Amazon S3. In fact, as far as I know, Dropbox works on the top of Amazon S3 to provide their services. If you would like to have an idea about how to upload/download a file to Amazon S3, you can take a look to this application example. If you want the same thing on Dropbox or Google Drive, there are a lot of examples on Internet. However, on these two providers you need a token to upload files, what I don't like. I prefer the way in which it works for Amazon S3 (just for programming purposes - GUI is better on GD or Dropbox). Amazon S3 is not totally free, but it is really, really cheap. Be aware of the network latency between your app and your File storage provider if you don't use the same physical infrastructure.
563e310261a80130652674f6	X	So basically, you want someone to read the docs for you and provide you with copypaste example for the things you want to do?
563e310261a80130652674f7	X	No, what I am saying is that somebody aware of this entire system could help push me in the right direction... that's all, nothing more.
563e310361a80130652674f8	X	I want to get product details i.e. their name,desctiption/features,price,all images(small,medium, large, whichever available),parent category, attributes such as weight, color, etc from amazon.com. Is this possible by any means? I am developing a website in php(wordpress) that will show products from amazon.com I came across amazon aws sdk for php and amazon PA API, but don't know how to use them to get what i want. Also found Amazon-ECS-PHP-Library but don't don't know how to use it. Can anyone tell me about any tutorial or any documentation or any kind of resourec that shows how to get the things that i want. i am registered at amazon affiliate.I see the services available ( like EC2,S3 and other) but not aware of using them. Please help.
563e310361a80130652674f9	X	The Product Advertising API falls under the Amazon Associates program, which is entirely separate from Amazon Web Services (AWS) which includes Cloud Computing solutions and server architecture. The AWS SDK for PHP is for AWS, and does not include support for the Product Advertising API. Since your question is about PHP and the PAAPI, let me move your question to the appropriate forum where you're more likely to get your question answered.
563e310361a80130652674fa	X	I'm dealing with a similar issue (see stackoverflow.com/questions/7796767/…). I figured out that if you add a delay of a few seconds before trying to alter another frame, the "about:blank" resolves to a nice-looking URL. Still got the same bug, though!
563e310361a80130652674fb	X	Thanks. It still throws that error but somehow I can log in.
563e310361a80130652674fc	X	I am using google contacts javascript api. I am trying to add contacts to the gmail account of the authenticated users using the code given in the http://code.google.com/apis/contacts/docs/1.0/developers_guide_js.html#Interactive_Samples. I am able to login and logout. But I try to create a new contact my chrome is given an error. I have hosted the javascript and html file in the amazon s3 bucket and also image. Unsafe JavaScript attempt to access frame with URL about:blank from frame with URL https://s3.amazonaws.com/googlecontacts/google_contacts.html. Domains, protocols and ports must match. And contacts are not created. HTML file javascript file
563e310361a80130652674fd	X	The problem was I was access https server from http server, so the protocol mis matched just changed the feedURi http://www.google.com/m8/feeds/contacts/default/full'; to https://www.google.com/m8/feeds/contacts/default/full';
563e310361a80130652674fe	X	Have you tried putting google.load( 'gdata', '1.x' ); in the html file? It worked for me.
563e310361a80130652674ff	X	My application requires storage for storing files like images and I dont want to store them in a database (neither RDBMS nor NoSQL) but as plain files. Is this possible? Cheers Mani
563e310361a8013065267500	X	File system isn't persistent, so content will be lost after a redeploy/restart. A possible workaround is to use local filesystem for quick access to file resources but store them in amazon S3 (or any other storage service). You can use jClouds API to avoid lock-in with a provider API, and be able to run locally for testing purpose switching to FileSystem implementation
563e310461a8013065267501	X	I am wondering if there is a way or process to correctly estimate the number of concurrent users which my app will be able to support with 2 Dynos on a Node.js (Hapi) based API server using MongoDB hosted on MongoLab (on a shared cluster). I think an important point of consideration (bottle-neck) might also be the fact that the users upload images to Amazon S3 using Cloudinary API. I understand that the answer depends and varies depending on the application and the way users interact with the DB during the game loop. But, is it possible to determine an approximate idea about how many concurrent users will be able to play using the present setup (without scaling up any further). Since, most of the services (other than Amazon S3) are not self-scaling, I was thinking it would be valuable to have an estimate to predict the infrastructure cost.
563e310461a8013065267502	X	Have you tried adding other sources besides just webcam video? I understand you only want webcam, but I'd be interested in seeing if anything changes. I was able to get it to work in chrome here: jsfiddle.net/Gy7nr
563e310461a8013065267503	X	Hi Brett thanks for the reply. I tried with 3 services. filepicker.SERVICES.VIDEO,filepicker.SERVICES.COMPUTER,filepicker.SERVICES.FACEB‌​OOK. Still no luck. This works fine on firefox. Also jsfiddle on chrome is showing this correctly. For some reason chrome & IE are not showing this.
563e310461a8013065267504	X	Reasearch : Also I extracted the iframe src dynamically assigned by filepicker code from firefox using firebug and force assigned to iframe src to check if chrome will be able display the widget.[filepicker.io/dialog/open/?m=*%2F*&amp;key=AeE9yA21xTpiOQ3JQ‌​m938z&amp;id=1349789071455&amp;referrer=&amp;iframe=true&amp;s=13,1,3]. This works fine on chrome & IE. So on chrome the src for the iframe was never set [atleast with the configuration I have.]
563e310461a8013065267505	X	Research : I placed the code in main update in this page, please try opening it on latest version of chrome [22.0.1229.79 m]. interviewcup.com/test.php .
563e310461a8013065267506	X	Excellent!! This works fine. I am proceeding with onReady call approach. Thanks for your help Bret !!
563e310461a8013065267507	X	I am trying to record video from user webcam using filepicker.io and store it in my Amazon S3 bucket. I am stuck up here.Below code works fine in firefox but not on chrome & IE. The abc iframe src is not set by filepicker api, so the contents remain blank. I verified that 3rd part cookies are enabled on chrome. Any help will be highly appreciated.
563e310461a8013065267508	X	It looks like the iframe wasn't included in the DOM yet. It worked for me when I put the javascript code inside the onReady call from jquery: http://www.filepicker.io/api/file/4wW3A6BwQYaPDNBENcau?dl=false
563e310461a8013065267509	X	stackoverflow.com/questions/13390343/…
563e310461a801306526750a	X	I'd like to set up a separate s3 bucket folder for each of my mobile app users for them to store their files. However, I also want to set up size limits so that they don't use up too much storage. Additionally, if they do go over the limit I'd like to offer them increased space if they sign up for a premium service. Is there a way I can set folder file size limits through s3 configuration or api? If not would I have to use the apis somehow to calculate folder size on every upload? I know that there is the devpay feature in Amazon but it might be a hassle for users to sign up with Amazon if they want to just use small amount of free space.
563e310561a801306526750b	X	There does not appear to be a way to do this, probably at least in part because there is actually no such thing as "folders" in S3. There is only the appearance of folders. Amazon S3 does not have concept of a folder, there are only buckets and objects. The Amazon S3 console supports the folder concept using the object key name prefixes. — http://docs.aws.amazon.com/AmazonS3/latest/UG/FolderOperations.html All of the keys in an S3 bucket are actually in a flat namespace, with the / delimiter used as desired to conceptually divide objects into logical groupings that look like folders, but it's only a convenient illusion. It seems impossible that S3 would have a concept of the size of a folder, when it has no actual concept of "folders" at all. If you don't maintain an authoritative database of what's been stored by clients (which suggests that all uploads should pass through an app server rather than going directly to S3, which is the the only approach that makes sense to me at all) then your only alternative is to poll S3 to discover what's there. An imperfect shortcut would be for your application to read the S3 bucket logs to discover what had been uploaded, but that is only provided on a best-effort basis. It should be reliable but is not guaranteed to be perfect. This service provides a best effort attempt to log all access of objects within a bucket. Please note that it is possible that the actual usage report at the end of a month will slightly vary. Your other option is to develop your own service that sits between users and Amazon S3, that monitors all requests to your buckets/objects. — http://aws.amazon.com/articles/1109#13 Again, having your app server mediate all requests seems to be the logical approach, and would also allow you to detect immediately (as opposed to "discover later") that a user had exceeded a threshold.
563e310561a801306526750c	X	Great ideas! I did not yet successfully implement them but it gives me some thing to try. I also explored the Javascript-sdk which is a dedicated sdk for sending files via the browser. So far I am having some trouble with the Secret Access Key not matching up. Thanks again!
563e310561a801306526750d	X	I am using PhantomJS 1.9.7 to scrape a web page. I need to send the returned page content to S3. I am currently using the filesystem module included with PhantomJS to save to the local file system and using a php script to scan the directory and ship the files off to S3. I would like to completely bypass the local filesystem and send the files directly from PhantomJS to S3. I could not find a direct way to do this within PhantomJS. I toyed with the idea of using the child_process module and pass in the content as an argument, like so: which would call a php script directly to accomplish the upload. This will require using an additional process to call a CLI command. I am not comfortable with having another asynchronous process running. What I am looking for is a way to send the content directly to S3 from the PhantomJS script similar to what the filesystem module does with the local filesystem. Any ideas as to how to accomplish this would be appreciated. Thanks!
563e310561a801306526750e	X	You could just create and open another page and point it to your S3 service. Amazon S3 has a REST API and a SOAP API and REST seems easier. For SOAP you will have to manually build the request. The only problem might be the wrong content-type. Though it looks as if it was implemented, but I cannot find a reference in the documentation. You could also create a form in the page context and send the file that way.
563e310561a801306526750f	X	have you considered the Same Origin Policy?
563e310561a8013065267510	X	Yes, that's taken care of. I'd get a different type of error from a preflighted options request. But that passes, I can see that in the debugging proxy.
563e310561a8013065267511	X	I'm trying to upload a file to Amazon AWS from Javascript with a signed URL obtained from a django api where I sign it with the help of Boto. This is my line in python: Using the URL generated from this, I can post to S3 with CURL like so: What does not work though, is using this URL to PUT a file to Amazon AWS via Javascript. Debugging with a proxy reveals a broken pipe error, while CURL would give me the regular Amazon Access Denied XML if something went wrong. This is my javascript code (file is a JS File object): Any ideas where to go from here? Maybe I'm not including all the required headers in the signing process, or FormData is not the right way to go.
563e310561a8013065267512	X	I'm working on a Rails app that accepts file uploads and where users can modify these files later. For example, they can change the text file contents or perform basic manipulations on images such as resizing, cropping, rotating etc. At the moment the files are stored on the same server where Apache is running with Passenger to serve all application requests. I need to move user files to dedicated server to distribute the load on my setup. At the moment our users upload around 10GB of files in a week, which is not huge amount but eventually it adds up. And so i'm going through a different options on how to implement the communication between application server(s) and a file server. I'd like to start out with a simple and fool-proof solution. If it scales well later across multiple file servers, i'd be more than happy. Here are some different options i've been investigating: So i'm looking for different (and possibly standards-based) approaches how file servers for web applications are implemented and how they have been working in the wild.
563e310561a8013065267513	X	Use S3. It is inexpensive, a-la-carte, and if people start downloading their files, your server won't have to get stressed because your download pages can point directly to the S3 URL of the uploaded file. "Pedro" has a nice sample application that works with S3 at github.com. I'm usually terribly incompetent or unlucky at getting these kinds of things working, but with Pedro's little S3 upload application I was successful. Good luck.
563e310561a8013065267514	X	you could also try and compile a version of Dropbox (they provide the source) and ln -s that to your public/system directory so paperclip saves to it. this way you can access the files remotely from any desktop as well... I haven't done this yet so i can't attest to how easy/hard/valuable it is but it's on my teux deux list... :)
563e310661a8013065267515	X	I think S3 is your best bet. With a plugin like Paperclip it's really very easy to add to a Rails application, and not having to worry about scaling it will save on headaches.
563e310661a8013065267516	X	I was trying to create kindle fire emulators to test apps for kindle fire tablet, fire phone, amazon TV. I've followed the documentation but i couldn't create emulators for these. Can anybody tell me that whether amazon is giving support for emulators or not. I need it very urgent. Any help would be much appreciated. Thank you :)
563e310661a8013065267517	X	You need a basic SDK development setup using the Android SDK Manager with the latest versions of the following downloaded: Then you will need to add the various Amazon Add-on Site urls to the Android SDK Manager. To do this open the SDK Manager and go to Tools -> Manage Add-on Sites.. on the main menu. Next go to the User Defined Sites and add the following URLs: Now close, and re-open your SDK manager (or use Reload from the Packages menu). Next download all of the Amazon/Kindle related stuff using the SDK manager. This includes: Once all of these have been downloaded, open your SDK folder and browse to \Extras\Amazon\AVDLauncherWindows and run amazonavd.bat. Now create your desired devices from there. For reference here are the Amazon documentation links: Kindle Fire Fire Phone Fire TV
563e310661a8013065267518	X	What was the solution for this 400 bad request issue? I have the same issue : stackoverflow.com/questions/29623898/…
563e310661a8013065267519	X	I'm trying to upload an image to a WordPress.com blog via their REST API. According to their documentation, I should be able to use the new post endpoint to do so. I'm currently using Rails and rest-client to successfully create text posts using the following code (post_hash contains all the post information): As I mentioned, this works perfectly. Now, I'd like to be able to upload a new image to the blog's Gallery by creating a new "post" with the image attached. I'm trying to use this code to do so (image is a CarrierWave file stored on Amazon S3): This does not work and keeps giving me a 400 Bad Request error. If anyone has any suggestions as to what I'm doing wrong or advice on a better way of doing this, I'd greatly appreciate it. Thanks!
563e310661a801306526751a	X	Questions looking for libraries or off site resources are off topic here.
563e310661a801306526751b	X	I intend to build a simple app to send files from a mobile phone to a remote server. File size can vary from 500kb to 10 MB. Is there any service available from Amazon or Google or any other company that will help with the server end side. I did some research about Google Cloud messenger, Pushbullet but they support only short messages.
563e310661a801306526751c	X	Best way is to upload files from Android (or iOS with similar APIs) to respective cloud storage directly (not streaming through your server process, but direct to the storage) Google Cloud Storage Java Client Library to upload to Google-Cloud-Storage by using GscService.createOrReplace(GcsFilename, GcsFileOptions) Amazon S3 Android TransferManager to upload to Amazon-S3 by using TransferManager.upload(bucketNmae, fileName, file) Both these services are meant for uploading files from a android app to respective cloud directly, without any server code. Then you can do either of Make the file available to download with a web URL (as CMS). Use the file inside the cloud in your application (as File System).
563e310761a801306526751d	X	Are there any functions in the S3 library that can select all files with a particular string in their filename?
563e310761a801306526751e	X	Yes via the delete_all_objects method. You can specify a regular expression to use to delete all files in a bucket that match the expression. Docs and examples here: docs.amazonwebservices.com/AWSSDKforPHP/latest/…
563e310761a801306526751f	X	You can accomplish this all with a single call to delete_all_objects instead of multiple calls. It would also be more atomic.
563e310761a8013065267520	X	@nategood There is no such thing as "more atomic". Its either atomic, or not. And both solutions aren't.
563e310761a8013065267521	X	Yes. Concise would be a better term. This operation is not atomic.
563e310761a8013065267522	X	I just started trying out Amazon S3 for hosting my website's images. I'm using the official Amazon AWS PHP SDK library. Problem: How can I delete all files located in a S3 'folder'? For example if I have a file named images/2012/photo.jpg, I want to delete all files whose filenames start withimages/2012/`. Thanks!
563e310761a8013065267523	X	S3 does not have "folders" as you would traditionally think of them on a file system (some S3 clients just do a nice job making S3 appear to have folders). Those / are actually part of the file name. As such, there is no "delete folder" option in the API. You would just need to delete each individual file that has the images/2012/... prefix. Update: This can be accomplished via the delete_all_objects method in the Amazon S3 PHP Client. Simply specify "/^images\/2012\//" as the regex prefix in the second argument (the first argument being your bucket name).
563e310761a8013065267524	X	The best way to delete a folder from S3 with all its files is using the API deleteMatchingObjects()
563e310761a8013065267525	X	Here is a function that will do what you are looking to do. Use: if I want to delete the directory foo
563e310761a8013065267526	X	thank you Prasanna Aarthi
563e310761a8013065267527	X	i have a web page which allows user to choose image file from local disk..what i want to do is that get image store it on amazon s 3 bucket and convert local link to amazon link and store it in my db.how do i do this.also please tell, is amazon for free and how to make bucket...please help i am a complete newbie to amazon. here is my code
563e310761a8013065267528	X	Regarding pricing As part of the AWS Free Usage Tier, you can get started with Amazon S3 for free. Upon sign-up, new AWS customers receive 5 GB of Amazon S3 standard storage, 20,000 Get Requests, 2,000 Put Requests, and 15GB of data transfer out each month for one year. This is from their official site, For creating buckets and uploading images.. Please go through their documentation ,they are pretty clear and elaborate.You can either use REST API'S to send request or make use of their SDK's in various languages(PHP,.NET etc) Please refer http://docs.aws.amazon.com/AmazonS3/latest/dev/MakingRequests.html
563e310861a8013065267529	X	Thanks for your response....yes ,only way is to traversing through whole addressbook
563e310861a801306526752a	X	I am developing an application to backup the whole address book into amazon s3,but i cant able to find any direct iphone api to get the whole address book into any data structure. I tried the following code,to write the address book array to a file in document directory But the problem is file is not getting created in document directory.... Any suggestions appreciated.... Thanks in advance...
563e310861a801306526752b	X	Im not 100% sure, but Accessing the entire addressbook at once probably is not allowed. Apple has the AddressBookPeoplePicker for a reason, though picking everyone in the AddressBook one person at a time would be a pain. You may be able to set up a loop to run through the address book programatically. Then add it to your own NSMutableArray.
563e310861a801306526752c	X	You would require this function: http://developer.apple.com/library/ios/#documentation/AddressBook/Reference/ABPersonRef_iPhoneOS/Reference/reference.html#//apple_ref/doc/uid/TP40007210 An Array of all the people is returned. Read the documentation for more. Best of Luck!
563e310961a801306526752d	X	I would ask the author if he could implement it in aws2js. I think it would be very easy to do and he has been recently active in the project. Or if you are able, implement it yourself.
563e310961a801306526752e	X	You can also implement this specific request through their REST API until there is support in one of the libraries.
563e310961a801306526752f	X	I can get something this to work only when specifying a MaxKeys value in the listObjects parameters
563e310961a8013065267530	X	Can anyone elaborate on Marker. Looked at the docs and am confused. If I omit it, I just get null for data.
563e310961a8013065267531	X	The Marker is a string that specifies the key to start with when listing objects in a bucket. It is optional: if you omit it you will see keys from the beginning (alphanumerically), so it sounds like there might be some other issue you're running into?
563e310961a8013065267532	X	As a general tip, I often go back to the HTTP API Reference to verify these things, because the documentation for the javascript SDK is sometimes incomplete or inaccurate: docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e310961a8013065267533	X	Although this is set as the right/selected answer, it should be noted that github.com/SaltwaterC/aws2js has been deprecated. Upon npm install it informs one that "aws2js is deprecated. Please use aws-sdk."
563e310961a8013065267534	X	Is there any Amazon S3 client library for Node.js that allows listing of all files in S3 bucket? The most known aws2js and knox don't seem to have this functionality.
563e310961a8013065267535	X	Using the official aws-sdk: see s3.listObjects
563e310961a8013065267536	X	In fact aws2js supports listing of objects in a bucket on a low level via s3.get() method call. To do it one has to pass prefix parameter which is documented on Amazon S3 REST API page: The data variable in the above snippet contains a list of all objects in the bucketName bucket.
563e310961a8013065267537	X	Published knox-copy when I couldn't find a good existing solution. Wraps all the pagination details of the Rest API into a familiar node stream: If you're listing fewer than 1000 files a single page will work:
563e310961a8013065267538	X	Although @Meekohi's answer does technically work, I've had enough heartache with the S3 portion of the AWS SDK for NodeJS. After all the previous struggling with modules such as aws-sdk, s3, knox, I decided to install s3cmd via the OS package manager and shell-out to it using child_process Something like: (Using the cmd_exec implementation from this question) This approach just works really well - including for other problematic things like file upload.
563e310a61a8013065267539	X	I'd like to say that finally I implemented Tomcat 8 with Spring to do it and I am passing data via Java 8 code which basically streams it from the NAS over the web. It is well scalable - it takes approx 20 cores for 10GBps transfers (downloads). It is not optimized in any way except for the read and write buffers which are somewhat like 8k.
563e310a61a801306526753a	X	Thank you very much for your answer. I have implemented some of it in Java and it works pretty much OK. Why I would need any abstraction in API (always asking this myself). I just need API for filesystem, so that it doesnt matter what filesystem is below (ext4, stornext, NTFS, glusterfs), so that I can navigate folders with respect to ACLs on it, and that it works thru web proxy, and it does support fail-over.
563e310a61a801306526753b	X	I finally designed solution which is using multiple storage APIs, so I am having MySQL, Mongo, Stornext and HDS Hitachi over REST as well jCIFS. This way, when storage connectivity has issues, it doesnt destroy the operating system (hangs Nagios checks, kernel panics etc), but instead I can handle these issues myself, e.g. connect to the different head. What I was asking above would be better described as "proxy to the filesystem", but now I can use CIFS/NFS server smbd and Java userspace client jCIFS.
563e310a61a801306526753c	X	Here is some more info in this context: unix.stackexchange.com/questions/45899/…
563e310a61a801306526753d	X	WebDav is not working well with large files. Swift is for cheap harddrives while I got already distributed, replicating file system. I just need software layer in form of API so that I can upload and download files, just this "little" part :-)
563e310a61a801306526753e	X	Swift isn't exclusively for "cheap hard drives". It can support almost any storage backend you want, provided you know how to integrate it. Why waste time re-implementing the wheel?
563e310a61a801306526753f	X	I need to build RESTful API for my files on cluster filesystem volumes. I have like 20 servers which do share the same filesystems. All I need is RESTful API services which would allow me to stat(), read(), write(), listFolder(), delete(), setacl() etc. Everything else is handled by cluster filesystem, so I just need to have the above functions. I need something which is pretty much mature so it supports access control lists, it has high performance API (like java ones), the library or project is maintained, and it runs Linux, also locking support would be very useful. I would like to put additional functions myself like getDuration(), so if it's open source that would be advantage. If you are aware of such code which would help me to build something like this I would be very grateful. The purpose of it is to allow BPM system to check if the files are OK on the various Stornext volumes. Since these systems are behind various firewalls and mouting NFS or SMB is not really good because of high availability, the best option seems to be RESTful API as single source to all file operations between firewall zones in some convenient way via HTTP(S) request instead of doing NFS or SSH.
563e310a61a8013065267540	X	If you want a very generic web based API to manipulate files Look into design of WebDAV api It's OK if you dont want to use it AS IS, you just look into it as an API inspiration. Look how stat(), listFolders() and setacl() might be just one command. If you looking into something time-tested - this is the one. This API was designed for web based file access, people put some wrappers around it to have it mountable just like any other file system - see davfs2, to me it's a proof of a solid and complete API. Now presuming you don't want full DAV - but something simpler, then I'd look into some libraries which can help me built a similar API. Check out these: Jackrabbit WebDAV Library, milton.io. There is also of course is Jigsaw project to steel code from. Use them to expose your ad-hoc APU or a selection of StorNext API calls over http. If you want a less generic API to manipulate blobs Check out Amazon S3 API as an inspiration, and a code like littles3 as an implementation example. There are plenty of projects like this, check this search Notice how what you want falls in between what is already available: If you want an API tailored to your domain Typically when faced with a similar challenge, like yours, people leverage their domain knowledge and use-cases. If you need this API for pictures storage and retrieval forget generic file operations and model your API around collection of images. You know up front a lot of information which make API design a much simpler job, for instance:
563e310a61a8013065267541	X	Have you looked at rails-api? I'm not sure if it supports all the functions you need but is maintained and open-source. https://github.com/rails-api/rails-api You could also include a ruby gem to handle access control lists. https://www.ruby-toolbox.com/projects/acl9
563e310a61a8013065267542	X	I'd recommend looking into a WebDAV implementation -- they're usually integrated into a web server (like Apache) and support most of the standard filesystem operations you require. If you really want to build it yourself, you could also fire up an object storage platform like OpenStack's "Swift" project, backed by your SAN or NAS appliance over NFS/iSCSI. EDIT: You want to store a large number of photos. There are various NoSQL databases that would also solve this problem. However you could also solve the problem using the a native network filesystem protocol like NFS. NFS will perform predictably well (v4.1+ anyway) on the majority of your typical read-and-write filesystem operations. However, you'll also need a way to index and retrieve photo metadata and provide access control mechanisms, and those are where performance can get complicated. When the file is uploaded to your HTTP API, you should calculate the MD5 hash of it's contents, while storing the original file name, owner UID and other metadata in a relational database. Then write the photo to your NFS mount in a specific "bucket". For example, assume you have a photo whose content has the MD5 hash: e240a38624f4a370bd2ec65cf771134b. Assuming your NFS mount is at /srv/content you would write the photo to the path /srv/content/e240/a38624f4/a370bd2ec65cf771134b.jpg -- splitting the MD5 hash to create prefixed folders. When your user later wants to retrieve the image, they can request it via the data stored in the relational database, your API can look up the photo's MD5 hash, and then locate it on the filesystem using a similar operation. Please be aware that using MD5 could result in collisions if you have a very large number of differing files, so you may want to use another hashing scheme or a combination of two or more to prevent that from occurring.
563e310a61a8013065267543	X	That sounds close to a workable solution, it keeps the credentials out of the visible application. Though what is to stop a request being forged for signing to the backend? IP restrictions?
563e310a61a8013065267544	X	To properly answer on your question, you should add more data about what kind of authorization is used at your service. Security measures that you should take depends on what kind of request signing is used in your service. You gave as example Amazon S3, which should not be accessed from vulnerable code. Frontend code is vulnerable. If any request to your service requires to be signed with "master" credentials, you should proxy all of them.
563e310a61a8013065267545	X	I am using angularjs to make REST requests to an API that requires authorisation. The authorisation requires that the request is signed (similar to Amazon s3) and that this signature is sent in the headers. I am unsure how to do this securely with angularjs. As this is client side the credentials need to be embedded in the js code which exposes a massive security hole. I assume I'm missing something obvious here? Any thoughts would be appreciated.
563e310a61a8013065267546	X	No, you aren't missing anything. I think possible solution here is write your own authorization backend that will hold your credentials and will return token to your application.
563e310b61a8013065267547	X	Check out aws.amazon.com/lambda
563e310b61a8013065267548	X	chiastic-security: Thanks for your response. It's really helpful. I'll try to implement the polling routine suggested by you.
563e310b61a8013065267549	X	@RajeevSingh you're welcome :)
563e310b61a801306526754a	X	I need to monitor a directory on Amazon S3 to check if any new file is added to this directory. I tried using Java NIO Watch Service, however it is not working properly. If I used following syntax with the provided path of S3: Then i get following error: If i remove the file:// prefix from the path then following error is generated: If i modify 'Line2' to Path path=Paths.get("https://abc/dir"); then following trace is generated: Kindly let me know what am i doing wrong here and if it is possible to monitor web resources like these using the Java watch service or if there is any other framework/api. Thanks
563e310b61a801306526754b	X	You've given it a mixture of a file protocol and an http protocol, which doesn't really make sense. Basically you can't do what you're trying to do, if the only way you have access to the resource is via HTTP. There's no general mechanism for being notified when an HTTP resource changes, because there's no general mechanism for having push notifications from an HTTP resource, and it's outside the operating system's control so it can't intercept changes as they happen. With a local file, your operating system can detect changes as they happen because it's ultimately responsible for dealing with writes to local disk, but that doesn't apply in your situation. You'd need something that polls for changes, unless S3 has something bespoke in place to push change notifications (but that's something you'd have to investigate separately). You can't do it with Java NIO.
563e310b61a801306526754c	X	And Google has announced that they plan to more tightly integrate Storage for Developers into GAE (code.google.com/appengine/docs/roadmap.html) but it is on the bottom of the roadmap.
563e310b61a801306526754d	X	Thanks. Yes, its a GAE app. Isn't it better to go with CDN storage for assets (which doesn't require updates) in terms of performance (& cost) compared with blobstore? Accessing assets from CDN won't need an instance of GAE app (of-course, tight integration will help on the secure access to assets).
563e310b61a801306526754e	X	Unlike datastore objects, blobstore objects can be served directly without invoking your App Engine app. It's designed for exactly this purpose.
563e310b61a801306526754f	X	Option A : From GAE database Option B : File System access in GAE Option C : File System access (CDN) outside GAE like Amazon S3? or any other options? Thanks.
563e310b61a8013065267550	X	If it's an App Engine app, you should use the the Blobstore. If you need stand-alone storage, try Amazon S3 or Google Storage for Developers. The first is tightly coupled to App Engine; the others are platform-agnostic and controlled via public APIs. They're all pretty similar in terms of behavior and pricing.
563e310c61a8013065267551	X	We have a website, which allows users to upload documents (word, pdf, PPT, etc.). We are uploading files to Amazon S3. So, all files will have it's own web URL. For these uploaded documents, we would like to generate thumbnails. This thumbnail needs to be generated based on it's content (like Google document viewer). Is there any Service/API, which generates thumbnails of documents by it's URL? Thanks and Regards, Ashish Shukla
563e310c61a8013065267552	X	I know Datalogics' PDF WebAPI should be able to do the trick for you. There's a Render Pages service that will take a URL and give you back a thumbnail of that PDF. Please keep in mind that this only works with PDF. They have a free account you can sign up for to get you started. (*Disclaimer for Stack Overflow community: Yes, I work for this company. I am posting this as I believe it will benefit the person asking the question. I am providing an explanation of how to use the service.) Here's a screenshot of how to make a request to Render Pages using Postman, something I use for testing out services all the time.  Hope this helps! *If anyone feels I posted this incorrectly, please let me know. Still reading up on all the rules.
563e310c61a8013065267553	X	You should also take a look at AWS Lambda. In fact, this presentation from the AWS re:Invent 2014 conference shows a live example of using Lambda to generate thumbnail images. This solution will be very reliable, and very cost-effective, but has the downside that you'll be responsible for maintaining the code, or debugging issues.
563e310c61a8013065267554	X	I have an API set up to receive (amongst other things) an image byte array from a mobile app. Once received, the image is streamed to Amazon S3 and saved as a png. All good, however I have an issue where some images that are received are different dimensions. Is it possible to determine the width and height of the image using only the byte array (ie, not saving it as an image to the server first), and if necessary crop the image?
563e310c61a8013065267555	X	Okay, so I figured it out in the end, and it goes something like this: The general gist of it I found here: http://salman-w.blogspot.co.uk/2009/04/crop-to-fit-image-using-aspphp.html
563e310c61a8013065267556	X	I did the changes you suggested but I unfortunately get errors. Here is the API server log: pastie.org/1805101.
563e310c61a8013065267557	X	I think you are on the right track. If I put a ! after @avatar.save I get this clue: pastie.org/1805120.
563e310c61a8013065267558	X	This is a correct output where I use the almost the same code in a webbased Ruby on Rails app. pastie.org/1805171.
563e310c61a8013065267559	X	Did you get it to work? That last log entry seems like it's working
563e310c61a801306526755a	X	Did you have a solution to this? I am stuck over here > stackoverflow.com/questions/5903565/…. Thanks for any advice!
563e310c61a801306526755b	X	I am developing an iPhone app in Appcelerator Titanium. My app i communicating with an API that I build using Rails 3. I want to be able to upload an image from the iPhone app to the API (and Amazon S3). I am using the gem called Paperclip. Along with the upload request I need to send the name of the file. But if I do it like below, the image_file_name is not recognized. What is wrong with the call? In the App: http://pastie.org/1805065 In the API model: http://pastie.org/1805071 In the API controller: http://pastie.org/1805073 Output on the API server: http://pastie.org/1805078 Please help!
563e310d61a801306526755c	X	Looks like in the API Controller the line: Should read: Explanation: This conclusions was made by looking at the server log output from the API Server, and checking the Parameters hash for the name of the submitted image. Instead of it being named "avatar" as your controller was expecting, it appears to actually be named "image".
563e310d61a801306526755d	X	If your proxy does not support the needed methods - the only things you can do is to change proxy to the one that can do that or set up NAT.
563e310d61a801306526755e	X	@zerkms: Can you please tell more about NAT setup?
563e310e61a801306526755f	X	try to google. It is a lot of articles there.
563e310e61a8013065267560	X	It's very likely that the OP is inside of a corporate firewall, thus making the choice of a caching proxy intentional. Nevertheless, this is a case of "then don't do that."
563e310e61a8013065267561	X	@Charles & @Steve-o: Yes my office has proxy server. I am supposed to work from behind Squid. Please suggest.
563e310e61a8013065267562	X	@Amit: updated answer, basically you are forced to proxy to an external server, and you have to implement that proxy protocol yourself.
563e310e61a8013065267563	X	What I have found till now is that:  How do I proceed from here ?
563e310e61a8013065267564	X	If the Squid proxy MUST be used AND you cannot fix Squid, then you only have one solution: tunnel the API calls through to a server outside of your network and have that server forward the API calls to Amazon S3 on your behalf. From a basic view you can just replicate all the S3 calls you use on your external server but you must be aware of the security implications, i.e. restricting the usage of the server to say the external IP address of your Squid server, or even API keys much like Amazon use themselves. If more flexibility is available try another proxy preferably non-caching like Pound.
563e310e61a8013065267565	X	I really hate patronizing answers that indicate the person asking the question is an idiot. You see it all the time on this site and it's getting annoying. for squid try this configuration directive:
563e310e61a8013065267566	X	Does anyone know if there is a possibility to write data to certain file position that is located on Amazon.S3? For example, how can i write 100 byte chunk of data to file on Amazon.S3 with offset of 1000 bytes?
563e310e61a8013065267567	X	No I believe it's not possible to edit any file in-situ on Amazon S3 without downloading it. It's a storage space, just like your hard-disk, where you usually have to fetch the file into memory for editing and then again put it back(save it). There are plenty of APIs available to do that. For java you have AWS-SDK.
563e310f61a8013065267568	X	Is it possible to retrieve the members of a google cloud project and their permissions via an api? I'm doing this because I want to see if the current signed in user of my app has permission to access the cloud storage section of the project. The user is authenticated via OAuth2. I interact with cloud storage through an Amazon s3 library that has the endpoint changed.
563e310f61a8013065267569	X	You can secure the page so that only admins can access it or check programmably.
563e310f61a801306526756a	X	If you identify your users, I recommand using a MySQL database and handling the requests with PHP. They are both easy to learn.
563e310f61a801306526756b	X	It can be totally anonymous. It is not an account based app. I will look into those though.
563e310f61a801306526756c	X	I have an app that I am working on, and I want it to go to my server, get a title, image and description and display it for the user. I have a good deal of experience with iOS app development, but the server side stuff...well not so much. I was thinking that I might just add a separate XML file on the server for each item that I want to display info for...but I know there are more effective options. Should I use a SQL database? I realize that this is quite a broad question, but I just need a push in the right direction. Thanks.
563e310f61a801306526756d	X	I don't really know how to set up your own server, so i'm using third parties. I'm using amazon s3, it's pretty simple to use, you can store there files that < 1gb. Create iPad, iPhone, and iPod applications that leverage AWS using the AWS SDK for iOS. The SDK helps remove complexity by providing iOS APIs for many AWS services including Amazon S3, Amazon SQS, Amazon SNS, and DynamoDB. The single, downloadable package includes the AWS iOS Library, code samples, and documentation. Here's manual. But there's a lot other good providers for storing your data, please read this article: Parse.com have good tutorials(photo server) for quick start with it - https://parse.com/tutorials
563e310f61a801306526756e	X	The only issue that I can think of is that it wont display an SSL cert in the browser bar to the user.
563e310f61a801306526756f	X	Your login form posts to HTTPS, but you blew it when you loaded it over HTTP
563e310f61a8013065267570	X	@SilverlightFox mind posting that as an answer?
563e311061a8013065267571	X	@tau sure, done.
563e311061a8013065267572	X	thank you for alerting me to this. here is a related question i started on the security stackexchange: security.stackexchange.com/questions/54744/…
563e311061a8013065267573	X	If I have a completely static frontend at http://domain.com and AJAX in the relevant data from https://api.domain.com, will all of the data transmitted between them be secure (as though both used https)? Is there a potential flaw with this model? The reason I'm asking is because then I can use a service like Amazon S3 to host my static webpage and simply pay for a cert on my API.
563e311061a8013065267574	X	Apart from the fact that this model results in only a partially encrypted page so the padlock and https cannot be shown to the user in the address bar, they cannot (rightly) verify that your site is the genuine one, it could also be possible for a MITM attack to be executed on your HTTP content and redirect further communications to the attacker. e.g. your AJAX code loaded over HTTP could be as follows: If the attacker intercepts the response from your server to the victim and modifies it to the user's credentials will be sent to the attacker instead of your website and if your attacker then forwarded them onto your website afterwards the victim would be non the wiser that their details have been stolen. Tools such as sslstrip can make it very easy for an attacker to accomplish this (assuming they are suitably positioned, which is a requirement of all MITM attacks).
563e311061a8013065267575	X	I think this approach is fine. Keep all your heavily accessed and static pages on S3 (even use Cloudfront to speed up further). One good example is the home page of your service, some help pages etc. These pages can be on http as there is nothing private in them. When you then require users to login and access "private" content, send them to another domain like secure.domain.com on https. Then on all the content is secured and users will get confidence to login, as they see the secure green lock in the browser when they are doing login etc. Since your APIs also hopefully be requiring privacy that also should operate on the secure domain.
563e311061a8013065267576	X	But is it the best approach when the ec2 instance with the api endpoint will be transferring a big amount of data per nonth? The data needs to be transferred before i can save it.
563e311061a8013065267577	X	I need some advise regarding my amazon cloud setup. I have a servicestack api which recieves streams in form of images and videos, and these needs to be saved somewhere. The same api also delivers these streams to clients (website, apps etc.) This means that a lot of data is being transferred over Internet and saved to disk somewhere. What is the best setup here when looking at pricing? I've tried the amazon cloud pricing calculator, but its difficult to figure out all the numbers. I was thinking.... - Normal ec2 instance for hosting api - S3 bukcet for saving images and videos - CloudFront for delivering images and videos Is there a better and cheaper approach? I was thinking that data transfer and disk space would be a huge cost in this setup.
563e311061a8013065267578	X	I think amazon s3 is very good for saving images and videos.The pricing based on data transferred "in" to and "out" of Amazon S3.Pay only for what you use. There is no minimum fee. We charge less where our costs are less, and prices are based on the location of your Amazon S3 bucket.
563e311061a8013065267579	X	Are you referring to CloudWatch Logs functionality?
563e311061a801306526757a	X	Yes, I wish to save then inside my pesonal bucket, instead of the inaccessible amazon's bucket
563e311061a801306526757b	X	Sup guyz, I would like to save my log files from CloudWatch inside my personal bucket and not in the default amazon's bucket, for easily download then. Somebody have already done it or just know if its possible?
563e311061a801306526757c	X	It is not possible. CloudWatch Logs stores its data in Streams, which are stored within CloudWatch Logs. It is not possible to point CloudWatch Logs to another data store (eg an Amazon S3 bucket). Logs can be retrieved from CloudWatch Logs via the GetLogEvents() API call. There is also a Retention setting to determine how long data should be retained within CloudWatch Logs.
563e311061a801306526757d	X	I think you'll have to change the image sizes on your own... I haven't looked at the API yet, though.
563e311161a801306526757e	X	I need from Instagram to serve me images on specific sizes. Is this possible?
563e311161a801306526757f	X	I think you will have to grab the images, say with standard_resolution, and then change the image size yourself... sitepoint.com/image-resizing-php
563e311161a8013065267580	X	okay, that's solution for less then standard_resolution. and is there any way to get bigger size than standard_resolution
563e311161a8013065267581	X	It depends... If you read the information here (help.instagram.com/276722745781769) you will see that Instagram always saves/uploads photos in the best resolution possible. However, that resolution differs for different devices, so you might not always be dealing with the same sized images...
563e311161a8013065267582	X	Thanks for the improvements guys
563e311161a8013065267583	X	I am trying to set custom width for images in Instagram, using Instagram API. I am using hashtag to grab all images related. Can I ask Instagram to serve me custom image sizes? I know there are 3 sizes: thumbnail, low_resolution, standard_resolution. Is that all what I can get? Thanks!
563e311161a8013065267584	X	Simple do a search and replace on _5 (or _6, ie) to get get your desired size (see below): Image sizes appear to be: http://distilleryimage7.s3.amazonaws.com/xxxxxxxxx_7.jpg 612px × 612px http://distilleryimage7.s3.amazonaws.com/xxxxxxxxx_6.jpg 306px × 306px http://distilleryimage7.s3.amazonaws.com/xxxxxxxxx_5.jpg 150px × 150px It appears _8,_4,_3 are not valid sizes.
563e311161a8013065267585	X	Instagram API returns three standard image resolutions. There is an additional undocumented image size that you could also use however, this might be a bit brittle/liable change in the future and it may also be against their terms of service (not sure). I noticed that this image was available when I saw that the Instagram website was serving up higher resolution images- I'm not really sure why Instagram choose not to make this image available. From the API, you can get these standard image sizes: thumbnail (150 x 150) https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/s150x150/e15/11821182_168255950173048_11130460_n.jpg low_resolution (320 x 320) https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/s320x320/e15/11821182_168255950173048_11130460_n.jpg standard resolution (640 x 640) https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/s640x640/sh0.08/e35/11821182_168255950173048_11130460_n.jpg Then, by looking at their website, I noticed they served up a larger version of the file at 1080 x 1080: undocumented large image (1080 x 1080) https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/s1080x1080/e15/11821182_168255950173048_11130460_n.jpg
563e311161a8013065267586	X	Instagram uses Amazon S3 to save its photo and has a certain naming convention that goes like this: By simply getting the photo URL via the API and then changing the _7, _6, _5 in the image URL to another value (via a search/replace), you can control the size of the image.
563e311161a8013065267587	X	Try it? A 401 with the proper headers will show an authentication popup, regardless how the resource is accessed (direct, img tag, script tag)..
563e311161a8013065267588	X	Good point! So I'll ignore 401's then, and send 400s or 403s instead.... How about the other Http codes... any other weird issues with them?
563e311161a8013065267589	X	I'll have to repeat my "try it". ;-) You could read the HTTP RFCs, but browsers can have their own quirks.
563e311161a801306526758a	X	and that's the reason I ask here - RFCs only get you so far... I'm asking if others have gone down this road and realized browser A has quirk B running on os X ;-) and if they do say so, then I can check that scenario myself and see if it really is a problem, and it'd be documented here for others to find instead of having to try it themselves ;-)
563e311161a801306526758b	X	I don't really understand the question. The http protocol doesn't know what it is sending/receiving. The error codes are not bounded to the shipped content. It is up to the browser to handle these errors accordingly.
563e311161a801306526758c	X	Still note quite an answer to my question - i want to know what different status's are well supported by browsers, but it appears that maybe the answer is all of them. At least this proves/shows an api sending a 403. Thanks!
563e311261a801306526758d	X	EDIT: In an attempt to clarify my question, here's what I'm trying to understand. If a web page embeds an image like so: How do browser handle receiving different HTTP error status codes from the image url? Is it very consistent across browser, and basically treated the same as if the image wasn't there (404) ? Note that I'm aware that I can "just try it", but I don't have every browser/os/phone around to try it out on, and I'd like to understand how this actually works in reasonably modern desktop and mobile browsers (~IE9 and newer as a fuzzy line). Plus if anyone else is every wondering the same thing, they could come here and see the answer too ;-) ORIGINAL QUESTION: I'm implementing a REST service that returns images and videos securely for a client. I was thinking it'd be nice to send out different HTTP response codes for different types of failures: Will responding with these different error codes work exactly the same as a 404 response for all reasonably modern browsers from both an HTML and Javascript perspective? I know the error code would be different of course, but what I'm trying to ensure is that no strange security errors pop up as a result of using these different HTTP responses.
563e311261a801306526758e	X	A 403 is given by lots of public APIs, for example, Amazon's S3 service responds with a 403 when an object with a temporary access time expires. e.g. An url like so would generate a 403 with the following response (Note: the below isn't a valid S3 url... keys/signature have been changed) https://somebucket.s3.amazonaws.com/test/4a828d8cf5633ab41e2fb8deba599178.jpg?AWSAccessKeyId=ACIAICGLRMBL2PTLALHQ&Expires=1424782953&Signature=sWko4DDDRaBS151iEhEZzfDRbU%3D
563e311261a801306526758f	X	Since you're using REST I assume you're using Ajax or Angular to request your images, right? In that case it is up to javascript to catch the errors. So, your browser will not show any popups when javascript handles it correctly. You could start with something like Note, the 401error can be found in the header of the response, and replaces the 'Content-Type: image/png' header. So the Client (browser) has no way to know that is an image was requested and handles it like any other page.
563e311a61a8013065267590	X	Do you have a link we can look at?
563e311a61a8013065267591	X	the file actually now called jqm.css
563e311a61a8013065267592	X	yeah I see it - ok upload manually
563e311a61a8013065267593	X	Have a look.... There shouldn't be any images now, so its ok. But you can click on ask and go to other pages...You see the dirrefence
563e311b61a8013065267594	X	can you put it back the other way now?
563e311b61a8013065267595	X	Problem solved! You saved the day dude!
563e311b61a8013065267596	X	I'm using com.amazonaws.services.s3.AmazonS3 to upload files from my server to amazon's one. Everything is working perfectly but I'm going crazy because of an issue: I am uploading several files and folders (mostly images, js and css files). The files get uploaded nicely however I have one particular css file (jquery-mobile-1.0.1.css) that gets uploaded however when an html relies on that file, the css is not loaded, until I go and manually upload that one file again and make it public. I literally tried everything (changed file name, location, encoding) but nothing seems to be working. Does anyone have an idea what can cause the problem? The files are uploaded dynamically, so the way that the particular css file gets uploaded does not differ from other css files. Any help is appreciated.
563e311b61a8013065267597	X	The mime type (HTTP content type header) on the files you uploaded is incorrect. S3 does not always set them correctly. Both JS files and CSS files are set to text/html - should be text/css and text/javascript. You need to set them appropriately on the upload API call. Some upload libraries will do this for you, yours clearly isn't. http://orensol.com/2009/07/14/google-chrome-2-css-content-type-and-amazon-s3/
563e311c61a8013065267598	X	possible duplicate of How to send PUT HTTP Request in Flex
563e311c61a8013065267599	X	Did you mean HTTPService?
563e311c61a801306526759a	X	Does Flex 4 support put request? I know that Silverlight 4 support put request using its client http stack.
563e311c61a801306526759b	X	In action script 3 we can use put method using below API http://livedocs.adobe.com/flash/9.0/ActionScriptLangRefV3/flash/net/URLRequestMethod.html I have used it on desktop apps to upload images to Amazon S3. Since it need air it can be used only on desktop and not on web apps.
563e311c61a801306526759c	X	yes, it does. You have to set the 'method' attribute to "PUT". i.e.: For more information have look on the Reference-Lib
563e311c61a801306526759d	X	there is no real way to do this.
563e311d61a801306526759e	X	This is a great idea! Yah. I'm not looking to make it more secure than like youtube or anything. I just want to defer like 95% of people from downloading the music.
563e311d61a801306526759f	X	yup, so come up with some hashing unique ID mechanism that both your server and your jukebox player would understand. maybe use mp3 song hash + some secret data.
563e311d61a80130652675a0	X	Do you know how you would implement this? I'm trying to think.. the jukebox would be loaded on the client's side - so he'll see the UID and just be able to type that right in to look like the jukebox. I'm a security novice by the way.
563e311d61a80130652675a1	X	Disregard half of the last statement.. I didn't see your most recent post..
563e311d61a80130652675a2	X	Wouldn't the client have access to mp3 song hash + some secret data that he could just send back to the server via play.php?hash=... ??
563e311d61a80130652675a3	X	It's harder to do in flash. You can't just look at the source and find the mp3 file in the URL.
563e311d61a80130652675a4	X	You can still record it, though. As long as anything DRM'ed can be played back it can be saved without the restrictions too. Fact of life (and of technology).
563e311d61a80130652675a5	X	Yah my goal is to defer like 95% of people from trying it.. that's all.
563e311d61a80130652675a6	X	I don't think it's any harder in Flash. Switch to program that records audio being played back (there's a million free ones), hit "record" button. Actually I think that's even easier than digging through an HTML for a URL. :-)
563e311d61a80130652675a7	X	I'm looking to build a jukebox and I am wondering how one would secure songs that are in <audio> tags with HTML 5. I don't want people to be able to download the song, but I'd like to stream it via those <audio> tags. Any suggestions?
563e311e61a80130652675a8	X	It's possible using Amazon S3 (similar to the way Soundcloud does it) to generate secure mp3 links for use in your HTML5 player. You generate a secure mp3 on S3 that is valid for a very short time (seconds or minutes), there by prevent someone from copying and sharing the link. You will need to generate the link dynamically using SDK/API. See example of how to use PHP to generate secure links.
563e311e61a80130652675a9	X	you could check referer, use some hashing mechanism (unique ID) to verify the streaming player is your jukebox, not the stream saver etc. BUT: whatever you do, some people will figure it out (or using the last resort - catching the whole stream, following on what kind of data your jukebox sends etc.)
563e311e61a80130652675aa	X	Whatever you give people to listen via a stream can be saved to disk too.
563e311e61a80130652675ab	X	This is not possible. In order for the client computer to be able to play the song, the song has to be transferred there. Period.
563e311e61a80130652675ac	X	Hi and thank you for your quick answer but when do you mean "storing on the filesystem in my app-root/data directory" do you mean localy on the smartphone?
563e311e61a80130652675ad	X	No, i mean storing it on your openshift gear.
563e311e61a80130652675ae	X	I would like to develop a mobile app which publish pictures online inside a mongodb on openshift.com. At the same time I would like to host website where some customers can see a "wall" with all the publications. My question is: Openshift.com is a good choice when you need to store datas (pictures, text) from smartphones and in the same time to host a website? p.s. the pictures will be stored directly in the mongodb, is it a good way to do? Thank you in advance
563e311e61a80130652675af	X	Yes, you would be best off writing a simple API that your mobile application will connect to (in your language of choice) to get the data into mongo. I personally would either store the images on the filesystem in your app-root/data directory, or store them on Amazon S3.
563e311f61a80130652675b0	X	This looks like it will do the trick. The process is already handled in a waterfall type workflow, so if any part of it fails before the last step (which would be updating this status) it terminates early, so the status will only get updated if everything else is successful. Thanks for the help!
563e311f61a80130652675b1	X	A year and a half later, I get an upvote for this, but I feel guilty: DynamoDB has released secondary indexes, which are made for this purpose without the complexity of you having to manage another table. Check them out!
563e311f61a80130652675b2	X	Lol, I noticed that as well and was also considering updating the post. But yeah, secondary index are nice... though there are still instances where your original answer does still apply. Anyone interested in this post, be sure to read the documentation when deciding if they are best for you, especially the "Use Indexes Sparingly" section of the documentation if you're wondering why (documention available here: docs.aws.amazon.com/amazondynamodb/latest/developerguide/…).
563e311f61a80130652675b3	X	I'm fairly new to Amazon's AWS and its API for Java, so I'm not exactly sure what the most efficient method for what I'm trying to do would be. Basically, I'm trying to setup a database that will store a project's ID, it's status, as well as the bucket and location when uploaded to an S3 bucket by a user. What I'm having trouble with is getting a list of all the project IDs that have a status of "ready" under the status attribute. Any projects that are of status "ready" need to have their ID numbers loaded to an array or arraylist for later reference. Any recommendations?
563e311f61a80130652675b4	X	The way to do this is to use the scan API. However, this means dynamo will need to look at every item in your table, and check if its attribute "status" is equal to "ready". The cost of this operation will be large, and will charge you for reading every item in your table. The code would look something like this: There is a way to make this better, though it requires denormalizing your data. Try keeping a second table with a hash key of "status", and a range key of "project ID". This is in addition to your existing table. This would allow you to use the Query API (scan's much cheaper cousin), and ask it for all items with a hash key of "ready". This will get you a list of the project IDs you need, and you can then get them from the project ID table you already have. The code for this would look something like: The downside to this approach is you have to update two tables whenever you update the status field, and you have to make sure that you keep them in sync. Dynamo doesn't offer transactionality, so you have to be ready for the case where the update to the master project table succeeds, but your secondary status table doesn't. Or vice-versa. For further reference: http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/QueryAndScan.html
563e311f61a80130652675b5	X	I'm getting a pdf stream as a result of an api call. I would like to use this stream as a pdf template, modify it with mPDF and save it to amazon s3 bucket. Is it possible to do that without saving the stream locally as tmp file and reading it with setSourceFile()? Is there any way to pass the stream to mPDF? I would really like to avoid saving the file locally, therefore I already tried to upload the stream directly to s3 and then pass the url to setSourceFile, but this results in an error: fseek(): stream does not support seeking when filesize() is being called in pdf_parser.php Thanks
563e311f61a80130652675b6	X	Ok thanks, that makes sense and puts me on the right path I think
563e312061a80130652675b7	X	I seem to be having trouble from Amazon's documentation (both general AWS and the .Net API references) how to accomplish getting a cloudfront url for a file uploaded to S3. I already have C# ready to upload files into our company's S3 into specific buckets and folders. However, I have been told that due to costs we shouldn't be serving out of S3, we should be serving from the cloudfront. So after I upload files (via code) to S3, how do I then generate/retrieve a cloudfront url for it?
563e312061a80130652675b8	X	I don't know about the .net specific code, but basically you create a distribution that points to your s3 bucket. The URL you use will be the domain you get back from creating the distribution + the s3 object's name that you want to download. The documentation on this is here
563e312061a80130652675b9	X	just wondering if you have a solution to simailr problem stackoverflow.com/questions/24431130/…
563e312061a80130652675ba	X	i have a web application running on tomcat7 and mySql, now i want to deploy it to aws.. the application need to write file on disk (such as images uploaded by users) some one can help me pointing out how to configure a good infrastructure in aws for my need? i read this: http://aws.amazon.com/elasticbeanstalk/ , i think that my needs are an EC2 instance for running tomcat and an Amazon RDS whit mySql... i need something else for R/W file ? i need to change my code in some way in order to make it work on aws? thanks in advance, Loris
563e312061a80130652675bb	X	Elasticbeanstalk is a good way to get started with an application deployment at AWS. For persistent file storage you can use S3 or an EBS volume. S3 allows you to read and write using amazon's SDK/API. I am using this on a java application running at AWS and it works pretty smoothly. http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html It is also possible to mount S3 over NFS, you can read some interesting points in this answer: How stable is s3fs to mount an Amazon S3 bucket as a local directory With EBS you can create a persistent storage volume attached to your EC2 node. Please note that EBS is a block level storage device so you'll need to format it before its usable as a filesystem. EBS allows you to help protect yourself from data loss by configuring EBS snapshot backups to S3. http://aws.amazon.com/ebs/details/ -fred
563e312061a80130652675bc	X	See if this post[stackoverflow.com/questions/12586147/… helps.
563e312161a80130652675bd	X	I'm building a new asp.net web api application using straight html 5 and a web api restful service. I'm already using forms authentication and the [Authorize] attribute to secure my web api calls. I'm trying to stay true as much as I can to the restful principles. I'm mimicking what an existing app does that uses two-factor authentication and uses asp.net web forms. The two-factor authentication is not used for logging in, but for an additional task of remoting into another machine through the site and a plugin. The existing web application uses session state to store a pin that is generated and emailed to the user. Then when the user enters in the pin it is checked against the pin in session state. So it seems like my options are... So what out of these options is the best option? Are there other possibilities?
563e312161a80130652675be	X	I'm not 100% sure I understand your architecture, but it seems very relevant that your security should at no point rely on the client. Assume the user has a javascript debugger in their browser (which most people actually do without realizing), and a custom build of your plugin. So the secondary PIN challenge should be embedded on the server side into the "remoting" protocol. If it is something based on RDP or VNC, it should be possible to change the connection password of the given user on the fly to the one-time generated PIN.
563e312161a80130652675bf	X	I bilieve, you should not pay too much attention to RESTful principles (wait, did I say that? :))... You see, the one side is the theory and the other side is the practice. In practice, you often need to break RESTful principles for security! Let's take the nonce - it's a big random number generated for each request, which gets then checked on the server, if the same nonce has not been sent before. This involves storing state (nonces) - i.e., it's not stateless, but it's sometimes critical for security. Also, by the way, OAuth, which is often used, is not RESTful. For encypting the PIN you should not use JavaScript client! You can implement some kind of encryption endpoint on the domain JavaScript client is used, though: Your coworker gave a good recommendation! Amazon is one of the best REST API implementations. Also check my idea: Looking for feedback on my REST-style API authentication design and two-factor authentication.
563e312161a80130652675c0	X	I suggest hiring someone since you've chosen not attempt anything yourself.
563e312161a80130652675c1	X	Yeah, try something yourself and show us your code - other than that we won't solve your troubles for you. Although I can assure you it is possible. The best way in my opinion would be uploading the file to your server and then move it to another (with some sort of remote service) and then delete it from your server.
563e312161a80130652675c2	X	You might find some info about uploading APIs here: developers.google.com/+/domains/api/media/insert
563e312161a80130652675c3	X	I have a website and I would like to allow visitors to upload images. I want these images to be stored somewhere so that I can check them out and maybe put them on the website. The 'problem' is that the server doesn't have much space, so if possible I don't want to keep the images on the server and instead have the website right away upload the images to for example a Google+ album.(Doesn't have to be Google+, just an example) So, what I want: * Have users upload images. * Store the images somewhere else so that it doesn't fill up the server * Be able to check out these images myself What would be the best way to achieve this? (If it's possible to begin with)
563e312161a80130652675c4	X	From what I learn that you can run an image hosting service for your website, and upload images via REST api. Take a look at ImageS3, https://github.com/images3/images3-play, with this, you can upload you images to Amazon S3. and manage these images via ImageS3 web admin console. Hope this answer can help you a little.
563e312261a80130652675c5	X	USE real time streaming from server side.
563e312261a80130652675c6	X	@VedantTerkar: how does that help?
563e312261a80130652675c7	X	@Qantas94Heavy look at this: therealtimeweb.com/index.cfm/2005/11/2/…
563e312261a80130652675c8	X	@VedantTerkar: the questioner wants to prevent users from being able to keep the music on their computer -- they can always get the information from the stream or record it off stereo mix.
563e312261a80130652675c9	X	if you're using php then it is .htaccess trick: stackoverflow.com/questions/14024877/…
563e312261a80130652675ca	X	So what about soundcloud for example ? Sometimes the Download button is disabled, but we still can listen.
563e312261a80130652675cb	X	My point is that, with all any form of DRM, if you are able to listen to or view the final content, the consumer can record the content. The best you can hope for is to obfuscate the link to the content. The SoundCloud example you gave certainly allows the listener to record the content, just not through the officially-supported download mechanisms. If the user can see/hear the content, they can record the content. Search for "the analog hole" for obvious examples.
563e312261a80130652675cc	X	Ok, I will add jingles to it. Thank's.
563e312261a80130652675cd	X	works fine! http://www.my-website.com/myaudio.mp3 redirects to the main page. thank you.
563e312361a80130652675ce	X	Your welcome! :) glad to help
563e312361a80130652675cf	X	hum... sorry but it seems <audio src="myaudio.mp3" /> does not work now. Direct download is blocked, but my page cannot get the file.
563e312361a80130652675d0	X	Try putting the song in a folder and not the main directory
563e312361a80130652675d1	X	I think (tested) that this is the best solution : stackoverflow.com/questions/10236717/…. It works as well
563e312361a80130652675d2	X	I mean, I want to share an audio file without letting users to download it. Here are my tries : Is it possible ? If so, how ?
563e312361a80130652675d3	X	The correct answer is that, if you allow your users to listen to the music, they will, by definition, be able to record said music. There is a very clear distinction between security, which you are requesting, and obfuscation, which you actually want.
563e312361a80130652675d4	X	Add a .httacess function to block users from downloading files in a path or a whole entire folder. You can also make a password for files also so they wont be able to be deleted without a password This goes in your .htacess file The last RewriteRule change the download-file.php to your actually php or html file with the download.
563e312361a80130652675d5	X	It's possible using Amazon S3 (similar to the way Soundcloud does it) to generate secure mp3 links for use in your HTML5 player. You generate a secure mp3 on S3 that is valid for a very short time (seconds or minutes), there by prevent someone from copying and sharing the link. You will need to generate the link dynamically using SDK/API. See example of how to use PHP to generate secure links.
563e312361a80130652675d6	X	@PhilippeBossu No idea. Did you try to ask Nomadesk support about this? Asking them would be more effective for both you and them.
563e312461a80130652675d7	X	Propose something like google docs or dropbox, with secured storage for enterprise
563e312461a80130652675d8	X	Documents are stored for offline access = they are stored on your local computer, or on any of your varied remote destinations for access.
563e312461a80130652675d9	X	Documents are encrypted = for transmission and if you pay the additional amount for their super secured service.
563e312461a80130652675da	X	Solution work on iOS, Android and if possible Windows / Mac OS
563e312461a80130652675db	X	So, in short. My solution ticks all the boxes. Stop voting me down! If you're voting me down, at least comment on WHY you think my answer deserves a down-vote.
563e312461a80130652675dc	X	Are there any software solutions or SDKs parts that handle the following: I know that google docs will propose offline access and distribute an application on IPad but I would like to know if there are alternatives ? Thank you Regards
563e312461a80130652675dd	X	See Nomadesk offerings - they do exactly what you need.
563e312461a80130652675de	X	You can try one of the following cloud services:
563e312461a80130652675df	X	Try crashplan http://www.crashplan.com/ .
563e312461a80130652675e0	X	I'd like to copy a file that's already on S3 to another bucket using the Zend Framework and without having to copy the file to the local filesystem first. How can this be achieved? If copying to another bucket is not possible, is it possible to just make a copy of an object (without having to copy the file to the local filesystem)?
563e312561a80130652675e1	X	Amazon S3 allows you to copy an object between buckets and to create a copy of the object in the same bucket using the PUT copy call - http://docs.amazonwebservices.com/AmazonS3/latest/API/index.html?RESTObjectCOPY.html To create the object copy using Zend Framework you can use the copyObject method in Zend_Service_Amazon_S3
563e312561a80130652675e2	X	Thanks for the help jterrace. The Service Account Authentication example seems to be close to what I need but I don't know what to include or what to do to get the Cloud Storage to work. Unfortunately I'm not using the App Engine but I'll use it if it makes it easier..
563e312561a80130652675e3	X	I have a private bucket on Google Cloud storage and I am having trouble figuring out how to read/write data to it. Right now I am using Amazon S3 and they make it VERY easy to do. Literally: But with Google Cloud Storage, I am very confused. I've come to the conclusion that I need a signed URL but I really don't want to have to use gsutil everytime I want to grab data (which is a lot). All of this is happening server-side so I can't have the customer do the whole browser pop up and sign into google sort of thing. Is there not a way to just use an API/Secret combination like Amazon S3? Could anyone help me figure out the easiest way to get this to work for my PHP server side application?
563e312561a80130652675e4	X	From App Engine, you can use the Google Cloud Storage PHP stream wrapper to write to GCS. If you want your PHP script to work outside of App Engine, there is also the Google APIs Client Library for PHP. There is a GCS PHP example you might want to take a look at. Specifically, in your case, you'd probably want to use Service Account Authentication, for which there is an example of here.
563e312561a80130652675e5	X	I need a recommendation for some image hosting with a fancy web-interface for uploading pics and an API (or even better — an iOS SDK) to get those photos in a mobile app. I'm aware of that question: Need recommendation for image hosting with API access. However, I don't need any image-uploading API, what I need is a way to populate galleries of photos to my iOS app users: upload the pics through a web interface, and then be able to retrieve all of them in the app. So there shouldn't be any user authentication in the mobile app, and the photos are meant to be public. I'm looking into several flickr iOS API libraries, but I'm not sure if any of them can be used without user authentication — e.g. by anonymously pulling the galleries of some specific app-hardcoded user (me). Or maybe I'm overthinking and there's another suitable solution that I'm missing? Any help is highly appreciated!
563e312561a80130652675e6	X	Ok, it turned out that Flickr is more or less the thing I needed — it obviously has a nice web-interface for managing your photos, and there are a bunch of components to access these galleries on iOS device (for instance, https://github.com/devedup/FlickrKit and https://github.com/lukhnos/objectiveflickr). Moreover, you don't need to log in to access publicly available photo sets and collections, so I decided to go for this option.
563e312561a80130652675e7	X	I am not sure if you would like to host this image hosting service by yourself. if you don't, then you can take a look at ImageS3, https://github.com/images3/images3-play. An open and free image hosting service run on top of Amazon S3. It is like other cloud image hosting service which provides REST api, web interface admin console.
563e312661a80130652675e8	X	Yes, you can sercurely let your visitors upload to S3, without giving away your Amazon AWS credentials, by creating a HMAC signature signature on your server, which then is used by the visitor's browser to upload directly to S3. Have a look here: docs.amazonwebservices.com/AmazonS3/latest/dev/… However, I do not know if that is possible to combine with pause/resume-upload.
563e312661a80130652675e9	X	But would it be possible to port this multipart stuff into javascript (or flash/actionscript) and do it in the browser, without giving away aws credentials?
563e312661a80130652675ea	X	Thank you Steffen, but my understanding is that the low level doesn't allow to pass from client to S3 directly (without web server), at least that's what the PHP example shows if I'm correct...am I missing something here?
563e312661a80130652675eb	X	@style-sheets: There is no way to avoid this other than exploring a client side JavaScript solution using the S3 REST API directly; I don't think it is much of a problem cost/performance wise, insofar EC2 to S3 connections are rather fast and free within one region. Obviously this approach offloads the pause/resume problem to HTML forms though, which again requires JavaScript as well as modern browsers supporting the File API - maybe How to resume a paused or broken file upload can get you started in case.
563e312661a80130652675ec	X	@style-sheets I believe you can accomplish this using a browser plugin like flash, silverlight or java and directly using the REST API. I currently use a silverlight plugin to upload large files (up to 5GB) directly to S3. I haven't implement pause/resume don't use the S3 large file support but it should be possible. Using a plugin is the only way to achieve broad browser coverage. Checkout this SO thread stackoverflow.com/questions/478799/…. There are lots of links there to various free and not free plugins.
563e312661a80130652675ed	X	There is an official JS SDK today. Also, there is an intelligent multipart upload API available.
563e312661a80130652675ee	X	(I'm new to Amazon AWS/S3, so please bear with me) My ultimate goal is to allow my users to upload files to S3 using their web browser, my requirements are: My two-part question is: Is it even possible to do this for large files? If so how? If it's possible to upload directly to S3, how can I handle pause/resume? PS. I'm using PHP 5.2+
563e312661a80130652675ef	X	The meanwhile available AWS SDK for JavaScript (in the Browser) supports Amazon S3, including a class ManagedUpload to support the multipart upload aspects of the use case at hand (see preceding update for more on this). It might now be the best solution for your scenario accordingly, see e.g. Uploading a local file using the File API for a concise example that uses the HTML5 File API in turn - the introductory blog post Announcing the Amazon S3 Managed Uploader in the AWS SDK for JavaScript provides more details about this SDK feature. My initial answer apparently missed the main point, so to clarify: If you want to do browser based upload via simple HTML forms, you are constrained to using the POST Object operation, which adds an object to a specified bucket using HTML forms: POST is an alternate form of PUT that enables browser-based uploads as a way of putting objects in buckets. Parameters that are passed to PUT via HTTP Headers are instead passed as form fields to POST in the multipart/form-data encoded message body. [...] The upload is handled in a single operation here, thus doesn't support pause/resume and limits you to the original maximum object size of 5 gigabytes (GB) or less. You can only overcome both limitations by Using the REST API for Multipart Upload instead, which is in turn used by SDKs like the AWS SDK for PHP to implement this functionality. This obviously requires a server (e.g. on EC2) to handle the operation initiated via the browser (which allows you to facilitate S3 Bucket Policies and/or IAM Policies for access control easily as well). The one alternative might be using a JavaScript library and performing this client side, see e.g. jQuery Upload Progress and AJAX file upload for an initial pointer. Unfortunately there is no canonical JavaScript SDK for AWS available (aws-lib surprisingly doesn't even support S3 yet) - apparently some forks of knox have added multipart upload, see e.g. slakis's fork, I haven't used either of these for the use case at hand though. If it's possible to upload [large files] directly to S3, how can I handle pause/resume? The AWS SDK for PHP supports uploading large files to Amazon S3 by means of the Low-Level PHP API for Multipart Upload: The AWS SDK for PHP exposes a low-level API that closely resembles the Amazon S3 REST API for multipart upload (see Using the REST API for Multipart Upload ). Use the low-level API when you need to pause and resume multipart uploads, vary part sizes during the upload, or do not know the size of the data in advance. Use the high-level API (see Using the High-Level PHP API for Multipart Upload) whenever you don't have these requirements. [emphasis mine] Amazon S3 can handle objects from 1 byte all the way to 5 terabytes (TB), see the respective introductory post Amazon S3 - Object Size Limit Now 5 TB: [...] Now customers can store extremely large files as single objects, which greatly simplifies their storage experience. Amazon S3 does the bookkeeping behind the scenes for our customers, so you can now GET that large object just like you would any other Amazon S3 object. In order to store larger objects you would use the new Multipart Upload API that I blogged about last month to upload the object in parts. [...]
563e312661a80130652675f0	X	I don't know of any flash uploaders offering anything more powerful than standard HTTP Post but you could develop your own flash software for the client with co-ordinating software on the server.
563e312761a80130652675f1	X	stackoverflow.com/a/14916525/6309 might now help, maybe as a zipped p2 update? I mention that in my edited answer below.
563e312761a80130652675f2	X	The problem I have is that github serves directories and files as HTML pages. To access the contents of a file there is a "raw" link. However, my update site is made up of several files and folders so I think I need to find a way to get a "raw" directory from github. Alternatively, can an update site be created in a zipped file where all the files are stored in a single archive?
563e312761a80130652675f3	X	@mchr: I don't think an update site can be one single zip file. As for the raw directory listing, that is a good question to support.github.com: support.github.com/discussions/site/… might help: replace 'tree' by 'raw' in your GitHub address.
563e312761a80130652675f4	X	It appears that if you use the root of the repository, and use "raw" in the url, as @BrunoMedeiros suggests in his answer, the update site works fine.
563e312761a80130652675f5	X	A note on big repositories and Github usage. This links describes what Github expects the limits to be: help.github.com/articles/what-is-my-disk-quota - "Rule of thumb: 1GB per repository, 100MB per file". That should be fine for most Eclipse update sites. If it gets too big, you can reset the underlying Git repo.
563e312761a80130652675f6	X	Thanks for sharing about the raw links to serve github folder publicly.
563e312861a80130652675f7	X	I tried this (here is my update site) but it seems, oddly enough, that this works for Eclipse on Linux, but from Windows computers I can neither install nor update. Is anyone else experiencing this?
563e312861a80130652675f8	X	I just tried to install your feature (Pig Latin) in Windows, from github.com/eyala/pig-eclipse-update-site/raw/master, and it installed fine.
563e312861a80130652675f9	X	Github pages are not a proper place for an update site. See my answer below. It may be fine if your jars are small but overall they advise against placing binaries there.
563e312861a80130652675fa	X	I am using github to develop an eclipse plugin. I would like to have a public eclipse update site for my plugin and wondered if I can use github for this? I know that github can be used for hosting individual files by using the "raw" links provided on the file information pages.
563e312861a80130652675fb	X	Github pages are not a proper place for an update site. Github pages may not properly serve large binary files as explained in this issue. It may be fine if your jars are small but overall they advise against placing binaries there. Instead they recommend placing binaries in the download section of the repository. I'd be happy if this situation changes because it would be very convenient to publish an update site by pushing to github. For now one would have to use their API to programatically upload files in the download section. Answers to this other question points to some libraries and scripts that uses this API for use within java/maven, perl, ruby, etc.
563e312861a80130652675fc	X	You may now try it in a release page (July 2013). See "Publish a project release (binary/source packages) on Github?" Original answer (January 2013) I have not tested it, but technically, a p2 repository can be defined in any shared path (either filesystem-shared or web-based-shared) You should only need to:
563e312861a80130652675fd	X	Forget the Github project releases feature, that won't work as a true update site (see notes at the end). To achieve what you want, you can create a Github repo, commit/push your p2 repository there and then serve it as an update site, using raw links. So for example, for the repository: https://github.com/some-user/some-repository/ you can serve it as an update site using the link: https://github.com/some-user/some-repository/raw/master/ Notes: Yes, if you open the update site link in a browser, github will give you no file listings, but rather a 404. But that's fine. The Eclipse update site mechanism doesn't need the parent link to be valid. Instead Eclipse will directly look for <update-site URL>/artifacts.jar (or .xml) and from the information in artifacts.jar, it will itself discover the URLs of the other artifacts stored in the update site. AFAIK, at no point does the Eclipse update mechanism need the web server to do file listings of a directory. Note2: if you use Github project releases, you can only attach a zipped p2 repository to it. That is not a proper update site because it is a static repository: there is no URL to which new releases can be uploaded to. Eclipse won't be able to automatically discover new updates, rather the user will need to download the zip for each new release he/she wants to update to. (Also with a proper update site, only the necessary artifacts for installation/update/query will be downloaded - a minor advantage)
563e312861a80130652675fe	X	http://pages.github.com/ The Github Pages feature allows you to host arbitrary folders of files without git turning each file into a github page.
563e312861a80130652675ff	X	No it is not possible anymore, the Downloads API has officially been deprecated. From the GitHub blog: However, some projects need to host and distribute large binary files in addition to source archives. If this applies to you, we recommend using one of the many fantastic services that exist exactly for this purpose such as Amazon S3 / Amazon CloudFront or SourceForge. Check out our help article on distributing large binaries. See this help article on distributing large binaries.
563e312861a8013065267600	X	Note that if the file is public, there's no need for the authorization token: curl -H 'Accept: application/vnd.github.v3.raw' https://api.github.com/repos/owner/repo/contents/path will return the raw file.
563e312961a8013065267601	X	Is the -H 'Accept: application/vnd.github.v3.raw' necessary? I was able to access a private file without that part.
563e312961a8013065267602	X	@NickChammas: without that header I get a JSON response with metadata and the actual file contents base64 encoded, rather than the file as plain text.
563e312961a8013065267603	X	What if the above command gives error as curl: (56) Proxy CONNECT aborted what does that mean.
563e312961a8013065267604	X	This does no longer work, github has changed the mechanism..
563e312961a8013065267605	X	On the CI server, I want to fetch a config file that we maintain on Github so it can be shared between many jobs. I'm trying to get this file via curl, but these approaches both fail (I get a 404):
563e312961a8013065267606	X	The previous answers don't work (or don't work anymore). You can use the V3 API to get a raw file like this (you'll need an OAuth token): curl -H 'Authorization: token INSERTACCESSTOKENHERE' -H 'Accept: application/vnd.github.v3.raw' -O -L https://api.github.com/repos/owner/repo/contents/path All of this has to go on one line. The -O option saves the file in the current directory. You can use -o filename to specify a different filename. To get the OAuth token follow the instructions here: https://help.github.com/articles/creating-an-access-token-for-command-line-use I've written this up as a gist as well: https://gist.github.com/madrobby/9476733 EDIT: API references for the solution are as follows:
563e312961a8013065267607	X	Or, if you don't have a token:
563e312961a8013065267608	X	I was struggling with this for a few minutes until I realized all that is needed is to wrap the url in quotes to escape the ampersand. That worked for me in my private repo.
563e312961a8013065267609	X	(I don't have rep points enough to comment on the "You can use the V3 API to get a raw file like this (you'll need an OAuth token):..." answer, so here's my comment: I ran into an authentication error when the url was redirected to Amazon S3: Only one auth mechanism allowed; only the X-Amz-Algorithm query parameter... Changing from the Authorization header to the ?access_token= param worked for me.
563e312a61a801306526760a	X	My website is hosted in yahoo small business, The database server is MySQL, I want to automate db backup, My main requirement is backup files must go to amazon bucket. Yahoo small business does not provide access to putty, which could be used to run the backup script. So, I have scheduled a task locally(windows machine) which calls the php backup script on the server and saves the backup files to folder in the server, Now i want to save these files to amazon bucket instead of folder, How to do this.
563e312a61a801306526760b	X	Don't quote me entirely on this but, Yahoo in general is very very very limiting in what they allow there clients to do. Best you can do that I can think of off the top of my head is run another scheduled task on your windows machine that will physically download the file then pass it to the bucket. Though pending yahoo doesn't limit it severely like they used to you may even be able to get away with the concept of another scheduled item on your windows machine. Where it just pings another script on your yahoo that will use the Amazon S3 api's to drop the file in a bucket. All in all though your best bet is get off of yahoo, get a cheap VPS where you are open to do a lot more with your server than a standard shared system especially one that limits its customers 10 fold. I host many of my sites through myhosting.com I have a vps account, i got something like 200 gigs of storage a terabyte of transfer and so many more perks for 36 dollars a month. And I can do virtually anything I want with the server as if it were my own linux based machine to do such with. Anyway thats off topic. The point I am ultimately trying to make is getting on an account like that you can run CRON jobs that is the same thing as the scheduler on your machine but on the same server as your site you want to back up, so no worries about a disconnect between server and home brew machine. You will also be able to install other 3rd party concepts that may secure your transfer to Amazon every night or do many of other things as well.. Just food for thought. All in all staying on Yahoo is going to really limit your ability.
563e312a61a801306526760c	X	I think a big reason to go with S3 would be object durability. How much engineering and management will need to go into a CouchDB deployment that has eleven 9's of durability and near infinite scalability?
563e312a61a801306526760d	X	I'm writing a simple web API that revolves entirely around file uploads. Users can upload files to the service via the HTTP-based API, and the service will generate files for users to access and will also need to store them along with the uploaded file. So there will be a lot of files at play. Basically, I'm trying to decide between storing these in CouchDB and storing these in something like Amazon's S3. With CouchDB, I'd probably have a single document for the initial uploaded file, by the user, with the attachment data inline in the _attachments collection. Additional files made by the system would be added to that document. (The service does document conversion, so they upload an Excel XLS and the system generates a PDF, TXT, etc.) I think this would be nice, because one delete on the uploaded document record will also delete the generated PDFs, TXTs, or any other attachments. With S3, I feel the security it knowing that I'm using a hosted solution dedicated entirely to individual file storage. It also dedicates that bandwidth exclusively to those files, and it wouldn't be coming from my API web server. The downsides are that it adds a lot of additional logic to my API code, and now I have to keep a lot of remote files in sync with what my local CouchDB database knows of them. Also, I'd have to deal with request signing and stuff if I wanted end-users to access the files directly off S3. Documents are all stored individually, so deleting the user's uploaded attachment from CouchDB would require me to make several delete queries to S3 for the other files, as well. I'm familiar with S3, and use it in a current project, but CouchDB looks really awesome in how it allows attachments. I'd love to use it, but are there any gotchas or downsides? Does CouchDB attachments make more sense than S3 in the scenario I described above, with a lot of uploaded files being stored? Thanks
563e312a61a801306526760e	X	I've used couchdb successfully for many projects and several a similar project. You get so much in the box of box with couchdb. The questions I have are what's the average size of your files, and how big do you think your db will go?
563e312a61a801306526760f	X	Both solutions are quite sensible: there are pros and cons. One advantage you didn't mention for storing files as CouchDB attachments is that they will be replicated with the data. It makes it easier for continuous backup, and in your snapshots your data will be coherent with your files.
563e312b61a8013065267610	X	Salesforce attachment file size is officially limited to 5MB (doc) but if requested they can increase this limit on a one to one cases. My question: Can I retrieve this newly allowed file size limit using the API. Context: Non-Profits are applying for grants via a web portal (.NET), all data is stored in Salesforce. They are asked to attach files. We read the file size they try to upload and send an error message if it exceeds 5MB as it will not be accepted by Salesforce. This is to avoid having them wait for few minutes to upload to only be told that the file size is too large. We would like to update our code so that it allows files bigger than 5MB if Salesforce allows it. Can we retrieve this information via the API? Thank you!
563e312b61a8013065267611	X	You can call the getUserInfo() function in the SOAP API, part of the returned data includes the field orgAttachmentFileSizeLimit (this appears to be missing from the docs, but is in the WSDL)
563e312b61a8013065267612	X	I'll recommend going away from Salesforce to store files, more if you'r expecting to hit limits, also there is a limit on the space for storing organization wide, a useful service like Amazon S3 would be very useful, you can then attach the S3 url to your record in case it is needed, it'll be also available for external applications without having to load your org's api consumption.
563e312b61a8013065267613	X	Why don't you use Google Drive? or Dropbox?
563e312b61a8013065267614	X	I want more like google cloud or amazon cloud.. is there any alternative
563e312b61a8013065267615	X	If your total storage requirements are very small, you can probably afford Google Cloud Storage. Storing a gigabyte over a month will cost you about 8 cents, and downloading that gigabyte every single day will cost you about 12 cents per day. That's about $4 for the whole month. Or you can just use Google Drive for free. It also has an API: developers.google.com/drive/v2/reference
563e312b61a8013065267616	X	I am a Mtech student and currently doing my project in cloud environment. I am looking for a free cloud storage with small memory to upload and access text files on to cloud. Is there any cloud storage which can help in my requirements. Thanking you in anticipation
563e312b61a8013065267617	X	Amazon S3 going to be one of the best solution, But you are talking about free. I would suggest you to go for dropbox it's free upto some extent. I already integrated with my rails application, they are providing API and Secret Key for app configuration. Have a look to below link 12 free cloud storage options Five Best Cloud Storage Providers
563e312c61a8013065267618	X	Several possibilities: Amazon S3, Dropbox, GoogleDrive.. Be aware of network latency if your app is deployed outside the same physical infrastructure. You can find a complete application example working on CloudBees + Amazon S3 (here) However, you can get a similar approach through the runtime and file system storage you want.
563e312c61a8013065267619	X	Thanks for you suggestions. Looks like multi-iteration hashing with sufficiently long salts will be secure enough (even when salt values are stored with hashed passwords).
563e312c61a801306526761a	X	That sounds fine as a storage system for all but the most stringent requirements. Remember to be careful with how passwords are sent across the wire, to expire people's session identifiers whenever they log in, and all of the other equally important security considerations that plague web authentication.
563e312c61a801306526761b	X	Thanks, I am looking at AWS docs now.
563e312c61a801306526761c	X	I'm building a non-browser client-server (XULRunner-CherryPy) application using HTTP for communication. The area I'm pondering now is user authentication. Since I don't have substantial knowledge in security I would much prefer using tried-and-tested approaches and ready-made libraries over trying to invent and/or build something myself. I've been reading a lot of articles lately and I can say all I have been left with is a lot of frustration, most of which contributed by this and this blog posts. What I think I need is: So the question is: what are the modern (headache-free preferrably) techniques and/or libraries that implement this? (No sensitive information, like credit card numbers, will be stored). I've been looking at OAuth and they have a new revision which they strongly recommend to use. The problem is the docs are still in development and there are no libraries implementing the new revision (?).
563e312c61a801306526761d	X	This may not be a complete answer, but I would like to offer some reassuring news about rainbow tables and the web. I wouldn't worry too much about Rainbow Tables with regards to the web for the following reasons: (1) Rainbow table cracks work by examining the hashed password. On the web, the hashed password is stored on your database so to even consider using rainbow tables one would first need to hack your entire database. (2) If you use a salt as most password storage systems do, then rainbow tables rapidly become unfeasible. Basically a salt adds a series of extra bits to the end of a given password. In order to use a rainbow table, it would need to accommodate the extra bits in each plaintext password. For example the first link you showed us had a rainbow table implementation that could crack up to 14 characters in a password. Therefore if you had more than 14 bytes of a salt that system would be useless.
563e312c61a801306526761e	X	Amazon Web Services, OpenID, and OAuth have examples of request signing. Amazon Web Services is an easy example to follow because there isn't a more complex protocol around the interactions. They basically involve having the client or server sign a request by hashing all of its fields with a previously set up key (or keypair), and having the other end verify the signature by doing the same. Replaying the hash is prevented by including a nonce or timestamp in the fields. Setting up keys or other credentials to allow this can be done over SSL, and it should be noted that one of the motivation of OAuth WRAP is to replace some or all of this request signing with SSL, for ease of implementation.
563e312c61a801306526761f	X	After a lot of poking around and trying to write my own prototype based on Amazon S3 design which (I thought) was pretty secure, I found this excellent website which has answers to all my questions, an Enterprise Security API Toolkit, and much, much more: OWASP.
563e312c61a8013065267620	X	My Spark applications take a folder as the input which contains lots of text file. How can I get filename of each input split programatically?
563e312d61a8013065267621	X	Normally you cannot retrieve the filename from which input originated. However if you use Hadoop HDFS FileSystem api you can list the content of the directory. And iterate over all files. But this is not pure spark program anymore. And it is dependent on the storage layer used (HDFS, amazon s3, etc).
563e312d61a8013065267622	X	try this.it worked for me. Hope it will help you.
563e313061a8013065267623	X	So, you're basically asking how to do asymmetric crypto?
563e313061a8013065267624	X	I am usure about the file formats that should be used to feed the App with the keys.
563e313061a8013065267625	X	You tell me. Which encryption toolset are you using? PGP/GPG, for example, uses keys that look like this y3xz.com/yuvadm.pgp
563e313061a8013065267626	X	Thanks. What about the file format for the keys?
563e313061a8013065267627	X	That completely depends on the encryption/toolset you are using. But I think PublicKey even implements Serializable, so you can just dump it into a file. Also, you can read through the official Java security doc.
563e313161a8013065267628	X	You cannot just accept every public key within an automated message. Your key needs some kind of verification, otherwise anybody can perform a man in the middle attack (among others). You can either do this out of band, or using a PKI method of verification (certificate chain).
563e313161a8013065267629	X	My Java app needs to handle encrypted files. This is the workflow: Amazon S3 provides the Java classes for download/upload, decryption/encryption. This API takes as input java.security.KeyPair. I am unsure how the customer should supply the key to My Java app, so that the app can get the key as java.security.KeyPair? What would be the proper way to exchange the keys between Customer and App? Which key file format could be used?
563e313161a801306526762a	X	Ussually, assymmetric encryption/decryption works like this: Now you have got a file from the customer. To be able to send back encrypted messages, you need another public/private key pair. This time, the customer must be the only one knowing the private key. He can - for instance - put the public key in his file that he has sent to you. Anyway, somehow you need to get a public key from him. With that key, you encrypt your files and send them to Amazon S3. The user picks them up and decrypts them with his private key. So, the customer must not give you a java.security.KeyPair, because those contain the private key. It's unsafe to send the private key. But he can send you the public key as a java.security.PublicKey. I think the best way would be to send it to you either within the file he supplies anyway, or within a separate file that he uploads at the same time and besides the supplied file.
563e313161a801306526762b	X	The problem is that you don't have a method of distributing trust yet. Fortunately there is one that works reasonably well: TLS. TLS certificates are stored within the browser (and in the JRE, if you require a thick client instead). Your key pair should be generated locally (or on a secured machine and imported). The private key should be kept safe the whole time. The customer connects to your site using TLS, and downloads your public key. Then the customer uploads the public key of his key pair. This can be performed during some setup/configuration phase. Now the customer can encrypt files for you, and you can encrypt files for the customer. Note that TLS already provides encryption (confidentiality). So what you have gained is that files are protected during storage, after they have been transported. Once you have trust in the public key (and a trustworthy system) you could send files over plain HTTP. Adding a signature is pretty important, otherwise anybody can replace the files in storage. Some audit logging is probably required as well, otherwise files may be removed. Other schemes are possible (I prefer a PGP scheme for file encryption/decryption), but they require out of band communication of the keys. Note that this is just the basic scheme, there are a lot of pitfalls, but working out a specific security architecture for you application is clearly off topic.
563e313161a801306526762c	X	Thanks a lot! Few weeks ago I tried to launch instance of opengeo and inserted this script link in user text and I was able to stop instance but I don't understand this script
563e313161a801306526762d	X	I'm a newbie and my knowledge is very low, so sorry if I'm not making myself clear. I would like to launch an EC2 instance of the OpenGeo AMI, but I noticed that I can't stop this instance because it doesn't use EBS. I tried to launch an instance of OpenGeo and then attach an EBS volume, but there is still no stop function available. I think I should create an EBS volume with commands inserted in user text when I'm launching the instance, but that is only a theory. Can you provide me a solution, thanks.
563e313161a801306526762e	X	Just realized that there appears to be a free OpenGeo Suite Community Edition on Amazon Web Services as well indeed. Consequently you might be able to succeed by means of the articles referenced in my initial answer below. Good luck! Given this appears to be a paid AMI, I'm afraid that it isn't possible, see the FAQ Can I create paid AMIs backed by Amazon EBS snapshots?: No. Currently Amazon DevPay only supports AMIs that are backed by Amazon S3. This means that your customers cannot use Amazon EC2 instances that leverage Amazon EBS backed root devices yet. Otherwise I'd recommend to first ask the AMI creator to provide EBS backed ones instead or as well, see Eric Hammond's excellent summary You Should Use EBS Boot Instances on Amazon EC2 for why this is almost always preferable. Finally, section Converting Amazon EC2 instance store-backed AMIs to EBS-Backed AMIs within Creating Amazon EBS-Backed AMIs answers your question, which is no easy process though: There's no simple API or button in the AWS Management Console that converts an existing Amazon EC2 instance store-backed AMI to an Amazon EBS-backed AMI. [...] The required steps are only briefly outlined in the following paragraphs there accordingly. There are quite some other posts around going into more detail though, e.g. Creating an EBS-backed AMI from an S3-backed AMI, Amazon EC2 – Boot from EBS and AMI conversion or Amazon EC2 - Swap root instance store device with EBS device.
563e313261a801306526762f	X	the only way to save more than one file at once is to zip them together.
563e313261a8013065267630	X	I want to know if they are a way to download multiple files with javascript and package it,i knew that there is the file system api but i can have only 5mb per object and it support is only for modern browsers. Now i have a Store server in amazon s3(Just for music and photos) and a Dedicated server in an other company, the user have the way to download multiple songs but its a slow process because the server have to request to amazon s3 download each song a zipped and then fired to the user to download so how i can do this with out download two times, i thought in javascript but im not sure how. Thanks
563e313261a8013065267631	X	In terms of s3 urls, are there really 2 kinds? And why? What are the different syntaxes? and Is this it? Why are there 2? Are there more? Are these correct?
563e313261a8013065267632	X	The additional functionality of providing multiple URL patterns for an object in the S3 is due to the Virtual Hosts and Website Hosting and publishing the data from the root directory. I got this info from In the Bucket starting URL style - bucket.s3.amazonaws.com/key you can simple add the files like favicon, robots.txt etc where as in the other URL pattern - s3.amazonaws.com/bucket/key - there is no notion of root directory where you can put those files. Content Snippet from AWS S3 Page - Virtual Hosting of Buckets : In general, virtual hosting is the practice of serving multiple web sites from a single web server. One way to differentiate sites is by using the apparent host name of the request instead of just the path name part of the URI. An ordinary Amazon S3 REST request specifies a bucket by using the first slash-delimited component of the Request-URI path. Alternatively, you can use Amazon S3 virtual hosting to address a bucket in a REST API call by using the HTTP Host header. In practice, Amazon S3 interprets Host as meaning that most buckets are automatically accessible (for limited types of requests) at http://bucketname.s3.amazonaws.com. Furthermore, by naming your bucket after your registered domain name and by making that name a DNS alias for Amazon S3, you can completely customize the URL of your Amazon S3 resources, for example, http://my.bucketname.com/. Besides the attractiveness of customized URLs, a second benefit of virtual hosting is the ability to publish to the "root directory" of your bucket's virtual server. This ability can be important because many existing applications search for files in this standard location. For example, favicon.ico, robots.txt, crossdomain.xml are all expected to be found at the root.
563e313261a8013065267633	X	Probably not possible, since CloudTrail streams data to CloudWatch Logs. CloudWatchLogs then generates a metric in CloudWatch. This can then trigger an Alarm, which sends a notification to SNS, which sends a message to SQS. Unfortunately, the instance information is not stored in the CloudWatch metric, so it can't be passed to the next process. Closest option is to configure Auto Scaling to send a notification when an instance is terminated, but that probably doesn't match your use-case.
563e313261a8013065267634	X	@john can you thinknof anyother way of doing it ?
563e313261a8013065267635	X	I basically need to know the information of nodes terminated automatically
563e313361a8013065267636	X	I have configured an Alarm on CloudTrail events. The metric of the alarm is to trigger it when it finds the information in the logs that an instance is terminated. The information sends a message to an SNS topic which in turn calls SQS. It is all working as of now. However, when I read SQS I can only see the information of the alarm, but I would like to obtain details of the instance that got terminated. For example, below is what I see: But I instead I want to see the instance id information which was there in the CloudTrail logs :
563e313361a8013065267637	X	AWS CloudTrail delivers log files to your Amazon S3 bucket approximately every 5 minutes. The delivery of these files can then be used to 'trigger' some code that checks whether a certain activity has occurred. And a good way to run this code is AWS Lambda. The basic flow is:  Here are two articles that describe such a setup:
563e313361a8013065267638	X	Why are you using S3 for file delivery. You should use CLoudFront.. which does.
563e313361a8013065267639	X	+1 That's a good trick, but CloudFront doesn't gzip content. It will serve gzipped content only if the origin serves gzipped content.
563e313361a801306526763a	X	Here's CloudFront's instructions for using gzipped content - and they don't automatically serve gzipped content, the URL for the gzipped version needs to be generated on the client side if you're using S3. docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/…
563e313361a801306526763b	X	Can I use JavaScript to detect if the user's browser supports gzipped content (client side, not node.js or similar)? I am trying to support the following edge case: There are a lot of possible files that can load on a particular web app and it would be better to load them on demand as necessary as the application runs rather than load them all initially. I want to serve these files off of S3 with a far-future cache expiration date. Since S3 does not support gzipping files to clients that support it, I want to host two versions of each file -- one normal, and one gzipped with content-type set to application/gzip. The browser of course needs to know which files to request. If JavaScript is able to detect if the browser supports gzipped content, then the browser will be able to request the correct files. Is this possible?
563e313361a801306526763c	X	Javascript can't, but you can use Javascript to detect wether or not the browser supports gzipped content. I commented above and would just like to reiterrate, you should use CloudFront anyway, which does gzip content. If you are using S3, then there is zero reason why you would not want to use CloudFront, however, for the purposes of answering your question... This blog post perfectly addresses how you would detect if the browser supports Gzip. http://blog.kenweiner.com/2009/08/serving-gzipped-javascript-files-from.html Here is a quick summary: 1) Create a small gzipped file, gzipcheck.js.jgz, and make it available in CloudFront. This file should contain one line of code: 2) Use the following code to attempt to load and run this file. You'll probably want to put it in the HTML HEAD section before any other Javascript code. If the file loads, it sets a flag, gzipEnabled, that indicates whether or not the browser supports gzip.
563e313361a801306526763d	X	Well cloudfront does not gzip content automatically. Till the time Amazon decides to do automatic gzip compression in S3 and Cloudfront one has to use the below workaround.
563e313361a801306526763e	X	I am trying to pass an image created by Cordova's camera plugin to Amazon Web Service's S3. In the past, I have used the HTML File API to create my S3 params, and been able to pass the file object. I can't directly link to how you do this, but there is an example on this page under the section 'example uses the HTML5 File API to upload a file on disk to S3'. But this file has not been inserted by an input element, so I can't access anything like files[0] - the file is returned by Cordova either as a base64 or an available file location. So I am trying to figure out how I would replicate that action using Cordova information. The solution I have worked off of is found here which results in the code below: This process works, but: What I would like to do is send the actual file object instead of messing with this base64. Alternatively, I would take a better/smarter/faster way to handle the base64 to S3 process.
563e313361a801306526763f	X	I was able to use the 'JavaScript Canvas to Blob' polyfill to create a blob from my Base64, and then send the returned Blob on to S3. Because it is now a blob instead of a coded string, I can use getSignedURL in the s3 APK to reference the image. The only change from above is: var theBody = btoa(evt.target._result); Becomes var theBody = window.dataURLtoBlob(evt.target._result); https://github.com/blueimp/JavaScript-Canvas-to-Blob
563e313461a8013065267640	X	What why would someone down vote this? Can you explain? This works for me!
563e313461a8013065267641	X	Voted down because this post specifically asks about S3 and S3 is not a normal ec2 instance so it is not running an SSH server. You need to use an HTTP protocol to talk to it.
563e313461a8013065267642	X	Is it possible to upload a file to S3 from a remote server? The remote server is basically a URL based file server. Example, using http://example.com/1.jpg, it serves the image. It doesn't do anything else and can't run code on this server. It is possible to have another server telling S3 to upload a file from http://example.com/1.jpg
563e313461a8013065267643	X	If you can't run code on the server or execute requests then, no, you can't do this. You will have to download the file to a server or computer that you own and upload from there. You can see the operations you can perform on amazon S3 at http://docs.amazonwebservices.com/AmazonS3/latest/API/APIRest.html Checking the operations for both the REST and SOAP APIs you'll see there's no way to give Amazon S3 a remote URL and have it grab the object for you. All of the PUT requests require the object's data to be provided as part of the request. Meaning the server or computer that is initiating the web request needs to have the data. I have had a similar problem in the past where I wanted to download my users' Facebook Thumbnails and upload them to S3 for use on my site. The way I did it was to download the image from Facebook into Memory on my server, then upload to Amazon S3 - the full thing took under 2 seconds. After the upload to S3 was complete, write the bucket/key to a database. Unfortunately there's no other way to do it.
563e313461a8013065267644	X	I think the suggestion provided is quite good, you can SCP the file to S3 Bucket. Giving the pem file will be a password less authentication, via PHP file you can validate the extensions. PHP file can pass the file, as argument to SCP command. The only problem with this solution is, you must have your instance in AWS. You can't use this solution if your website is hosted in other Hosting Providers and you are trying to upload files straight to S3 Bucket.
563e313461a8013065267645	X	Technically it's possible, using AWS Signature Version 4, Assuming your remote server is the customer in the image below, you could prepare a form in the main server, and send the form fields to the remote server, for it to curl it. Detailed example here. 
563e313461a8013065267646	X	you can use scp command from Terminal. 1)using terminal, go to the place where there is that file you want to transfer to the server 2) type this: N.B. Add "ec2-user@" before your ec2blablbla stuffs that you got from the Ec2 website!! This is such a picky error! 3) your file will be uploaded and the progress will be shown. When it is 100%, you are done!
563e313461a8013065267647	X	I'm using the LitS3 library to help upload photos from my ASP.NET MVC application to Amazon S3. I've read through all the documentation, googled around, and i can't figure out how to set the cache-control header for the photos when i upload them. I know you can do it with the REST API, but as i'm using the LitS3 library, that's not an option (unless i scrap the library altogether). Has anyone figured out how to do it? I see the documentation there is a section for "want more flexibility" - which seemingly gives access to nearly 100% of the API, but can't see how i can apply that to my situation. Here's how i'm currently uploading: Where inputStream is a Stream, that i get from the HttpPostedFileBase.InputStream in my MVC action. None of the AddObject overloads support setting any other header apart from content-type. So it looks like i need to dig deeper and use a lower-level method, but as i said - just can't find out how. Can anyone help?
563e313461a8013065267648	X	Got it! Thanks to this thread, which doesn't concern cache-control, but it shows how to use AddObjectRequest with a given Stream. Here's the working code, if anyone else is interested:
563e313561a8013065267649	X	I wanted to go the S3 route initially, but the volume of uploads is fairly low plus it's tough to sell this sort of thing to our non-technical customers who are super concerned about security. I would think it would be relatively simple to access files on the filesystem of a server on the same local network.
563e313561a801306526764a	X	In my Rails app, for HIPAA reasons, I need to keep all my data stored on a separate server from my web application. This is simple to do with the database, but what's the best way to allow my rails app (on the web server) to access uploads on the filesystem of the database server? Or should I just store the uploads in the database (mysql)? I'm using Rails 3.2 and Paperclip, but could switch to Carrierwave or another solution Thanks!
563e313561a801306526764b	X	Or should I just store the uploads in the database (mysql) No. I will point you here for a better explanation Storing Images in DB - Yea or Nay? what's the best way to allow my rails app (on the web server) to access uploads on the filesystem of the database server I would probably create another web server with a separate rails app that was responsible solely for serving up files from it's local filesystem through some authenticated API. It looks like people do use Amazon S3 successfully with HIPAA-compliant websites for file storage.
563e313561a801306526764c	X	you can configure paper clip to store your documents on amazon s3 https://github.com/thoughtbot/paperclip#storage http://rubydoc.info/gems/paperclip/Paperclip/Storage/S3
563e313661a801306526764d	X	Is it possible to have growing files on amazon s3? That is, can i upload a file that i when the upload starts don't know the final size of. So that I can start writing more data to the file with at an specified offset. for example write 1000 bytes in one go, and then in the next call continue to write to the file with offset 1001, so that the next bytes being written is the 1001 byte of the file.
563e313661a801306526764e	X	Amazon S3 indeed allows you to do that by Uploading Objects Using Multipart Upload API: Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. [...] One of the listed advantages precisely addresses your use case, namely to Begin an upload before you know the final object size - You can upload an object as you are creating it. This functionality is available by Using the REST API for Multipart Upload and all AWS SDKs as well as 3rd party libraries like boto (a Python package that provides interfaces to Amazon Web Services) do offer multipart upload support based on this API as well.
563e313661a801306526764f	X	Is it possible to expose Amazon S3 account bucket (shared by ACL setings) to the users setup using new Amazon AIM API under different account? I'm able to create working IAM policy when related to the users and objects belonging to a single account. But as it seems this no longer works when two different accounts are involved - despite account 2 being able to access account 1's bucket directly. Sample policy is: In this case AIM user is able to list test.doom bucket (owned by the same AWS account) and not 'test1234.doom' bucket (owned by the different AWS account). This is despite one account having correct ACL permissions to access the other bucket.
563e313661a8013065267650	X	It looks like this can't be done. http://aws.amazon.com/iam/faqs/#Will_users_be_able_to_access_data_controlled_by_AWS_Accounts_other_than_the_account_under_which_they_are_defined Although it looks like in the future they might be allowed to create data under another account. http://aws.amazon.com/iam/faqs/#Will_users_be_able_to_create_data_under_AWS_Accounts_other_than_the_account_under_which_they_are_defined
563e313661a8013065267651	X	The more obvious way to avoid the potential problem seems like it would be to use the virtual url format for an S3 bucket, https://bucket-name.s3-us-west-2.amazonaws.com/key (substituting the appropriate region, or just "s3" for US-Standard) unless your bucket name has a dot in it... in which case, create a bucket name without a dot... no?
563e313661a8013065267652	X	Good idea. That does work exactly how I had hoped S3 would and avoids any possible same-origin problems. Thank you!
563e313661a8013065267653	X	I have a web page that uses JavaScript to access users' Gmail accounts through the Gmail API using OAuth 2.0, and I'd like to host the HTML and JS files on Amazon's S3 service. For this reason, I've authorized https://s3-us-west-2.amazonaws.com/ as one of the origin URIs in the Google Developer Console's credential page. My page works great, but it has me wondering: If my page shares the same origin URI as every other web page hosted on S3, wouldn't this mean that another page would satisfy the same-origin policy and be allowed access to my page's access tokens? For example, say a user just granted my app access to his Gmail account. Google sees an approved origin URI and sends back an access token. Now imagine some other page hosted on S3 had copied my app's Client ID (publicly stored as a variable in JS) and was repeatedly requesting for an access token from Google. Once my user grants that access to my page, won't the other page also receive the token from Google since it satisfies the same origin URI? I realize one solution to this potential problem is to host my web page on a domain that I have control over, but I'm curious if that is the only way.
563e313661a8013065267654	X	I also couldn't figure this out. I'm not sure if Google's docs are to blame, but it's hard to construct such a solution. I ended up using the Datastore's Image API instead.
563e313661a8013065267655	X	Brandon, just to clarify, if I have an App Engine app, and I want users to upload to the Google Cloud Storage, I use Blobstore Python API?
563e313661a8013065267656	X	You certainly can, yes.
563e313761a8013065267657	X	I'm trying to set up a basic python-based google app engine site that allows users to upload files to google cloud storage (mostly images) I've been going through the documentation for the JSON API and the GCS client library overview (as well as blobstore etc) and still don't have a good handle on which is the best method and how they are related. Would be great if someone could give an overview of this or point me to some resources I can check out Also, any sample code that's relevant would be really helpful. I've been able to run the upload samples here but not sure if they're useful for an app engine setup: https://github.com/GoogleCloudPlatform/storage-file-transfer-json-python Thanks!!
563e313761a8013065267658	X	Google Cloud Storage has two APIs -- the XML API and the JSON API. The XML API is XML based and very like the Amazon S3 API. The JSON API is similar to many other Google APIs, and it works with the standard Google API client libraries (for example, the Google API Python library). Both of these APIs can be used from anywhere, with or without App Engine, and are based on RESTful HTTP calls. App Engine provides a couple of standard ways to access Google Cloud Storage. The first is built into App Engine's API as a feature called the "Google Cloud Storage Python API". This does not directly use either the XML or the JSON API. It's deprecated and no longer recommended. The second App Engine library is called the "Google Cloud Storage Python Client Library" and is not part of the core App Engine API. Instead, it's a Python library put out by Google that you can download and add to your application like any other library. This library happens to be implemented using the XML API. It provides a few extra features that are useful for App Engine users, such as the ability to serialize an upload while it's in progress. There's an example of using this library included as part of the download, in the python/demo directory. You can also see it online. Equivalents of these tools also exist in Java and Go. There's no need for users to use the App Engine-specific libraries unless they find them to be useful. The standard Python library or even just hand-written HTTP calls using urlfetch will work just as well. The App Engine library merely provides some useful extras for App Engine users. App Engine also have a "Blobstore Python API". This is a feature specific to App Engine and distinct from Google Cloud Storage, except that it provides a few hooks into Google Cloud Storage, such as the ability to store files in Google Cloud Storage using the Blobstore API.
563e313761a8013065267659	X	interesting plugin for the S3 you got there.
563e313761a801306526765a	X	I can't agree more.
563e313761a801306526765b	X	philsturgeon is quite the programmer, I really trust his work.
563e313761a801306526765c	X	@IamJohnGalt I can't agree with you more I found it fairly annoying ones I opened up cake template files for the actually pages and what did i see? __d(code code) this kind of syntax is just plain stupid and in a way will make huge mess as this is fairly large project with allot of code.
563e313761a801306526765d	X	@burzum actually Yii seemed to have had a bigger file size I also considered Laravel Framework. But as I said Yii seemed-ed to be bigger of the two aw 23.xx MB, and from I read people are not very found of CakePHP. So I will probably go with CI or Laravel in the long term.
563e313761a801306526765e	X	@IamJohnGalt to be honest John, Cake reminds me of PERL ones it becomes a total hacked together mess ones the code becomes larger and larger. Also would you recommend Laravel or Symphony? If not I will just go with CI or Laravel as I do have a developer on my team familiar with LARAVEL and he said its pretty amazing
563e313761a801306526765f	X	@burzum OOP can become a mess on its own, not mentioning that this project got 3 developers of different skill level and the way they write code in general. Having odd syntax which I never seen before trying out CakePHP was annoying. It took me seconds to create new pages with CI and I did not have such luck with CakePHP.
563e313761a8013065267660	X	@IamJohnGalt actually I seen more and more articles pooping up about FuelPHP played with it today also seems like a great framework.
563e313761a8013065267661	X	I am building a web app designed to provide users with file-sharing ability, this is a safe app as the service is paid and will be using Stripe Payment API However I can't decide which php framework would be the best for this task. I am very clear that this can be done with both, but I want the framework to be light on the server and not eat allot of server resources like RAM and CPU as I am limited at this moment, but should the app grow higher then resources will be increased. I looked at Code Igniter and CakePHP. In my view CakePHP seems much more lighter then CodeIngiter but I never used either one so I can't judge. All I need the framework to do is Provide up-most security possible against XSS, Injections Able to work with file system and amazon s3 Play nice with amazon s3 and provide easy upload method to s3 Provide simple template scheme Provide fastest possible database and general response time Work with Stripe Payment API Work with PHP 5.3.x and MySQL 5.3.x as efficiently This is really it, if you can provide some pro feedback on which one would do the best or just do great for the task of file-sharing and dealing with files in s3 please let me know. Thanks in advance.
563e313861a8013065267662	X	If you want something small go for Yii if it is still small at all or any other micro framework. The questions is if it offers you everything you need to get it done in time and is solid. Cake had just two security issues in the last few years AFAIR, the latest one (1 year ago) has just hit ruby on rails. It is solid and has thousands of unit tests for the core. What is a template scheme for you? Something like twig? There are plugins for virtually any template engine. But template engines are a waste of resources. Go for plain php/html views. I know that some template engines support caching to increase performance but thats pointless because cake already offers caching techniques and also full page caching. So you can cache any output. If you want to go for OOP views checkout the Ctk plugin for Cake. I've written a plugin for storing files in any kind of storage system. It supports more storage engines than the few you list. And it was exactly written for the task you ask for. I'm storing files first locally and move them to S3 later. I don't know if there is any free available plugin for the stripe API, I think you'll have to do a little coding own your own... ;) If you go for cake you can use this plugin as a base and maybe you would like to make it open source, in this case I would help you with it. But I'm sure that any php framework can made to work with this API.
563e313861a8013065267663	X	There are not much new improvement done in CodeIgniter. To take advantage of the new PHP 5.3, I would recommend you to use CakePHP instead. They are actively working in version 3 which will fully support 5.3 . The following post might help you making a good decision. http://philsturgeon.co.uk/blog/2012/05/laravel-is-awesome
563e313861a8013065267664	X	I would suggest codeigniter. I have used both heavily and I really have a lot of issues with Cake. Cake is not the lighter of the two, I found that Cake wants you to do things cakes way otherwise you are in for a really sore time. This is not really how a framework should work. Codeigniter on the other hand hs a framework to support you but allows you to write unobstructed php all day as you need it. Cake is really good at getting in the way and has a much steeper learning curve. Codeigniter will let you do all of those things you need, I know for instance that there are handy functions like $this->security->xss_clean($your_data); that return data cleaned of xss issues. So its support for stuff like that is super handy. Templating is less robust in codeigniter, but the reason I like codeigniter is that it stays out of your way and lets you code PHP. This is good, because you know how to code in PHP, whereas when you code in Cake, you are writing weird cake code. This has the nasty side effect of making the code hard to manage should more people come on to the team and its impossible to ask people who do not know cake for help with cake stuff. Also cake does not guarantee backwards compatibility, your installation is your forever, if you use say version 1.6 then it might not be possible for you to move to 2.0 without serious rewrites (which seems short sighted on their part.) Cake will offer you more features though, so if your in a hurry it will help you along with large parts of the setup, provided your always cool with doing everything its way. If your not the best at database stuff cake totally abstracts that away. I personally hate that, because I'm serious about db fine-tuning and lots of things I want to do to make better use of the db cake just flat-out forbids. Codeigniter's freedom why it is the one I would pick. Its docs are great two. Oh yeah the docs. Codeignite's docs are so much better than cake, so that is a good reason to pick it too.
563e313861a8013065267665	X	Why do you need to talk to the cloud storage directly - why's that better than talking to a cloud-hosted web service acting as an interface / gatekeeper to the storage?
563e313861a8013065267666	X	It is an issue of performance, however hypothetical. I just feel like it would be a waste of resources.
563e313861a8013065267667	X	Am researching same general topic. AWS has introduced IAM that alleges to "Manage access for federated users" but have not had time to fully delve into it. Similarly AWS appears to be waiting-and-seeing attitude before implementing CORS (en.wikipedia.org/wiki/Cross-origin_resource_sharing) which would greatly contribute to this area.
563e313861a8013065267668	X	@tillda Curious: why are you trying to do everything via JavaScript? Is it cost-related (don't want to pay for a web or worker role - though they are now as cheap as $0.02/hour) - or something else?
563e313861a8013065267669	X	cloud storage is supposed to be for the users, under their account, or for you, under your secret account. just like being able to read js file's source, you can't have both secrecy and client-side-only code.
563e313861a801306526766a	X	Remember that JSONP is confined to GETS ... so it's only 1/2 a loaf.(en.wikipedia.org/wiki/…)
563e313861a801306526766b	X	That Azure thing looks like a good approach, but JSONP is in my opinion absolutely deprecated technique.
563e313861a801306526766c	X	@codingoutloud JSONP works and we'll need it in either way. This is a great article by Gaurav on how to use JavaScript to upload large files directly from client browser to Azure Storage. the JS files absolutely need to be hosted on the same storage account where you target the upload, because we cannot use JSONP to upload to the Storage. But we can use JSONP to get the SAS from the Server we control.
563e313861a801306526766d	X	I'm researching a possibility of using some cloud storage directly from client-side JavaScript. However, I ran into two problems: Security - the architecture is usually build on per cloud client basis, so there is one API key (for example). This is problematic, since I need a security per my user. I can't give the same API key to all my users. Cross-domain AJAX. There are HTTP headers that browsers can use to be able to do cross domain requests, but this means that I would have to be able to set them on the cloud-side. But, the only thing I need for this to work is to be able to add a custom HTTP response header: Access-Control-Allow-Origin: otherdomain.com. My scenario involves a lots of simple queue messages from JS client and I thought I would use cloud to get rid of this traffic from my main hosting provider. Windows Azure has this Queue Service part, which seems quite near to what I need, except that I don't know if these problems can be solved. Any thoughts? It seems to me that JavaScript clients for cloud services are unavoidable scenarios in the near future. So, is there some cloud storage with REST API that offers management of clients' authentication and does not give the API key to them?
563e313861a801306526766e	X	Windows Azure Blob Storage has the notion of a Shared Access Signature (SAS) which could be issued on the server-side and is essentially a special URL that a client could write to without having direct access to the storage account API key. This is the only mechanism in Windows Azure Storage that allows writing data without access to the storage account key. A SAS can be expired (e.g., give user 10 minutes to use the SAS URL for an upload) and can be set up to allow for canceling access even after issue. Further, a SAS can be useful for time-limited read access (e.g., give user 1 day to watch this video). If your JavaScript client is also running in a browser, you may indeed have cross-domain issues. I have two thoughts - neither tested! One thought is JSONP-style approach (though this will be limited to HTTP GET calls). The other (more promising) thought is to host the .js files in blob storage along with your data files so they are on same domain (hopefully making your web browser happy). The "real" solution might be Cross-Origin Resource Sharing (CORS) support, but that is not available in Windows Azure Blob Storage, and still emerging (along with other HTML 5 goodness) in browsers.
563e313961a801306526766f	X	Yes you can do this but you wouldn't want your azure key available on the client side for the javascript to be able to access the queue directly. I would have the javascript talking to a web service which could check access rights for the user and allow/disallow the posting of a message to the queue. So the javascript would only ever talk to the web services and leave the web services to handle talking to the queues. Its a little too big a subject to post sample code but hopefully this is enough to get you started.
563e313961a8013065267670	X	I think that the existing service providers do not allow you to query storage directly from the client. So in order to resolve the issues: Update: Looks like Google already solves your problem. Check this out. On https://developers.google.com/storage/docs/json_api/v1/libraries check the Google Cloud Storage JSON API client libraries section.
563e313961a8013065267671	X	This can be done with Amazon S3, but not Azure at the moment I think. The reason for this is that S3 supports CORS. http://aws.amazon.com/about-aws/whats-new/2012/08/31/amazon-s3-announces-cross-origin-resource-sharing-CORS-support/ but Azure does not (yet). Also, from your question it sounds like a queuing solution is what you want which suggests Amazon SQS, but SQS does not support CORS either. If you need any complex queue semantics (like message expiry or long polling) then S3 is probably not the solution for you. However, if your queuing requirements are simple then S3 could be suitable. You would have to have a web service called from the browser with the desired S3 object URL as a parameter. The role of the service is to authenticate and authorize the request, and if successful, generate and return a URL that gives temporary access to the S3 object using query string authentication. http://docs.aws.amazon.com/AmazonS3/latest/dev/S3_QSAuth.html A neat way might be have the service just redirect to the query string authentication URL. For those wondering why this is a Good Thing, it means that you don't have to stream all the S3 object content through your compute tier. You just generate a query string authenticated URL (essentially just a signed string) which is a very cheap operation and then rely on the massive scalability provided by S3 for the actual upload/download. Update: As of November this year, Azure now supports CORS on table, queue and blob storage http://msdn.microsoft.com/en-us/library/windowsazure/dn535601.aspx
563e313961a8013065267672	X	With Amazon S3 and Amazon IAM you can generate very fine grained API keys for users (not only clients!); however the full would be PITA to use from Javascript, even if possible. However, with CORS headers and little server scripting, you can make uploads directly to the S3 from HTML5 forms; this works by generating an upload link on the server side; the link will have an embedded policy document on, that tells what the upload form is allowed to upload and with which kind of prefix ("directories"), content-type and so forth.
563e313961a8013065267673	X	In order to have contents you're going to need to upload it in some fashion (iframe, ajax, flash, or traditional form).
563e313961a8013065267674	X	The file must be uploaded to the server first.
563e313961a8013065267675	X	Not necessarily, if the browser supports the new File API (see html5rocks.com/en/tutorials/file/dndfiles)
563e313a61a8013065267676	X	I'm actually using this plugin jasny.github.com/bootstrap/javascript.html#fileupload and I can get a preview of the file so the data are somewhere.
563e313a61a8013065267677	X	in that case the "data" will be on the server. You'll have to output the data to the client (browser) before you can access it via Javascript/jQuery
563e313a61a8013065267678	X	Hey, I tried your solution and that works. I retrieved the information but it's not well encoded. For a file I get \xc3\xbf\xc3 instead of getting this encode \xff\xd8\xff for the 3 first characters of my picture. What should I use to get the second encode for my picture?
563e313a61a8013065267679	X	@kschaeffler try fr.readAsDataURL(file); this is function reading Base64 datas, but i haven't tested it yet.
563e313a61a801306526767a	X	actually this function give me not enough data. that's already what I have from the preview that I use in the plugin jasny.github.com/bootstrap/javascript.html#fileupload. I think that the data I retrieve are not Base64 encoded and this is the problem, but I'm not sure
563e313a61a801306526767b	X	I actually need the same encode data that I retrieve when I post through html form with the "multipart/form-data" encode type
563e313a61a801306526767c	X	You don't get the actual content with this...
563e313a61a801306526767d	X	I actually have a file input and I would like to retrieve the Base64 data of the file. I tried: to retrieve the data. But it only provides the name, the length, the content type but not the data itself. I actually need these data to send them to Amazon S3 I already test the API and when I send the data through html form with encode type "multipart/form-data" it works. I use this plugin : http://jasny.github.com/bootstrap/javascript.html#fileupload And this plugins gives me a preview of the picture and I retrieve data in the src attribute of the image preview. But when I send these data to S3 it does not work. I maybe need to encode the data like "multipart/form-data" but I don't know why. Is there a way to retrieve these data without using an html form?
563e313a61a801306526767e	X	you can try FileReader API something like this.
563e313a61a801306526767f	X	input file element: get file :
563e313a61a8013065267680	X	I created a form data object and appended the file: and i got: in the headers sent. I can confirm this works because my file was sent and stored in a folder on my server. If you don't know how to use the FormData object there is some documentation online, but not much. Form Data Object Explination by Mozilla
563e313b61a8013065267681	X	I'm storing images in Amazon S3 using Fog and Carrierwave. It returns a url like bucket.s3.amazonaws.com/my_image.jpg. DNS entries have been set up so that images.mysite.com points to bucket.s3.amazonaws.com. I want to adjust my views an APIs to use the images.mysite.com/my_image.jpg URL. Carrierwave, however, only spits out the Amazon based one. Is there a simple way to tell Carrierwave and/or Fog to use a different host for its URLs than normal? If not, how would I modify the uploader to spit it out?
563e313b61a8013065267682	X	Come to find out that, as of June 6th, 2012, Amazon AWS does not support custom SSL certs, which makes this a moot point.
563e313b61a8013065267683	X	How you plans store the secret key in your clients?
563e313b61a8013065267684	X	Ismael, thanks for your interest. That's one of our main concerns.
563e313b61a8013065267685	X	My company is developing a SaaS application using PHP/Zend Framework. In order to provide access to different mobile devices, we have created a RESTful API and right now we are dealing with the authentication mechanism. After several meetings, we have agreed to use an "Amazon S3 REST API like" implementation. I mean to include in every request an APIKey (public) and a signature generated from a secretKey only known by client and server. All request are made using HTTPS protocol. My questions are: Thank you very much in advance. Best regards
563e313b61a8013065267686	X	I'm pretty new to Map/Reduce world and trying to evaluate the best option to figure if I can leverage it to create index in Solr. Currently, I'm using a regular crawl to fetch data and index it in Solr directly. This is working without any issues. But going forward, we need to access a sizable data residing in Amazon S3. There are around 5 million data presently stored in S3 , which needs to be indexed. I'm thinking of using Amazon Elastic Map/Reduce (EMR) to directly access the content from S3 and subsequently create the index in Solr. The data structure is simple, the url (which is unique) is the S3 key, the value is a XML file. The url will be used as the doc id in Solr while relevant portion of the XML data will be stored as fields in Solr index. My question is whether EMR is the right approach? The task is to access the data from S3, extract certain elements from XML, do some processing and then call Solr API to generate the index. The processing part requires few classes, possibly a chain of command pattern, before indexing the data. Is it something achievable? Doo I require a reducer or can use a mapper to do the process? If reducer is need, what will the scope of it? Currently, I've a single index which is storing the data. Any pointers on this will be highly appreciated. Thanks
563e313b61a8013065267687	X	You can try using MapReduceIndexer Tool. You can download it from apache-sole. It is part of contrib module.
563e313b61a8013065267688	X	did you manage to do this yet?
563e313b61a8013065267689	X	Im working on it, i think i will have to write a library that will handle it, extend the current api using decorator design pattern. Still have dificulties to think about the system design.
563e313c61a801306526768a	X	Thats awesome when i assume that all my files are public, but its not the case.
563e313c61a801306526768b	X	Ths problem I see here is the use of absolute url, which is a pain to handle in a prod env when switching servers
563e313c61a801306526768c	X	I edited my answer with a possible solution: add a flag to the SavedFile object.
563e313c61a801306526768d	X	As you may know Laravel uses Flysystem PHP package to provide filesystem abstraction. Ive started to use this feature in my project, just for fun uploaded some images to my Amazon s3 bucket, I have also instance of Cloudfront linked to this bucket. My problem is when im trying to display those images in my html page, i need a url. I could not find any "clean" way to do it, as flysystem is generic library i throught that i will be able to do something like this: for 'public' files and its easy to determine if file is "public" or not because its included in their api, so if im using s3 bucket as my driver for the disk i should get: "https://s3.amazonaws.com/my_awesome_bucket/path/image.png" or alternatively: for 'private' files - i should get a temporary url that will expire after some time. Is this something i can achieve only with specific implementation? for example if im using Amazon S3 i can easily run: But i dont want to do ugly "switch cases" to determine what driver im using. And how can i get a url from a cdn (like cloudfront) through the FileSystem interface? Any suggestion?
563e313c61a801306526768e	X	I think of Flysystem as an interface to a disk (or other storage mechanism) and nothing more. Just as I would not ask my local filesystem to calculate a public URI, I would not ask Flysystem to do it either. I create objects that correspond to the files that I save via Flysystem. Depending on my needs, I might save the public URI directly in the database record, or I might create a custom getter that builds the public URI based on runtime circumstances. With Flysystem, I know the path to the file when I write the file. In order to keep track of these files I'll typically create an object that represents a saved file: When I save the file, I create a record of it in the database: Whenever I need the public URI, I can just grab it off the SavedFile model. This is handy for trivial applications, but it breaks down if I ever need to switch storage providers. Another neat trick is to define a method that will resolve the public URI based on a variable defined in the child of an abstract SavedFile model. That way I'm not hard-coding the URI in the database, and I can create new classes for other storage services with just a couple of variable definitions: Now if I have a bunch of files stored on my local filesystem and one day I move them to Amazon S3, I can simply copy the files and swap out the dependencies in my IoC binding definitions and I'm done. No need to do a time consuming and potentially hazardous find-and-replace on a massive database table, since the calculation of the URI is done by the model: Edit: Just add a flag to the object:
563e313d61a801306526768f	X	Nice @theblackbenzkid I'll take a look at this
563e313d61a8013065267690	X	@ReynierPM please let me know how you get on. Would be interesting to see. Multi Channel Commerce is a huge entity within the eCommerce/Commerce sector as it migrates and creates the bridge of Non Web Sales to Online sales - OpenCart Database Layer can offer this with customisation.
563e313d61a8013065267691	X	well my client buy a extension in OpenCart site at 200 USD and this only can synchronize from OpenCart to Amazon but not from Amazon to OpenCart. Then I get a report as a flat file from Amazon, read that file, insert in a temporary table at my DB and then lookup at WorldCat because Google Books change their API and can't get books by ISBN as far as I know and all this is a module for OpenCart that I'm planning to sell in the near future when it's done and tested because it give me a lot of headaches. This is what I'm doing right now.
563e313d61a8013065267692	X	opencart.com/index.php?route=extension/extension/… opencart.com/index.php?route=extension/extension/…
563e313d61a8013065267693	X	Two new mods I have seen to help and aid in code bases. The only issue with external and commercial mods within Opencart IMO is trust factor. vBulletin and Magento have commercial companies that use the platform and environment therefore commercial and copyright licences and full commercial developers in the community - Opencart community is to closed and bloated with enthusiasts rather than pros.. you cannot even sign up as the developer or contribute to their forum.
563e313d61a8013065267694	X	I'm a book seller and have a storefront in Amazon and also have another store outside Amazon. I'll like to show my Amazon books in my other store with proper link to buy directly trough Amazon, is this possible using Amazon MWS? Any sample using PHP?
563e313d61a8013065267695	X	It is possible. Using Amazon MWS - you need to use the Amazon API with PHP - It would be not entirely complex to do; but making a plugin for Amazon, eBay and Opencart direct management of uploading and sync for multi commerce has always been on community pipelines. No plugin exists at the moment; free or commercial; you have to do this yourself. Alternative is to create a native app for opencart. Someone did this via a freelancer site; worth speaking to them: Google Cache of Mayoousa Amazon Store and Amazon Product Feed Integration PDF Or edit this (or uses code bases from this): eBay for OpenCart eBay for OpenCart Demo ebay for Opencart Module on Opencart Amazon S3 module for opencart Amazon Payments module for Opencart
563e313d61a8013065267696	X	My implementation is scalable, I have multiple servers, and each has a Sidekiq, running workers. I just need to run the Sidekiq job on the same server that received the file. This might lead to over use of some server, but I think that the load balancer will sort that out.
563e313d61a8013065267697	X	If the load balancer works well, then the solution 2 should have better performance as it only requires one upload.
563e313d61a8013065267698	X	So I'm faced with a choice of whether to implement a direct upload to S3 or to proxy images through my servers and then to S3. I do need to process images once they are uploaded, creating thumbs and different versions. So, in short what I would like to know is which approach is better, from those that might have done this before. Choice 1. Direct upload to S3 Choice 2. Upload through servers Looking at this, it would seem that uploading directly to S3 is actually slower than through my servers first. However, does the load that initial upload places on server outweigh the extra download step when going directly to S3? Also are there additional factors involved, such as network performance in each case and similar?
563e313d61a8013065267699	X	Regards to performance, uploading directly to S3 is a better solution. The reason is that S3 is highly scalable, which means: if you use one thread to upload files to S3, the speed is 1M/s, if you use 10 threads, the speed will be about 10M/s. For you problem, if you have a lot of users uploading files at the same time, S3 can handle these requests with less performance personality.As uploading files directly to the server, it requires your server can handle many requests at the same time, if your implementation is salable, it may have a chance to beat S3 on performance. But solution 1 has more costs as it has more transactions, and also data transfer out.
563e313d61a801306526769a	X	One issue with uploading directly to S3 is that you're uploading to the location of the bucket, whereas your servers may be distributed across multiple locations throughout the world. (For example, if you're running your servers in multiple EC 2 regions). In that case, you find you have lower latency (higher transfer rates) going to your server than going to S3 directly. Also, there may not be transfer out charges from S3 if you're transferring data to a server running on EC2 in the same region. Otherwise Matt's right - you'll pay anywhere from $0.02 - $0.25 / GB for transferring data from S3. Mike
563e313d61a801306526769b	X	I was a member of a big project using S3 storage. S3 upload is not 100% perfect officially. It is very important that whether you choose one of them you mentioned. Because each one has different benefits and drawbacks. You should choose first one if you want to make overload in client side instead of server side because of some reason like 1. Server is low spec or busy 2. There are huge upload files in huge clients side. 3. Customer want to know the result of upload immediately and want to do next action with uploaded files. But you should make strong error handling logins in upload program. Otherwise, you might be suffered from customer's complain. . You should choose second one if you want to have higher reliability of uploading process in terms of structure or development environment. Because S3 uploading is HTTP protocol so it is very slow and sometime it may make overload in upload side. If there are some abnormal cases in client side like stopping upload or network problem, you might get into problem handling the situation. So it is stable to upload files in server side again because error handling is essential as amazon mentioned below link. http://docs.aws.amazon.com/AmazonS3/latest/dev/ErrorBestPractices.html The URL mentioned "When designing an application for use with Amazon S3, it is important to handle Amazon S3 errors appropriately ..." Also I recommend you use just only official APIs from Amazon to handle errors instead of plug-in or S3 application. :)
563e313e61a801306526769c	X	Thanks Norm for your answer, I guess I have to rework my code here.
563e313e61a801306526769d	X	I'm trying to upload files to Amazon S3. Works well, as long as the files don't exceed 1GB in size. If so, Powershell runs into some Memory Loop when it processes the 1st byte after 1 GB and never makes it to my write-host line I just can't figure out why there's a limit and how to handle that. Any ideas? (Code snippet below opens a file stream Reader + writer. The real code for this function on my home computer contains an additional cryptostream writer, which I removed here for the sake of readability of the code, just in case someone's wondering what the code should be good for. Result however is the same -> Memory Loop)
563e313e61a801306526769e	X	Using the S3FileInfo to write to a stream does not work well for large files. All the data is needed for computing the signature to S3 so the S3FileInfo buffers the data written to its streams until the stream is closed. I would suggest using the TransferUtility and use the Upload(stream, bucketName, key) method that takes in your stream, see Using the High-Level .NET API for Multipart Upload for details and example snippets. Since you are using a Crypto stream you might want to take a look at the S3 encryption support added to version 2 of the SDK, see Client Side Data Encryption with AWS SDK for .NET and Amazon S3.
563e313e61a801306526769f	X	I would like to decrypt a CSV dump of an Amazon Redshift table locally. I m using the unload command and client side encryption since the data contains sensitive information. The command i am using is like this: to generate a master_key i used the follwing command: This outputs: I used the key as the `master_symmetric_key. I copy the s3 data locally and try to decrypt like this: But get: How do I decrypt an Amazon Redshift CSV dump?
563e313e61a80130652676a0	X	The Key is stored as metadata and is available in x-amz-meta-x-amz-key, and The IV is stored as metadata and is available in x-amz-meta-x-amz-iv. From the Redshift documentation: ... UNLOAD then stores the encrypted data files in Amazon S3 and stores the encrypted envelope key and IV as object metadata with each file. The encrypted envelope key is stored as object metadata x-amz-meta-x-amz-key and the IV is stored as object metadata x-amz-meta-x-amz-iv. When you get the S3 object you will also get these meta-data fields. Here are some example of S3 GET-Object example: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html
563e313f61a80130652676a1	X	streisand effect in 3... 2... 1...
563e313f61a80130652676a2	X	@Charlie "The Streisand effect is a primarily online phenomenon in which an attempt to censor or remove a piece of information that has the unintended consequence of causing the information to be publicized widely and to a greater extent than would have occurred if no censorship had been attempted..." Sorry, I dont get it?
563e313f61a80130652676a3	X	A better question would be "What are the characteristics of good Web APIs?". I think that's what you're trying to get at. Or that's what you should be trying to get at. My attempt to answer, below.
563e313f61a80130652676a4	X	@Pongus eh, close enough. I was talking about Jeff posting this question on Twitter, causing an influx of upvotes to him. It's not exactly the Streisand Effect, but it's got the same jist
563e313f61a80130652676a5	X	@Charlie Somerville, with you! Probably more the Katie Price effect... :o)
563e313f61a80130652676a6	X	+5. Now what was I saying about the Streisand effect? :P
563e313f61a80130652676a7	X	-1 Whilst those may be "popular" API's, many of them are definately not "gold standards". The Flickr API is particularly hilarious, for example. It advertises itself as RESTful and yet is nothing of the sort!
563e313f61a80130652676a8	X	@nathan see where it says "please feel free to edit"? Well.. please feel free! Just click the edit link!
563e313f61a80130652676a9	X	I'm interested in the topic as well. I recently found out about Mashery - they seem to add a layer on top of your public facing API which helps with the permissions and user's request throttling.
563e313f61a80130652676aa	X	The Sun Cloud API is by far the closest thing to a REST style API.
563e313f61a80130652676ab	X	A comment on the Twitter having "Good REST practice". Twitter does not use hypermedia to discover links and due to its use of generic media types the responses are not self-descriptive. It is far from being RESTful.
563e313f61a80130652676ac	X	@Darrel - agreed but a pure RESTFUL service is only necessary if its going to be consumed and updated by robots - otherwise you spend too much time worrying about being RESTful instead of delivering a product IMO
563e314061a80130652676ad	X	@schmoopy Most static HTML websites are pure RESTful services. Are those only consumed by robots?
563e314061a80130652676ae	X	@Darrel - yes, only google reads them, we read google ;-) -- my only point was that being purely restful may not be ideal since your API is used by developers and not robots -- typically - if websites required users to read them with a decoder ring then I am sure the web would be heading in a different direction.
563e314061a80130652676af	X	took the words right out of my mouth :)
563e314061a80130652676b0	X	Doesn't look RESTful whatsoever though. So loses points big time there.
563e314061a80130652676b1	X	Yea was just checking it out for reference, but I cant take it serious since I can pass in an invalid method route and still get back a 200 ResponseCode - ws.audioscrobbler.com/2.0/… returns an HttpStatusCode of 200 - should be 404 since its not a found 'resource' IMO
563e314061a80130652676b2	X	SOAP? Excuse me while i go over here and throw up...
563e314061a80130652676b3	X	+1 for strict and safe!
563e314061a80130652676b4	X	Seems like there are two categories of APIs for websites today. APIs which allow the functionality of the site to be extended like Facebook, Myspace, etc. These APIs seem to be very diverse. APIs which allow interaction with the existing site functionality like Twitter, Flickr, etc. These all claim to be REST based, but are in reality simply "data over HTTP". If you were creating a website that allowed both functional extension and outside interaction, what existing APIs would you use as a reference model?
563e314061a80130652676b5	X	We're doing some research in this area ourselves. Not a lot out there in terms of "gold standard" for website API references. The most common website APIs referenced are: Another list here: http://www.pingable.org/the-top-15-web-apis-for-your-site/ Someone recommended the book Restful Web Services as a good reference on this. (please feel free to edit the above list to add other high profile websites with APIs)
563e314061a80130652676b6	X	How To Design A Good API and Why it Matters, a 60 minute Google tech talk by Joshua Bloch, is relevant.
563e314061a80130652676b7	X	Having worked with a few, I'll get right down to it Facebook Myspace Photobucket (disclaimer: I wrote the server side of the photobucket api) Twitter Ideal characteristics So that being said... something between Facebook and Twitter. Of course I'm partial to some of the stuff we have on Photobucket, but I also hate some of it.
563e314061a80130652676b8	X	It would seem to me that the documentation of the API is just as (or more) important than the actual design of the API. Well written, simple documentation will make up for any design flaws. That's what I've learned after looking at the various links posted already. Specifically, Last.fm's documentation seems very good: easy to navigate and easy to understand.
563e314061a80130652676b9	X	Last.fm's api continues to be one of the most well maintained apis on the net. It's also been around longer than most, because it started out basically as just that. http://www.last.fm/api
563e314061a80130652676ba	X	Regarding Jeff's list of big APIs: I am pretty sure that common does not mean "Gold Standard". No need to keep a manual list of widespread API. To get a list check out http://www.programmableweb.com/apis/directory/1?sort=mashups . Since I like REST as loose standard, I'd say that an API is "Golden" when it makes sense and is intuitive. … all make the most sense for me and are well thought out (as Brian already pointed out). In my current daily work I also work a lot with OpenSocial, where URIs feel very natural but also extend the REST standard in its own way. If you like it strict and safe, use SOAP.
563e314061a80130652676bb	X	I would check out OpenSocial, a movement to create an API Standard for social network sices. They use REST for this and have a 'user' centric approach. But this is a very well documented approach which might help for even a site that is not totally Social based. If you are looking for some internal code implementations look at Drupals hook system and Wordpress. http://code.google.com/apis/opensocial/
563e314061a80130652676bc	X	I think the best way to answer is to list the characteristics of good web APIs instead of citing examples. If you like Twitter/Facebook/etc APIs, what aspect of these APIs do you find attractive? I'll take a first stab: Please add more to comments.
563e314161a80130652676bd	X	I don't have any experience with the others, but even as it has evolved over the years, the Facebook API is still awful. It doesn't come anywhere near being a "gold standard." Rather, it is something people struggle through and grit their teeth because once they finally get it right, it can add a lot of value.
563e314161a80130652676be	X	Some APIs which are notably good:
563e314161a80130652676bf	X	It will depend on what your target audience is. If it is .net shops then soap is probably okay other wise focus on REST since it has a much lower bar of entry. From there look at website APIs that target the same people you would like to. This way your api will feel familiar.
563e314161a80130652676c0	X	Force (previously known as SalesForce) API: http://www.salesforce.com/us/developer/docs/api/index.htm
563e314161a80130652676c1	X	AtomPub is the gold standard because it was designed by some of the brightest minds on the internet. You can't go too far wrong using iit as a basis. That is what google and msft does.
563e314161a80130652676c2	X	Had a similar question to this which didn't get much action, but thought it would be good to link to it. http://stackoverflow.com/questions/775988/what-web-apis-would-you-most-want-to-replicate-or-are-the-most-popular
563e314161a80130652676c3	X	If I were designing a web api today for an existing web site, assuming the web site was well designed with respect to its proper usage of HTTP, I would use the existing web site as the design guideline. Take Stack Overflow as an example, it has the entire URI space already mapped out. It has a complete set of interconnections defined between the different representations. Users of the site are already familiar with the site structure and therefore the API structure would already be familiar. The only part that needs to change is the content of the representations, to eliminate all unnecessary markup. It would be necessary to add in a few extra templated links to allow for searches that are currently only accessible via javascript. For example, searching for a user is not easily discoverable via navigating because currently the link is built via javascript. The really tricky decision is what media-type to use. You could use bare bones html with RDFa style metadata markup, or go wild and use the new Microdata format in Html5. Or, you could return a custom media type based on xml or Json. Something like application/vnd.stackoverflow.question+xml, etc. The custom media type makes versioning really easy, but it is less accessible to clients that were not designed to access StackOverflow directly. The custom types could be used in combination with Atom feeds which are mostly already there in StackOverflow, Designing a web api is really no different than designing a website, other than the fact that you are delivering content that will be consumed by a program that is not a web browser. What you don't want to do is create an Http based data access layer. That is just like showing your underwear to the world. The existing web site is optimized for all the common usage scenarios, many of the api access patterns are going to be similar, so reuse the "views" that have already been created. It may be necessary to add a few extra links here and there to make it a bit quicker for programs to get the data they want, but those can be added incrementally as the need arises. Well written web sites are already very effective APIs for web browser clients, it is really not necessary to go back to the drawing board to support any other type of client. The API structure does not need to change, just the delivered content.
563e314161a80130652676c4	X	But how can I know ulrs for files at CloudFront? For example I have an image /images/test.png at my EC2 server. And CloudFront was configurated with custom origin option. So how can I know url to access this image at CloudFront?
563e314161a80130652676c5	X	http://*.cloudfront.net/images/test.png * = your CloudFront ID
563e314161a80130652676c6	X	if you set up a CNAME record say cdn.example.com to point to DISTRIBUTION_ID.cloudfront.net , then your will be able to access your file by going to cdn.example.com/images/test.png
563e314161a80130652676c7	X	Just for understanding. How to configurate CF? Is this an EC2 instance? I should to upload files to server and load balancing work will be done by Amazon? (I mean that I have only one CF instance and if load will grow Amazon will add another instance? How can I upload files? Can I make git repository and commit files? If I have another EC2 instance with web application is there any way to get access to my files at Cloud Front through file system? I mean if I can to use something like php: file_get_contents('/cloud_front/images/some_image.png') What links should I use to get, for example, images at Clod Front server? I mean that may be there are urls like http://distilleryimage10.s3.amazonaws.com/d2e89af606cc11e3b81c22000a1fbca3_6.jpg to get file access. So how can I generate this url? Any API? Thanks for your time.
563e314161a80130652676c8	X	CloudFront is a Content Delivery Netwerk. You can not upload (single) files. Instead, it makes a copy of your existing files (that you host on S3 or something similar) when that file is requested. This is overkill when a particular file is only accessed a few times. However, if a file is accessed over a thousand times, it can be cheaper and faster to store a copy on CloudFront (this is done automatically, you cannot select individual files) because the roundtrip a computer makes when connecting to the CloudFront datacenter (e.g. AMS) is much shorter than when connecting to your S3 files (e.g. in the worst case US west). Do note that CloudFront is not one datacenter, so it's also not really suitable if your files are accessed often, but all from different locations (so that after all, every file is only accessed once or so at a CF datacenter).
563e314161a80130652676c9	X	i've got an old debian root server, that is hosted at an provider in germany. The maschine is now about 6 years old and i want to move it to a cloud. the question is if there is an easy way to clone the root server (dd or something else) and use this an amazon (or something else) cloud server. are there any way's to this or do i have to migrate the whole server to a new instance in the cloud. thx
563e314161a80130652676ca	X	Look into this tool http://aws.amazon.com/ec2/vmimport/ it should do what you're asking. The first method is to import your VM image using the Amazon EC2 API tools. To get started, simply:
563e314261a80130652676cb	X	Make asynchronous versions of your web API methods and use those methods. If that's not possible, call the methods from separate tasks/threads.
563e314261a80130652676cc	X	Have you looked at the AWS documentation for the asynchronous APIs using C#?
563e314261a80130652676cd	X	Don't create async void methods unless you're sure that you have to, and that it's the correct option at the time. Usually it's not. You should almost always be returning a Task instead.
563e314261a80130652676ce	X	First off I apologize for terrible wording of that question...here's the scenario: I built a WEB API method that receives a ProductID and then uploads that products images to Amazon S3. This part is working just fine. I am now trying to get a console app running that will grab a range of ProductIDs and loop through them, calling the API method, and not wait for the results... Can anyone point me in the right direction? I suppose another caveat would be to not eat up all the resources on the machine running the console app...so maybe a thread cap? UPDATE (This still seems to be synchronous):
563e314261a80130652676cf	X	There are a couple easy ways to do this. I'd recommend using Parallels. It makes the most optimized use of your environments many threads/cores. For your example, you'd simply do something like this: The other method would be to use Tasks, and send each call as a separate task. Be careful with this approach though because you could overload the system with pending tasks if you have too many iterations.
563e314261a80130652676d0	X	I do not recommend using Parallel.For. It does not give an satisfactory control of parallelism (you probably don't want to hammer away hundrades of requests which will start to timeout) also it requires unnecessary context switching. Threads/cores isn't the limiting factor in case http requests. In the example change to and when using real web api calls also remember to wait in Main otherwise the program will terminate before the calls have been made. Then to control the level of parallelism I think the easiest solution is to use a Semaphore to count the number of outstanding calls, waiting in the main loop for the semaphore to be signaled again before issuing new requests.
563e314261a80130652676d1	X	This could explain the strange issues we were seeing. Thanks. My theory is that while uploading two files to the same s3 key concurrently, Amazon chooses one to "win". Sometimes, it chooses "incorrectly", meaning it chooses the one which ends up getting killed (incomplete file), which causes the S3 key to remain empty (no file ends up getting saved).
563e314261a80130652676d2	X	I think I'm having a problem with concurrent s3 writes. Two (or more) processes are writing almost the same content to the same s3 location at the same time. I'd like to determine the concurrency rules that govern how this situation will play out. By design, all of the processes but one will get killed while writing to s3. (I had said they are writing "almost" the same content because all but one of the processes are getting killed. If all processes were allowed to live, they would end up writing the same exact content.) My theory is that the process getting killed is leaving an incomplete file on s3, and the other file (which presumably was written fully) is not being chosen as the one that gets to live on s3. I'd like to prove or disprove this theory. (I'm trying to find out if the issues are caused by concurrency issues during write to s3, or some other time). From the FAQ at http://aws.amazon.com/s3/faqs/ : Q: What data consistency model does Amazon S3 employ? Amazon S3 buckets in the US West (Oregon), US West (Northern California), EU (Ireland), Asia Pacific (Singapore), Asia Pacific (Tokyo), Asia Pacific (Sydney) and South America (Sao Paulo) Regions provide read-after-write consistency for PUTS of new objects and eventual consistency for overwrite PUTS and DELETES. Amazon S3 buckets in the US Standard Region provide eventual consistency. I'm using the US Standard Region.
563e314261a80130652676d3	X	I don't think that the consistency statements in that FAQ entry say anything about what will happen during concurrent writes to the same key. However, it is not possible to have an incomplete file in S3: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html says Amazon S3 never adds partial objects; if you receive a success response, Amazon S3 added the entire object to the bucket. This implies that only the file that is completely uploaded will exist at the specified key, but I suppose it's possible that such concurrent writes might tickle some error condition that result in no file being successfully uploaded. I'd do some testing to be sure; you might also wish to try using object versioning while you're at it and see if that behaves differently.
563e314361a80130652676d4	X	In the default US Standard region, S3 provides eventually consistent writes. Making a GetObjectMetadata() call immediately following a PutObject() may give a 404, because the object has not been copied to all of the datacenters yet. Your code above may throw an exception, even when the put was successful. The AmazonS3 client will throw an exception if there was a failure, either on the client side or the server side.
563e314361a80130652676d5	X	That seems to work fine, I'll deploy it and see how it holds up in production.
563e314361a80130652676d6	X	Calculate the MD5 against the key or really the data sent?
563e314361a80130652676d7	X	Approximately once\week a file upload fails when saving to Amazon S3 (1\300). The following code works well enough to confirm that the file saved correctly, but I can't help but think there's a better way. When a file does fail, no exception is thrown so I'm never really certain where the problem lies. Any suggestions for better confirmation?
563e314461a80130652676d8	X	According to the API documentation it recommends you check the ETag value against the a calculated MD5 hash of the data you sent. They obviously should match. "To ensure an object is not corrupted over the network, you can calculate the MD5 of an object, PUT it to Amazon S3, and compare the returned Etag to the calculated MD5 value." http://docs.amazonwebservices.com/AmazonS3/latest/API/SOAPPutObject.html Hope that helps
563e314461a80130652676d9	X	Well, if you can compress the data, that might reduce it and therefore it would be better. But apart from that, POST looks like the valid HTTP method to send the data. There is also PUT but it should be much similar.
563e314461a80130652676da	X	I, personally, would use an AJAX-based method such as jsonp as opposed to CURL. Curl is exceedingly slow, while AJAX is virtually instant.
563e314561a80130652676db	X	@Austin: I bet that's not the case here.
563e314561a80130652676dc	X	True, it depends on his goal and how he plans to use the data.
563e314561a80130652676dd	X	@Austin i am sending data between 2 servers , no browsers , no javascript to use ajax . and even json requires some sort of connection , so what would it be ?
563e314561a80130652676de	X	FTP would be slow , i am not sending FILES i am sending DATA which has to be processed , and for sure i won't use a 3rd party !!
563e314561a80130652676df	X	Used curl . divided the file to many pieces and sent it
563e314561a80130652676e0	X	I would advise against using MySQL. It's not really designed for transporting files (esp. large ones).
563e314561a80130652676e1	X	FTP would be slow , because i am sending DATA to be processed and not files ! .
563e314561a80130652676e2	X	MySQL would be a performance loss
563e314561a80130652676e3	X	I am using CURL to send large amounts of data between servers , am using POST , is this OK or is there any better/standard way to send large serialized data with curl ? the problem is in the max-post-size in the php settings , i have to change it (default 2MB) . i didn't encounter any problems with this yet , but when the system will be online it is possible that data larger than 50MB will be sent each time ! Any ideas ? Thank you . EDIT : I am sending DATA and not FILES , a data that once received should be processed by the second server and saved to database/file/do some action and might need to send a response after processing the data . I just would like to know , will i face any other problem except max-post-size ? (forget about timeouts of both curl and php) , and is there anyway to make the server not look at max_post_size ? maybe by using PUSH ? or PUT ? does that post_size affect the PUSH or PUT ?? and how to use it via curl ? so many questions !
563e314661a80130652676e4	X	Using cURL is perfectly fine. Personally, I would prefer to not having to do it through web server (eg. Apache) as there can be too many potential faults along the way, eg. PHP timeout, web server timeout, memory limit, no write privileges, limited to web root, etc. I would prefer to do it through mechanisms designed for file transfers:
563e314661a80130652676e5	X	The way is ok. Two more ideas for you:
563e314661a80130652676e6	X	Does anyone know how I can store large binary values in Riak?
563e314661a80130652676e7	X	For now, they don't recommend storing files larger than 50MB in size without splitting them. See: FAQ - Riak Wiki If your files are smaller than 50MB, than proceed as you would with storing non binary data in Riak. Another reason one might pick Riak is for flexibility in modeling your data. Riak will store any data you tell it to in a content-agnostic way — it does not enforce tables, columns, or referential integrity. This means you can store binary files right alongside more programmer-transparent formats like JSON or XML. Using Riak as a sort of “document database” (semi-structured, mostly de-normalized data) and “attachment storage” will have different needs than the key/value-style scheme — namely, the need for efficient online-queries, conflict resolution, increased internal semantics, and robust expressions of relationships.Schema Design in Riak - Introduction
563e314661a80130652676e8	X	@Brian Mansell's answer is on the right track - you don't really want to store large binary values (over 50 MB) as a single object, in Riak (the cluster becomes unusably slow, after a while). You have 2 options, instead: 1) If a binary object is small enough, store it directly. If it's over a certain threshold (50 MB is a decent arbitrary value to start with, but really, run some performance tests to see what the average object size is, for your cluster, after which it starts to crawl) -- break up the file into several chunks, and store the chunks separately. (In fact, most people that I've seen go this route, use chunks of 1 MB in size). This means, of course, that you have to keep track of the "manifest" -- which chunks got stored where, and in what order. And then, to retrieve the file, you would first have to fetch the object tracking the chunks, then fetch the individual file chunks and reassemble them back into the original file. Take a look at a project like https://github.com/podados/python-riakfs to see how they did it. 2) Alternatively, you can just use Riak CS (Riak Cloud Storage), to do all of the above, but the code is written for you. That's exactly how RiakCS works -- it breaks an incoming file into chunks, stores and tracks them individually in plain Riak, and reassembles them when it comes time to fetch it back. And provides an Amazon S3 API for file storage, for your convenience. I highly recommend this route (so as not to reinvent the wheel -- chunking and tracking files is hard enough). Yes, CS is a paid product, but check out the free Developer Trial, if you're curious.
563e314661a80130652676e9	X	Just like every other value. Why would it be different?
563e314661a80130652676ea	X	Use either the Erlang interface ( http://hg.basho.com/riak/src/461421125af9/doc/basic-client.txt ) or the "raw" HTTP interface ( http://hg.basho.com/riak/src/tip/doc/raw-http-howto.txt ). It should "just work." Also, you'll generally find a better response on the riak-users mailing list than you will here. http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com (No offense to z8000, who seems to also have answers.)
563e314661a80130652676eb	X	I'm not entirely clear on your requirements: Do you (1) want the user to login to your own backend authentication service? Or (2) do you want the user to not need to login, but still be able to upload to S3 (without embedding your AWS credentials)?
563e314661a80130652676ec	X	Could I use STS without cognito? Would that be a simpler solution?
563e314761a80130652676ed	X	Well, you could, but again Amazon Cognito provides a super-set of the functionality of what web identity federation provides. Some benefits include, allowing users to have guest access and later link a social login to the same user. Cognito also provides data synchronization capabilities across devices. You can read this article(blogs.aws.amazon.com/security/post/Tx3SYCORF5EKRC0/…) to further understand the benefits of using Amazon Cognito.
563e314761a80130652676ee	X	So I'm a bit confused by the Amazon documentation on Cognito concerning one of their stated use cases: "use your own identity system... allowing your apps to save data to the AWS cloud". In my case I want to give them aws tokens to upload directly to s3 from the mobile client without putting my aws keys on the client device. In order to implement this on the server side - how do I generate the proper credentials so that the client can use this identity on the client app to upload to s3? Do I first call getId() (what values do I pass if I'm using my own login - since I'm not providing a facebook or twitter ID? How do I pass in my own db's generated user ids? AWS.CognitoIdentity.getCredentialsForIdentity() method from the congito API... or maybe I have to new up an AWS.CognitoIdentity? Any links to a good example? I couldn't find any full examples in the documentation itself. For example in their documentation amazon says that var identityId = AWS.config.credentials.identityId; retrieves an identityid for your end user immediately, however looking at it, it seems to be a property and not an id factory. How does it generate unique ids, or is one identity id shared by all of my users? Are there credentials of some sort that I can derive from this that I can then pass on to my mobile client to get upload privileges to s3? I also read something about AWS STS service - is that an alternative to using Cognito?
563e314761a80130652676ef	X	You can find an example in this AWS Mobile blog post and the differences between developer authenticated identities and regular identities in this other blog post. Basically, the flow is that your app will authenticate against your backend, then your backend will call GetOpenIdTokenIdTokenForDeveloperIdentity and send the resulting token and Identity ID to the user's app. The user's app can use this token to obtain Cognito credentials using the SDK, and with this credentials make calls to S3 or other AWS services. Each user will have it's own credentials, so they only have access to their own resources in S3. About STS, that's what the SDK will internally use to obtain the credentials, but as long as you use the SDK you don't need to worry about it. It's not an alternative to Cognito, but they both work together.
563e314761a80130652676f0	X	It is possible to the .JAR to run the code of httpclient from the jar inside my jar only ?
563e314761a80130652676f1	X	Yes, to make the code clear, you could merge all HttpClient invocation in to one Class (let's call it HttpClientHelper); Then create a class loader which has 4.1.1 in its class path, and use this class loader to load HttpClientHelper. If you need some sample codes, give me your email address.
563e314761a80130652676f2	X	Is it possible to use the same lib with different versions ? The thing is: i have the httpclient-4.0.1 into my application in the WEB-INF/lib directory. I made an API for the Amazon S3 service, which use the httpclient-4.1.1. But i don't want to update my application library to use the newer version, because i don't have enough time to test and garantee that the application will run properly. So, is there a way, that my API i`ve made (actually a jar) to use the httpclient-4.1.1 without need to upgrade the library of my application (4.0.1)?
563e314761a80130652676f3	X	you cannot use 2 versions of same Lib in the same class loader; but you can use different class loader to load th different versions. For example, you can use a sub classloader to loader httpclient-4.0.1, and shield 4.1.0 in the super class loader.
563e314a61a80130652676f4	X	Okay, first off the ecommerce API was renamed to the associates API. Here's a link to the latest: developer.amazonwebservices.com/connect/…
563e314a61a80130652676f5	X	apisigning.com claims: "As of August 17th, 2009, Amazon now requires that all requests to their Product Advertising API be signed."
563e314b61a80130652676f6	X	Register for Amazon Web Services. Then try the following URL: http://ecs.amazonaws.com/onca/xml?Service=AWSECommerceService&Operation=ItemLookup&AWSAccessKeyId=YOURKEY If you have my experience you'll see: What is this trying to tell me? What are "Signature" and "Timestamp" parameters? This is a n00b problem of some sort, but finding the answer is not obvious. I Googled for "MinimumParameterRequirement". I looked at http://docs.amazonwebservices.com/AWSEcommerceService/2007-01-17 and http://s3.amazonaws.com/awsdocs/ECS/20080819/QRC-AAWS-2008-08-19.pdf and http://docs.amazonwebservices.com/AWSEcommerceService/2007-01-17/ApiReference/ErrorCodesAndMessages.html. In fact, even a link to the latest API doc for ecommerce would be nice. Is 2008-08-19 the latest?
563e314b61a80130652676f7	X	Well here's the signature parameter: http://docs.amazonwebservices.com/AmazonFPS/latest/FPSAdvancedGuide/index.html?APPNDX%5FGeneratingaSignature.html It's a hash of the other parameters. Strange that they give lots of examples without it and claim that those examples work.
563e314b61a80130652676f8	X	anyone can grab the client side code from your page and modify it to circumvent the xml check.
563e314b61a80130652676f9	X	Any thoughts on how this should be tackled?
563e314b61a80130652676fa	X	unless S3 allows you to run your code on their side, where you can check the contents of the files, you could post to your own server and do the check there and then post to S3 from your server.
563e314b61a80130652676fb	X	Yes I can do that, but i do not think one can run code on s3 other than html; as for the server side processing I thought since i can directly upload to s3 I would save the server side processing time and resources for something else
563e314c61a80130652676fc	X	Will read on the info you posted; will get back to you ASAP...thank you
563e314c61a80130652676fd	X	I allow users to upload to s3 directly without the need to go through the server ; Everything works perfectly; my only worry is the security. In my javascript code I check for file extensions . However I know in javascript code Users can manipulate script- in my case to allow upload of xml files - (since client side upload) and thus would'nt they be able to replace the crossdomain.xml in my bucket and accordingly be able to control my bucket? Note: I am using the bucket owner access key and secret key. Update: Any possible approaches to overcome this issue...?
563e314c61a80130652676fe	X	If you are not adverse to running additional resources, you can accomplish this by running a Token Vending Machine. Here's the gist: A simple example of creating a "dropbox"-like service using per-user access is detailed here.
563e314c61a80130652676ff	X	That's right. Would definitely be a problem since I am using the CurrentUser to protect the data.
563e314d61a8013065267700	X	Consider using this, then: stackoverflow.com/questions/3681493/…
563e314d61a8013065267701	X	I need to store user credentials in my app. I can store and retrieve the password with protectdata. But as soon as I push a new revision of my app I loose the credentials. It seems like appharbor cleans the ProtectedData Store. Is this behavior on purpose? Is there a better way to store user credentials on appharbor. OAuth is not an options since it's a ftp account.
563e314d61a8013065267702	X	Changes to the local instance file system are not persisted across deploys on AppHarbor, so you'll have to store the data someplace else. We generally recommend using Amazon S3. You can use System.Security.Cryptography.ProtectedData.Protect() from the same API to encrypt data written there. Here's some sample code: protect bytes data .net I'm not super familiar with this API, but you might get into trouble if you app us running on multiple AppHarbor instances using different user accounts.
563e314d61a8013065267703	X	Here is the files schema github.com/eknowles/mongoose-crate-example/blob/master/app/… and the api github.com/eknowles/mongoose-crate-example/blob/master/app/…
563e314d61a8013065267704	X	Amazing! Thanks so much for the effort you put into answering this. This goes a long way towards helping me understand express routing in general. I owe you one. Stack says I have to wait 20hrs before rewarding the bounty, but rest assured it's yours!
563e314d61a8013065267705	X	No worries! Express is an awesome framework, loads of really great stuff out there!
563e314d61a8013065267706	X	Any chance you could modify your example for Express 4? express.multipart is removed from the Express 4.x library so I can't use the built in parser like your example does. I've been using body-parser, but maybe I will need to use multiparty
563e314d61a8013065267707	X	You can use github.com/expressjs/multer for Express4, it's very simple to setup
563e314d61a8013065267708	X	I am building a RESTful API using Node + Express 4 + MongoDB + Mongoose. One thing my API needs to do is store and retrieve files. Which I will store in Amazon S3. Mongoose has a specific plugin for attaching files to Mongo documents called Mongoose-Crate, which in turn has a storage provider Mongoose-Crate-S3 that uploads files to S3. I've done my best to adapt the example code from the Mongoose-Crate-S3 npm page to work as an express route, but so far I've not gotten an image to successfully upload to my S3 storage. Documents of my 'file' model are being created in my mongo database, but the only have an '_id' and '__v' fields. No 'title', no 'description', nothing to indicate that the .post endpoint is actually receiving the files I try to post. I keep making slight adjustments to my code but I am generally getting some variation on "Could not get any response". Here is my mongoose schema file.js (with my S4 credentials removed of course) And here is the relevant snippet of my api routes file. I'm fairly certain the code I need to fix goes in here. I fully expect I am missing something obvious, but MEAN stack programming is new to me, and I've scoured stackoverflow & the web at large looking for more examples or anything to hint at what I am missing. Please help!
563e314e61a8013065267709	X	First of all you should set up your location path of where you want the file to be saved on S3, currently in that example it's using the same path as the origin file (which could be /var/tmp/y7sday... or C:/Users/SomeGuy/Pictures/..) so we need to sort that out first. In my code I'm using the same name as the file they gave, in production you might want to sort these by date and add a random uuid to them. full example Next in your API endpoint you want to include any form information in the body and add those to the object, these will be in the req.body part. The most important thing to note here is that I've set the attachment as file, this needs to match the field you have declared in your mongoose schema or it will die. Next include req.files.file as the second argument. full example I've uploaded my workings to GitHub, so please do clone that and try it out. All you need to do is POST to /file/ Make sure that you set the field in the form to be file like the image below  Tip: If you ever run into any problems it's always good to do a console.log(req) and see where the bits are that you need.
563e314e61a801306526770a	X	I have refered to the amazon site -'http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html'to upload a audio file directly to the S3 server. The file gets uploaded successfully, but I am unable to handle the return status. I used the option 'success_action_status' and set it to 200 to make sure that it is returned in the success block. But it always gets returned in the error block. I am using Dot Net 3.5 with MVC. I have used It always ends up in the error block. I want to process the request further based on the return status (success/error). Any help will be highly appreciated.
563e314e61a801306526770b	X	If you know the name of the bucket you want to use, why do you need to list them? putObject to the bucket directly.
563e314e61a801306526770c	X	putObject can get bucketName as String: (docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/…, java.lang.String, java.io.File)
563e314e61a801306526770d	X	@Guy what if I'm using Bucket reference for some other purposes? So please learn the difference between a text and object reference.
563e314e61a801306526770e	X	Some concepts are probably too hard for me to understand... I like simple life with API calls that are using text... but this is only me.
563e314e61a801306526770f	X	Thank you for the answer. +1.
563e314e61a8013065267710	X	Can you tell me how can I check with AmazonS3Client.getS3AccountOwner() whether I'm the owner?
563e314e61a8013065267711	X	AmazonS3Client.getS3AccountOwner() gets the owner of the current account the code is using - that is, yourself. AmazonS3Client.getBucketAcl(bucketId).getOwner() gets the owner of the bucket. Comparing the two should show whether you are the owner of the bucket.
563e314e61a8013065267712	X	thank you for the explanation :)
563e314f61a8013065267713	X	You are referring to the method createBucket() of AmazonS3 interface, and I'm using an implementation of that method and I get an exception when I use that method. Firstly, I was thinking that the method should return the bucket if it already exists, but it does not. Exception message: com.amazonaws.services.s3.model.AmazonS3Exception: Status Code: 409, AWS Service: Amazon S3, AWS Request ID: ****, AWS Error Code: BucketAlreadyOwnedByYou, AWS Error Message: Your previous request to create the named bucket succeeded and you already own it., S3 Extended Request ID: ****
563e314f61a8013065267714	X	So basically, your answer doesn't help. Thanks, anyway.
563e314f61a8013065267715	X	Thank you for the answer, but I'm using Java as specified above.
563e314f61a8013065267716	X	Also, the "no request made" suggests that you'll get a valid bucket object even if the bucket does not actually exist, which is not quite what the question's code does.
563e314f61a8013065267717	X	While I was trying to upload objects to a specific bucket in Amazon S3 storage via Amazon Web Services API, I obviously needed a reference to that existing bucket. I was using AmazonS3Client to perform that, however I couldn't see any method such as getBucket(String bucketID). Please check here: AmazonS3Client I applied a brute-force search to get a reference to the desired bucket: Obviously, this works fine. But I wonder why the API doesn't provide such a trivial method or might I be missing something or do you know any other method(s) from the same API which will replace the lines above with one-line code? EDIT: For a simple task like uploading a file, the bucket name seems to be sufficient: However, my question is as the title implies, why isn't there a method returning a reference to a specific Bucket instance?
563e314f61a8013065267718	X	Your brute force method gets a Bucket object if the bucket exists and you own it, and otherwise results in null. To get the same result without listing the buckets: If you already know you own a bucket with the relevant name, you can skip all but the last step. Not exactly a one line result, but it does avoiding listing the buckets, which could be good if you have a large number of buckets, and I think it's the API's intended way to do what you want.
563e314f61a8013065267719	X	Quoting my own article: The createBucket call is idempotent: It if exists, no exception is thrown. Hope it helps
563e314f61a801306526771a	X	I don't think you're missing anything. There are no APIs in that list that return a single bucket. The Ruby SDK seems to do what you want. http://docs.aws.amazon.com/AWSRubySDK/latest/frames.html Before you can upload files to S3, you need to create a bucket. s3 = AWS::S3.new bucket = s3.buckets.create('my-bucket') If a bucket already exists, you can get a reference to the bucket. bucket = s3.buckets['my-bucket'] # no request made
563e314f61a801306526771b	X	I'll give it a stab. According to the Java Doc and the REST API doc (v 1.7.1), there is simply nothing that you can pass in to get anything less than a full list of buckets. If I had to guess, I would say that its probably because Amazon cautions in their docs to create/delete buckets sparingly and thus pushes the burden onto the dev of caching that response or of searching through it themselves. Theres also a limit on the number of buckets per account, so the number of buckets in the response is reasonably small. One of the comments on the question mentioned that most of the calls use a String and not a Bucket, which caused me to take a look at the Bucket class. The only fields that it contains other than its name are the owner and creation date. If that's what you're trying to get at, then its probably in your best interest keep the full response somewhere. The bucket owner and dates aren't going to change very often, if at all. Perhaps I'm assuming too much here, but it might be best if you just poll the service at app startup or on a regular basis to see if anything changed, updating your cache when needed and to use this cache instead of hitting S3 every time you need this info. Its also worth mentioning that if you don't need an owner and creation date, there's nothing stopping you from instantiating a Bucket yourself and setting the name on it.
563e314f61a801306526771c	X	I have a senario where we have many clients uploading to s3. My question is what is the best approach to knowing that there is a new file. Is it realistic/good idea, for me to poll the bucket ever few seconds?
563e314f61a801306526771d	X	UPDATE: Since November 2014, S3 supports the following event notifications: These notifications can be issued to Amazon SNS, SQS or Lambda. Check out the blog post that's linked in Alan's answer for more information on these new notifications. Original Answer: Although Amazon S3 has a bucket notifications system in place it does not support notifications for anything but the s3:ReducedRedundancyLostObject event (see the GET Bucket notification section in their API). Currently the only way to check for new objects is to poll the bucket at a preset time interval or build your own notification logic in the upload clients (possibly based on Amazon SNS).
563e314f61a801306526771e	X	Push notifications are now built into S3: http://aws.amazon.com/blogs/aws/s3-event-notification/ You can send notifications to SQS or SNS when an object is created via PUT or POST or a multi-part upload is finished.
563e314f61a801306526771f	X	Your best option nowadays is using the AWS Lambda service. You can write a Lambda using either node.js javascript, java or Python (probably more options will be added in time). The lambda service allows you to write functions that respond to events from S3 such as file upload. Cost effective, scalable and easy to use.
563e315061a8013065267720	X	You must configure a shared folder where you can store user generated content. Check here for same example about shared folder. Hope this help
563e315061a8013065267721	X	I want to deploy a php application from a git repository to AWS Opsworks service. I've setup an App and configured chef cookbooks so it runs the database schema creation, dumping assets etc... But my application has some user generated files in a sub folder under web root. git repository has a .gitignore file in that folder so an empty folder is there when i run deploy command. My problem is : after generating some files (by using the site) in that folder, if I run 'deploy' command again 'Opsworks' adds a new release under 'site_name/releases/xxxx' folder and symlink to it from 'site_name/current' folder. So it makes my previous 'user generated stuff' inaccessible. What is the best solution for this kind of situation? Thanks in advance for your kind answers.
563e315061a8013065267722	X	You have a few different options. Listed below in order of personal preference: When using OpsWorks think of replicable/disposable servers. What I mean by this is that if you can create one server (call it server A) and then switch to a different one in the same stack (call it server B), the result of using server A or server B should not impact how your application works. While it may seem like a good idea to save your user generated files in a directory that is common between different versions of your app (every time you deploy a new release directory is generated) when you destroy your server, you run the risk of destroying your files. Benefits and downsides of using S3? Benefits: Downsides: Benefits and downsides of using EBS? Benefits: Downsides: Downside of using a database? My preferred choice would be to use S3, but ultimately this is your decision. Good luck! EDIT: Take a look at this repository opsworks-chef-cookbooks it contains some recipes to deploy Symfony2 application on OpsWorks. I have been using it for over a year and works quite well.
563e315061a8013065267723	X	Use Chef templates, and use them in a recipe in the opsworks deploy lifecycle event.
563e315061a8013065267724	X	Presumably your server is an EC2 instance, because there is no such thing as a server 'in S3' - right?
563e315061a8013065267725	X	check it before. Couldn't find any valuable information for this
563e315061a8013065267726	X	Did you check the Code Samples? They have an example of downloading a file from S3 in there: $response = $s3->get_object('aws-sdk-for-php', 'demo/big-buck-bunny.mp4', array( 'fileDownload' => './downloads/big-buck-bunny.mp4' ));
563e315061a8013065267727	X	It is ok if I can download the particular file in the particular account to my account. What do u mean by " download from the source account and upload to the target account". The target account is my account and my server is running their. May be I didn't get the exact architect of the S3. Can't I just download that file to my account?
563e315061a8013065267728	X	There might be some confusion regarding the different AWS services: Amazon S3 is storage for the Internet - it is really just that, storage, so you can't run a server on it etc.; on the other hand, Amazon EC2 provides resizable compute capacity in the cloud - slightly simplified this boils down to running servers on demand, and your server is probably such an EC2 instance accordingly. Both EC2 and S3 (and in fact all AWS services) are usually accessed via a single AWS account.
563e315061a8013065267729	X	So the question is whether you want to download from an S3 bucket stored in your AWS account to an EC2 instance running in your AWS account, or want to copy an object (file) from one S3 bucket in your AWS account to another S3 bucket in your AWS account.
563e315061a801306526772a	X	Yes the requirement is this. Users have videos on their S3. I want to download a copy of that video from their S3 to my EC2 directory.
563e315161a801306526772b	X	Hi I'm really new to amazon s3. I want to download a file to my server (which is in s3) from a given another s3 location (credentials will be provided). This must be done using a php (cakephp 1.2) method and there is no user interface for this. (this could be a cron job probably). I couldn't find any good article regarding this by googling. Any sample code that I can do this work?
563e315161a801306526772c	X	Check out AWS SDK for PHP.
563e315161a801306526772d	X	If I understand you correctly, you want to copy an Amazon S3 object (file) from one AWS account to a different AWS account without downloading and uploading it to a separate system (apparently an Amazon EC2 instance)?! It is possible to copy an object within a single account by means of copyObject(), but cross account operations aren't supported by this API (and neither for any other AWS resources, as far as I know, which is likely a deliberate decision to ease and streamline the security architecture and process). So while your use case is sound, there is no other solution than channeling this process through your server, i.e. download from the source account and upload to the target account. This shouldn't be much of a problem cost or performance wise though, because There is no Data Transfer charge between Amazon EC2 and other Amazon Web Services within the same region (i.e. between Amazon EC2 US West and Amazon S3 in US West) (see section Data Transfer in Amazon EC2 Pricing) and these operations will facilitate Amazon's decent internal network infrastructure (rather than crossing the public internet).
563e315161a801306526772e	X	You'll need to provide a lot more information. For example, what does the request look like? What version of Fine Uploader are you using? What sort of parameters are being associated with the file?
563e315161a801306526772f	X	I am using FineUploader to upload to S3. I have everything working including deletes. However, when I upload larger files that get broken into multi-part uploads, I get the following error in the console (debugging turned on): Can someone point me in the right direction as what I should check for settings, or what additional info you might need?
563e315161a8013065267730	X	Since you haven't included anything really specific to your setup, code, or the failing request, my best guess is that your server isn't returning a proper signature response for uploads made to the S3 REST API (which is used for larger files). You'll need to review that procedure for generating a response to this type of signature request. Here's the relevant section from Fine Uploader's S3 documentation: Fine Uploader S3 uses Amazon S3’s REST API to initiate, upload, complete, and abort multipart uploads. The REST API handles authentication by signing canonically formatted headers. This signing is something you need to implement server-side. All your server needs to do to authenticate and supported chunked uploads direct to Amazon S3 is sign a string representing the headers of the request that Fine Uploader sends to S3. This string is found in the payload of the signature request: { "headers": /* string to sign */ } The presence of this property indicates to your sever that this is, in fact, a request to sign a REST/multipart request and not a policy document. This signature for the headers string differs slightly from the policy document signature. You should NOT base64 encode the headers string before signing it. All you must do, server-side, is generate an HMAC SHA1 signature of the string using your AWS secret key and then base64 encode the result. Your server should respond with the following in the body of an ‘application/json’ response: { "signature": /* signed headers string */ }
563e315161a8013065267731	X	Cyberduck is spelled in one word with no camel case.
563e315261a8013065267732	X	Thanks . It's working fine for me .
563e315261a8013065267733	X	worked fine with standard S3
563e315361a8013065267734	X	I am trying to use Cyberduck CLI to connect to an S3 compatible S3-compatible CEPH API by UKFast (https://www.ukfast.co.uk/cloud-storage.html). It has the same function as Amazon but uses a different url/ server obviously. The connection is via secret key and pass phrase the same as S3. Cyberduck CLI protocols are listed here: https://trac.cyberduck.io/wiki/help/en/howto/cli I have tried using the below command the windows command prompt. The problem is that Cyberduck auto adds amazon AWS URL. So how do I use all the S3 options with a custom end point?
563e315361a8013065267735	X	The s3:// scheme is reserved for AWS in Cyberduck CLI. If you want to connect to a third party services compatible with the S3 protocol you will need to create a custom connection profile. A connection is a XML property list .cyberduckprofile file that you install, providing another connection scheme. An example of such a profile is the Rackspace profile shipped within the application bundle in Profiles/Rackspace US.cyberduckprofile adding the rackspace:// scheme to connect to OpenStack Swift compatible Rackspace Cloud. You can download one of the other S3 profiles available and use it as a template. Make sure to change at least the Vendor key to the protocol scheme you want to use such as ukfast and put in the service endpoint of UKFast as the value for the Default Hostname key (Which corresponds to s3.amazonaws.com; I cannot find any documentation for the S3 endpoint of UKFast. When done, verify the new protocol is listed in duck --help. You can then use the command duck --list ukfast://bucket/ --username <AccessKey> --password <Secret Key> to list files in a bucket. You might also want to request UKFast to provide such a profile file for you and other users to make setup simpler. The same connection profile can also be used with Cyberduck.
563e315361a8013065267736	X	+1 because I would also like to know, but I suspect the answer will be an emphatic no.
563e315461a8013065267737	X	Thanks David - I suggest making this an RFE. I’ve used MLJAM in the past. My preference is to host the Java code in a Tomcat cluster and communicate with MarkLogic via RESTful APIs. In either case, it complicates the deployment footprint and should be avoided.
563e315461a8013065267738	X	In general - in most companies I have worked with - the more customers explicitly and directly making it known to their support or sales or account reps what future features are important and valuable to them, especially if given a priority, urgency and rating comparison to other important and valuable features they may be asking for is the most productive way to influence the the prioritization of new features. Associatiating and communicating a tangible 'value' of a feature to the company is very helpful vs 'would love to have xxx'.
563e315461a8013065267739	X	I'd like to use MarkLogic's JVM to run some custom Java code. Using the MarkLogic JVM would dramatically reduce the infrastructure/deployment footprint. The custom Java code that I want to run within the JVM will extract data from legacy excel spreadsheets. I will be using the Apache POI Java APIs to do this. Apache POI Spreadsheet API => http://poi.apache.org/spreadsheet/index.html I know that there's some undocumented ways to call the MarkLogic JVM directly. I assume this is being done for the HDFS and Amazon S3 features. Is there a way to use the MarkLogic Java VM and if so how?
563e315461a801306526773a	X	"Empathetic No" Where "No" means that if there was such a feature that was safe, efficient, tested, and supported and guaranteed to work with with the same enterprise quality and reliability as documented features, it would be documented. Since its not - don't do it, no matter how tempting. Alternatively: There is the excellent and well used public domain "MLJAM" library http://developer.marklogic.com/learn/2006-05-mljam And this uses 100% documented features. You can also go the other way and start from java, query and push to ML ... the overhead can actually be minimal if you design it right. Sometimes even more efficient by splitting out the workload then by doing it all in one process or system. Take a look at the Java Client API, this can run on the same host as MarkLogic or on a different host. -David
563e315461a801306526773b	X	This is such a good idea, thank you!
563e315461a801306526773c	X	Excellent answer. Thanks!
563e315461a801306526773d	X	I'm running a website that handles multimedia uploads for one of its primary uses. I'm wondering what are the best practices or industry standard for organizing alot of user uploaded files on a server.
563e315461a801306526773e	X	I don't think you are going get any concrete answers unless you give more context and describe what the use-case are for the files. Like any other technology decision, the 'best practice' is always going to be a compromise between the different functional and non-functional requirements, and as such the question needs a lot more context to yield answers that you can go and act upon. Having said that, here are some of the strategies I would consider sound options: 1) Use the conventions dictated by the consumer of the files. For instance, if the files are going to be used by a CMS/publishing solution, that system probably has some standardized solution for handling files. 2) Use a third party upload solution. There are a bunch of tools that can help guide you to a solution that solves your specific problem. Tools like Transloadit, Zencoder and Encoding all have different options for handling uploads. Having a look at those options should give you and idea of what could be considered "industry standard". 3) Look at proved solutions, and mimic the parts that fit your use-case. There are open-source solutions that handles the sort of things you are describing here. Have a look at the different plugins to for example paperclip, to learn how they organize files, or more importantly, what abstractions do they provide that lets you change your mind when the requirements change. 4) Design your own solution. Do a spike, it's one of the most efficient ways of exposing requirements you haven't thought about. Try integrating one of the tools mentioned above, and see how it goes. Software is soft, so no decision is final. Maybe the best solution is to just try something, and change it when it doesn't fit anymore. This is probably not the concrete answer you were looking for, but like I mentioned in the beginning, design decisions are always a trade-off, "best-practice" in one context could be the worst solution in another context :) Best off luck!
563e315561a801306526773f	X	Your question is exceptionally broad, but I'll assume you are talking about storage/organisation/hierarchy of the files (rather than platform/infrastructure). A typical approach for organisation is to upload files to a 3 level hierarchical structure based on the filename itself. Eg. Filename = "My_Video_12.mpg" Which would then be stored in, Or another example, "a9usfkj_0001.jpg" This way, you end up with a manageable structure that makes it easy to locate a file's location simply based on its name. It also ensures that directories do not grow to a huge scale and become incredibly slow to access. Just an idea, but it might be worth being more explicit as to what your question is actually about.
563e315561a8013065267740	X	From what I understand you want a suggestion on how to store the files. If is that what you want, I would suggest you to have 2 different storage systems for your files. The first storage would be a place to store the physical file, like a directory on your server (w/o FTP enabled, accessible or not to browsers, ...) or go for Amazon s3 (aws.amazon.com/en/s3/), Rackspace CloudFiles (www.rackspace.com/cloud/cloud_hosting_products/files/) or any other storage solution (you can even choose dropbox, if you want). All of these options offers APIs to save/retrieve the files. The second storage would be a database, to index and control the files. On the DB, that could be MySQL, MSSQL or a non-relational database, like Amazon DynamoDB or SimpleSQL, you set the link to you file (http link, the path to the file or anything like this). Also, on the DB you can control and store any metadata of the file you want and choose one or many @ebaxt's solutions to get it. The metadata can be older versions of the file, the words of a text file, the camera-model and geo-location of a picture, etc. Of course it depends on your needs and how it will be really used. You have a very large number of options, but without more info of what you pretend to do is hard to suggest you a solution. On Amazon tutorials area (http://aws.amazon.com/articles/Amazon-S3?browse=1) you can find many papers about it, like Netflix's Transition to High-Availability Storage Systems, Using the Java Persistence API with Amazon SimpleDB and Petboard: An ASP.NET Sample Using Amazon S3 and Amazon SimpleDB Regards.
563e315561a8013065267741	X	See similar question stackoverflow.com/questions/1312087/…
563e315561a8013065267742	X	Your answer is correct in spirit. A REST interface should be discoverable as you describe. However, all media-types used by representations should have formal definitions. REST is not an excuse avoid documenting your API. The difference is what you are documenting. Do document the data structures that flow between the client and the server, just not the endpoints (ie. URLS).
563e315561a8013065267743	X	Machine readable definitions are useful for defining types and other support code in various programming languages.
563e315561a8013065267744	X	Most REST interfaces I see are described with a simple web page describing the URL, the method, accepted input and returned result. For example the Amazon S3 or the Twitter API documentation. But why should I settle with what is apparently good enough for Amazon or Twitter... So, is it worth describing a REST API in a machine readable format? And if yes, which one? WSDL 2.0 claims is capable of describing REST. WADL is explicitly created for describing REST services. Both WSDL 2.0 and WADL seem to have a rather small following atm and it seem to be little return for the effort of creating and maintaining the description documents. My uestion is basically to validate or negate my assumption. Do you use WSDL/WADL to describe your services? Do you rely on WSDL/WADL to consume others' services? Does your tool of choice support either one at the moment?
563e315561a8013065267745	X	Yes, you should. You will be able to generate your client code, tests and documentation using a set of tools supporting WADL. Some examples can be found here. Also, I think you should stick with WADL, rather than WSDL 2.0 because it is less verbose and way simpler (IMHO). In fact, in WADL you describe exactly what the user sees on the documentation page, just using WADL XML syntax. BTW, that is why it's so easy to write XSLT-based documentation generators for WADL.
563e315561a8013065267746	X	The following is just my personal opinion: I think WADL is similar to site maps for html pages. Site maps are considered theoretically a good practice, but rarely implemented and even more rarely used by people. I think the reason is simple - wandering around a site and pushing strategically placed buttons is often significantly more rewarding than browsing a complex map. REST API methods should not require a formal description. So if API is created thoughtfully it is pretty easy to discover all the resources just by following strategically placed uri links of a 'home' RESTful resource.
563e315561a8013065267747	X	There's a chicken/egg phenonenon here. WADL is useless without tools that produce or consume it. The tools are useless unless sites publish WADL. etc. For me, The Amazon model works fine. Depending on your audience you will get more return on an effort to produce samples, including snips od sample dialogs (what does request1 look like on the wire, same for response 1, then request 2, response 2, etc), and code in vvarious languages that are important to you. If you want to go to a machine-readable definition, you can use XSD if it is an XML message format. Obviously this is not WADL but coupled with your english description, it may provide a little extra utility for developers.
563e315661a8013065267748	X	What is the benefit of a machine-readable REST API definition? The point of REST is for the API to be relatively simple and easy-to-understand. Natural language works well for this. If you mean "API Type Definitions" when you say "API Definition" then there may be some value in providing metadata. This, however, is only one piece of an API definition. Having "machine readable" API can easily repeat the API source code, violating the DRY principle. It's often simpler to write English descriptions of what the REST verbs do and what the URI's are. Send the type's which are marshalled through JSON (or YAML or JAXB) as source code. That's the perfect machine-readable API -- actual working source for the message object class.
563e315661a8013065267749	X	The most popular usage of WSDL (and WADL in the same way) is code generation. It sure helps speed up development, but nothing can replace plain old documentation. For humans and not for machines.
563e315661a801306526774a	X	Thank you for the detailed answer! I've actually started implementing the protocol you detailed before, but haven't gotten around to actually finishing it yet: github.com/keichan34/s3uploader/tree/2.0-wip
563e315661a801306526774b	X	Great, I've noticed you are using Ruby in your project and also found another Ruby project which I have added to the answer for your reference.
563e315661a801306526774c	X	Here is another example in PHP https://github.com/ienzam/s3-multipart-upload-browser
563e315661a801306526774d	X	@BausTheBig this is/was the first project linked in the answer. Obviously it wasn't clear so thanks to your comment I have updated the answer to make the link more clear.
563e315661a801306526774e	X	@BausTheBig actually, the bulk of the project is written in Javascript, it only needs a small server-side endpoint, which can be written in any language easily. The "canonical" endpoint is written in Python though (Flask).
563e315661a801306526774f	X	Not supported by S3? docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
563e315661a8013065267750	X	It certainly is supported by S3
563e315661a8013065267751	X	There is NO support for browser-based multi-part uploads, folks... which is what the question was about. Browser-based single-part uploads are supported and multi-part uploads via the web service API are supported. But not both together.
563e315661a8013065267752	X	You CAN initiate multipart uploads from a browser via the S3 REST API. using XMLHttpRequest level 2. Not sure what you are talking about when you say there is no support for "browser-based" multi-part uploads.
563e315661a8013065267753	X	Is it possible to use the HTML 5 File API (for example, this library: https://github.com/23/resumable.js ) in conjunction with the S3 multi-part upload feature? http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
563e315661a8013065267754	X	Yes, but you will need some kind of server backend to handle the Amazon API keys in a more secure way if you are going to make it part of a public website. You can find what looks like a complete example implementation of this these projects: Please note that I have not used, tested or reviewed these projects. A rough description of the sequence is as follows: Notes:
563e315761a8013065267755	X	Browser-based POST? Yes. Multi-part uploads? No, it isn't currently supported by S3. :(
563e315761a8013065267756	X	Ok, that makes sense. permanent_url is the property to use, of course :-) Missed that one. It is not mentioned in the documentation either, maybe it should be added.
563e315761a8013065267757	X	We are trying to download attachments using the Asana REST API. For attachments in Google Drive and Dropbox the view_url property gives a permanent, non-expiring link, but for attachments in Asana itself (which uses Amazon S3) this is not the case. For S3 the view_url looks like this: https://s3.amazonaws.com:443/prod_object_assets/assets/19422864231098/Time_travelling_-_How_does_the_world_look_like_in_10_years.docx?AWSAccessKeyId=AKIAI7NUHQYARXR2GGCQ&Expires=1415262240&Signature=TCpmP6kKbxl5YQQ554P0MlMw6%2BY%3D#= Notice the "Expires" section in the link. We would very much like to have a permanent link to attachments in S3, is this possible with the Asana REST API? When reading the API documentation it seems like this should be possible: https://asana.com/developers/api-reference/attachments. There's a distinction between download_url and view_url, where it is clearly stated that the download_url may only be valid for one hour. But for view_url there is no such warning, indicating that view_url is a permanent, non-expiring link. But this is not consistent with what we see when using the API (we use the /attachments/attachment-id endpoint). Does anyone know if this is a bug, or is the documentation incorrect?
563e315761a8013065267758	X	I think the documentation is incomplete - you're right we should call out that both view_url and download_url should be treated as temporary. If you want a permanent url, use permanent_url.
563e315761a8013065267759	X	There is now Amazon WorkMail aws.amazon.com/workmail
563e315761a801306526775a	X	Was that available in November of 2013? Just wondering; thanks.
563e315761a801306526775b	X	No, it wasn't. It's new to AWS this year.
563e315761a801306526775c	X	I'm relatively new to AWS, but I am trying to figure out how to get AWS to receive emails. According this post How to configure email accounts like support@xyz.com or feedback@xyz.com on AWS SES only handles outbound email. What I am hoping to achieve is the ability to filter aliases. For example, if the alias is "xyz12alias", then any email sent to "xyz12alias@mydomain.co", can see the email and process the content appropriately. Which in my case will be storing it in account associated with the filter. Can anybody direct me to a strategy or service within AWS that would allow me to implement inbound email on Amazon AWS? https://postmarkapp.com/inbound appears to give me what I want, but is there anything within the AWS framework itself? Are there alternate services to postmarkapp? Thanks.
563e315761a801306526775d	X	You'd have to set up your own server; that's the way to handle it using AWS. They don't provide anything other than their bulk email delivery service. A few links below: http://jeffreifman.com/how-to-install-your-own-private-e-mail-server-in-the-amazon-cloud-aws/ http://cerebellumstrategies.com/2012/04/15/amazon-linux-postfix-dovecot/ Update: there is now a solution available in AWS, as referenced in the comments below.
563e315861a801306526775e	X	Amazon Simple Email Service just introduced incoming e-mail support: https://aws.amazon.com/about-aws/whats-new/2015/09/amazon-ses-now-supports-inbound-email/ In addition to offering a scalable, cost-effective email-sending platform, Amazon SES can now accept your incoming emails. You can configure Amazon SES to deliver your messages to an Amazon S3 bucket, call your custom code via an AWS Lambda function, or publish notifications to Amazon SNS. You can also configure Amazon SES to drop or bounce messages you do not want to receive. If you choose to store your messages in Amazon S3, Amazon SES can encrypt your mail using AWS Key Management Service (KMS) before writing it to the bucket. You configure all of these actions by defining receipt rules, which you set up by using the Amazon SES console or the Amazon SES API. Receipt rules enable a single message to trigger multiple actions. Your rules can be as broad or as specific as you choose because you can configure them to apply to specific email addresses or entire domains. You can also use receipt rules to control which messages Amazon SES can accept on your behalf. Another filtering method is to set up custom IP address block lists and allow lists. If you know that you don’t want to receive mail originating from a particular IP address range, simply add it to your account's IP address block list. You can also override block lists by adding IP address ranges to your allow list, which provides fine-grained control over your inbound email traffic.
563e315861a801306526775f	X	Still doesn't appear to be possible on SES. I'd recommend looking at Mandrill and Sendgrid though. http://mandrill.com/features/ https://sendgrid.com/docs/API_Reference/Webhooks/parse.html
563e315861a8013065267760	X	This really did seem like my best bet, but I'm still consistently hitting my host's limits. Back to the drawing board. Thanks!
563e315861a8013065267761	X	I'm running a backup script using AWS CLI to perform an S3 sync command every night on my MediaTemple server. This has run without fail for months, but I updated my Plesk installation and now every night, when the backup script runs, MediaTemple disables my server due to excessive usage. The limits I seem to be crossing are as follows: They also include a networking snapshot at the time they take the server offline which includes many open connections to Amazon IP addresses (9 at time of the snapshot). Is there anything I can do to throttle the connections to AWS? Preferably I'm looking for an option within the AWS API (though I haven't seen anything useful in the documentation), but barring that, is there something I can do on my end to manage the connections at the network level?
563e315861a8013065267762	X	The AWS CLI S3 transfer commands (which includes sync) have the following relevant configuration options: This isn't so granular as throttling packets per second, but it seems like setting a lower concurrent request value and lowering both multipart threshold and chunksize will help. If the values you pasted are close to average, I would start with these values and tweak until you're reliably not exceeding the limits anymore:
563e315861a8013065267763	X	I ended up using Trickle and capping download & upload speeds at 20,000 kb/s. This let me use my existing script without much modification (all I had to do was add the trickle call to the beginning of the command). Also, it looks like bandwidth throttling has been added as an issue to AWS CLI, so hopefully this will all be a non-issue for folks if that gets implemented.
563e315861a8013065267764	X	Thanks, it looks really good, but what about licensing issues? Can I use Temboo freely in my projects?
563e315961a8013065267765	X	Yes you can - there are no licensing issues. We simply wrap access to publicly available APIs, making them appear consistent and easier to use. We adhere to API terms of service.
563e315961a8013065267766	X	Alright, so I'm only subject to APIs terms, right?I'll take a deep look, but it seems like you did a good work... you can't tell me about the compatibility with Google App Engine, can you?
563e315961a8013065267767	X	Exactly, you're subject to the API terms. Also, our free tier limits you to a certain number of calls (10,000) and data transfer (512MB) per month. As regards Google App Engine, we have used Temboo in that context before, but unfortunately it's not something we have documented.
563e315961a8013065267768	X	I need to create a Java web app that uses the API of at least two different cloud storage providers (Google Drive, Dropbox, SkyDrive, Mega, ...). I'm wondering if there's someone with experience using these APIs who can tell which are the easiest to use and which are the most difficult...
563e315a61a8013065267769	X	Temboo supports a number of cloud storage APIs, and can generate the Java source code you need to access them. See here: https://live.temboo.com/library/keyword/storage/ By normalizing API access, Temboo makes talking to one API as easy as talking to the next, so it sounds like something that you'll find useful for this project. Full disclosure: I work at Temboo.
563e320e61a801306526776a	X	Kloudless provides a common API to several different cloud storage APIs (Dropbox, Box, GDrive, OneDrive, etc.). Kloudless also provides SDKs in popular languages and UI widgets to handle authentication and other user interactions. You can find more information and sign up here: https://developers.kloudless.com/ Full disclosure: I work at Kloudless.
563e320f61a801306526776b	X	The Amazon S3 service is very simple, and I've had great experiences working with it for large files and large numbers of files in the context of web services. Once you've signed up for the service, you can use the RESTful API to create buckets and upload objects to them. The Java library is the reference library for interfacing with the services, although there are ports to other languages as well (such as boto for Python).
563e320f61a801306526776c	X	After a very little research on this subject, I've found out that probably the cloud storage provider with the simplest API is MediaFire, which offers really simple interaction through RESTful services. You can see the API documentation. I've not yet started working deeply with this API, but it seems to provided all the basic functionalities. The API served by Box seems to be also quite simple. It uses OAuth 2.0, which makes it more secure. See developers website. The SOAP API from 4sync is also really simple. There's no much documentation and the samples in the website seem to be from an older version, but anyway it is very easy to use. See documentation here.
563e321061a801306526776d	X	Do you want to send an E-Mail to your own G-Mail Account with a file (attachment)?
563e321061a801306526776e	X	i want to create a tool like this viksoe.dk/code/gmail.htm
563e321161a801306526776f	X	Which is fully based around sending an e-mail to your G-Mail account with a file attachment...
563e321161a8013065267770	X	A much better approach than using GMail. Plus Google Docs offers more than enough space for most users as well and if you already have a GMail account, all you need to do is activate it (sign up with the same e-mail address).
563e321161a8013065267771	X	I am creating an application which uploads file to gmail account avilable space can any one please tell me the best way to do it ? i read somewhere about using IMAP protocol is the best way to upload files or is there any other good way around ? regards
563e321261a8013065267772	X	GMail is not suitable for keeping generic files. Google offers Documents and Data API mechanisms for storing files. You might want to look at them. BTW we have products, which combined let you create a virtual drive with Google backend storage in a couple of hours. Callback File System offers a virtual drive, and CloudBlackbox lets you store data on Amazon S3, MS Azure and on GMail storages.
563e321261a8013065267773	X	see gist.github.com/Offbeatmammal/3718414 for a working sample
563e321261a8013065267774	X	I am attempting to play an HTML5 video within my WebView app. It works as expected on every device I have tested that is running Android 5.x, but does not work on any device running 4.x, meaning it essentially doesn't work at all. I have turned on hardware acceleration and I have set a WebChromeClient as the docs say to do, but the video still will not play. In order to support inline HTML5 video in your application, you need to have hardware acceleration turned on, and set a WebChromeClient. AndroidManifest.xml MyFragment.java Is there something else I need to do that is not documented in the developer reference?
563e321261a8013065267775	X	The problem is WebKit poorly handles redirects for videos. There are videos within webpages from my company's proprietary API. When a video is clicked, the call goes to our API, then redirects to Amazon S3 to get the actual video file. WebKit then tries to "chunk" the video (instead of pre-loading the entire thing) as you would expect. However, S3 has already done that, which causes the playback to be completely broken. Android 5.x works fine because WebView is based upon Chromium starting in 4.4, which handles the redirect appropriately.
563e321361a8013065267776	X	Why can't you catch SocketException directly? The fact that it isn't a checked exception doesn't mean that you cannot catch it.
563e321361a8013065267777	X	I think that's only true for exceptions that inherit from RuntimeException. Trying to catch SocketException here results in a compilation error because that exception itself is not thrown.
563e321361a8013065267778	X	Ok I see what you mean now. SocketException is a checked exception but the AmazonS3Client is not passing it up, right?
563e321361a8013065267779	X	The javadoc says that putObject() throws AmazonClientException when this kind of error occurs. AmazonClientException has a method called isRetryable(), maybe you can try with that.
563e321361a801306526777a	X	@DavidLevesque You're right and that basically answers my question. Write it up as an answer and I will accept it.
563e321461a801306526777b	X	So basically crank up the timeouts and retries to avoid the exception being thrown in the first place? It would seem to greatly decrease the likelihood of the exception being thrown, but not eliminate it.
563e321461a801306526777c	X	I have a ThreadPoolExecutorService to which I'm submitting runnable jobs that are uploading large (1-2 GB) files to Amazon's S3 file system, using the AWS Java SDK. Occasionally one of my worker threads will report a java.net.SocketException with "Connection reset" as the cause and then die. AWS doesn't use checked exceptions so I actually can't catch SocketException directly---it must be wrapped somehow. My question is how I should deal with this problem so I can retry any problematic uploads and increase the reliability of my program. Would the Multipart Upload API be more reliable? Is there some exception I can reliably catch to enable retries? Here's the stack trace. The com.example.* code is mine. Basically what the DataProcessorAWS call does is call putObject(String bucketName, String key, File file) on an instance of AmazonS3Client that's shared across threads.
563e321461a801306526777d	X	The javadoc says that putObject() throws AmazonClientException when this kind of error occurs. AmazonClientException has a method called isRetryable(), you can try with that.
563e321561a801306526777e	X	'Connection reset' means the connection is hosed. Close the socket, or whatever higher-level construct you're using. Probably the server has decided the upload is too large, or it's overloaded, or something. Whether you can retry the operation is something only you can know.
563e321561a801306526777f	X	
563e321561a8013065267780	X	Did you manage to fix this problem? If so, please, share your experience
563e321561a8013065267781	X	Hi, Did you manage to get this solved?
563e321561a8013065267782	X	I'm not using S3 to set cookies, I'm using a django backend to set cookies on the S3 domain but for some reason that's not happening..
563e321661a8013065267783	X	S3 is ignoring sent cookies and not sending back any other cookies. Doesn't matter on how you achieved that the browser is accepting a cookie on the "S3 domain", the S3 service is not working like this.
563e321661a8013065267784	X	I have an api endpoint for static S3 hosted site. The S3 site lives on the domain name: www.mysite.com My api (django) runs on the site the domain name: api.mysite.com When I use my login button on my site and sign in using proper username/password django sends back response with a Set-Cookie but the browser doesn't set any cookies. You can see the full response below, note the line Set-Cookie:sessionid=3kn2hovtweeofalf00ld3lowb6yvete; Domain=.mysite.com; expires=Mon, 27-May-2013 22:21:54 GMT; Max-Age=1209600; Path=/ In Django I have the SESSION_COOKIE_DOMAIN = '.mysite.com' but I've tried changing it to 'mysite.com' and '' neither of which has allowed my browser to set this returned cookie. www.msyite.com is a static site hosted on Amazon S3 but I'm using Django as my api/backend for data. When I render my login pages using Django they work just fine (login/logout cookies and sessions all work fine, so I know it's not my django code) but when using S3 or even a python SimpleHTTPServer the browser doesn't set the returned cookie. Thanks in advance!
563e321661a8013065267785	X	AFAIK the purpose of a static S3 site is to not set/accept any cookies at all? We're using that for static content like images in order to get rid of all that cookie stuff and not have the overload in the request during image or CSS requests. And: S3 is not a usual web server, it behaves very different, RTM is highly recommended. It's weired, true, but the answer I distilled from this thread Can S3 set a user cookie? is just "No."
563e321661a8013065267786	X	+1 Thanks for your fast answer! The client is concerned about privacy, and I was trying to not pay some 3rd parties for upload services...
563e321661a8013065267787	X	All I can say is find a better server :/ 10MB is an extremely small amount of space.
563e321761a8013065267788	X	Without any extra information, I'm not sure. Amazon EC2 offers a free 612MB RAM server free (not sure how many gigs of storage) for one year. If you have experience with Linux servers, you may want to give it a try.
563e321761a8013065267789	X	Thank you! I'll leave this topic opened a bit before accepting. Thanks! I'm really out of imagination how to solve this.
563e321761a801306526778a	X	I have a classic form and an upload image input that allows visitors to enter details and one image. currently I store images on the client server and send just the image URL to the client email, but recently the folder reached the limit of 10MB and no further images uploads were possible and further emails failed submission too. I tried for a long time to somehow send the form with an uploaded image the the client email, without storing it server side but with no success. Googled all around and tried lot of suggestions - I was just able to send the form but with no image attached to it. What can I do?
563e321761a801306526778b	X	You should be able to get the binary contents of the image with file_get_contents($image) using the tmp location when you submit the form and then use that data in the e-mail. The approach to display in the e-mail will vary depending on if you are using html e-mails or not. This link may help php: recreate and display an image from binary data
563e321761a801306526778c	X	It is not possible to attach a file to an e-mail without first storing it on the server. You may consider using a service such as Amazon S3 to host the files. That way, you don't have to handle the files and you can still e-mail the URLs, which makes everyone happier. Alternatively, you may be able to find a service such as ImageShack that provides an API to upload pictures to. Good luck!
563e321961a801306526778d	X	Thanks Peter, Is there any example or a tutorial to show the method of accessing Google Storage with java? That will be a great help for me. thanks again.
563e321961a801306526778e	X	I am developing an application on google app engine with gwt. There is a requirement for this application to store files(eg: pdf, msword,.zip ect). I tried to use amazon S3 in Google app engine. But it fails because app enigine does not allow me to write file handling code if the app is running on Google app engine. One another option I tried was using Blobstore which limit to store files lesser than 1Mb per API call. Is there any other option to store large files at least 10mb with Google app engine?
563e321c61a801306526778f	X	Possible solutions: Blobstore API is limited to 1Mb per API call. You could chunk up your file into multiple calls: 1MB quota limit for a blobstore object in Google App Engine? Use Google Storage and it's API.
563e321d61a8013065267790	X	When I store files with GAE I use next aproach: 1 In my JSP page create a form with file input: 2 Write a servlet/controller that read binary data from request 3 Write this data to DataStore (limited with 1MB) or into BlobStore (one call to API is limited with 1MB, but total file size can be up to 2GB). For files with size > 1MB you have to do few API calls.
563e321d61a8013065267791	X	Ok sounds great and I will definitely do it like this. How would you go ahead if you wanted to store information about the file on GAE Data Store? Like the user who stored it and the mime-type for example?
563e321d61a8013065267792	X	Store that information when the user requests the 'redirect' page. You can do a HEAD request on the newly-uploaded file to fetch the metadata, if necessary.
563e321d61a8013065267793	X	What about security issues? I mean there is no way to validate the data (except AJAX) in the form before submitting it to S3 right? So basically if I set the max. file-size within the form like Amazons suggests it you can just write your own form and upload to my bucket? And the meta-data I would add within the form can also easily be modified...
563e321d61a8013065267794	X	Never mind. I found the encrypted policy file ;)! Thanks so much for your help guys! I am new to Stack Overflow but this is amazing!
563e321d61a8013065267795	X	Okay, so far so good. But if I have a 20MB file and I use the Amazon S3 Python Library to send that file to S3...won't GAE kill the process because it takes longer than 30 seconds?
563e321d61a8013065267796	X	To be honest, I dont really know GAE's limitations, I just looked at it briefly and its flaws were way to apparent and limiting for my particular uses. To be honest, outside the fact they have a free edition available, I see very little to recommend it.
563e321d61a8013065267797	X	I'm pretty sure he already knew all this - and it's not what he was asking.
563e321e61a8013065267798	X	That link states maximum object size: 2 gigabytes. I don't know when it changed, but it's still good news :)
563e321e61a8013065267799	X	Thanks for the shout-out ;)
563e321e61a801306526779a	X	Also, I should note, that there's a decent chance it will work with S3 as well. Give it a try just by removing the filter from the build.xml file and testing it out. I'd be curious what the results are.
563e321e61a801306526779b	X	I know this has been asked before but there is really not a clear answer. My problem is I built a file upload script for GAE and only found out after, that you can only store files up to aprox. 1MB in the data store. I can stop you right here if you can tell me that if I enable billing the 1MB limit is history but I doubt it. I need to be able to upload up to 20mb per file so I thought maybe I can use Amazon's S3. Any ideas on how to accomplish this? I was told to use a combination of GAE + Ec2 and S3 but I have no idea how this would work. Thanks, Max
563e321e61a801306526779c	X	From the Amazon S3 documentation: The user opens a web browser and accesses your web page. Your web page contains an HTTP form that contains all the information necessary for the user to upload content to Amazon S3. The user uploads content directly to Amazon S3. GAE prepares and serves the web page, a speedy operation. You user uploads to S3, a lengthy operation, but that is between your user's browser and Amazon; GAE is not involved. Part of the S3 protocol is a *success_action_redirect*, that lets you tell S3 where to aim the browser in the event of a successful upload. That redirect can be to GAE.
563e321e61a801306526779d	X	Google App Engine and EC2 are competitors. They do the same thing, although GAE provides an environment for your app to run in with strict language restrictions, while EC2 provides you a virtual machine ( think VMWare ) on which to host your application. S3 on the other hand is a raw storage api. You can use a SOAP or REST api to access it. If you want to stick with GAE, you can simply use the Amazon S3 Python Library to make REST calls from Python to S3. You will, of course, have to pay for usage on S3. Its amazing how granular their billing is. When getting started I was literally charged 4 cents one month.
563e321e61a801306526779e	X	For future reference, Google added support for large file upload (up to 50 MB): The new feature was released as part of the Blobstore API and is discussed here.
563e321e61a801306526779f	X	Thomas L Holaday's answer is the correct answer, I suppose. Anyway, just in case, here's a link to Amazon Web Services SDK for App Engine (Java), which you can use e.g. to upload files from App Engine to Amazon S3. (Edit: Oh, just noticed -- excepting S3) http://apetresc.wordpress.com/2010/06/22/introducing-the-gae-aws-sdk-for-java/ Written by Adrian Petrescu. From his web site: [It is] a version of the Amazon Web Services SDK for Java that will run from inside of Google App Engine. This wouldn’t work if you simply included the JAR that AWS provides directly into GAE’s WAR, because GAE’s security model doesn’t allow the Apache Commons HTTP Client to create the sockets and low-level networking primitives it requires to establish an HTTP connection; instead, Google requires you to make all connections through its URLFetch utility
563e321e61a80130652677a0	X	Some Google App Engine + S3 links: Previous related post... 10mb limit. This link demonstrates small file uploads. I haven't found an example of large uploads yet... This link shows a different approach, (with a fix for a known issue)
563e321e61a80130652677a1	X	I'm working on integrating an existing app with File Picker. In our existing setup we are relying on md5 checksums to ensure data integrity. As far as I can see File Picker does not provide any md5 when they respond to an upload against the REST API (nor using JavaScript client). We are using S3 for storage, and as far as I know you may provide S3 with an md5 checksum when storing files so that Amazon may verify and reject storing request if data seems to be wrong. To ensure that data is not corrupted traversing the network, use the Content-MD5 header. When you use this header, Amazon S3 checks the object against the provided MD5 value and, if they do not match, returns an error. Additionally, you can calculate the MD5 while putting an object to Amazon S3 and compare the returned ETag to the calculated MD5 value. I have investigated the etag header which Amazon returns a bit, and found that it isn't clear what actually is returned as etag. The Java documentation states: Gets the hex encoded 128-bit MD5 hash of this object's contents as computed by Amazon S3. The Ruby documentation states: Generally the ETAG is the MD5 of the object. If the object was uploaded using multipart upload then this is the MD5 all of the upload-part-md5s Another place in their documentation I found this: The entity tag is a hash of the object. The ETag only reflects changes to the contents of an object, not its metadata. The ETag is determined when an object is created. For objects created by the PUT Object operation and the POST Object operation, the ETag is a quoted, 32-digit hexadecimal string representing the MD5 digest of the object data. For other objects, the ETag may or may not be an MD5 digest of the object data. If the ETag is not an MD5 digest of the object data, it will contain one or more non-hexadecimal characters and/or will consist of less than 32 or more than 32 hexadecimal digits. This seems to describe how etag is actually calculated on S3, and this stack overflow post seems to imply the same thing: Etag cannot be trusted to always be equal to the file MD5.
563e321e61a80130652677a2	X	
563e321f61a80130652677a3	X	That layout looks a lot like Shoji: aminus.org/rbre/shoji/shoji-draft-02.txt
563e321f61a80130652677a4	X	I'm designing a REST API. The part I'm working on now involves simply reading objects in the form of JSON data off of the server and modifying them. The resource that I am thinking of using for this looks like: /data/{table name}/{row key} I would like to allow GET and PUT operations on this resource. The question that I am wrestling with is that I would like to return other data along with the JSON object such as customer messages, the amount of time it took for the round trip to the data base, etc... I would also like to allow for sending query arguments with the payload in cases where the URL would be too long if they were included there. So the resources would work like this: GET Server returns: PUT Client sends as payload: I'm afraid this might violate the rules for proper RESTful GET and PUT resources because what you are sending to the server is not exactly what you are getting back out since other information is being included in the payloads. I'd rather not have every operation be a POST as a remedy. Am I being too much of a stickler with this? Is there some other way I should be structuring this? Thanks! I should note that in the resource: /data/{table name}/{row key}, I used 'table name' and 'row key' for simplicity. This is for use with a noSQL database. This resource is intended to work similar to Amazon S3. "uuid" would actually be a better description than 'row key'.
563e321f61a80130652677a5	X	I see nothing wrong with your approach. But if I was implementing this scenario, I would have asked myself the following questions: Let's take as an example "response time". If it's part of your resource, your approach is perfect and nothing else should be done. However, if it's not part of the resource, return it as a HTTP header. Fair enough.
563e321f61a80130652677a6	X	As for me it just depends on how additional info is going to be used. For my customers responseTime is not a question (or at least I think so :), they just need that response. For me as developer it can help debugging. So when customer gives me slow request I can test it easy, and that extra information could help. Here I mean that it is possible to create simple url as you specified /data/{table name}/{row key} and send just response according to that request, and you can make one more url /data/{table name}/{row key}/debug or whatever else to get the same data with additional info like "reponseTime". Just an idea ;) UPDATE: Ah yes, forgot: do not use table-name as part of your url, at least modify its name. I don't like to tell anybody how my tables are called, if somebody is going to hack my DB injecting extra code I would like her to spend more time looking for any information, instead of giving her info on a plate :)
563e321f61a80130652677a7	X	I don't see anything wrong with this, looks pretty standard to me. I'm not sure what you are planning to pass in queryArguments, is this where you would specify a callback to execute for JSON-P clients? Only thing I'd recommend you keep in mind is that REST deals with resources, and that does not necessarily map 1-to-1 with tables. Instead of using a row key you might want to have some type of GUID or UUID that you can map to that resource.
563e321f61a80130652677a8	X	I doesn't appear in the meta data--whether an object is a folder or not. Is there a specific method you guys on SO know of? I can't find anything of worth in Google search.
563e321f61a80130652677a9	X	Objects are not folders. From the docs: An Amazon S3 bucket has no directory hierarchy such as you would find in a typical computer file system. You can, however, create a logical hierarchy by using object key names that imply a folder structure. The best you can do is use GET Bucket (list objects) to get some info about the contents of the bucket: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e321f61a80130652677aa	X	There is no folder concept on S3. All objects have a key (like name) and keys can contain special characters like / (slash). This gives us a feel of folders. When you list the bucket contents it returns the list of all the objects (and the keys). Then you can see if the key string contains slash (/). If it contains, then understand the object is in a "folder like" structure. That way you get the full details. "A response can contain CommonPrefixes only if you specify a delimiter. When you do, CommonPrefixes contains all (if there are any) keys between Prefix and the next occurrence of the string specified by delimiter. In effect, CommonPrefixes lists keys that act like subdirectories in the directory specified by Prefix. For example, if prefix is notes/ and delimiter is a slash (/), in notes/summer/july, the common prefix is notes/summer/. All of the keys rolled up in a common prefix count as a single return when calculating the number of returns. See MaxKeys." http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e322061a80130652677ab	X	using AWS-CLI, this should work: aws s3 rm s3://bucket/folder1/folder2/ --recursive. This will remove everything from folder2 and below.
563e322061a80130652677ac	X	will this delete all versions permanently and not leave delete markers in the objects' place?
563e322061a80130652677ad	X	No, above command will leave the marker behind. For deleting versioned objects, the version number has to be specified in the delete command. You may have to write a quick hack to get the object IDs along with versions and then run a delete command in the loop them
563e322061a80130652677ae	X	OK you're confirming my suspicions in the original post... thanks.
563e322261a80130652677af	X	Thanks John, I've read about that as well but we need to target specific objects in our bucket on an ad hoc basis so a lifecycle rule won't apply here.
563e322361a80130652677b0	X	According to the documentation objects can only be deleted permanently by also supplying their version number. I had a look at Python's Boto and it seems simple enough for small sets of objects. But if I have a folder that contains 100 000 objects, it would have to delete them one by one and that would take some time. Is there a better way to go about this?
563e322361a80130652677b1	X	An easy way to delete versioned objects in an Amazon S3 bucket is to create a lifecycle rule. The rule activates on a batch basis (Midnight UTC?) and can delete objects within specified paths and it knows how to handle versioned objects. See: Such deletions do not count towards the API call usage count, so it can be cheaper, too!
563e322361a80130652677b2	X	dupe: stackoverflow.com/questions/651171/usermode-file-system
563e322361a80130652677b3	X	not a dupe, I am not looking for a library to do this for me, I am looking to do it myself.
563e322361a80130652677b4	X	although I might end up having to use that ;)
563e322361a80130652677b5	X	By the way although I hardly care about whether it is going to be user mode or not (I'd even prefer system-mode actually so that the data would be available to system services without need for a user to log-in) I am really curious about writing a file system driver in C# and could find nothing interesting on this subject so far...
563e322461a80130652677b6	X	+1 this worked for me....
563e322461a80130652677b7	X	To the best of my understanding Dokan needs to be installed as a kernel-mode driver (Dokan.sys), and only after this can you implement your own user-mode code (optionally in .Net) layered on top...
563e322461a80130652677b8	X	@user424257 LGPL allows use in commercial projects (GPL does too in fact). Dokan has it's own shortcomings, though, such as a huge tail of bugs and lack of support and maintenance.
563e322461a80130652677b9	X	how is this pure usermode? It's a kernel driver, isn't it??
563e322461a80130652677ba	X	Corection - you don't need Microsoft signing for the driver to be loaded. The driver must be signed and countersignature must be included, but it's a different matter. Microsoft signing (WHQL certification, to be correct) is a different story.
563e322461a80130652677bb	X	Rather than talking to local hardware, I am actually looking from the other end of it .. how to make my abstracted file system appear as a drive in Windows.
563e322461a80130652677bc	X	I appreciate it, although it is your product and you can charge whatever you want for it, but $4K for a 'discounted' private business vendor license is a little too steep for me
563e322461a80130652677bd	X	@esac writing drivers never was cheap, and maintaining them is quite expensive as well. "Free" Dokan is full of bugs with no release cycle. So you pay with either your money or your disappointed customers.
563e322461a80130652677be	X	I agree fully, and you obviously should charge for your product. As a developer who writes a lot of tools in his free time, most of them which only I use personally and they never see the light of day, I can't justify 4K. I would be glad to pay a MUCH smaller fee for a personal use, non-redistributable copy, and if for some reason I ever wanted to release something, I would be far more willing to shell out the 4K required.
563e322461a80130652677bf	X	Have an evaluation key that works for 6 months. After 6 months, the user would need to request a further evaluation key. If you see a copy of a program out in the wild using one of these evaluation keys, you don't give them out to that user anymore, and their software stops working.
563e322461a80130652677c0	X	@MatBee we have private business licenses and free non-commercial licenses. The only restriction is no open-source.
563e322461a80130652677c1	X	Is it possible to write a filesystem for Windows in pure usermode, or more specifically purely in managed code? I am thinking of something very similar to GMAILFS. Excluding what it is doing under the covers (GMAIL, Amazon, etc..) the main goal would be to provide a drive letter and support all of the basic file operations, and possibly even adding my own structures for storing metadata, etc..
563e322461a80130652677c2	X	It's difficult. I'd take a look at some projects which have done some of the hard work for you, e.g. Dokan.
563e322561a80130652677c3	X	Yes. It's possible and has been successfully done for the ext2 filesystem. Note that you will need to write your own driver which will require Microsoft signing to be run on some OSes.
563e322561a80130652677c4	X	Sure, you can abstract the regular file operations and have them running in the cloud (see Google Apps, Amazon S3, Microsoft Azure etc.). But if you'd like to talk to local devices - including the local HD - you'll have to use system APIs and those use drivers (system/kernel mode). As long as all you want is a storage service -no problem. If you want a real OS, you'll need to talk to real hardware and that means drivers.
563e322561a80130652677c5	X	Just as a reference - our Callback File System is a maintained and supported solution for creation of filesystems in user-mode.
563e322561a80130652677c6	X	How about an URL so we can have a look at it?
563e322561a80130652677c7	X	thanks, i didn't know it was called that. I will get jquery from google.
563e322561a80130652677c8	X	i have to try moving my scripts to the bottom of the page. Does this include css scripts as well?
563e322561a80130652677c9	X	No, CSS style sheets go at the top of the page. For more info and other best practices, see developer.yahoo.com/performance/rules.html
563e322561a80130652677ca	X	re: free web account idea: If I move my images to a Google Sites account, do you know if Google Sites' servers are just as fast as Google.com?
563e322561a80130652677cb	X	They should be served using the same infrastructure, so I reckon that it will be just as fast. They do have the option for additional bandwidth at cost, but you can just use the free bandwidth until you get an idea of the bandwidth needed.
563e322561a80130652677cc	X	I am thinking to save server load, i could load common javascript files (jquery src) and maybe certain images from websites like Google (which are almost always never down, and always pretty fast, maybe faster than my server). Will it save much load? Thanks! UPDATE: I am not so much worried about saving bandwidth, as I am reducing Server Load because my server has difficulty when there are a lot of users online, and I think this is because there are too many images/files it loads from my single server.
563e322561a80130652677cd	X	This is known as a content delivery network, and it will help, although you should probably make sure you need one before you go about setting it all up. I have heard okay things about Amazon S3 for this (which Twitter, among other sites, use to host their images and such). Also, you should consider Google's API cloud if you are using any popular javascript libraries.
563e322661a80130652677ce	X	You might consider putting up another server that does nothing but serve your static files using an ultra efficient web server such as lighttpd
563e322661a80130652677cf	X	Well, there are a couple things in principle: So if you are truly not experiencing any bandwidth problems, I don't think offloading your images, etc will do much for you. However, as you move stuff off to Google, then it frees your server's bandwidth up for more concurrent requests and faster transfer on the existing ones. The only tradeoff here is that clients will experience a slight (most likely unnoticable) initial delay while DNS looks up the other servers and initiates the connection to them.
563e322661a80130652677d0	X	It really depends on what your server load is like now. Are there lots of small web pages and lots of users? If so, then the 50K taken up by jQuery could mean a lot. If all of your pages are fairly large, and/or you have a small user base, caching jQuery with Google might not help much. Same with the pictures. That said, I have heard anecdotal reports (here on SO) that loading your scripts from Google does indeed provide noticeable performance improvement. I have also heard that Google is not necessarily 100% uptime (though it is close), and when it is down it is damned inconvenient. If you're suffering from speed problems, putting your scripts at the bottom of the web page can help a lot.
563e322861a80130652677d1	X	I'm assuming you want to save costs by offloading commonly used resources to the web at large. What you're suggesting is called Hotlinking.. that means directly linking to other people's content. While it can work in most cases, you do lose control of the content, that means your website may change without your input. Since image hosted on google are scoured from other websites, the images may be copyrighted, causing some (potential) concern, or they may have anti-hotlinking measures that may block the images from your webpage. If you're just working on a hobby website, you can consider hosting your resources on a free web account to save bandwidth.
563e322961a80130652677d2	X	I would remove your personal comments about Perl and Java
563e322961a80130652677d3	X	I need to create RESTful API for uploading media data. I need to be able to handle hundreds (thousands) of simultaneous requests. Once data is uploaded to my server, we are going to store it on Amazon S3 and populate some meta data into database. Could you advice in a few questions: 1) Which language is better for these kind of tasks ? (I'm familiar with PHP and Perl) 2) What about server? (nginx ?) 3) We need to be able to scale easily in case there are a lot of requests 4) Anything else you could point out and advice ? Thank you
563e322a61a80130652677d4	X	See stackoverflow.com/a/16537396/165673
563e322a61a80130652677d5	X	did you just do this or it is from your working code?
563e322a61a80130652677d6	X	Approximately 2-3 days of work some months ago. But happy to share it (I also submitted a patch to plupload because of issues I had during implementation).
563e322a61a80130652677d7	X	Does anyone have any sample code (preferrably in rails) that uploads to s3, using s3's servers. Again, uploading directly to s3, where the actual upload/streaming is also preformed on amazon's servers.
563e322a61a80130652677d8	X	Requirements: Idea: I posted the code as a gist at https://gist.github.com/759939, it misses commments and you might run into some issues due to missing methods (had to rip it from our codebase). stored_file.rb contains a model for your DB. Has many of paperclips helper methods inlined (which we used before we switched to direct upload to S3). I hope you can use it as a sample to get your stuff running.
563e322b61a80130652677d9	X	If you are using Rails 3, please check out my sample projects: Sample project using Rails 3, Flash and MooTools-based FancyUploader to upload directly to S3: https://github.com/iwasrobbed/Rails3-S3-Uploader-FancyUploader Sample project using Rails 3, Flash/Silverlight/GoogleGears/BrowserPlus and jQuery-based Plupload to upload directly to S3: https://github.com/iwasrobbed/Rails3-S3-Uploader-Plupload
563e322b61a80130652677da	X	To simply copy files, this is easy to use: Smart Copy Script into S3
563e322b61a80130652677db	X	Amazon wrote a Ruby library for the S3 REST API. I haven't used it yet. http://amazon.rubyforge.org/
563e322b61a80130652677dc	X	No, it doesn't work because it requires a String in the parameter. My problem is that I couldn't get the path of each file in my bucket.
563e322c61a80130652677dd	X	If it requires a string then it must expect that string to be a path to a local file. In that case, the only option is to copy the file from S3 to your local filesystem (using key.get_contents_to_filename) and then pass the path to the local file to the Mp3AudioFile class.
563e322c61a80130652677de	X	Well, I need to get the paths + mp3 metadata of these files to store it in a MySQL database for my Django app, so later I'll be able to play them in on my website. Is there any other library (other than eyeD3) that allows me to read tags from a non-local file?
563e322c61a80130652677df	X	I don't, sorry. If you want the full URL for the MP3 files as stored in S3 you can call key.generate_url(1, query_auth=False, force_http=True) for each key. This will give you a URL pointing to the object in S3. Hope that helps.
563e322c61a80130652677e0	X	That means the files are not publicly readable. Do you want them to be? Or are you intended these to be accessible only through your application? You can create signed URL's that expire in a period of time. For example, key.generate_url(60, force_http=True) would generate a signed URL that will expire (i.e. become unusable) in 60 seconds. I don't really know what your application is so I'm not sure what to suggest.
563e322c61a80130652677e1	X	I'll be running the update (meta data extraction from mp3 files in AS3 and upload the data to MySQL database) from the Django admin panel. So I should download the files temporarely into my server and then get the metadata from them?
563e322c61a80130652677e2	X	@MohamedTurki - Is this a one-off task? If yes, then I'd write a completely separate script in whatever language you prefer to do the indexing? I wouldn't want to download the files to my web server even if temporarily.
563e322c61a80130652677e3	X	emm, I'm not sure how frequently I'll be doing this, perhaps once in three months. Problem is, i'll need to fill the database from different sources, AS3 to get the url of each mp3 file, and from local drive to get metatags of the same mp3 file. Maybe I'm not giving this task as much attention as it deserves, as I thought it wouldn't be this complicated, maybe I should write a script that uploads files and extract metal data at the same time? Is that feasible?
563e322c61a80130652677e4	X	@MohamedTurki I would absolutely go down the route of 'indexing' the files during the upload process. S3 buckets can be somewhat difficult to manage with lots of files if you don't already have a firm grip of what's in there. This should be easy to implement. Upload to your server, extract the tags and add to DB along with any other info about file (incuding details of S3 location eg bucket subfolder etc in case you want to move some files in time), move to S3. If your server is in EC2 then this is even better as transfers between EC2 and S3 are free and very fast.
563e322c61a80130652677e5	X	I'm trying to get mp3 tags from my files that stored in Amazon S3 using Boto. Here is my script : However, I could list all the files in my bucket and so on. The error i'm getting is Is there any problem with my code?
563e322c61a80130652677e6	X	You are passing key.name to the eyeD3 functions but I think you want a file-like object for the call to eyeD3.Mp3AudioFile. I haven't used eyeD3 and it doesn't seem to want to install via pip so I can't try this but something like this should work:
563e322c61a80130652677e7	X	There is no way to get the tags from the files without downloading them from S3. You might consider using EC2 to process the files or Amazons Elastic MapReduce but you're still going to be downloading the file to read the tags.
563e322c61a80130652677e8	X	I had to write a script that the meta data of the mp3 files from my local drive, uploads the songs to Amazon S3 (Using Boto API) and set privileges to "public", generates a URL, then store the URL and metal data into a MySQL database. So just in case some runs into same problem, this solved my issue as I now don't need to upload the songs and then run an update for my database.
563e322c61a80130652677e9	X	Thanks, I already thought so. Amazon seems a pretty good bet, thanks for the hint!
563e322c61a80130652677ea	X	We are implementing a leveleditor for our game and want to enable submitting the user-generated levels. All other users of the game should be able to play these levels. Is it possible to store the levels in GameCenter or is the only way to achieve this to set up a dedicated server? In case we have to go with our own server, are there any preconfigured services for this scenario? Something like dropbox, with a nice API instead of having to code everything ourselves from the ground up. Thanks a lot!
563e322d61a80130652677eb	X	GameCenter does not currently have support for downloadable content. You could peer-to-peer share small payloads of data between players using the matching API, but that's pretty clearly not what you want. In thinking about other services, the first/easiest thing that comes to mind for me would be to use Amazon S3. It's super-simple, reasonably cheap, has good content distribution, availability, etc. (Sure beats running your own server, anyway.)
563e322d61a80130652677ec	X	Great, thank a lot!
563e322d61a80130652677ed	X	how you have used the Security token service in AWS
563e322d61a80130652677ee	X	I have a server, which should provide temporary AWS credentials to the client. The credentials will be transmitted using HTTPS. The client should be able to upload S3 files, as well as download them. The concern I have is the following: I have multiple users accessing ONLY their own directory: /Users/someUser/myfile.png You can set policies to allow or deny S3 in general, but you can't grant only the access to a specific path. What should I do about this? Will the HTTPS transmission be enough? Then my second question. If I hear "temporary credentials", I have a key in mind, that is valid for a couple of hours and then expires. But I'm not sure if IAM is really built for that. Should I provide the same credentials for all users? Or do I generate a key-pair for each client? The server runs with PHP, the client with Objective-C.
563e322d61a80130652677ef	X	You can specify permissions on a path in Amazon S3. For more details see the following: Using IAM Policies Also, if you want to create "temporary credentials" you can use the AWS Security Token Service. This service allows you to create credentials that last from 1 - 36 hours and you can put a policy on those credentials to limit their access. For more details about the service see: Security Token Service API Reference Finally, there is an article written for the AWS Mobile SDKs that does something similar. It has a server to issue temporary credentials to users that use an Amazon S3 bucket. It limits the users to a "sub-folder" of the bucket and also limits their actions. You can read and this sample here: Credential Management for Mobile Applications Hope this helps you get to the information you need.
563e322d61a80130652677f0	X	So what is the financial cost of this operation? As well, is this algorithm efficient? Say I have 10000000 objects, since S3 is flat does it search for all objects?
563e322d61a80130652677f1	X	It's almost free (in terms of cost); 1/2 cent per 10k requests. It's generally advisable to not do this operation as a standard part of your code- there are typically much better way to structure things so this isn't something you need to do often.
563e322d61a80130652677f2	X	Thanks for the information. What would be the better practice then?
563e322d61a80130652677f3	X	It depends what you are trying to do. If you post your use case as a new question, link it here. You can use SNS/SQS and possibly Lambda.
563e322d61a80130652677f4	X	I would like to get all the URLs of objects stored in a folder. I will only have one level of folders so I am not concerned with nested folders. I have read the PHP client API (http://docs.aws.amazon.com/aws-sdk-php/v2/api/class-Aws.S3.S3Client.html) for S3 but can't seem to find a way to accomplish this. I found this code from StackOverflow to get the size of contents: Which is close to something I want, except I do not want all the items in a bucket, I want all the items in a certain bucket's folder. My second question is how much would I need to spend to accomplish this as there doesn't seem to be an get/put commands being used so I am not sure how much Amazon charges to for this operation?
563e322d61a80130652677f5	X	ListObjects takes a prefix argument. That prefix is the "directory" inside a bucket.
563e322d61a80130652677f6	X	I doubt there's a straightforward way. Do the folder names follow a pattern?
563e322e61a80130652677f7	X	TJ, there is no pattern on the folder names, they are all UUIDs. I am trying to use "file-named-a" as a flag to indicate another processor that the folder is ready for processing.
563e322e61a80130652677f8	X	Thank you John. I think that is the best I can do for now.
563e322e61a80130652677f9	X	I understand that s3 does not have "folder" but I will still use the term to illustrate what I am looking for. I have this folder structure in s3: my-bucket/folder-1/file-named-a my-bucket/folder-2/... my-bucket/folder-3/file-named-a my-bucket/folder-4/... I would like to find all folders containing "file-named-a", so folder-1 and folder-3 in above example will be returned. I only need to search the "top level" folders under my-bucket. There could be tens of thousands of folders to search. How to construct the ListObjectsRequest to do that? Thanks, Sam
563e322e61a80130652677fa	X	An Amazon S3 bucket can be listed (ListBucket()) to view its contents, and this API call can be limited by a Prefix. However, it is not possible to put a wildcard within the prefix. Therefore, you would need to retrieve the entire bucket listing, looking for these files. This would require repeated calls if there are a large number of objects. Example: Listing Keys Using the AWS SDK for Java
563e322e61a80130652677fb	X	may I ask how you resolved this problem? I know its more than a year old but I am facing the same issue and any help would be appreciated.
563e322e61a80130652677fc	X	... or the server certificate isn't trusted (see this question).
563e322e61a80130652677fd	X	Thanks @Bruno. I was looking for that earlier and could not find it.
563e322e61a80130652677fe	X	I'm using Simpl3r, a simple high level Android API for file uploads using the Amazon S3 service, to upload media files to my bucket. On some uploads, I'm getting a SSLException error. Here's the code where the exception is thrown: When it is come the app is stuck in upload state and this exception is not caught in my exception block.When i am searching for this problem, i found somewhere that your JVM is obsoleted you have to update your JVM. So how to resolve this problem, any ideas?
563e322e61a80130652677ff	X	javax.net.ssl.SSLPeerUnverifiedException: No peer certificate Either you are using Anonymous Diffie-Hellman (ADH), the certificate is not installed on the server, SSL/TLS is not enabled for the server, or there's a trust issue with the server. You should verify that SSL/TLS is available with OpenSSL (or another tool). For example: Here's the OpenSSL docs on s_client(1). If there is a SSL/TLS server available, then see Bruno's answer at Apache HTTPClient SSLPeerUnverifiedException. There's another potential problem: no client certificate. But I don't believe that's a problem here. It does not look like you configured for mutual authentication, and I don't see the TLS alert in the back trace.
563e322e61a8013065267800	X	If the base64'd data is not an image, then imagecreatefromstring will return FALSE. That's probably all the "validation" you need?
563e322e61a8013065267801	X	Why are you base64 encoding the images in the app? Why not just send the binary data in the payload? Or post it like form data so you can get the image via $_FILES?
563e322e61a8013065267802	X	I'm building a RESTful JSON API to serve as the backend for an iPhone app with some camera/photo functionality. I'm trying to determine what are useful and reliable ways to handle the image uploads. We are going to be base64 encoding the images within the app and POSTing with the rest of the payload to the API, and then on the server we want to resize the image to multiple different dimensions (thumbnail, etc.) and push all of the files to Amazon S3. Some of the resizing may be done asynchronously. What useful ways to do this, taking into account any file format conversion (we want all images to be JPEG, but they may not come directly from the camera), and that we do not want to permanently store the files on the server? I have done this much: But I am mostly unsure about how do I validate that the base64 string represents a JPEG, or do I need to? I create the image using imagecreatefromstring(), but what should be the next steps after that?
563e322f61a8013065267803	X	Thanks for responding. I am using the s3handler.js that you linked to already. It works, except when I enable the chunking option browser-side. Any other suggestions for me to continue troubleshooting this?
563e322f61a8013065267804	X	Your next step should be to verify that your endpoint is returning the correct signature to fine uploader, and then that this signature is being sent to s3 properly without any additional headers not accounted for when generating the signature.
563e322f61a8013065267805	X	Not sure if it is relevant, but I added two lines of code to enable CORS in your s3handler.js file.
563e322f61a8013065267806	X	The nodeJS endpoint is returning the signature generated by the code provided in s3hanlder.js for multi-part uploads. It is provided this by the fileUploaderS3 object in the browser. I verified the response to the post matches in the browser and console.log() output from node.
563e322f61a8013065267807	X	This is all F.U. code involved, nothing I wrote. Since the client access key works when chunking is turned off I'm not sure how to troubleshoot this. It's either a bug in the example code, or perhpas something is not configured correctly on the S3 CORS?
563e322f61a8013065267808	X	For files less than 5mb my configuration is working to upload. For files larger than 5mb they always fail with Post response from AWS: I am using NodeJS, with the relevant functions from the demo s3handler.js unchanged eg: the signRestRequest() function appears to be working as expected per this documentation: http://blog.fineuploader.com/2013/08/16/fine-uploader-s3-upload-directly-to-amazon-s3-from-your-browser/#support-chunking For files less than 5mb with this configuration, resume works if the connection is closed and then re-open automatically. I am hosting the page and NodeJS in the same local Ubuntu VM. My browser side configuration: This is the response before it is SHA1 HMac encoded, then base64 encoded: My temporary test S3 Bucket CORS configuration: I can provide any additional information as required and have purchased a license. Thanks!! This is the header sent to the AWS post for a multi-part upload:
563e322f61a8013065267809	X	If AWS reports that your signature is incorrect, then the issue is with your signature generation code. As documented, chunked requests (by default files greater than 5 MB) use S3's multipart upload API, and the signature requirements are different. The following is from Fine Uploader's documentation, specific to chunked uploads: Fine Uploader S3 uses Amazon S3's REST API to initiate, upload, complete, and abort multipart uploads. The REST API handles authentication by signing canonically formatted headers. This signing is something you need to implement server-side. All your server needs to do to authenticate and supported chunked uploads direct to Amazon S3 is sign a string representing the headers of the request that Fine Uploader sends to S3. This string is found in the payload of the signature request: { "headers": /* string to sign */ } The precense of this property indicates to your sever that this is, in fact, a request to sign a REST/multipart request and not a policy document. This signature for the headers string differs slightly from the policy document signature. You should NOT base64 encode the headers string before signing it. All you must do, server-side, is generate an HMAC SHA1 signature of the string using your AWS secret key and then base64 encode the result. Your server should respond with the following in the body of an 'application/json' response: { "signature": /* signed headers string */ } Fine Uploader's server-examples repo even includes a full node.js example that handles chunked and non-chunked signing logic.
563e322f61a801306526780a	X	Obviously you can use Facebook for this purpose. P.S.: I don't think you can upload photos using Javascript.
563e322f61a801306526780b	X	well i am referring to this - webdevhub.net/facebook-api/picture-upload-facebook-api and looks like it's possible.
563e322f61a801306526780c	X	Here you are submitting to Facebook end-point directly! The example described in the article is NOT recommended as it'll move your visitors away from your website/app!
563e322f61a801306526780d	X	you are right. in fact, I just realized that this is not the right approach... would you suggest something better ? or will it solve my problem if i use ajax submit ?
563e322f61a801306526780e	X	since I am using socket.io in node.js, I can certainly send these information to the server and process it there.. Please suggest if there is a better approach.
563e322f61a801306526780f	X	edited my post after "EDIT"...
563e322f61a8013065267810	X	I am building a site with nodejs and mongodb and using facebook for authentication.. my users will be required to upload certain photo and others will be able to view them. Since I am using facebook, I am wondering if I should allow users to upload their photos to their facebook profile and I'll save the links into the database so that later on other users can view this. Is this the right approach ? or should I use flickr or picasa or something else ? I have to do this in javascript and I know facebook has support for this. Please let me know what you think. EDIT: Hi, Finally I found the module connect-form to upload the file to the server and then uploading the file to facebook using the module facebook-js. I found fb.api for "/me/feed" works perfectly in node.js server. But when I tried to use the graph api for "/me/photos" as mentioned in http://developers.facebook.com/docs/reference/api/album/, I got the error. That is because it expects the source to be in "multipart/form-data". In my html, it is already "multipart/form-data", otherwise I'll not get the file in the server side. In the server side, however, I am not very sure about how to embed this in FB.api... what should be the "source" parameter in FB.api "/me/ photos" ? I tried this with "source:files.source" as mentioned in my example. But it does not work. may be i am missing something very silly...
563e322f61a8013065267811	X	You have the option to upload photos to Facebook using the url, read the following article: https://developers.facebook.com/blog/post/526/ I suggest to use some CDN for photos like Amazon S3 which is very reliable and scalable. Then you can optionally have it on Facebook too with a simple API call (don't forget about extended permissions to upload photos in this case) hope this helps
563e323061a8013065267812	X	I need to share this app with a few friends for testing. I need an online solution.
563e323061a8013065267813	X	Well I found this site and it is good for my purpose: hostinghood.com
563e323061a8013065267814	X	For specific reasons I need to store a small files (up to 1MB) in a web server so that my application can read data from it. I don't need any server side computing. I know there are commercial solutions for this but since this is a test app I need to manage costs down. Does any one know a free solution to store files like a server? Thanks in advance.
563e323061a8013065267815	X	If you have Python installed on the computer, just go to the directory where you have the files and type python -m SimpleHTTPServer - it will start a HTTP server on port 8000. If you want this to be hosted online, then you can use any number of free services: Dropbox/Google Drive/OneDrive - upload the file there an use their API or a simple direct link to it. Use Amazon S3 - its available for free for 12 months if you are a new subscriber.
563e323061a8013065267816	X	If you need that to speed up your website, you can use CDN at: https://www.cloudflare.com/ It stores automatucly your website's static data.
563e323161a8013065267817	X	Unfortunately btoa is not available from within Deployd event scripts. ReferenceError: btoa is not defined
563e323161a8013065267818	X	hmm, the, the only way is that you write your own base64 coding/decoding function...
563e323161a8013065267819	X	I'm using Deployd to build an API to be consumed by an AngularJS app. I'm attempting to integrate the ng-s3upload module to save some images on Amazon S3. The ng-s3upload module requires the backend server, in this case deployd, to generate a Base64 encoded policy. I created a GET event to generate the policy but haven't figured out how I can Base64 encode it form within the Deployd event script. Any help or ideas is appreciated. I tried to use the NodeJS Buffer function, Deployd is based on Node, but it is not available form the event script environment.
563e323161a801306526781a	X	You can use the btoa() function to encode strings to base64 format. EDITED As you say you can't use btoa, I wrote an implementation of a base64 encoding function. You use it just like this: Here is the code, you can see it in action in this jsfiddle.
563e323161a801306526781b	X	Try toString('base64');
563e323161a801306526781c	X	sounds like a good idea, is it easy to integrate with an uploader... like plupload?
563e323161a801306526781d	X	it is easier to use the plain api, and it all depends how you would like to use, it, only from frontend, choose js plain api, from backend, with java and python its very easy, with go its easy and with php i dont know, if you choose from backend then you should be able to easy integrate it with uploaders like plpload
563e323161a801306526781e	X	I'm looking for a cloud based service which will allow my customers to upload very high resolution and print pdfs (sometimes about 60mb), store the images and create low resolution images very quickly I've started looking at Amazon S3 but know this doesn't do anything with the files uploaded and started looking at google app engine. I did think about using dropbox core api but i think this is really for 1 to 1 users rather than hundreds of users daily. Any suggestions for services would be great Thanks David
563e323161a801306526781f	X	have a look at google cloud storage: https://cloud.google.com/products/cloud-storage there you can upload files up to 5 tb, and as many as you can pay. it works perfectly with lots of users. you can use buckets or folders per user, its up to you. also its possible to reach that files with an own domain, apis are available for many languages as well
563e323261a8013065267820	X	Are there third party tools that you would recommend? Thanks
563e323261a8013065267821	X	Not really. I think just hitting Google and seeing what's out there would be your best bet.
563e323261a8013065267822	X	Which cloud management consoles? Thank you very much
563e323261a8013065267823	X	Thanks datasage. Do you know good scripts that would allow me to define a retention period/number of versions I want to keep and that would also send an email allowing me to quickly know if a backup went wrong?
563e323261a8013065267824	X	You could quote some fragments or provide some context to this link. Please read stackoverflow.com/questions/how-to-answer
563e323261a8013065267825	X	Thanks! Added some more context.
563e323261a8013065267826	X	Skeddly works fine and is pretty much free if you do not backup too often.
563e323261a8013065267827	X	Does it mean you have to have an instance running Scalr?
563e323261a8013065267828	X	You can have an instance running on AWS or on a machine running on prem which is my configuration. I'm using it for automated snap shots right now but plan to create scripts to save the snapshots in S3 or glacier
563e323261a8013065267829	X	It is not readable.
563e323261a801306526782a	X	@Andy Check now ? is it fine ?
563e323261a801306526782b	X	Yes thanks! ...
563e323261a801306526782c	X	I'm looking for a backup solution for Amazon EC2 instances. I found this: http://www.n2ws.com and I wanted to know if there were other ones. Thank you PS: It's possible to automatically backup RDS databases using Amazon solution but there isn't anything for EC2 instances... Is there?
563e323261a801306526782d	X	I've been using Skeddly for several months now to automatically backup the EBS volumes attached to my EC2 instances. I'm really happy with it so far. I liked the way I could define which instances to backup: only instances with a specific tag are backed up. I just have to add this tag to the instances I want to back up. No need to do any change in Skeddly each time I add an instance. I had to define 2 actions in Skeddly: one to backup the instances and one to delete the old snapshots. And I receive emails to inform me the actions (backup and expiration) have been successful or not.
563e323261a801306526782e	X	If by "EC2 Instances" you really mean "EC2 Instances with EBS Drives" then the Snapshot features of EBS, available through the AWS Console and the AWS API, are what you're looking for. From the EBS Docs: Amazon EBS also provides the ability to create point-in-time snapshots of volumes, which are persisted to Amazon S3. These snapshots can be used as the starting point for new Amazon EBS volumes, and protect data for long-term durability. The same snapshot can be used to instantiate as many volumes as you wish. These snapshots can be copied across AWS regions, making it easier to leverage multiple AWS regions for geographical expansion, data center migration and disaster recovery. Amazon doesn't offer any scheduling or retention type policies around snapshots, but there are some third party tools that leverage the AWS API's.
563e323261a801306526782f	X	The company I work for has been using Amazon's S3, EBS, and EC2 almost since their inception. It became painfully obvious, after losing 2 (1 development and 1 production) virtual servers 4 days after they were completed and scheduled to be let loose on EC2 the next night. To make a long story short, we did not find a standalone application that was very small, lightweight, and nearly configurable to any situation. Using AWS .NET SDK, we were able to write the above application in less than a day and then using the Task Scheduler on our in-house Windows Server 2008 R2 server. We have gone through a number of scenarios and settled on the following schedule: EC2 instances images are created weekly, EBS snapshots are created daily. EC2 instances older than 31 days are dropped and EBS snapshots are dropped after 60 days, per our contract we entered in a contract with a client who had been burned previously with a standalone application that was supposed to run the backups on its own internal scheduling code/mechanism. It never ran, and no one looked at it after they set it up. As the application matures we plan on having Simple E-Mail Service (SES) for backup summary/log e-mail to our developers, and Simple Queuing Service (SQS) to record the process. Hope this helps.
563e323261a8013065267830	X	The cloud protection manager product you found (www.n2ws.com) does support automoated backups of full EC2 instances, beyond backing-up EBS volumes individually, as well as RDS snapshots. It also has the scheduling, data retention policies and automated alerts options you were looking for and other backup related features for AWS. Couldn't find other 3rd party products providing comparable automated backups for EC2 instances, but some of the cloud management consoles allow snapshot scheduding & creation of data retention policies.
563e323361a8013065267831	X	Sort of. You can snapshot EBS volumes on a regular interval. While there isn't anything in the UI to do this for you automatically, the API will allow you to do it. You can either roll your own backup script, or search for one that has been publicly released.
563e323361a8013065267832	X	For critical applications, a backup solution should be more than just scheduling snapshots. You'd expect features like application-support, backup policies and powerful recovery options and more. You can ream about it in my post: http://www.n2ws.com/blog/tier-1-application-backup-on-amazon-cloud.html It's from the n2ws site and also references the CPM product.
563e323361a8013065267833	X	For easy management with a GUI, there is also Skeddly. It is pay as you go with CAD 0.15 for most actions. It is also possible to do all this things free. A good script to start from is this.
563e323561a8013065267834	X	There is an opensource project called Scalr that I just started using for about a week and it has features that enable you to scheduled automated snapshots/backups of your EBS volumes. Scalr is actually a cloud management solution and has many fabulous features that I've yet to play with but I'm looking forward to it. There is a pay version but I'm just kickn the tires on the free open source version for now. The Scalr installer is available on Github: https://github.com/Scalr/installer-ng The Scalr source code is on Gitub too: https://github.com/Scalr/scalr Installation instructions are on the Scalr wiki: https://scalr-wiki.atlassian.net/wiki/x/0Q8b
563e323561a8013065267835	X	You can use AutomatiCloud to backup your EC2 volumes and RDS instances. AutomatiCloud allows you to define schedules for backups and cleans up after a retention period you can configure. It also sends out email notifications in case of success/failure. And it is free! www.automaticloud.net Disclaimer: I am the author
563e323561a8013065267836	X	Here is script Script to Automate AMI backup ! It will find instance-id of all instance in your VPC n create AMI backup !
563e323561a8013065267837	X	I can probably add to this, I've used skeddly for about 12 months it works, kinda, but it has a pretty bad UI and I can never reach their guys if I've needed support or have feature improvement ideas. I've worked with the team over at www.cloudmgr.com for a fair while too and have been much more impressed. CloudMGR's UI is better both from a UI design and responsiveness perspective, its easier for non techs. It handles elements beyond backups such as cost optimisation and scheduling and more. The support team is also stellar which is v. valuable. Give them a try. (full disclosure - not related to Skeddly/cloudMGR or any other vendor, I work for a company who has used all of the above).
563e323561a8013065267838	X	What specific request is failing?
563e323561a8013065267839	X	Sorry I am not sure exactly what you are asking for but: whenever I drag and drop a JPG file on the browser it generates the thumbnail but fails to upload. it just returns the error message: Reason: XHR returned response code 0 and fails to upload the image to my S3 bucket. I am not sure what part is failing, if there is a way to review my JSON policy and signed data or failing CORS configs/values somewhere that is what I am looking for advice on.
563e323561a801306526783a	X	I guess whenever I have something partially working/failing I can enable verbose logging somewhere to figure out whats broken (or at least google error messages). For this, the only error I see the javascript alert with UPLOAD FAILED Reason: XHR returned response code 0. Just trying to figure out what else I can test/inspect/review.
563e323561a801306526783b	X	The first step is to figure out which specific request is failing. If you look at the network tab of your browser's dev tools, you will be able to make this determination. You will also see more useful information in the console.
563e323561a801306526783c	X	Gotcha thanks. So digging around I see the following error: XMLHttpRequest cannot load s3.amazonaws.com/dev-pre-content. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin '192.168.1.215'; is therefore not allowed access. The test box I am running this off is a VM guest on my laptop. Its a full mirror of my production server. From this laptop IP 192.168.1.215 it goes directly to my S3 bucket named 'dev-pre-content' which is on US-WEST-2 AWS.
563e323561a801306526783d	X	Editing the server side code s3demo-cors.php and changing header('Access-Control-Allow-Origin: {$_SERVER['SERVER_NAME']}'); rather than * seemed to move me a step further. Now Im hitting the following errors: [Fine Uploader 5.2.0] Error attempting to parse signature response: SyntaxError: Unexpected token < [Fine Uploader 5.2.0] Received an empty or invalid response from the server! [Fine Uploader 5.2.0] Policy signing failed. Received an empty or invalid response from the server!.
563e323561a801306526783e	X	Im stepping through the policy signing code for POST method and not sure what exactly is required. It has my AWS credentials already, looks like it constructs the JSON policy (in code) but cant seem to gain visibility into what the JSON output looks like or how to validate what is being generated by the s3demo-cors.php example you provide.
563e323561a801306526783f	X	I see this documentation here: blog.fineuploader.com/2013/08/16/… which explains the json policy values but not sure if this is to be posted to s3demo-cors.php OR where/how this is supposed to be implemented.
563e323661a8013065267840	X	The PHP example handles the signing for you.
563e323661a8013065267841	X	SUCCESS: Verdict was my request endpoint. Originally it was: "s3.amazonaws.com/dev-pre-content"; which I thought I read was supposed to be a valid format. I had to change it to "dev-pre-content.s3.amazonaws.com"; in order for it to work over HTTPS and recognize it.
563e323661a8013065267842	X	Nothing in your answer relates to CORS. Perhaps you are confusing terms?
563e323661a8013065267843	X	so I ended up putting corsag to what I meant is the policy,
563e323661a8013065267844	X	The issue, as described by the question poster, was an invalid S3 bucket endpoint over HTTPS. Not CORS or policy-related. stackoverflow.com/questions/29779346/…
563e323661a8013065267845	X	I have been kicking around trying to implement the S3 uploader into my application and getting closer but no cigar. Here is my setup in a nutshell: I have followed the blog post here: http://blog.fineuploader.com/2013/08/16/fine-uploader-s3-upload-directly-to-amazon-s3-from-your-browser/ multiple times (hoping I am not missing something). I know my IAM permissions are valid because other PHP tests allow various PutObject and list commands successfully. I have verified my CORS config is setup as follows for testing:  I have a few files I am using for this: s3.php = my test page with the fineuploader instance My PHP server side code is from your examples s3demo-cors.php (sorry code formatting got a little garbled when pasting in here)
563e323661a8013065267846	X	Based on your follow-up comment explaining the specific issue: So digging around I see the following error: XMLHttpRequest cannot load s3.amazonaws.com/dev-pre-content. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin '192.168.1.215'; is therefore not allowed access. The problem is with your bucket's CORS configuration. You'll need to be sure you have appropriate CORS rules associated with the specific bucket you are uploading to.
563e323661a8013065267847	X	It was exactly what you said, I believe that should implement the policy signature. When you send the file directly to the S3 server, you must first inform the política assignature, so your javascript will pass the credentials that will be on your php file, so that after you send the files you want. Here is a working example of política signature 
563e323661a8013065267848	X	I'd add price -- most storage providers charge for both space and traffic, which can lead to enormous bills.
563e323661a8013065267849	X	Simple question, doesn't seem to have been directly asked yet. What are the benefits to storing files (images/videos/documents/etc) on Amazon S3-and-related vs. on the File System your app is deployed on? It seems that the general consensus is to store files on the File System over the Database (here and here), but where does the Cloud fit in? Should all files just be stored on the File System? I really would like to use the Cloud, so there's more of an API and I could get the docs from multiple apps, but I'm unsure about the performance tradeoff.
563e323661a801306526784a	X	The pros and cons of storing files in "the cloud" IMHO: The pros: The cons: So, bearing this in mind, you decide what's best for your particular project.
563e323661a801306526784b	X	Konamiman's answer is a good one, I just wanted to add one thing to the pros: some cloud providers have Content Delivery Network integration, like Rackspace with Limelight, which can give end users very fast content delivery.
563e323661a801306526784c	X	So, you have a private key that you would like to use directly from the client side browser?
563e323761a801306526784d	X	On every request I am encrypting the URL before making the CORS request and sending on a header. The requirement is that I need to send an encrypted header on the request. At this moment I have no way to hide the private key that I am using to generate the encrypted header. If you know a better way of doing this let me know.
563e323761a801306526784e	X	Ask your server for the header. Keep the private key on the server. AFAIK, there is no secure way to store a private key in the browser.
563e323761a801306526784f	X	I don't have a server right now. I think that I will have a node.js server. Is it the right way? Even if I ask the key to the server, it will be available in memory, how can I fix that?
563e323761a8013065267850	X	No, I didn't say ask the server for the key. I said ask the server for the header (already encrypted by the key). Again, to my knowledge, this is the only way to protect the key.
563e323761a8013065267851	X	I want to hash on client side. But in order to hash I need a private key. I am using HMAC-SHA1. Any help?
563e323761a8013065267852	X	@Tony you could encrypt either with public or private key. Take a look on the Crypto-JS library
563e323961a8013065267853	X	I have a static AngularJS file. It's deployed on Amazon S3. I am accessing a API which has a hash authentication mechanism. I have to use a private key in order to create a hash of the URL and send on the header. It works great, but the problem is that I have to find a way to keep the private key out of the reach of hackers. I would like to know if anyone knows a way to keep the key secure. I have thought about running my site on a node.js server. The only requirement is that I should be able to deploy it on Amazon web services. Does it make sense?
563e323a61a8013065267854	X	There isn't a secure way to store secrets on client side. Private keys should always stay in the server. The common authentication mechanism for APIs is to hash (not encrypt) authentication on client side. But if you need encrypt something, you should use a asymmetric encryption algorithm with a public key on client. http://searchsecurity.techtarget.com/definition/asymmetric-cryptography
563e323a61a8013065267855	X	I haven't tried this feature myself, but I've found CloudFormation in general to lag behind the API on the order of months. If there was a change some time back as to how this works, it's likely it just hasn't migrated to CF... yet?
563e323a61a8013065267856	X	Kerim, thanks for the update. I noticed this was added just a few days back. Anyone working on same problem should be able to leverage this solution.
563e323a61a8013065267857	X	I am trying to set up my S3 to notify my SQS Queue for a "PUT" Object Creation Event. I am able to achieve this using CLI by: Also able to do the same using Java: However when I tried to something similar using CloudFormation template, I cannot find any way to trigger a notification to SQS. The only option I see that works and is documented is to trigger notification to SNS. I have referred the Cloud Formation Documentation: I tried doing something like this: But this as expected threw an error: "Encountered unsupported property QueueConfiguration" from amazon. Looked at this API documentation I would like to know if someone has been able to do this using CloudFormation Templates as thats how I am maintaining all the other AWS resources and do not want to do anything special for this particular feature. Any help is appreciated.
563e323a61a8013065267858	X	There is no need "Id" in Cloudformation Template ( You can check from QueueConfiguration Doc ) and your second mistake, that is not "QueueConfiguration", it's "QueueConfigurations". Because of that you get an error that says "Encountered unsupported property QueueConfiguration" It must be something like that. While you are reading cloudformation template documents, you must be careful about "Required:" sections. If it is not required, you don't need to fill it, just remove that line from your template if you don't use it( Like S3 Tags ). Other Docs about it: S3BucketDocs NotificationConfigurationDocs
563e323a61a8013065267859	X	the best solution is the simpliest :) Thank you
563e323b61a801306526785a	X	I'm researching this for a project and I'm wondering what other people are doing to prevent stale CSS and JavaScript files from being served with each new release. I don't want to append a timestamp or something similar which may prevent caching on every request. I'm working with the Spring 2.5 MVC framework and I'm already using the google api's to serve prototype and scriptaculous. I'm also considering using Amazon S3 and the new Cloudfront offering to minimize network latency.
563e323b61a801306526785b	X	I add a parameter to the request with the revision number, something like: The 'ver' parameter is updated automatically with each build (read from file, which the build updates). This makes sure the scripts are cached only for the current revision.
563e323b61a801306526785c	X	Like @eran-galperin, I use a parameter in the reference to the JS file, but I include a server-generated reference to the file's "last modified" date. @stein-g-strindhaug suggests this approach. It would look something like this: The server ignores the parameter for the static file and the client may cache the script until the date code changes. If (and only if) you modify the JS file on the server, the date code will change automatically. For instance, in PHP, my script to create this code looks like this: So then when your PHP file includes a reference to a CSS file, it might look like this: ... which will create ...
563e323b61a801306526785d	X	With regards to cached files, I have yet to run into any issues of bugs related to stale cached files by using the querystring method. However, with regards to performance, and echoing Todd B's mention of revving by filename, please check out Steve Souders' work for more on the topic: "Squid, a popular proxy, doesn’t cache resources with a querystring. This hurts performance when multiple users behind a proxy cache request the same file - rather than using the cached version everybody would have to send a request to the origin server." "Proxy administrators can change the configuration to support caching resources with a querystring, when the caching headers indicate that is appropriate. But the default configuration is what web developers should expect to encounter most frequently." http://www.stevesouders.com/blog/2008/08/23/revving-filenames-dont-use-querystring/
563e323b61a801306526785e	X	Use a conditional get request with an If-Modified-Since header
563e323b61a801306526785f	X	This is actually a very hard issue, and something that you can spend a while engineering the correct solution for. I would recommend publishing your files using a timestamp and/or version built into the url, so instead of: /media/js/my.js you end up with: /media/js/v12/my.js or something similar. You can automate the versioning/timestamping with any tool. This has the added benefit of NOT breaking the site as you roll out new versions, and lets you do real side-by-side testing (unlike a rewrite rule that just strips the version and sends back the newest file). One thing to watch out for with JS or CSS is when you include dependent urls inside of them (background images, etc) you need to make sure the JS/CSS timestamp/version changes if a resource inside does (as well as rewrite them, but that is possible with a very simple regex and a resource manifest). No matter what you do make sure not to toss a ?vblah on the end, as you are basically throwing caching out the window when you do that (which is unfortunate, as it is by far the easiest way to handle this)
563e323b61a8013065267860	X	If you get the "modified time" of the file as a timestamp it will be cached until the file is modified. Just use a helper function (or whatever it is called in other frameworks) to add script/css/image tags that get the timestamp from the file. On a unix like system (wich most survers are) you could simply touch the files to force the modified time to change if necessary. Ruby on Rails uses this strategy in production mode (by default I beleave), and uses a normal timestamp in development mode (to be really sure something isn't cached).
563e323b61a8013065267861	X	If you use MAVEN, you can use this, ADD on you pom.xml: With this you can acess ${timestamp} in your view. Like this sample:
563e323b61a8013065267862	X	I was wondering if there is a similar method in PHP to combine multiple zip files into one new zip-file without recompressing the contents - like this zip file library: http://www.example-code.com/vb/zip_appendFilesToExistingZip.asp Reason I ask is because of this article on fast file zipping for Amazon S3: http://www.w2lessons.com/2012/01/fast-zipping-in-amazon-s3.html See why: Upon inspecting the Chilkat API, I noticed the existence of a QuickAppend method which serves to append one zip to another. I began wondering how the compression time would be affected if we pre-zipped each file in S3, in its destination directory structure, and then simply appended them all together to form the final zip. To my dismay, the difference in compression time was astonishing. Small zip files in the 100kb-300kb range saw a 2x-3x speed improvement, while those larger than 10mb saw a 10x – 15x improvement. For example, a 14mb zip with 25 files varying in size from 100kb to 8mb took a mere 120ms to compress into the final zip, while building the zip from scratch took over 1.5 seconds. Anybody know a similar technique in PHP?
563e323c61a8013065267863	X	Ok, I didn't look to deep into the Chillkat extensions, they seem to have a PHP extension as well. See: http://www.example-code.com/phpExt/zip_appendFilesToExistingZip.asp However, my question now would be - is there an open-source PHP library that does the same thing?
563e323c61a8013065267864	X	I'm concerned about the undocumented restrictions on object (key) names. Amazon claims any unicode works, but clearly '../../word' does not. I'm wondering what else isn't supported...
563e323c61a8013065267865	X	Looks like the answer is "No, there is not a document". I would recommend asking your question on the AWS forums. On a side note, here is a similar question (and answer :) ) : stackoverflow.com/questions/3146380/…
563e323c61a8013065267866	X	@Downvoter: it would be good to have feedback as why you think the answer does not address the question. Or even better, an edit to the answer.
563e323c61a8013065267867	X	docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html
563e323c61a8013065267868	X	From the AWS docs, I understand that: However, these rules seem too permissive. For instance, if I make a key called '../../d', a 400 ERROR occurs when I attempt to access it with the GET OBJECT API. Interestingly, I have no problem accessing '../d'. Is there a document specifying what is and is not legal?
563e323c61a8013065267869	X	The only restrictions provided by Amazon is (as found on their Technical FAQ): What characters are allowed in a bucket or object name? A key is a sequence of Unicode characters whose UTF-8 encoding is at most 1024 bytes long. Additional restrictions apply for Buckets (as found on the Rules for Bucket Naming section of their Bucket Restrictions and Limitations FAQ): In all regions except for the US Standard region a bucket name must comply with the following rules. These result in a DNS compliant bucket name. Less permissive restrictions apply to the US standard region. Please see the FAQs for additional information and some examples. Hope it helps!
563e323c61a801306526786a	X	According to AWS S3 documentation: Although you can use any UTF-8 characters in an object key name, the following key naming best practices help ensure maximum compatibility with other applications. Each application may parse special characters differently. The following guidelines help you maximize compliance with DNS, web safe characters, XML parsers, and other APIs. Please find below the The following character sets are generally safe for use in key names: NOTE ABOUT THE DELIMITER ("/") The following are examples of valid object key names: 4my-organization my.great_photos-2014/jan/myvacation.jpg videos/2014/birthday/video1.wmv Note that the Amazon S3 data model is a flat structure: you create a bucket, and the bucket stores objects. There is no hierarchy of subbuckets or subfolders; however, you can infer logical hierarchy using keyname prefixes and delimiters as the Amazon S3 console does. e.g if you use Private/taxdocument.pdf as a key, it will create the Private folder, with taxdocument.pdf in it. Amazon S3 supports buckets and objects, there is no hierarchy in Amazon S3. However, the prefixes and delimiters in an object key name, enables the Amazon S3 console and the AWS SDKs to infer hierarchy and introduce concept of folders. The following characters in a key name may require additional code handling and will likely need to be URL encoded or referenced as HEX. Some of these are non-printable characters and your browser may not handle them, which will also require special handling: You should avoid the following characters in a key name because of significant special handling for consistency across all applications.
563e323c61a801306526786b	X	when I follow those examples I get errors like: undefined method `write' for #<Aws::S3::Object bucket_name="my_bucket", key="image.png"> (NoMethodError)
563e323c61a801306526786c	X	I ended up using this answer (stackoverflow.com/questions/130948/ruby-convert-file-to-string) then used object = bucket.object('image.png') object.put(body: contents)
563e323c61a801306526786d	X	@EldadMor You linked to the v1 documentation. The v2 documentation is found here: docs.aws.amazon.com/sdkforruby/api/index.html
563e323c61a801306526786e	X	You're my freaking hero! Spent the last 3-4 hours trying to make this work to no avail. Doing this in v1 of the aws-sdk was so simple but v2 seems unnecessarily complicated.
563e323d61a801306526786f	X	@mmichael I'm curious in what way v2 uploads are more complicated than v1? The syntax is very similar, no?
563e323d61a8013065267870	X	Well, with v1 you didn't have to deal with the Client or Resource classes, you could just run s3 = AWS::S3.new and then s3.buckets['bucket_name'].objects['key'].write(file: '/path/to/file'). I actually misspoke, the v2 version isn't more complicated. I meant to say that the v2 docs are just more confusing. Nowhere in the new docs does it show you how to do the steps in your answer. And if they do, then it's not as clear as the instructions in v1 where it shows you everything you need on one page.
563e323d61a8013065267871	X	I agree, they have raised the knowledge level above beginner in the v2 documentation
563e323d61a8013065267872	X	yeah, the docs are a nightmare. would be great to have a better overview of common use cases for things like Client and Resource
563e323d61a8013065267873	X	I'm having a hell of a time working with the aws-sdk documentation, all of the links I follow seem outdated and unusable. I'm looking for a straight forward implementation example of uploading an image file to an S3 bucket in Ruby. Any advice is much appreciated.
563e323d61a8013065267874	X	Here is how you can upload a file from disk to the named bucket and key: That is the simplest method. You should replace 'key' with the key you want it stored with in Amazon S3. This will automatically upload large files for you using the multipart upload APIs and will retry failed parts. If you prefer to upload always using PUT object, you can call #put or use an Aws::S3::Client: Also, the API reference documentation for the v2 SDK is here: http://docs.aws.amazon.com/sdkforruby/api/index.html
563e323d61a8013065267875	X	this is what AWS states:"To verify an email address, make an API call with the email address as a parameter. This API call will trigger a verification email, which will contain a link that you can click on to complete the verification process. " ...need to know how to make that API call. thanks
563e323d61a8013065267876	X	I am signed up for AWS SES (with instance, S3 and my website running nicely). I also have rec'd approval for sending out email without receiver verification and "mass production" OK. The only thing I'm left with is having my 3 "from" email addresses verified. Started to download Perl, as was suggested to run email-verification scripts -- but got no where with the installation. Do have my credentials ready to use. There is an AWS SES API to use for verification which I can't find... suspect that it has something to do with AWS's sdk which I could figure out how to install. So my question: is there a simple, straight forward way to get my email addresses to Amazon for verification via a response email they send? Their documentation is somewhat confusing.
563e323d61a8013065267877	X	You have to go validate your email address through their web service (like the perl script is doing). You can also use their SDK's that they publish, which are wrappers around their web service. For example, if you have Visual Studio, you can use the AWS SDK for .NET (also available as a Nuget package: PM> Install-Package AWSSDKForNET) and set up a simple console application to do something like this: They also have SDK's available in PHP and Java that work pretty much the same.
563e323e61a8013065267878	X	Assuming you've already looked through the AWS SDK for PHP, what have you got so far? As for resizing and cropping, I believe your application's server should be taking care of that and then send to your S3 bucket.
563e323e61a8013065267879	X	1st and 3rd answer are fine. 2nd > resize/crop is for remote image (hosted in asw ). eg ... asw.com/abc.jpg?h=200&w=200&q=100
563e323e61a801306526787a	X	You can use s3 library to get object S3::getObject($bucketName, $uploadName) then use the image library
563e323e61a801306526787b	X	For my website i want to store all image using AWS S3 service using API. How to do that thing via API/SDK 1) How to upload image/file in different folder using API (from my website). 2) How to resize/crop image on the fly. eg 50x50 px, 250x250 px. 3) Force download. Thanks
563e323e61a801306526787c	X	Your question is a bit vague, so the answer cannot be precise. I'll give some advice or library to start: This library should be a nice place to start working with S3 and files: https://github.com/tpyo/amazon-s3-php-class This should do the trick: https://github.com/eventviva/php-image-resize Or you can try PHP Imagick: http://php.net/manual/en/book.imagick.php Stackoverflow already has the answer for you: How to force file download with PHP Hope this helps get you started.
563e323e61a801306526787d	X	I don't think AWS has a built-in feature to resize any stored image in S3. As eldblz mentioned, you have to do the resizing on your own. You can use S3 stream wrapper. The S3 stream wrapper is pretty amazing: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-s3-stream-wrapper.html It will allow you to use built-in PHP functions like file_get_contents and file_put_contents. Get details of the original file: Get the image data with file_get_contents(or fopen): create an image resource from the data and resize: Output: Hope this helps!
563e323e61a801306526787e	X	We have an archive stored in Amazon s3. For one of our application I need to get the list of folders under a given URL. Instead of parsing the output from the web query to the URL, Is there a way to get the list of folder names in JSON or XML format using AWS REST APi's? If its not possible with REST API's is there any alternative better than parsing the web request return?
563e323e61a801306526787f	X	Can anyone with experience using cloud encoding services like Encoding.com with asp.net MVC point me in the right direction workflow wise? I will be storing my videos in Amazon S3 so I would like to avoid forcing the user to upload to my web server only to immediately be pushed to S3. Encoding.com has made this uploader script available here I am wondering if this is the proper start to the workflow and where would I go from here? I would imagine I could take this result and call their API to begin the processing. Just looking for general info/help on the workflow as a whole and what to show the user while all of this is taking place. Any insight would be greatly appreciated.
563e323e61a8013065267880	X	@GeogeGreen i need to upload large videos to s3 bucket, most likely it would be 5GB , can i do it using NSURLSession coz what i have read is that background sessions wont be execute long time
563e323f61a8013065267881	X	Thanks. This works very well
563e323f61a8013065267882	X	How would you do this using pre-singed authentication? The client only has accessKey and signature, this assumes you have key and secret.
563e323f61a8013065267883	X	I am actually using FederationToken to generate temporary credentials before (docs.aws.amazon.com/STS/latest/APIReference/…;. I have never used pre signed url, but as far as I understand the documentation it is a simple url request to the generated url. No need to use AWS SDK methods for that, just create an NSUrlSessionUploadTask without any AWS integration at all.
563e323f61a8013065267884	X	Good that it works..Strangely ive been unable to add prefixes to the file uploaded- i.e s3.amazonaws.com/NSURLessionUploadTest/image.jpg works fine but s3.amazonaws.com/NSURLessionUploadTest/Prefix/image.jpg fails.. anyone faced this?
563e323f61a8013065267885	X	Since this is any other string for S3, results should be the same with or without prefix. Maybe it is the bucket policy that is preventing you from uploading. What exactly is the error? A second idea: Please print out the url of request2 just before starting the upload task, maybe the slash is being changed somehow.
563e323f61a8013065267886	X	i want to upload videos to S3 bucket using V2 api and it should support pause and resume capability , Can use objc version of this code snippet to upload
563e323f61a8013065267887	X	Awesome, that really helped. Apple were adding an extra header field after I had signed the request!
563e323f61a8013065267888	X	@GeorgeGreen can you provide some more information? How did you overcome this eventually?
563e323f61a8013065267889	X	@GeorgeGreen I am very interested too.
563e323f61a801306526788a	X	@GeorgeGreen can you please expand on this?
563e323f61a801306526788b	X	From the docs for "Uploading Body Content using a file": "The session object computes the Content-Length header based on the size of the data object. If your app does not provide a value for the Content-Type header, the session also provides one." This can mess up your s3 signature.
563e323f61a801306526788c	X	Link only answers are discouraged. Please include the salient aspects of your solution in your answer, or delete this answer and just leave a comment.
563e323f61a801306526788d	X	@Genady is thr anyway that i can resume uploading if the internet connection drops ??
563e324061a801306526788e	X	Interesting question, I didn't try it yet.
563e324061a801306526788f	X	@Genady Okrain currently if the connection drops the upload process will stop , do u think is thr any chance to resume upload from where it stopped ??
563e324061a8013065267890	X	I have an app which is currently uploading images to amazon S3. I have been trying to switch it from using NSURLConnection to NSURLSession so that the uploads can continue while the app is in the background! I seem to be hitting a bit of an issue. The NSURLRequest is created and passed to the NSURLSession but amazon sends back a 403 - forbidden response, if I pass the same request to a NSURLConnection it uploads the file perfectly. Here is the code that creates the response: And then this signs the response (I think this came from another SO answer): Then if I use this line of code: It works and uploads the file, but if I use: I get the forbidden error..!? Has anyone tried uploading to S3 with this and hit similar issues? I wonder if it is to do with the way the session pauses and resumes uploads, or it is doing something funny to the request..? One possible solution would be to upload the file to an interim server that I control and have that forward it to S3 when it is complete... but this is clearly not an ideal solution! Any help is much appreciated!! Thanks!
563e324061a8013065267891	X	I made it work based on Zeev Vax answer. I want to provide some insight on problems I ran into and offer minor improvements. Build a normal PutRequest, for instance Now we need to do some work the S3Client usually does for us Now copy all of that to a new request. Amazon use their own NSUrlRequest class which would cause an exception Now we can start the actual transfer This is the code that creates the background session: It took me a while to figure out that the session / task delegate needs to handle an auth challenge (we are in fact authentication to s3). So just implement
563e324061a8013065267892	X	The answers here are slightly outdated, spent a great deal of my day trying to get this work in Swift and the new AWS SDK. So here's how to do it in Swift by using the new AWSS3PreSignedURLBuilder (available in version 2.0.7+):
563e324061a8013065267893	X	I don't know NSURLSessionUploadTask very well yet but I can tell you how I would debug this. I would use a tool like Charles to be able to see HTTP(S) requests that my application makes. The problem is likely that the NSURLSessionUploadTask ignores a header that you set or it uses a different HTTP method than Amazon's S3 expects for the file upload. This can be easily verified with an intercepting proxy. Also, when Amazon S3 returns an error like 403, it actually sends back an XML document that has some more information about the error. Maybe there is a delegate method for NSURLSession that can retrieve the response body? If not then Charles will certainly give you more insight.
563e324061a8013065267894	X	Here is my code to run the task: I open sourced my S3 background uploaded https://github.com/genadyo/S3Uploader/
563e324061a8013065267895	X	For background uploading/downloading you need to use NSURLSession with background configuration. Since AWS SDK 2.0.7 you can use pre signed requests: PreSigned URL Builder** - The SDK now includes support for pre-signed Amazon Simple Storage Service (S3) URLs. You can use these URLS to perform background transfers using the NSURLSession class. Init background NSURLSession and AWS Services Implement upload file function NSURLSession Delegate Function:
563e324061a8013065267896	X	I just spent sometime on that, and finally succeeded. The best way is to use AWS library to create the request with the signed headers and than copy the request. It is critical to copy the request since NSURLSessionTask would fail other wise. In the code example below I used AFNetworking and sub-classed AFHTTPSessionManager, but this code also works with NSURLSession. Another good resource is the apple sample code hereand look for "Simple Background Transfer"
563e324061a8013065267897	X	Recently Amazon has updated there AWS api to 2.2.4. speciality of this update is that , it supports background uploading , you don't have to use NSURLSession to upload videos its pretty simple , you can use following source block to test it, i have tested against with my older version , it is 30 - 40 % faster than the previous version in AppDelegate.m didFinishLaunchingWithOptions method // ~GM~ setup cognito for AWS V2 configuraitons in handleEventsForBackgroundURLSession method in upload class More references - : http://mobile.awsblog.com/post/Tx283AGGIL76PKP/Amazon-S3-Transfer-Utility-for-iOS
563e324061a8013065267898	X	thanks for answer
563e324061a8013065267899	X	can I use CDN with images ? and if can then how to use it with upload from website to CDN server
563e324161a801306526789a	X	Yes, and you can check with your CDN provider on the methods they allow for uploading, such as pull (CDN server download the files from your website/server) or push (sent from your website/server to the CDN server) Example : automatic push to CDN deployment strategy
563e324161a801306526789b	X	Seems like there are a few options to accomplish this. The first one would be using the CDN as Origin. In which case, there is already an answer with some advice. The second option would be using your current website as Origin for the images. In which case you will need to do some DNS work that would look something like this: Published URL -> CDN -> Public Origin Step 1 - images.yoursite.com IN CNAME images.yoursite.com.edgesuite.net --- This entry will send all traffic requests for the images subdomain to Akamai's CDN edge network. Step 2 - origin-images.yoursite.com IN A or IN CNAME Public front end for the images So the way it works is that in step one you get a request for one of your images, which will be then sent via DNS to the edge network in the CDN (in this case Akamai HTTP only). If the CDN does not already have the image in cache or if its cache TTL is expired, it will then forward the request to the public origin you have setup to pull the file, apply any custom behavior rules (rewrites, cache controls override, etc), cache the content if marked as cacheable and then serve the file to the client. There is a lot of customization that can be done when serving static content via CDN. The example above is very superficial and it is that way to easily illustrate the logic at a very high level.
563e324161a801306526789c	X	In common CDN setups you actually don't upload images to the CDN. Instead, you access your images via a CDN, quite like accessing resources via an online Proxy. The CDN, in turn, will cache your images according to your HTTP cache headers and make sure that subsequent calls for the same image will be returned from the closest CDN edge. Some recommended CDNs - AWS CloudFront, Edgecast, MaxCDN, Akamai. Specifically for images, you might want to take a look at Cloudinary, http://cloudinary.com (the company I work at). We do all of this for you - you upload images to Cloudinary, request Cloudinary for on-the-fly image transformations, and get the results delivered via Akamai's high-end CDN.
563e324161a801306526789d	X	Do you mean you want to use a CDN to host images? And you want to upload images from your website to the CDN or use the website run by the company hosting the CDN to upload the images? Ok, firstly yes you can use a CDN with images. In fact it's advised to do so. Amazon CloudFront and RackspaceCloud's Cloudfiles are the two that immediately spring to mind. Cloudfiles you can upload either by their API or through their website and CloudFront you upload to Amazon's S3 storage which then hooks into the CloudFront CDN.
563e324161a801306526789e	X	Iam using AWS to upload images, css and some zip files for my site and they are fine to upload them.But now I want like I first upload a zip on localhost and I will extract them into one folder and I want to upload that entire folder into aws.Can anyone help me to do it.Thanks in advance. Iam using function to upload files like It is working fine for single images.But I dont know how to do it for entore folder.
563e324161a801306526789f	X	There is no API call for Amazon S3 that can upload an entire folder. You can loop through your list of local files and then upload each individually to S3. If you're capable, doing it in parallel can greatly speed the upload, too. You could also cheat by calling out to the AWS Command Line Interface (CLI). The CLI can upload/download a recursive list of files and can also do multi-part upload for large files. There is also an aws s3 sync command that can intelligently upload only new/modified files.
563e324161a80130652678a0	X	what about the same domain but on a subdomain.. like assets.mysite.com...?
563e324161a80130652678a1	X	Domains/Subdomains have no bearing on the physical server or its filesystem.
563e324161a80130652678a2	X	I'm looking for some quick info about best practices on storing user's uploaded files on different servers or sub domains... For example, photos on facebook of course arent on facebook.com/files/users/453485 etc... but rather on photos.ak.fbcdn.net or whatever... I'm wondering how with php i can upload to a different server whilst maintaining a mysql connection to my original... is it possible?
563e324161a80130652678a3	X	Facebook uses a content delivery network (cdn, hence fbcdn or facebook content delivery network) and probably uses webservices to pass binary data (photos) from server to server. Rackspace Cloud offers a similar service. Here is an example application of their PHP library to access their webservice api: http://cloudfiles.rackspacecloud.com/index.php/Sample_PHP_Application
563e324161a80130652678a4	X	I'm going to make the assumption that you have multiple webservers, and want to be able to access the same set of files on each one. In that case, some sort of shared storage that each machine can access might be a good place to start. Here are a couple options I've used: If you don't have control over the hardware or aren't able to install extra software, I'd suggest Amazon S3. There is an api that you can use to shuttle files back and forth. The only downside is that you don't get to use storage that you may already use, and it will cost you some money. If you do have access to the hardware and software, MogileFS is somewhat like S3, in that you have an api to access the files. But is different in that you get to use your existing storage and get to do so for no additional cost. NFS is a typical place where people will start, because it's the simplest way to get started. The downside is that you'll have to be able to configure servers, and setup a NFS volume for them to mount. But if I were starting a high-volume photo hosting service, I'd use S3, and I'd put a CDN like Akamai in front of it.
563e324261a80130652678a5	X	I've seen the recently Google Drive pricing changes and they are amazing. This changes everything ! We have a SaaS website in which we keep customer's files. Does anyone know if Google Drive can be used to keep this kind of files/service or it's just for personal use? Does it have a robust API for uploading, downloading, and create public URL's to access files as S3 have ? Edit: I saw the SDK here (https://developers.google.com/drive/v2/reference/). The main concern is if this service can be used for keeping customer's files, I mean, a SaaS website offering a service and keeping files there.
563e324261a80130652678a6	X	This doesn't really change anything. “Google Drive storage is for users and Google Cloud Storage is for developers.” — https://support.google.com/a/answer/2490100?hl=en The analogous service with comparable functionality to S3 is Google Cloud Storage, which is remarkably similar to S3 in pricing. https://developers.google.com/storage/pricing
563e324461a80130652678a7	X	Does anyone know if Google Drive can be used to keep this kind of files/service or it's just for personal use? Yes you can. That's exactly why the Drive SDK exists. You can either store files under the user's own account, or under an "app" account called a Service Account. Does it have a robust API for uploading, downloading, and create public URL's to access files as S3 have ? "Robust" is a bit subjective, but there is certainly an API. There are a number of techniques you can use to access the stored files. Look at https://developers.google.com/drive/v2/reference/files to see the various URLs which are provided. Por true public access, you will probably need to have the files under a public directory. See https://support.google.com/drive/answer/2881970?hl=en NB. If you are in the TB space, be very aware that Drive has a bunch of quotas, some of which are unpublished. Make sure you test any proof of concept at full scale.
563e324461a80130652678a8	X	Sorry to spoil your party, before you get too excited, look at this issue. It is in Google's own product, and has been active since November 2013 (i.e.4 months). Now imagine re-syncing a few hundred GB of files once a while. Or better, ask your customers to do it with their files after you recommended Drive to them.
563e324461a80130652678a9	X	A Service. Definitely. If you manage your AsyncTask inside an Activity, it may get killed when the activity gets in background
563e324461a80130652678aa	X	stackoverflow.com/questions/6957775/…
563e324461a80130652678ab	X	you said it. "the OS kills the app to free memory.". That's a serious disadvantage.
563e324561a80130652678ac	X	@fiddler Thanks for your suggestion. So i think i shall use IntentService instead of AsyncTask.
563e324561a80130652678ad	X	Thank you for your answer. I shall better use IntentService.
563e324561a80130652678ae	X	Good Android citizen.. Key words.. 👍
563e324561a80130652678af	X	I have a requirement where a user is able to upload a video to Amazon S3. I have achieved this using the java high-level api in amazon sdk. During the upload process if the user clicks the home button the upload must continue in the background. What would be a better approach :? *1 Using AsyncTask: I have tried using AsyncTask and it works fine. But if the uploading process continues for long interval at the background, the OS kills the app to free memory. Is there any way that i can handle this situation, and let my app complete the upload process. *2 Using Service: Someone suggested me to use a Service + ui notification. I feel like using AsyncTask, because it works fine for me. Is there any advantage of using a Service over AsyncTask.
563e324561a80130652678b0	X	Most of the time an AsyncTask is coupled to your UI, the Activity that started it etc. Those will stay in memory until the task is finished. This upload scenario of yours begs for implementation through an IntentService. This will decouple the uploading from a specific activity and make your App a good Android citizen in regard to the Android life cycle. You can now create a Notification that is periodically updated from the Service that shows the status of the upload and lets the user cancel the upload from his status bar.
563e324561a80130652678b1	X	An upload library I maintain, Fine Uploader handles uploads directly to S3 in all browsers, including IE7. It also supports chunking, auto-resume, retry, and a bunch of other features. A live demo of the upload-to-s3 function can be found on the page I just linked to.
563e324561a80130652678b2	X	This is not a browser-based solution. It's a Microsoft Windows software.
563e324561a80130652678b3	X	I'm looking for a front-end solution for uploading files to amazon s3 (that is, not passing them through my server. The solution I have found is https://code.google.com/p/swfupload/ It might do the job, but it requier flash and this is the first sentence of the project description is: SWFUpload has not been under active development for several years. Here are my desired features, though none of them are nessesary
563e324661a80130652678b4	X	You could start by using this tutorial as a baseline , if you are asking about uploading from your web app - http://aws.amazon.com/articles/1434
563e324661a80130652678b5	X	kgu87 is correct, this article pretty much explains the entire process to upload files directly to S3 without passing them trough your own server. You can also check out the AWS docs related to this on: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html If you're looking for an upload tool that supports HTML5 uploads directly to S3, check out Plupload They have a great article that explains how to set it up: https://github.com/moxiecode/plupload/wiki/Upload-to-Amazon-S3 The documentation describes a PHP service that's used to generate a policy and signature (both are required for S3 to accept your download) but you can use any language to generate those. Also, in certain use cases, you can just generate a one-time policy with a very high expiration time and hard code it into your upload form.
563e324661a80130652678b6	X	You could use this tool: http://aws.amazon.com/customerapps/Amazon-S3/Consumers/2069 free, and works in most browsers; doesn't require a server. S3 Browser is a free client interface for users of the Amazon S3 service. It provides an easy to use GUI to manage your storage buckets and allows you to: - Browse, create, delete Amazon S3 buckets - Upload and download files to and from Amazon S3 - Create public URLs to share the files. - Keep your files backed up on a multiple data centers. - Set Access Control on buckets and files. S3 Browser is free for non-commercial use.
563e324661a80130652678b7	X	I would say that REST and WSDL/SOAP-like services have different usage / areas where they differ in strength.
563e324661a80130652678b8	X	Very much agreed.
563e324661a80130652678b9	X	I'm having trouble understanding why a WSDL would be so beneficial, when the truth is that the service definition is not that human-readable, and most developers will use automated tools to consume it to generate objects in the application. Why isn't it easier for a service provider to define a simple XML schema, and just tell me in the documentation what I can get and how I can get it? I understand the whole "universal definition" aspect, but it all just seems so overcomplicated to me. Most APIs for social sites use a basic RESTful architecture, and all they do is give you a schema representation of the data you will get back. Seriously, I must be missing something here.
563e324661a80130652678ba	X	At one time, WSDL was popular and I'm sure for internal tools for many companies (and large SOA systems), WSDL is still in use. But you're correct, the adoption of REST has taken WSDL off the map a bit once it "hit the scene". Take for example Amazon S3. They offer a WSDL along with the REST API. I had read somewhere that 98% of S3 users are using the REST API and Amazon is considering dropping the WSDL support. REST is clean. WSDL often depends on other tools to parse it out, or to automatically build functions for your application to benefit from the services offered by the WSDL. REST also has the benefit of being much more natural by taking advantage of HTTP and not really relying on anything more. One you get SOAP into the mix and the many other acronyms that go along with WSDL, you end up with a lot on your hands....
563e324761a80130652678bb	X	So what "simple XML schema" would you propose that would let the tools give the same level of code generation support that they do now? I think the WSDL designers would argue that they're already giving the simplest schema they could which exposed everything they needed to. I'm not saying I'd necessarily agree, but being able to autogenerate clients is very powerful.
563e324761a80130652678bc	X	To me WSDL seems like another example of over-engineered "onion architecture", like original Java EJBs. Lots of layers and tears.
563e324761a80130652678bd	X	This link should help you out. http://www.prescod.net/rest/rest_vs_soap_overview/ This is a great resource to help those who do not understand the SOAP vs REST contention. They are different tools. Use them so that you solve your problem in the most efficient way.
563e324761a80130652678be	X	For one thing, WSDL is what the automated tools use to generate objects. And it already is a pretty simple XML format, but I'm beginning to believe that a tool that makes it easy to write XML won't ever exist. I don't think anybody is saying it's a better protocol than REST, but it came out first, has great tool support (Visual Studio completely it abstracts away when creating and consuming services), and it's a standard so it will probably stay popular for a while.
563e324761a80130652678bf	X	WSDL is XML representation file and a communication standard for any external system to communicate with your webservice regardless its implementation technologies or platforms. FYI, RESTful services can use a definition language file called WADL to describe the service as well. So, it's not about webservices only.
563e324761a80130652678c0	X	This should be the accepted answer.
563e324761a80130652678c1	X	Like to know if anyone been able to stream HLS video via AWS Cloudfront with Signed URL. My experience so far is, it is not possible. AWS documentation is not clear. AWS Forum is silent. I seem to be able to get the first file ( .m3u8 ) then it stops. Using JW player, which complains cannot get media file. If answer is yes, please point me in the right direction. Thanks.
563e324761a80130652678c2	X	The scenario with unsigned URLs will most definitely work. I've successfully segmented video streams with ffmpeg onto S3 and served from CF. It's all HTTP after all. If you wish to restrict access to your HLS content, your playlist file would need to include signed URLs as well. You would typically compute these as the playlist is requested, based on whatever credentials you wish to authenticate the user with. Thus, you need a server-side implementation that generates session-unique m3u8's for you in order for the signed-URL scheme to make sense. Depending on your needs, another option would be to look into DRM. JW Player supports single/rotating key fragment decryption, which arguably tends to be a more complicated solution. You would then be left with the matter of securely distributing decryption keys to your clients. I hope this somewhat addresses your concerns. If not, feel free to leave a comment.
563e324761a80130652678c3	X	According to this article, cloudfront does support HLS. I am currently attempting to implement this on my site using flowplayer with html5 video. I will update this answer once it is running. Amazon Web Services site also had this to say about HLS Support although it does not seem as neat and tidy as the rtmp approach.
563e324861a80130652678c4	X	According to CloudFront's description of its streaming: Streaming of pre-recorded media: You can deliver your on-demand media using Adobe’s Real Time Messaging Protocol (RTMP) streaming via Amazon CloudFront. You store the original copy of your media files in Amazon S3 and use Amazon CloudFront for low-latency delivery of your media content. Amazon CloudFront integrates with Amazon S3 so you can configure media streaming by making a simple API call or with a few clicks in the AWS Management Console. You also benefit for the high throughput delivery of your media when using Amazon CloudFront, so you can deliver content in full HD quality to your viewers. The short answer is pretty much no. Streaming from CloudFront is RTMP. Link: http://aws.amazon.com/cloudfront/ That said, AWS's Elastic Transcoder can make HLS filesets and playlists, and those can be served from CloudFront. So then the answer becomes "yes if you can do the work/figure it out." Here's a link to their FAQ telling you how to do it: http://aws.amazon.com/elastictranscoder/faqs/#Can_I_get_segmented_output_for_HTTP_Live_Streaming_(HLS)
563e324861a80130652678c5	X	Most of these would be considered Integration Tests, right? Also, I'm not sure what you mean by mocking https.
563e324861a80130652678c6	X	By mocking https I meant mocking the configuration manager to return both HTTPS and HTTP urls.
563e324861a80130652678c7	X	And yes, you are right a lot of these are integration tests, but that is because of how your code is coupled. Mike Z's suggestions offer ways to help curb that.
563e324861a80130652678c8	X	Both of your answers have been incredibly helpful to get me to start decoupling my code. Thank you!
563e324861a80130652678c9	X	You give a lot of good suggestions. I like the ConfigurationManager separation and the IKeySource. I'm using Ninject instead of a factory method. I see what you mean - adding a new layer for TestClient is just going to add up exponentially. I assume I could use integration tests here, instead.
563e324861a80130652678ca	X	I have a fairly simple class that I'm trying to unit test. I'm very new to unit testing in general, and I'm not sure what I should be testing here. The only test case that I can figure out how to code is a null argument of stream. Besides that, I'm not sure how to test the results of a PutObjectRequest or what else. If I should be using mocks here, how?
563e324861a80130652678cb	X	Things I would look at: Mock your configuration manager to return invalid data for the bucket and the URL. (null, invalid urls, invalid buckets) Does S3 support https ? If so mock it, if not, mock it and verify you get a valid error. Pass different kinds of streams in (Memory, File, other types). Pass in streams in different states (Empty streams, streams that have been read to the end, ...) I would allow the timeouts to be set as parameters, so you can test with really low timeouts and see what errors you get back. I would also test with duplicate keys, just to verify the error message. Even though you are using guids, you are storing to an amazon server where someone else could use the S3 API to store documents and could theoretically create a file that appears to be a guid, but could create a conflict down the road (unlikely, but possible)
563e324861a80130652678cc	X	You are having trouble unit testing UploadImage because it is coupled to many other external services and state. Static calls including (new) tightly couple the code to specific implementations. Your goal should be to refactor those so that you can more easily unit test. Also, keep in mind that after unit testing this class, you will still need to do the big tests involving actually using the Amazon S3 service and making sure the upload happened correctly without error or fails as expected. By unit testing thoroughly, hopefully you reduce the number of these big and possibly expensive tests. Removing the coupling to the AmazonS3Client implementation is probably going to give you the biggest bang for your testing buck. We need to refactor by pulling out the new AmazonS3Client call. If there is not already an interface for this class, then I would create one to wrap it. Then you need to decide how to inject the implementation. There are a number of options, including as a method parameter, constructor parameter, property, or a factory. Let's use the factory approach because it is more interesting than the others, which are straight-forward. I've left out some of the details for clarity and read-ability. A unit test might look like this in MSTest: Now, we have one test verifying that the correct input stream is sent over in the request object. Obviously, a mocking framework would help cut down on a lot of boilerplate code for testing this behavior. You could expand this by starting to write tests for the other properties on the request object. Error cases are where unit testing can really shine because often they can be difficult or impossible to induce in production implementation classes. To fully unit test other scenarios of this method/class, there are other external dependencies here that would need to be passed in or mocked. The ConfigurationManager directly accesses the config file. Those settings should be passed in. Guid.NewGuid is basically a source of uncontrolled randomness which is also bad for unit testing. You could define an IKeySource to be a provider of key values to various services and mock it or just have the key passed from the outside. Finally, you should be weighing all the time taken for testing/refactoring against how much value it is giving you. More layers can always be added to isolate more and more components, but there are diminishing returns for each added layer.
563e324861a80130652678cd	X	There is a legacy web site whose tags point to a Microsoft .NET Web API endpoint which currently dumps the bytes of an image to HTTP response output. I want to re-write this Web API and instead of dumping the image on the screen I would like to put the generated image somewhere on Amazon S3 and just return a Cloud Front url which points to that image. In more crude terms I want the new API to return the path of the image rather than its binary data. My question is that if my API returns an HTTP Redirect , will the image be able to retrieve the image from the new location without redirecting the whole page? What is the best way of replacing the legacy API in such a scenario?
563e324961a80130652678ce	X	I think a better approach could be that the ViewModel hold the final URLs. Then, in your view, you just set the proper URL to the img tag. So, in your controller, you call the API to do the logic of putting the image in S3 and get the URL. Then you put that URL in the viewmodel with the other data required by your view. This way, you don't need to worry about redirection... and your view is clean of logic.
563e324961a80130652678cf	X	A redirect result (301 or 302) would work just fine, the image would still load correctly. I would caution you to use the 301/Permanent redirect in case you have search engine rankings in play. You could always try this yourself. In a test project, setup a controller action that returns a redirect result to a physical image. Then test! In your view: This should return whatever image you pointed to in your Redirect()
563e324961a80130652678d0	X	If they are stored in the app (especially in clear text), then they can be retrieved. Can your app retrieve the key on first run (using https) and then store it using a KeychainItemWrapper. If so, you should also make sure you also set the appropriate level of app security for accessing the keychain.
563e324961a80130652678d1	X	That is starting to sound like a good idea. I could even work out something to force it to expire after a week, in case I need to change to a different set of credentials.
563e324961a80130652678d2	X	For this use case, the Token Vending Machine sounds like the best option. Thanks!
563e324961a80130652678d3	X	This doesn't answer the question. How does this hide the key? This still requires the key to be in the code.
563e324961a80130652678d4	X	@rmaddy Is there any way to securely include API keys in an app?. Probably I misunderstood
563e324961a80130652678d5	X	Read the code in the question. He has a key in his code. He is asking how to hide that key from the code so a hacker can't find it. Using the keychain doesn't hide the key from the app's code.
563e324961a80130652678d6	X	This answer is not relevant to the question that was asked.
563e324961a80130652678d7	X	You would be either relying on obfuscation which is reversible or encryption. If you encrypted the keys, you would still have to store the key for decryption which is then a recursive problem. How would you store the encryption key?
563e324961a80130652678d8	X	@StephenJohnson Obfuscating the key is still a big improvement over using plain text.
563e324a61a80130652678d9	X	@rmaddy Agreed!
563e324a61a80130652678da	X	@rmaddy - nice approach to obfuscation: iosdevelopertips.com/cocoa/…
563e324a61a80130652678db	X	Amazon has an AWS SDK for iOS, along with several sample apps. In their samples, they put the API credentials in a Constants.h file: My concern is that these can be extracted by a determined hacker. Is there any way to securely include API keys in an app? The one option I've seen is to include a server of my own as a go-between: the app talks to my server, my server talks to S3. I can see the value in doing this, but one is still presented with the problem: do I allow the app to make API calls on my server without any kind of authentication? Including my own API key in the app has the same problem as including AWS API keys.
563e324a61a80130652678dc	X	There are a couple of credential management options to help you avoid embedding credentials in your app. The first is Web Identity Federation, which allows users to log in to your app with Facebook, Google, or Login With Amazon. Another option is to use a Token Vending Machine, which is a server component that distributes temporary credentials to your app. There is a high-level overview with pointers to the relevant documentation and code samples on the AWS Mobile Development Blog: http://mobile.awsblog.com/post/Tx3UKF4SV4V0LV3/Announcing-Web-Identity-Federation
563e324a61a80130652678dd	X	You'll probably want to create temporary write credentials using AWS STS tokens instead of passing keys all the way to the client. You can also create OAIs for CloudFront endpoints so no users directly access S3. http://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html
563e324a61a80130652678de	X	Did you try a KeychainItemWrapper ? to Set to Get Keychain Services Programming Guide Before import Security.framwork I didn't check this code, if something doesn't work, let me know
563e324a61a80130652678df	X	Probably you colud store them in an encoded form and encode them as needed.
563e324a61a80130652678e0	X	Welcome to Stack Overflow. This isn't really a good question for Stack Overflow. Explaining how something works doesn't fit well into the question categories because it can generate a small book. Instead, you need to read their documentation carefully and follow any examples, and, if that doesn't help, consult their tech support who are better equipped to answer specific questions about their tools.
563e324a61a80130652678e1	X	I am trying to parse describe_instances api's response from Aws::EC2::Client. But verion 2 of aws ruby sdk comes with response paging feature. I dont understand what exactly is this!
563e324b61a80130652678e2	X	Response paging is a feature where you can enumerate calls to an API, yielding one response as a time, until all results have been received. This can be very important for API calls that return a large amount of data, such as enumerating objects in a bucket in Amazon S3. Without response paging you would have to do something like this: Some APIs have more complex paging requirements. Response paging eliminates the need to understand the paging requirements of every API call, and provides an #each method on the response.
563e324b61a80130652678e3	X	Hello Dear thanks for giving this knowledge to me.
563e324b61a80130652678e4	X	I am using XAMP(PHP 5.3,mySQL) on Windows 7, now in order to upload my App on EC2 is thr any additional requirement . So in this scenario should i need to install their SDK?
563e324b61a80130652678e5	X	Can you please tell me about Amazon EC2. I am having a fully functional small web application -- about 20-25 pages -- completed locally. But now the owners want to get it uploaded on the cloud rather than on a simple server. So please tell me, should I make any changes in my app? Is there any need to use the PHP SDK on Amazon Cloud. What steps exactly are required to manage the instance on the cloud? Please provide me some link from where I can get details about this. I am having my application in PHP and MySQL.
563e324b61a80130652678e6	X	Amazon EC2 is the Elastic computing cloud by Amazon. EC2 is a platform for hosting dedicated servers in the cloud. This differs from platform as a service models, like Google App Engine, where you definitely need to use their SDK. If your local server is running SUSE, for instance, and the EC2 server is running SUSE, then in theory your app should run the same on both servers. You should be able to access the EC2 server with an SSH connection just like you would a local server. You should be able to copy the app using secure copy (scp). Additionally, assuming they're running SUSE, you'd need to make sure Apache is configured to run PHP scripts, and you would need to install and configure MySQL, just like you would on your local server. Your app should run just fine on Amazon. You're still dealing with a dedicated server. The main difference is that you can't physically touch it as it's somewhere in Virginia I think. With that said, there is an SDK for PHP for Amazon, but it's not immediately clear what purpose it serves. I've run PHP just fine on EC2 without an SDK. But if you are interested, the link is below: http://aws.amazon.com/sdkforphp/ EDIT: The main advantage of the PHP API is for cases where the application will integrate with Amazon services. As an example, let's say your application will save files to Amazon S3. S3 uses a REST interface to interact with resources on the S3 Cloud. Instead of writing a wrapper around the REST interface yourself, the PHP API includes some pre-packaged API's that make development faster. You can learn more here at the Amazon PHP SDK FAQ
563e324c61a80130652678e7	X	All right, I am pretty newbie to network and storage things, but in my research, we need to use AWS S3 to backup data, sounds simple enough! So I follow the "AWS storage gateway user guide (API version 2013-06-30). Below are details I could provide based on my best knowledge: And After all the above completed, I tried to use my Windows 8 iSCSI to connect to VM. It shows as a disk in folder, so I did a initial formatting. But after this, it asks for formatting again. I followed the guide, but unfortunately it didn't work for me this time. Could anyone provide any insight on this issue? Thanks very much in advance.
563e324c61a80130652678e8	X	It turns out that I don't understand how iSCSI works. Quoted from Amazon Storage Gateway User Guide: Each of your storage volumes is exposed as an iSCSI target. Connect only one iSCSI initiator to each iSCSI target Since I thought iSCSI target as a network shared drive, I let multiple machine connect to the iSCSI target, resulting keeping asking for formatting.
563e324c61a80130652678e9	X	I figured out why it didn't work. Turns out I was using an old version of the "Amazon S3 PHP Class". I updated and used your suggested code, and now the new files have a Cache-Control set. Great! I will also look into your second link to set all the Cache-Control headers for the files that are already in the bucket. This should solve all my problems. Thanks for everything!
563e324c61a80130652678ea	X	I recently started using Amazon S3 to serve images to my visitors since this will reduce the server load. Now, there is a new problem: Today I looked into my AWS billings. I noticed that I have a huge bill waiting for me - there has been a total of 4TB AWS Data Transfer in 20 days. Obviously this is because the high amount of outgoing Amazon S3 traffic (to Cloudflare which then serves it to the visitors). Now I should to reduce the amount of requested files by setting a Cache header (since Cloudflare's Crawler will respect that). I have modified my code like this: to Still, it does not work. Cloudflare does not respect the Cache because the Cache-Control does not show up as "Cache-Control" in the Header but instead as "x-amz-meta-cachecontrol". Cloudflare ignores this. Does anyone have an easy solution for this? TL;DR: I have more or less the same problem as this guy: http://support.bucketexplorer.com/topic734.html (that was in 2008) EDIT: I have stumbled upon this: Amazon S3 not caching images but unfortunately that solution does not work for me. EDIT 2: Turns out it didn't work because I was using an old version of the "Amazon S3 class". I updated and the code works now. Thank you for your time.
563e324c61a80130652678eb	X	If you are getting "x-amz-meta-cachecontrol", it is likely you are not setting the headers correctly. It might just be the exact way you are doing it in your code. This is supposed to work. I am deducing this is php using Amazon S3 PHP Class? Try this: In the S3 PHP docs putObjectFile is listed under Legacy Methods: Compare to this: You need to set cache-control as a request header, but appears that there is no way to set request headers with putObjectFile, only meta headers. You have to use putObject and give it an empty array for meta headers and then another array with the request headers (including cache-control). You can also try some of the other working examples I have listed below. See also: How to set the Expires and Cache-Control headers for all objects in an AWS S3 bucket with a PHP script (php) Updating caching headers for Amazon S3 and CloudFront (python) Set cache-control for entire S3 bucket automatically (using bucket policies?) http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html?r=5225
563e324c61a80130652678ec	X	I can see there are examples with server side only and I am looking for only javascript based upload to s3. I am not sure how to add policy to request as in all examples they are using server side code. please help!!!
563e324c61a80130652678ed	X	@KrishnaBhatt I have explained how to do this in my answer. Also, the link provided by Burak explains in detail how to achieve this.
563e324c61a80130652678ee	X	I am trying upload files to directly to s3 but as per my research its need server side code or dependency on facebook,google etc. is there any way to upload files directly to amazon using fineuploder only?
563e324c61a80130652678ef	X	Yes, with fine uploader you can do.Here is a link that explains very well what you need to do http://blog.fineuploader.com/2013/08/16/fine-uploader-s3-upload-directly-to-amazon-s3-from-your-browser/
563e324c61a80130652678f0	X	There are three ways to upload files directly to S3 using Fine Uploader: Allow Fine Uploader S3 to send a small request to your server before each API call it makes to S3. In this request, your server will respond with a signature that Fine Uploader needs to make the request. This signatures ensures the integrity of the request, and requires you to use your secret key, which should not be exposed client-side. This is discussed here: http://blog.fineuploader.com/2013/08/16/fine-uploader-s3-upload-directly-to-amazon-s3-from-your-browser/. Ask Fine Uploader to sign all requests client-side. This is a good option if you don't want Fine Uploader to make any requests to your server at all. However, it is critical that you don't simply hardcode your AWS secret key. Again, this key should be kept a secret. By utilizing an identity provider such as Facebook, Google, or Amazon, you can request very limited and temporary credentials which are fed to Fine Uploader. It then uses these credentials to submit requests to S3. You can read more about this here: http://blog.fineuploader.com/2014/01/15/uploads-without-any-server-code/. The third way to upload files directly to S3 using Fine Uploader is to either generate temporary security credentials yourself when you create a Fine Uploader instance, or simply hard-code them in your client-side code. I would suggest you not hard-code security credentials.
563e324d61a80130652678f1	X	Here is what you need. In this blogpost fineuploader team introduces serverless s3 upload via javascript. http://blog.fineuploader.com/2014/01/15/uploads-without-any-server-code/
563e324d61a80130652678f2	X	I am doing an upload via CORS to Amazon S3 with the Kendo Upload control. I'm having an issue with the fact that I need to grab a signature from my server, then add it to the 'data' for the event object of 'upload' handler I created. The problem is, of course, that in the handler I fire off an async request to get the signature, and the upload handler continues on it's merry way without the signature data i need. The published API has no 'upload()' or something command that I could call when my async request returns. I saw an ASP-Kendo-S3 example somewhere, but it's not exactly clear from that code, how that signature is being obtained, and of course, I'm not using ASP.
563e325061a80130652678f3	X	Kendo Upload has an onUpload event. In Kendo's asp.net example there really isn't anything specific to that framework that wouldn't port to anything else. They populate the page initially with the profile (base64 encoded JSON). To get a signature for that base64 encoded json profile they use this method (C#): It looks pretty self explanatory to the point where you could port it to another language.
563e325061a80130652678f4	X	I have an account in the Amazon S3 where I upload files - among them video files. What I need to do next, is to show my clients a specific video, and I need something that support all kinds of encoding. Most of the time, the user uses a smartphone to watch the video, and I need some software to stream the video-file right into his/her smartphone - and no matter in which format the video file was uploaded, I need that the user will be able to watch it. I saw some solutions such as Vimeo, Vzaar, etc. but I'm looking for something else - without too many limits on size and bandwidth, with a good API to upload the files into it, and with cross-platform ability to view the file. Is there a solution in Youtube? in Amazon EC2? etc.? Any other solution? Thanks a lot, Danny
563e325061a80130652678f5	X	One solution for you is to use a conversion utility like ffmpeg for your conversion needs. One format of course cannot support all end users (PC browsers, mobile browsers and so on), so you need to convert the originally uploaded file to multiple formats and render to the user what is best based on the device type which accesses it. You can also look at Darwin streaming server and host it on Amazon or any other platform for that matter to suit your need If you are looking for a solution directly from Amzon by uploading your files on S3 and stream it, then currently Amazon supports only Adobe server as its streaming server in production mode, so you are pretty much restricted to the formats that the streaming server supports. Still the ownus is on you to convert in to different formats.
563e325061a80130652678f6	X	thank you, it's cool answer. But what I want is to enter the keyword "iphone", for example, and get some iphone products on amazon with its description and price; you said about paying only for amout of usage, what does it mean: I don't understand, because I'm not going to save information on Amazon, I just wanna get information about amazon products through amazon web servises
563e325161a80130652678f7	X	1) I want to enter the keywork "iphone" and get some iphone products on amazon with its description and price; 2) I'm interesting in price: should I pay for each request or may be I should pay monthly, and where can I find the price list
563e325161a80130652678f8	X	Sometimes your bills might surprise you - we must be very careful on how we use AWS.
563e325161a80130652678f9	X	for example, to use ebay web services I shouldn't pay. How much should I pay? Should I pay for request number or monthly? Where can I get the price list?
563e325161a80130652678fa	X	Amazon charges almost for everything. They will charge you for requests also. There're many things you should read about (how to save costs, for example). And prices info could be found here: aws.amazon.com/pricing
563e325161a80130652678fb	X	I'm new in iOS development, and I faced an issue with amazon. I wanna gain information about amazon products with amazon web servises. I wanna enter the keyword and get information about proper products. I looked at http://aws.amazon.com/mobile/ and saw that I should register. During the registration Amazon asked me about my Visa card information and then tried to withdraw 1 dollar. The questions are:
563e325161a80130652678fc	X	The pricing differs between the various services and is typically listed in http://aws.amazon.com/<service name>/pricing/. Here are couple of examples - EC2, S3. Note that for some of the services there is a free tier for about a year, as long as you stay under certain amount of usage. So, while you WILL get a bill every month, that bill might be for $0. More about the AWS Free Usage Tier. You can download the client SDKs freely and write code against it. However, to actually run it against AWS, you will need AWS Access Key ID and Secret Access Key, so that AWS servers can authenticate the requests from your application (and incidentally also bil you properly for your usage). You should start with the Getting Started with the AWS SDK for iOS and the AWS SDK for iOS FAQs. The SDK also contains bunch of sample apps into the <SDK install folder>/samples folder. Update: Ah, you want to search the Amazon catalog? That's different from AWS. AWS is intended to provide you access to computing resources (storage, CPU, load balancing, and so on) for your own services. For your scenario you need to use the Amazon Affiliate Program Product Advertising API. While that API does share credentials with AWS (it uses the AWS Access Key ID and Secret Key), it most likely is free (but double check to be sure), as amazon will be making money on any product your users buy. Also, the Product Advertising API does not have client SDKs (as far as I know), so you will have to deal with making the HTTP requests yourself. The API supports both REST and SOAP, so you can choose your own poison. There's also bunch of samples for both server and client apps, in PHP, C#, Java, Node.js, Ruby, and so on.
563e325161a80130652678fd	X	AWS is great! Its totally worth the price. So you can download the AWS iOS SDK and integrate it into your project; however, before it will work you need to signup. I would give you some examples but I don't fully understand what you're asking. The AWS iOS SDK has tons of code samples in it. If you want, you can comment on this post what you want to use AWS for and then I can help you come up with the code to achieve it :) I hope you have fun with iOS Development, its great :) Good Luck!
563e325161a80130652678fe	X	Are you maybe confusing Amazon web services with a request API? You said: I want to enter the keywork "iphone" and get some iphone products on amazon with its description and price That is what an amazon web API would do (from this question, I understand there is maybe no such thing for Amazon?). AWS is a cloud service where you can run your programs and pay according to the resources you use. Think of that as a web host. All in all, AWS is not directly related to Amazon content, if I understood correctly this is not what you want.
563e325161a80130652678ff	X	
563e325161a8013065267900	X	I have about 400 GB data on an Amazon EBS volume and I need this data in a S3 bucket for Hadoop EMR usage. How can I move/copy data from an EBS volume to a S3 bucket (both S3 bucket and EBS volume are in the same AWS region)? Thanks
563e325161a8013065267901	X	The AWS Command Line Interface is meanwhile the recommended choice for all things AWS: The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. On top of this unified approach to all AWS APIs, it also adds a new set of simple file commands for efficient file transfers to and from Amazon S3, with characteristics similar to the well known Unix commands, e.g. for the task at hand: So cp would be sufficient for your use case, but be sure to check out sync as well, it is particularly powerful for many frequently encountered scenarios (and sort of implies cp depending on the arguments).
563e325161a8013065267902	X	I'm pretty sure they haven't enabled this functionality yet. I've been waiting for them to add the ability for a menu action like share or send or open website to a card insert from GDK.
563e325261a8013065267903	X	That was actually the first thought I had. I tried that. I had a Card with an image and no text and pushed it to the timeline. The issue is that there's nothing you can do with that card as a user once it's in there. Unless I'm doing something wrong here: Card card2 = new Card(this); card2.setImageLayout(Card.ImageLayout.FULL); card2.addImage(R.drawable.glass_share); TimelineManager.from(this).insert(card2);
563e325261a8013065267904	X	Take a look at the grid here (bottom of the page) : developers.google.com/glass/develop/gdk/ui/index. You can't access the user input with Static Cards. You probably should try with a Live Card. There is a good example : developers.google.com/glass/develop/gdk/ui/live-cards
563e325261a8013065267905	X	Ok so let's say I have a live card and I can access the user input. How then would I share the image out other people?
563e325261a8013065267906	X	My GDK app generates an image that I'd like the user to share with other people. I tried creating a Card that contained an image and inserted it into the timeline. There is no Share menu item for that card. I also tried adding the image into the Media Gallery and that does not produce an error but the image does not appear in the timeline: And lastly I've tried the ACTION_SEND intent which doesn't seem to be supported: So what's the right way to do this? --edit A couple more things I've tried unsuccessfully : -Upload to Amazon S3. It doesn't look like I can use the Amazon .jar file in a GDK app. The app builds but I get a ClassNotFoundException on runtime. -use the JavaMail API to mail that image to a specific email address. It doesn't look like some of the required classes for JavaMail are included in the GDK environment.
563e325261a8013065267907	X	I don't know if a 'Share' system menu exists, but if you want to push a card on the timeline, you should probably use the TimelineManager class, that allow you to interact with the Glass timeline : https://developers.google.com/glass/develop/gdk/reference/com/google/android/glass/timeline/TimelineManager#insert(com.google.android.glass.app.Card) Be careful, you can't use the same insertion strategy if you're pushing static cards or live cards. Julian
563e325261a8013065267908	X	OK I think I've got a solution here. I can post the image to a service and then on the backend do whatever I need to do with it. I had to Base64 encode the image content b/c as far as I can tell it doesn't look like Multipart is supported. Wrap this bad boy in an AsyncTask and you should be good to go:
563e325261a8013065267909	X	I am trying to create an application where I host a few static html files that a user authors and uploads to GAE. Is it possible to upload these files to WAR folder for hosting without a redeploy. Uploading to WAR may not be a good idea as a redeploy would wipe out the user authored files. Would like these files to be stored across deployments. May be a GCS bucket ? But in case of GCS bucket, how would you serve these static html files over a GAE/page.html url ? I was reading that GCS bucket would serve as a web folder by turning a knob on that bucket. But how scalable would that be ? Is that even a good approach to use GCS bucket as a web url ? Would be good to get some ideas on addressing this as I am new to GAE ! Thanks a lot
563e325261a801306526790a	X	You can use Google Cloud Storage to serve static websites and a Google AppEngine application to update the bucket when user edit/upload some content to the website. You would probably need to have different subdomains for this like: This should be pretty scalable & cheap to serve traffic. Apart from GCS you also can considering storing static files on Amazon S3 or any other hosting that has API. Actually you even can develop a solution that upload files to multiple storages like GCS, S3, etc. and serve them using fault-tolerant DNS or reverse-proxy (like CloudFlare) so if Google or Amazon goes down your website is up and running.
563e325261a801306526790b	X	What are the pitfalls you are looking to avoid?
563e325261a801306526790c	X	I don't know exactly. I just know it's complicated doing video streaming to android and I don't want to spend months going in one direction only to find out it's not compatible with it.
563e325261a801306526790d	X	I'm thinking that the first thing I should do is make an android application that uploads a video and a rails app that accepts that video and downloads it to Amazon. Any idea how to do that?
563e325261a801306526790e	X	On SO, you need to try something first. See: stackoverflow.com/questions/how-to-ask
563e325261a801306526790f	X	I am currently building an app that will allow users to upload videos and view other users videos in a stream. Sort of like Vine. I have been using rails for over a year now but I am not sure how to go about implementing the backend for the android application. My understanding of the situation is that I must use a json call to my rails api that will upload the video file to Amazon s3 or CloudFront. I then need to make the Amazon video file or url be stored or linked to a URL that the Rails app creates for the user. After that, I would need to play the video (and other people's videos) back to the android application. It looks like there are a lot of pitfalls to this. If anyone knows the correct way to go about doing this, I would be really grateful. Thank you.
563e325261a8013065267910	X	Probably want to use a REST API. Should host your videos on a CDN. Can use VideoView to stream some types of videos on Android.
563e325361a8013065267911	X	I'm using s3cmd 1.1.0beta to upload files that are larger than 5 GB to Amazon S3. This is because s3cmd older than 1.1.0 is not able to upload files larger than 5 GB (Amazon single-part upload limit), and the latest beta version is able to upload these files to S3 using multi-part upload. The problem is: I am not able to perform ANY operation on the files larger than 5 GB uploaded through s3cmd 1.1.0. I suspect that this may be happening because Etag set by s3cmd does not match the Etag that Amazon expects: The specific problems are as follows (both through the web console): Is there any way to fix the Etags in the larger-than-5-GB-files so that I am able to perform operations on these files?
563e325361a8013065267912	X	OK, after some investigation, I found that the problem has to do with Amazon S3's inability to natively handle files that are larger than 5 GB in size. In order to copy or do any operation on a file larger than 5 GB in size, you have to specifically use Amazon's multi-part upload and related APIs for working on large files. Apparently, even Amazon's AWS web console uses only the simple APIs, which work only on files that are less than 5 GB in size, so if you want to do anything with files larger than 5 GB in size, you need to write your own code with the AWS API to operate on those files!
563e325561a8013065267913	X	Great, this worked, thanks a lot! But only one thing, getObject requires a parameter, I added empty string (""), and worked.
563e325561a8013065267914	X	I have this code to get files from rackspace cloud files: In the open php cloud documentation says you can change 'GET' to 'PUT' to what I imagine is being able to put a file, instead of getting it, the problem is that the file doesn't exist yet, and apparently the only way to create a file is uploading it first? PHP SDK Container In Amazon s3 I can do the following to get what I want: Then I can write to the presignedUrl anyway I prefer, like with Java: So, basically what I want to do, is get the upload URL from RackSpace and write to it like I do with Amazon s3. Is it possible? And if it is, how can you do it? I need to do it this way because my API will provide only download and upload links, so no traffic goes directly through it. I can't have it saving the files to my API server then upload it to cloud.
563e325661a8013065267915	X	Yes, you can simulate a file upload without actually uploading content to the API - all you need to do is determine what the filename will be. The code you will need is: The getObject method returns an empty object, and all you're doing is setting the remote name on that object. When a temp URL is created, it uses the name you just set and presents a temporary URL for you to use - regardless if the object exists remotely. The temp URL can then be used to create the object.
563e325661a8013065267916	X	If the answer below is correct, can you mark it as such ? This would help other customers having the same question and looking for answers. Thanks
563e325661a8013065267917	X	take a look at Uploadcare. it allows uploading huge files and copying them to your s3 bucket without exposing it.
563e325761a8013065267918	X	I have a pretty tough problem here: I need to allow the users of my site to upload very large files to their accounts and I want to store these files on a AWS S3 filesystem. I can't just write a web service to receive these and save them in the S3 fs, because all kinds of things can go wrong during the upload and I need a sophisticated uploader client. The kind of client that Amazon provides to upload files into S3, but of course I can't give my users direct access to that. I'd seriously appreciate any ideas for this! Thank you
563e325761a8013065267919	X	Best practice would be to let your client application to upload directly to S3, not flowing through your own web infrastructure. This would leverage ether massive parallel nature of S3 and off-load your web infrastructure. Offloading your infrastructure will allow to use less instances or smaller instances to serve the same amount of traffic. Hence a lower infrastructure cost for you. You would need to write an S3 IAM Policy that would limit access for each of your user to their own "directory" (aka key prefix) on S3 Have a look at this blog post : http://blogs.aws.amazon.com/security/post/Tx1P2T3LFXXCNB5/Writing-IAM-policies-Grant-access-to-user-specific-folders-in-an-Amazon-S3-bucke If your application is a web app, you can even let your customers' browsers upload directly to S3. See how to implement this securely at http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html Just a last note about your question : S3 is not a file system, S3 is an object store. Read more about the differences between object storage and file system at http://www.infoworld.com/article/2614094/data-center/what-is-object-storage-.html
563e325761a801306526791a	X	I am uploading objects to amazon s3 using AWS iOS SDK in Iphone, sometime error occurs and some of the objects are uploaded, remaining are not uploaded. I have created bucket and inside bucket i have created folder in which i have store my objects. I want to delete folder and all its object. Can anyone help me?
563e325761a801306526791b	X	First of all, there is not such thing as "folders" in S3. Most S3 clients (including the AWS web console) show them as folders only for convenience (grouping stuff), but in fact, what you see as a "folder name" is merely a prefix. Being that said, my suggestion to you is using the listObjectsInBucket API call, passing in your "folder name" as prefix in the S3ListObjectsRequest parameter. When you have obtained all the keys (file names including prefix) matching that prefix, use the deleteObjects API call, passing in the keys in S3ListObjectsRequest parameter. For more details on folder/prefix and deleting stuff, please see these related links: Delete files, directories and buckets in amazon s3 java Thread on AWS forum regarding this subject
563e325c61a801306526791c	X	Thanks Wizzard, but I'm looking for a storage service provider so I don't have to spend lots on my own infrastructure. Sorry if that wasn't clear.
563e325c61a801306526791d	X	I have a requirement to store files remotely from a cordova/phonegap application. I have been looking at: However, these seems to want my application to be registered with them, but I can't see whey this is required (see here). I have also looked at Amazon S3. Howeever, I'm not sure my users will be happy to create an Amazon account just so that their files can be persisted. Question: Are there other service provider API's available that will allow me to create a user account on the service provider and upload/download files?
563e325c61a801306526791e	X	Take a look at various Open Source cloud solutions (example: OpenStack). Using provided API you can implement user registration in your app. To have a very basic (OpenStack) installation you really need only 1 server and then you can easily extend your cloud adding additional servers / resources.
563e325d61a801306526791f	X	Thanks, that's useful code to look at. Did you have some client code too?
563e325d61a8013065267920	X	@James: only timestamp seems not sufficient much, during short of time they may simulate the request and sent to server, I have just edited my post, use both would be the best.
563e325e61a8013065267921	X	Are you sure this is working as it should? you are hashing the timestamp with the message and caching that message. This would mean a different signature each request which would render your cached signature useless.
563e326061a8013065267922	X	@FilipStas: seems I don't get your point, the reason to use Cache in here is to prevent relay attack, nothing more
563e326061a8013065267923	X	@ChrisO: You can refer [this page] (jokecamp.wordpress.com/2012/10/21/…). I will update this source soon
563e326061a8013065267924	X	Thanks - I'll take a look at this, though for now I've rolled my own HMAC-based solution.
563e326061a8013065267925	X	@CraigShearer - hi, you say you've rolled your own.. just had a few questions if you don't mind sharing. I'm in a similar position, where i have a relatively small MVC Web API. The API controllers sit alongside other controller/actions which are under forms auth. Implementing OAuth seems an overkill when I already have a membership provider i could use and I only need to secure a handful of operations. I really want an auth action that returns an encrypted token - then used the token in subsequent calls? any info welcome before I commit to implementing an existing auth solution. thanks!
563e326061a8013065267926	X	@Maksymilian Majer - Any chance you can share how you've implemented the provider in more detail? I'm having some problems sending responses back to the client.
563e326061a8013065267927	X	Thanks for the example @Dalorzo, but I have some issues. I looked at the attached link, but following that instructions doesn't quite work. I also found needed info missing. Firstly, when I create the new project, is it right to choose Individual User Accounts for authentication? Or do I leave it at no authentication. I'm also not getting the mentioned 302 error, but am getting a 401 error. Lastly, how do I pass the needed info from my view to the controller? What must my ajax call look like? Btw, I'm using forms authentication for my MVC views. Is that a problem?
563e326061a8013065267928	X	Are you sharing/providing source code for this framework as open source?
563e326061a8013065267929	X	I want to build a RESTful web service using ASP.NET Web API that third-party developers will use to access my application's data. I've read quite a lot about OAuth and it seems to be the standard, but finding a good sample with documentation explaining how it works (and that actually does work!) seems to be incredibly difficult (especially for a newbie to OAuth). Is there a sample that actually builds and works and shows how to implement this? I've downloaded numerous samples: I've also looked at blogs suggesting a simple token-based scheme (like this) - this seems like re-inventing the wheel but it does have the advantage of being conceptually fairly simple. It seems there are many questions like this on SO but no good answers. What is everybody doing in this space?
563e326161a801306526792a	X	We have managed to apply HMAC authentication to secure Web Api and it worked okay. Basically, HMAC authentication uses a secret key for each consumer which both consumer and server both know to hmac hash a message, HMAC256 should be used. Most of cases, hashed password of consumer is used as secret key. The message normally is built from data in the HTTP request, or even customized data which is added into HTTP header, message might include: Under the hood, HMAC authentication would be: Consumer sends a HTTP request to web server, after building the signature (output of hmac hash), the template of HTTP request: Example for GET request: The message to hash to get signature: Example for POST request with querystring (signature below is not correct, just an example) The message to hash to get signature Please note that form data and query string should be in order, so the code on server get querystring and form data to build correct message. When HTTP request comes to server, an authentication action filter is implemented to parse the request to get information: HTTP verb, timestamp, uri, form data and query string, then based on these to build signature (use hmac hash) with secret key (hashed password) on the server. The secret key is got from database with username on the request. Then server code compares the signature on the request with the signature built, if equal, authentication is passed, otherwise, it failed. The code to build signature: So, how to prevent replay attack? Add constraint for the timestamp, something like: (servertime: time of request comming to server) And, cache the signature of request in memory (use MemoryCache, should keep in limit of time). If the next request comes with the same signature with previous request, it will be rejected. The demo code is put as here: https://github.com/cuongle/Hmac.WebApi
563e326161a801306526792b	X	Have you tried DevDefined.OAuth? I have used it to secure my WebApi with 2-Legged OAuth. I have also successfully tested it with PHP clients. It's quite easy to add support for OAuth using this library. Here's how you can implement the provider for ASP.NET MVC Web API: 1) Get the source code of DevDefined.OAuth: https://github.com/bittercoder/DevDefined.OAuth - the newest version allows for OAuthContextBuilder extensibility. 2) Build the library and reference it in your Web API project. 3) Create a custom context builder to support building a context from HttpRequestMessage: 4) Use this tutorial for creating an OAuth provider: http://code.google.com/p/devdefined-tools/wiki/OAuthProvider. In the last step (Accessing Protected Resource Example) you can use this code in your AuthorizationFilterAttribute attribute: I have implemented my own provider so I haven't tested the above code (except of course the WebApiOAuthContextBuilder which I'm using in my provider) but it should work fine.
563e326161a801306526792c	X	I would suggest starting with the simplest solutions first - maybe simple HTTP Basic Authentication + HTTPS is enough in your scenario? If not (for example you cannot use https, or need more complex key management) you may have a look at HMAC-based solutions as suggested by others. A good example of such api would be Amazon S3 (http://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html) I wrote a blog post about HMAC based authentication in ASP.NET Web API.It discusses both Web API service and Web API client and the code is is available on bitbucket. http://www.piotrwalat.net/hmac-authentication-in-asp-net-web-api/ Here is a post about Basic Authentication in Web API: http://www.piotrwalat.net/basic-http-authentication-in-asp-net-web-api-using-message-handlers/ Remember that if you are going to provide an API to 3rd parties, you will also most likely be responsible for providing client libraries. Basic authentication has a great advantage here as it is supported on most programming platforms out of the box. HMAC on the other hand is not that standardized and will require custom implementation. These should be relatively straightforward, but still require work. PS. There is also an option to use HTTPS + certificates - http://www.piotrwalat.net/client-certificate-authentication-in-asp-net-web-api-and-windows-store-apps/
563e326161a801306526792d	X	Web API introduced an Attribute [Authorize] to provide security. This can be set globally (global.asx) Or per controller: Of course your type of authentication may vary and you may want to perform your own authentication, when this occurs you may find useful inheriting from Authorizate Attribute and extending it to meet your requirements: And in your controller: Here is a link on other custom implemenation for WebApi Authorizations: http://www.piotrwalat.net/basic-http-authentication-in-asp-net-web-api-using-membership-provider/
563e326161a801306526792e	X	If you want to secure your API in a server to server fashion (no redirection to website for 2 legged authentication). You can look at OAuth2 Client Credentials Grant protocol. https://dev.twitter.com/docs/auth/application-only-auth I have developed a library that can help you easily add this kind of support to your WebAPI. You can install it as a NuGet package: https://nuget.org/packages/OAuth2ClientCredentialsGrant/1.0.0.0 The library targets .NET Framework 4.5. Once you add the package to your project, it will create a readme file in the root of your project. You can look at that readme file to see how to configure/use this package. Cheers!
563e326161a801306526792f	X	in continuation to @ Cuong Le's answer , my approach to prevent replay attack would be // Encrypt the Unix Time at Client side using the shared private key(or user's password) // Send it as part of request header to server(WEB API) // Decrypt the Unix Time at Server(WEB API) using the shared private key(or user's password) // Check the time difference between the Client's Unix Time and Server's Unix Time, should not be greater than x sec // if User ID/Hash Password are correct and the decrypted UnixTime is within x sec of server time then it is a valid request
563e326161a8013065267930	X	Server should allow some time difference unless server and client are perfectly synced. Checking logs can be complicated too. It's better to have a nonce (unique string) generated with each request and check nonces for duplicates.
563e326161a8013065267931	X	EJP's answer is correct, you should use SSL. And this answer is NOT secure. With this attempted solution, a man in the middle attack can easily intercept messages and replay them to the server while preventing the requests from reaching the server. Signing the timestamps does nothing to secure against this hole.
563e326161a8013065267932	X	HMAC isn't going to prevent replay attacks, if the same message and HMAC are played again.
563e326261a8013065267933	X	@Bruno thanks for pointing that out. I don't know what I was thinking. I guess including Cryptographic nonce can solve the problem.
563e326261a8013065267934	X	Let's say Bob sent this HTTP request to an API to update his email: /user/update?email=bob@example.com&userid=1234&sig=x1zz645 Now a sniffer named Zerocool recorded this request for later use. After a few days later, Bob updated his email again to email=newbob@example.com. Few hours later Zerocool now decides to use what he sniffed a few days ago and runs the request: /user/update?email=bob@example.com&userid=1234&sig=x1zz645 The server accepts it and Bob is now confused why is the old email back. How can we prevent this from happening without using SSL?
563e326461a8013065267935	X	Keep a log of recent requests. Embed a timestamp into such requests, and reject any that are present in the log or older than the log. For good measure, sign the timestamps with a private md5 checksum, so they can't be fabricated.
563e326461a8013065267936	X	Use SSL as stated in your tags. It is already immune to both sniffing and replay attacks. It exists. Using it is free. It works. It's done. If you can't use SSL please remove it from your tags.
563e326461a8013065267937	X	You can use Hash based message authentication code (HMAC) to secure the API so that replay attacks like the one you mentioned can be avoided. Both the server and the client will have a shared secret API key. Amazon S3 Rest API uses the same procedure to Authenticate and Validate requests. See the Documentation here. UPDATE: As Bruno pointed out HMAC itself cannot prevent replay attacks. You will have to include some unique identifier signed with secret key with the message and validate it at the server.
563e326561a8013065267938	X	I have an option for users to share videos. I use to filepicker.io to handle my uploads. I am using zend gdata plugin, it works well for videos stored on my server. But it does not work for videos saved on amazon s3 servers. Can I upload videos on s3 to youtube. I appreciate any help.
563e326561a8013065267939	X	The YouTube Data API requires that you send the actual bytes of the video you're uploading as part of the upload request. You can't just provide a reference to a remote URL as part of an upload request and expect the YouTube API servers to retrieve the video file from that URL. If your video is on Amazon S3 and your code is running from a place that doesn't have direct access to the video file, then you're going to have to download it temporarily to someplace your code does have access to, and then include the video file in your YouTube upload request.
563e326561a801306526793a	X	Asked them on twitter. Let's see what they say.
563e326561a801306526793b	X	thank you kindly for answwering my question
563e326561a801306526793c	X	I've recently begun experimenting with Deployd. It is (kind of) similar to meteor. This may be amateurish question, but what happens if my collection consists of images? How will I upload it to MongoDB @ deployd dashboard?
563e326561a801306526793d	X	The only real way to use the Collection Resource Type to do this right now would be to base64 encode the image and store it as a string property. There are some limitations and performance issues with base64 images though. Alternatively, @dallonf has created an Amazon S3 resource to make it easy to integrate deployd apps with S3. http://docs.deployd.com/docs/using-modules/official/s3.md There have been a lot of requests for storing binary files in collections, and hopefully someone (core committer or otherwise) can work on this after the forthcoming deployd release which includes significant improvements to the module API. This Github issue is worth watching: https://github.com/deployd/deployd/issues/106
563e326561a801306526793e	X	I created a module for deployd to upload files (images included). https://github.com/NicolasRitouet/dpd-fileupload It lets you store the files in a local folder and in a collection to query them.
563e326661a801306526793f	X	Just want to know more, is your app a web app? If it is, you have the option to offload the uploading task to the web client interface, meaning that your users will upload images directly to S3, instead of to your backend app and then to S3.
563e326661a8013065267940	X	IN the putObject call, what is the 'my-key' referring to? That is one of those things that the AWS docs seem to just pop in there but I can't get a decent explanation of what it is...
563e326661a8013065267941	X	So in S3 there are two concepts - a bucket and a key. The bucket is the container you store things in, e.g. "my-bucket" and the key is the path to it. It may look like a folder and file path, e.g. "path/to/my/file.txt" - so the full AWS resource path would be in this case s3://my-bucket/path/to/my/file.txt. Treat it as a "filename"
563e326661a8013065267942	X	AHA! So that just made a light go on over my head! Thank you so much - for some reason I never quite grasped that idea until you explained it.
563e326661a8013065267943	X	No worries - it can seem a bit complicated but it's actually quite a simple system.
563e326661a8013065267944	X	Okay, this is driving me nutty. Some code : require 'vendor/autoload.php'; $sharedConfig = [ 'region' => 'us-west-2', 'version' => 'latest' ]; $sdk = new Aws\Sdk($sharedConfig); It all works until it hits that last line and then it just fails...
563e326661a8013065267945	X	I have a PHP app running on a AWS EC2 instance. I want to upload a file to unit and then save that file to a S3 bucket. I can get the file up to the Ec2 instance with now problem but for the life of me I cannot figure out how to use the puObject call to move it to the S3 instance... I am very new to AWS so if anyone can give me a pointer, that'd help out a lot!
563e326661a8013065267946	X	You have three options for how to transfer data from the EC2 instance to S3 using PHP: This is the preferred option. The SDK is released by Amazon and includes an easy to use API interface for all of their services. It can also allow you to use the EC2 instance's IAM role for credentials, meaning you don't necessarily need to store your API keys etc in the code. Example use: This is less ideal, but still doable. I wouldn't recommend doing this as it would mean you use PHP to access the shell, and assume that the shell has the AWS CLI configured (most if not all EC2 instances would do by default). Example: This is probably the next best option after using the SDK, although it requires that you store the credentials in code, have to formulate your HMAC signatures and ensure that your API structure matches Amazon's guidelines. Note that the PHP SDK does this all for you, but this way you don't need the whole SDK installed. If you go this way, you'll need to read up about how to sign your requests.
563e326661a8013065267947	X	My question is how to effectively upload the data, not how to effectively store them. The point is that if the frontend and backend are different servers, the upload is duplicated — first it's uploaded from client to frontend and then from there again whole file via network to the backend. The data of course need to be on the backend system.
563e326761a8013065267948	X	I edited the question with possibility to upload to S3.
563e326761a8013065267949	X	if your data need to be only on the backend system then you can directly upload it to your backend server from the client browser. If you read the Amazon S3 API documentation then you will get it.
563e326761a801306526794a	X	What if I don't want the backend server to be accessible from the whole internet? Not speaking of the fact that the "password" for access to the backend would be out there in browser. Maybe I could solve the second problem somehow (how?), but is this really the clean way how it's done? I mean, for example how does Google Drive upload files?
563e326861a801306526794b	X	Another thing.. can you link to specific part of the S3 Documentation you meant? And I don't mean to discuss the way to upload data, but the design, the architecture of the whole system. How it's properly done.
563e326961a801306526794c	X	Let's say I have a website with frontend and backend. The website allows users to upload some data ranging from hundreds of MB to GB. How do you effectively upload these data? So, do you have any ideas? Or is there some kind of general way to do this effectively? Does it include CDNs? Or should I store the data in a database as base64 strings if it'd only be few hundreds of MB pre file? Thanks. EDIT: I just thought of using a Amazon S3 for this. This raises similar question where is the upload to S3 going to happen. In browser (and reference to S3 object will be sent to FE and BE) or in FE (and reference will be saved in BE DB) or in BE? Which one of them and why?
563e326b61a801306526794d	X	from your question i understood that you are allowing users to upload files ranging from MB to GB. In that case u need to save the files in a particular folder and save the reference to that folder in your backend database. This will be the effective way of storing large data. If you save the data in the backend then it will overload your backend unnecessarily.
563e326c61a801306526794e	X	One of the option is to mount S3 bucket as local directory on your server (using RioFS for example), and let your web application save uploaded files to that folder. Files will be sent to your S3 bucket in parallel.
563e326d61a801306526794f	X	Amazon S3 file size limit is supposed to be 5T according to this announcement, but I am getting the following error when uploading a 5G file This makes it seem like S3 is only accepting 5G uploads. I am using Apache Spark SQL to write out a Parquet data set using SchemRDD.saveAsParquetFile method. The full stack trace is Is the upload limit still 5T? If it is why am I getting this error and how do I fix it?
563e326d61a8013065267950	X	The object size is limited to 5 TB. The upload size is still 5 GB, as explained in the manual: Depending on the size of the data you are uploading, Amazon S3 offers the following options: Upload objects in a single operation—With a single PUT operation you can upload objects up to 5 GB in size. Upload objects in parts—Using the Multipart upload API you can upload large objects, up to 5 TB. http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html Once you do a multipart upload, S3 validates and recombines the parts, and you then have a single object in S3, up to 5TB in size, that can be downloaded as a single entitity, with a single HTTP GET request... but uploading is potentially much faster, even on files smaller than 5GB, since you can upload the parts in parallel and even retry the uploads of any parts that didn't succeed on first attempt.
563e326d61a8013065267951	X	If you're sending the username and password with every request, use HTTPS.
563e326e61a8013065267952	X	Nothing to do with security, but a RESTful API would not use getrating and andrating; it would just be rating, and you would GET, POST, PUT, or DELETE to that resource.
563e326e61a8013065267953	X	I don't find Amazon's way hard to copy per say... like I don't find writing JavaScript from scratch "hard", but jQuery sure help writing it well. Reading about it left me wondering isn't there a framework to abstract this?
563e326e61a8013065267954	X	Google it. I wrote an implementation for AWS, but it's not complete I believe: code.google.com/p/sabredav/source/browse/lib/Sabre/HTTP/…
563e326e61a8013065267955	X	Sorry but the answer is not correct. Cookies and HTTP Digest are complementary but orthogonal - you can use the second to authenticate user and issue a cookie. Comparing to OAuth, cookie based security won't work when you have cross-domain services, or you are letting other untrusted people to write client to your service (3rd parties involved) and want to allow your users to revoke application access. But in other cases it works exactly like OAuth, you can think about a cookie as an OAuth access token, which by the way you need to store somewhere too.
563e326e61a8013065267956	X	My answer was written before OAuth2, when every OAuth token would be a digest of a bunch of information related to the request. This is not true for a OAuth2 bearer, which indeed pretty much puts it on-par to a token in a Cookie.
563e326e61a8013065267957	X	But then don't you force the user to login before accessing an API every single time the "session" expires? In case of a mobile device what do you do? Give them a permanent "key" so they don't have to login everytime they use the application use the API?
563e326e61a8013065267958	X	I force them to log in. It's either that or use a cookie, or map the fingerprint to their UDID, but the UDID thing means the user has to access the service from the same device.
563e326e61a8013065267959	X	Regarding step 3: this means that if I'm using an MVC framework that uses "Actions" inside "Controllers", I should put in every Action two extra parameters (fingerprint and sessionid)?
563e326e61a801306526795a	X	@sports Regarding adding functionality at the beginning of method calls: If you're using MVC/WebApi, intercept actions with OWIN. If you want to allow anything through the endoint, but instead want to secure the domain logic, use AOP like PostSharp. If you're rolling your own, just use Attributes and break a bunch of principles to put functionality in attributes on methods you want to secure.
563e326e61a801306526795b	X	How is that not on top of Don't worry about being "RESTful", worry about security. answer?
563e326f61a801306526795c	X	It's too late, and I don't know what you are talking about, sorry. :D I'll read this post tomorrow, I don't remember on it...
563e326f61a801306526795d	X	I managed to read the post again. Yepp, I think it's because my post has a 2 years delay compared to the post you mentioned. That's all, have a nice evening! :-)
563e326f61a801306526795e	X	When designing REST API is it common to authenticate a user first? The typical use case I am looking for is: I would like to build it once and allow say a web-app, an android application or an iPhone application to use it. A REST API appears to be a logical choice with requirements like this To illustrate my question I'll use a simple example. I have an item in a database, which has a rating attribute (integer 1 to 5). If I understand REST correctly I would implement a GET request using the language of my choice that returns csv, xml or json like this: Say we pick JSON we return: This is fine for public facing APIs. I get that part. Where I have tons of question is how do I combine this with a security model? I'm used to web-app security where I have a session state identifying my user at all time so I can control what they can do no matter what they decide to send me. As I understand it this isn't RESTful so would be a bad solution in this case. I'll try to use another example using the same item/rating. If user "JOE" wants to add a rating to an item This could be done using: At this point I want to store the data saying that "JOE" gave product {id} a rating of {givenRating}. Question: How do I know the request came from "JOE" and not "BOB". Furthermore, what if it was for more sensible data like a user's phone number? What I've got so far is: 1) Use the built-in feature of HTTP to authenticate at every request, either plain HTTP or HTTPS. This means that every request now take the form of: 2) Use an approach like Amazon's S3 with private and public key: http://www.thebuzzmedia.com/designing-a-secure-rest-api-without-oauth-authentication/ 3) Use a cookie anyway and break the stateless part of REST. The second approach appears better to me, but I am left wondering do I really have to re-invent this whole thing? Hashing, storing, generating the keys, etc all by myself? This sounds a lot like using session in a typical web application and rewriting the entire stack yourself, which usually to me mean "You're doing it wrong" especially when dealing with security. EDIT: I guess I should have mentioned OAuth as well.
563e326f61a801306526795f	X	No, there is absolutely no need to use a cookie. It's not half as secure as HTTP Digest, OAuth or Amazon's AWS (which is not hard to copy). The way you should look at a cookie is that it's an authentication token as much as Basic/Digest/OAuth/whichever would be, but less appropriate. However, I don't feel using a cookie goes against RESTful principles per se, as long as the contents of the session cookie does not influence the contents of the resource you're returning from the server. Cookies are evil, stop using them.
563e326f61a8013065267960	X	Don't worry about being "RESTful", worry about security. Here's how I do it: Step 1: User hits authentication service with credentials. Step 2: If credentials check out, return a fingerprint, session id, etc..., and pop them into shared memory for quick retrieval later or use a database if you don't mind adding a few milliseconds to your web service turnaround time. Step 3: Add an entry point call to the top of every web service script that validates the fingerprint and session id for every web service request. Step 4: If the fingerprint and session id aren't valid or have timed out redirect to authentication. READ THIS: RESTful Authentication
563e326f61a8013065267961	X	Use a cookie anyway and break the stateless part of REST. Don't use sessions, with sessions your REST service won't be well scalable... There are 2 states here: application state (or client state or session s) and resource state. Application state contains the session data and it is maintained by the REST client. Resource state contains the resource properties and relations and is maintained by the REST service. You can decide very easy whether a particular variable is part of the application state or the resource state. If the amount of data increases with the number of active sessions, then it belongs to the application state. So for example user identity by the current session belongs to the application state, but the list of the users or user permissions belongs to the resource state. So the REST client should store the identification factors and send them with every request. Don't confuse the REST client with the HTTP client. They are not the same. REST client can be on the server side too if it uses curl, or it can create for example a server side http only cookie which it can share with the REST service via CORS. The only thing what matters that the REST service has to authenticate by every request, so you have to send the credentials (username, password) with every request. Cookies are not necessarily bad. You can use them in a RESTful way until they hold client state and the service holds resource state only. For example you can store the cart or the preferred pagination settings in cookies...
563e326f61a8013065267962	X	I too found that method- bit it is defined as #grants ⇒ Array (readonly) and as such cannot be used to SET grants- just GET grants
563e326f61a8013065267963	X	just scroll down to the bottom of the page and see different methods like grant_full_control etc
563e326f61a8013065267964	X	I have a bucket that already has a few grantees with full access permission. I want to add a new grantee (with full access) to the bucket using the aws-sdk gem (aws api v2). I can't seem to figure out how to do this. I am able to get the bucket and it's associated ACL- but I can't figure out how to update the grants list of the ACL and put it back to S3?
563e327061a8013065267965	X	The amazon api documentation lists all the methods through which you can do this: http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/BucketAcl.html#grants-instance_method Hope this helps
563e327061a8013065267966	X	I have an inbound payload in JSON format. I'm converting it using the "JSON to Object" converter, and then passing on the data to a component (as a JsonData object.) My component then returns the same JsonData object with modifications. I'm trying to use the Amazon S3 component as the next step in my flow, and trying to tie the bucket name and other values to elements accessible in the JsonData object. Here is the expression for the bucket name for instance: From experience this has worked with JSON. However when I run this, here is what I get: Message : Failed to invoke getObjectContent. Message payload is of type: JsonData Code : MULE_ERROR-29999 Is there a way I can use my JsonData object and pull information from it, or do I have to convert it back to something else before passing it on to the Amazon S3 component? Thanks,
563e327061a8013065267967	X	Remove the empty space from your expression: #[json:TopKey/BucketName]
563e327061a8013065267968	X	After trying a little more to play with my expression, I figured out I can just access elements the way I do it in my Java component already: and I have my BucketName!
563e327061a8013065267969	X	You can set the "Return Class" to java.util.Map in the "JSON to Object" processor, you can then access the value via #[payload.TopKey.BucketName]
563e327061a801306526796a	X	What you described has been done by doodle.com Keep working at it though, you will learn a lot.
563e327161a801306526796b	X	I agree with Nick that you need a web service of your own. Don't let your client (presentation layer) talk directly to your persistence layer (SDB, S3, SQL). Put your own web service in between. FYI, AWS recently started a free tier, so you can get started on either GAE or AWS for free now.
563e327161a801306526796c	X	@Spike nice about AWS for free! I'll test that out too now :)
563e327261a801306526796d	X	Cool, thanks for the feedback! I have decided to go with GAE on this one. You were absolutly correct though in saying what I am describing is a web app that uses an Android device as the main client. I wasn't looking at it that way where I most def should have been.
563e327261a801306526796e	X	I'm currently developing my first Android application and still in the designing stage trying to come up with a solid model. My application will use the GCal data from a users Google calendar and sync it up with one or more other users to determine common meeting times between all without the tedious back and forth of scheduling over email. I vision this working by storing each user and their calendar data in a database that will be refreshed daily. When a query to determine the optimal meeting times between a group is issued, I want to select the calendar data of each user from the database, perform the computation to find optimal times, and display the results back to the user who made the query. The AWS SDK for Android supports Amazon SimpleDB and S3, in which case I would use SimpleDB for my database. Where I am getting lost is using the Amazon EC2 web service in concert with the SimpleDB to perform the computation. First off, any feedback on my approach and/or design is appreciated. Second, how does using non-Android, but Java based APIs/SDKs effect applications, or is it even possible to do so? The API typica for Java looks interesting and useful if it is possible to use with Android for instance. Thanks!
563e327361a801306526796f	X	So, I think its important to note a couple of things. What you are describing is not an 'android application'. Its a web service application with an android client. The reason I'm being pedantic is that many of the design decisions you need to make are completely besides the fact that your primary client will run on android. I'm concerned about the viability of storing the users calendar in a non-relation database. I don't know if you've already looked through this, but the problem you are trying to solve (calendaring) seems like it would benefit from the relational benefits of a relational database. For instance, i'm not sure how you would structure for storage the data of past, present and future events/meetings in a non-relational. Its probably possible, but i'm not sure if its optimal. Depending on the amount of data you may also need to consider the maximum record size. While its true that AWS SDK for android supports writing to S3 or SimpleDB, I think there is a lot to consider. The reason you are confused about the interaction with EC2 is that normally, your EC2 web service will be interacting with S3 or SimpleDB. By using the AWS SDK you can, in theory, remove the requirement for a web service. My main issue with that is that you're now forced to do lots more on each client because there is no common access pattern. Your ios client or web client needs to have all the same logic that your android client has to make sure its accessing your s3 and simple db data the same. If that doesn't make sense i can elaborate. Using non-android api's and sdks is a mixed bag. Sometimes it works fine if the classes compile to Davlik. If they don't it doesn't work. One thing I might point out, since you'll already possibly be tied to a Google technology is Google App Engine. The nice part about it is that there is a free level of service which lets you get your app up and running without cost. Based on the technologies you are suggesting, it might be something for you to look into. Other than that, my other strong suggestion is that you focus on building out the web service first and independently of the android client. Take the time to model what the client server interaction would be and move as much of the 'logic' to the server as is possible. Thats what I felt like was missing from your initial description. Where the crunching would be.
563e327361a8013065267970	X	my solution is that you use O-O principles. store your db on amazon dynamoDB and then sync user data with the mobile app. then you do processing of the data/computation on the device before displaying the results
563e327461a8013065267971	X	possible duplicate of Reliable data serving
563e327461a8013065267972	X	For the sake of brevity consider a facebook style image content serving app. Users can upload content as well as access content shared by other people. I am looking at best ways of handling this kind of file serving application through Java servlets. There is surprisingly little information available on the topic. I'd appreciate if someone can tell me their personal experiences on a small setup (a few hundred users). So far I am tempted to use the database as a file system (using mongodb) but the approach seems cumbersome and tedious and will need replicating part of the functionality already provided by OS native filesystems. I don't want to use commercial software or have the bandwidth to write my own like facebook. All I want is to be able to do this through free software on a small server with a RAID or something similar. A solution that scales well to multiple servers would be a plus. The important thing is to serve it through java servlets (I am willing to look into alternatives but they have to be usable through java). I'd appreciate any help. Any references to first hand experiences would be helpful as well. Thanks.
563e327461a8013065267973	X	Guru - I set up something exactly like this for members of my extended family to share photos. It is a slightly complicated process that includes the following: 1) Sign up for Amazon Web Services, notably their S3 (Simple Storage Service). There is a free storage tier that should cover the amount of users you described. 2) Set up a web application that accepts uploads. I use Uploadify in combination with jQuery and ajax, to upload to a servlet that accepts, scans, logs, and does whatever else I want with the file(s). On the servlet side, I use ESAPI's upload validation mechanism, part of the validation engine, which is just built on top of Commons File Upload, which I have also used by itself. 3) After processing the file(s) appropriately, I use JetS3t as my Java-AmazonS3 API and upload the file to Amazon S3. At that point, users can download or view photos depending on their level of access. The easiest way I have found to do this is to use JetS3t in combination with the Web Application Authentication to create Temporary URL's, which give the user access to the file for a specific amount of time, after which the URL becomes unusable. A couple of things, if you are not concerned with file processing and trust the people uploading their files completely, you can upload directly to Amazon S3. However, I find it much easier to just upload to my server and do all of my processing, checking, and logging before taking the final step and putting the file on Amazon S3. If you have any questions on the specifics of any of this, just let me know.
563e327461a8013065267974	X	While Owens suggestion is an excellent one, there is another option you can consider - what you are describing is a Content Repository. Since you have sufficient control of the server to be able to install a (non-commercial) piece of software, you may be interested in the Apache Jackrabbit* Content Repository. It even includes a Java API, so you should be able to control the software (at least as far as adding, and extracting content) from your Servlets. Actually, if you combine this idea with Owens and expand on it, you could actually host the repository on the Amazon S3 space, and use the free-tier Amazon EC2 instance to host the software itself. (Although, I understand that the free-tier EC2 instance is only free for the first year) HTH NB. I'm sure other content repositories exist, but JackRabbit is the only one I've played with (albeit briefly).
563e327561a8013065267975	X	What is the endpoint ? Your S3 bucket ? If so this means you allow anyone to upload file. You would need AWS client to do it or use an intermediary server.
563e327561a8013065267976	X	@inmyth I am uploading through a rails server. Plus I am able to upload files using POSTMAN perfectly. So I am guessing there shouldn't be an issue with the server.
563e327561a8013065267977	X	I am using Retrofit library to upload media files (multipart) from my Android application. The servers are on Amazon using S3. I am getting this following error : Few points : 1. I have tested the API using POSTMAN and it is working perfectly. (no issues with max upload size as well.) 2. Weirdly this is running(uploading) successfully in one of my phones ie Moto E. The phone it is not working includes Moto G2 and Xperia SP as of now. 3. I am able to make normal requests through Retrofit successfully. Its uploading media files that is an issue. Here is the code to upload : I have been researching on this issue for a while now and cannot come to a solution. I don't want to switch to another library as this should work for me as well. Any help will be highly appreciated. TIA
563e327561a8013065267978	X	are the tables all/predominantly MyISAM or InnoDB?
563e327561a8013065267979	X	InnoDb for most tables... I have kept MyISAM on the 1 table we perform full text searches on
563e327661a801306526797a	X	have you tried using mk-query-digest (in maatkit) or mtop to determine where the resource usage is heaviest? the slow query log and the query profiler?
563e327661a801306526797b	X	are you implying sprocs are bad ?
563e327661a801306526797c	X	In MySQL, the implementation is pretty horrible. The code is not parsed, the ASCII source of the procedure is stored on disk. Each connection parses the procedure and caches the parsed code in its connection structure, there is no sharing between connections. Additionally, the implementation is incomplete (somewhat corrected in 5.5) and hard to debug. Even without these problems, stored procedures are code that is running on the most expensive and least easily scaled CPUs in a system, and hence are generally a bad idea (counterexamples exist).
563e327661a801306526797d	X	and yet, it will still be significantly faster (and require less network resources) than bringing the data to your app to do the processing and then sending he results back over the network
563e327661a801306526797e	X	guess i'd better stick to 100's of middle tier to db svr calls then
563e327661a801306526797f	X	I have an Amazon s3 instance and the project we have on the server does a lot of INSERTs and UPDATEs and a few complex SELECTs We are finding that MySQL will quite often take up a lot of the CPU. I am trying to establish whether a higher memory or higher cpu is better of the above setup. Below is an output of cat /proc/meminfo Current Setup: High-CPU Extra Large Instance 7 GB of memory 20 EC2 Compute Units (8 virtual cores with 2.5 EC2 Compute Units each) 1690 GB of instance storage 64-bit platform I/O Performance: High API name: c1.xlarge Possible Setup: High-Memory Double Extra Large Instance 34.2 GB of memory 13 EC2 Compute Units (4 virtual cores with 3.25 EC2 Compute Units each) 850 GB of instance storage 64-bit platform I/O Performance: High API name: m2.2xlarge
563e327661a8013065267980	X	I would go for 32GB memory and maybe more harddisks in RAID. CPU won't help that much - you have eough cpu power. You also need to configure mysql correctly. Defragment the query cache to better utilize its memory. FLUSH QUERY CACHE does not remove any queries from the cache, unlike FLUSH TABLES or RESET QUERY CACHE. However I noticed that the other solution has the half disk space: 850GB, which might be reduced number of hard disks. That's generally a bad idea. The biggest problem in databases is hard disks. If you use RAID5 - make sure you don't use less hard disks. If you don't use raid at all - I would suggest raid 0.
563e327661a8013065267981	X	It depends on the application. You could use memcached to cache mysql queries. This would ease cpu usage a bit, however with this method you would want to increase RAM for storing the queries. On the other hand if it's not feasible based on type of application then I would recommend higher CPU.
563e327661a8013065267982	X	There are not many reasons for a MySQL to use a lot of CPU: It is either processing of stored routines (stored procedures or stored functions) or sorting going on that can eat CPU. If you are using a lot of CPU due to stored routines, you are doing it wrong and your soul cannot be saved anyway. If you are using a lot of CPU due to sorting going on, some things can be done, depending on the nature of your queries: You can extend indexes to include the ORDER BY columns at the end, or you can drop the ORDER BY clauses and sort in the client. What approach to chose depends on the actual cause of the CPU usage - is it queries and sorting? And on the actual queries. So in any case you will need better monitoring first. Not having monitoring information, the general advice is always: Buy more memory, not more CPU for a database.
563e327761a8013065267983	X	Doesn't the on-demand nature of EC2 make it rather straightforward to rent the possible setup for a day, and do some load testing? Measurements speak louder than words.
563e327761a8013065267984	X	Use "High-CPU Extra Large Instance". In your current setup, MySQL is not constrained by memory: Out of 7 GB memory, 2 GB is unused and being used by OS as I/O cache. In this case, increasing CPU count would give you more bang for buck.
563e327761a8013065267985	X	Use vmstat and iostat to find out whether CPU or I/O is the bottleneck (if I/O - add more RAM and load data into memory). Run from shell and check results:
563e327761a8013065267986	X	Well running management server on premise is not a prime requirement.
563e327761a8013065267987	X	Then Cloudify should meet your needs. Give it a shot. The support forum is at: cloudifysource.zendesk.com/home - in case you have any questions.
563e327761a8013065267988	X	I am planning to migrate few products on Cloud which will be used as a platform for the developer community. In short I am trying to host PaaS vendor for my products which can be consumed by developers for build and development process. The plan is as below: What I am trying to achieve is as follows: Evaluation so far: I tried looking at Eucalyptus and it sounds promising, but I am still not able to find out if this will be supporting the public cloud setup as my requirement is. I believe this is more like a private cloud setup. If anyone can help me compare the other available options, that would help me solving my issue. For e.g. RightScale, OpenStack, CloudStack, Nimbula etc.
563e327761a8013065267989	X	There are several PaaS providers out there. There is a comparison here: Looking for PaaS providers recommendations Disclaimer: I work for GigaSpaces, developing the Cloudify open-source PaaS stack. Cloudify answers most of your requirements, especially vendor independence - it supports a large number of IaaS providers, including: EC2, HP, Rackspace, Azure and others. Cloudify does require its management server to run in the same cloud as the applications it runs so it can collect monitoring information using private communications rather then over the internet. Why do you want to run your management server on-premise?
563e327861a801306526798a	X	What would you recommend from easy of use/administration/maintance perspective: Luwak or RiakCS? I only need fault-tolerant storage for large (gigabytes range) files.
563e327861a801306526798b	X	@Moonwalker - I would highly suggest trying out RiakCS. It's basically designed to do exactly what you're asking, built on top of Riak. Trying to us Luwak inside Riak was our first go at things ... this is better.
563e327861a801306526798c	X	Brian, thank you.
563e327861a801306526798d	X	Say, if I store movies, etc., using a Riak database, how do I stream the binaries in chunks to any client (which could be a download-then-play or direct play?) Is Riak recommended for storing large binary files? Also, I've read somewhere that the maximum file is 50 MB or otherwise it will cause problems, but that seems to be old documentation. Could anyone provide more information?
563e327861a801306526798e	X	We do not recommend storing objects over 50M for performance reasons. Nothing has changed in that regard. Given that, the answer is Riak is not well suited for what you are describing. We have developed an enterprise product, RiakCS, for a distributed file storage solution (Amazon S3 compatible API) but this is not an open source project. Edit to add: We were attempting to incorporate Luwak for large object/file support but unfortunately we are no longer doing so. That project is of course available on github should others want to continue work. Updated: We have now open-sourced Riak CS. See: http://basho.com/riak-cloud-storage/
563e327861a801306526798f	X	Any updates on this @Edmar Miyake? Did you figure out a way to backup ES on GCE?
563e327961a8013065267990	X	I found that this is a feature currently scheduled to be supported in elasticsearch-cloud-gce 2.5.1 (the next release of the plugin) github.com/elastic/elasticsearch-cloud-gce/issues/11
563e327961a8013065267991	X	@chrishiestand Unfortunately I ended up using Amazon S3. It's nonsense since Amazon and GCE are competitors.
563e327961a8013065267992	X	Thanks for your suggestion. I understand the interoperability between S3 and GCS. But It doesn't necessary mean that elasticsearch is interopable with GCS by using Amazon S3 plugin for elastic search. By normal backup you mean creating a disk snapshot? That would make me create a backup for every elasticsearch node I have, right?
563e327961a8013065267993	X	@EdmarMiyake not so much a full disk snapshot, just do a normal, local filesystem backup with elasticsearch then copy that to cloud storage. Hopefully this is making sense, as I'm not overly familiar with how elasticsearch works.
563e327961a8013065267994	X	I have e elasticsearch environment configured on GCE (Google Compute Engine) with two nodes, therefore two VM's and I need to create a backup strategy for this. I first thought I could use the elasticsearch snapshot API to backup all my data to a given storage as the API supports a few ways to store the snapshot. I tried to used the shared filesystem option, but it requires that the store location be shared between nodes. Is there a way I can do this on GCE? }' nested: RepositoryVerificationException[[backup] store location [/elasticsearch/backup] is not shared between node I know there is a AWS plugin for elasticsearch for storing the backups. Is there any plugin for Google Cloud Storage? Is is possible to do that? If any of those alternatives above are not possible, is there any other recommended strategy for backing-up my data?
563e327961a8013065267995	X	You may be able to use the S3 plugin with Google Cloud Storage by way of interoperability. See this page for more details. Alternatively, you can just create a normal backup in the filesystem then upload it to cloud storage using gsutil.
563e327961a8013065267996	X	I am having the same problem with my ES cluster (5 nodes) on Google Cloud. We can't use local backups on the actual disk as Jon mentioned above since not every node has all the data in my case. It seems to me that only way is to create a small machine with a large disk and mounting that disk as a shared drive on all 5 ES nodes I have in the same path so that we can use the "Shared filesystem" option.
563e327a61a8013065267997	X	I'm developing an application that uses (lots) of image processing. The general overview of the system is: My current situation is that I have almost no expertise in image hosting nor large data uploading and managing. What I plan to do is: My doubts are regarding the process time. I don't want to upload it directly to my server, because it would require a lot of traffic and create a bottleneck, so using Amazon S3 would reduce that. And hosting images with Amazon would not be that good, since they don't provide specific API's to deal with images as Cloudinary does. Working with separate servers for uploading, and only triggering my server when upload is done by the browser is ok? Using Cloudinary for hosting images is also something that makes sense? Sending to Amazon, instead of my own server (direct upload to my server) should be avoided? (This is more a guidance/design question)
563e327a61a8013065267998	X	Why wouldn't you prefer uploading directly to Cloudinary? The image can be uploaded directly from the browser to your Cloudinary account, without any further servers involved. Cloudinary then notifies you about the uploaded image and its details, then you can perform all the image processing in the cloud via Cloudinary. You can either manipulate the image while keeping the original, or you may choose to replace the original with the manipulated one.
563e327a61a8013065267999	X	Update: ended up using api.imgur.com as a storage and management tool for images. Unfortunately response time for about 600 images is very slow (10 - 20 sec.) and returning json is heavy (147 kb). So I've built middleware layer on EC2 to filter out redundant json attributes and cache result json on S3.
563e327a61a801306526799a	X	Thanks, will take a look at S3 and write if it's acceptable for me.
563e327b61a801306526799b	X	I'm trying to find appropriate image hosting for my mobile app. I have been using flickr but it's not possible to proceed with it because due to (new?) terms and conditions I'm not allowed to upload images that do not produced by me. I'm using the following flickr features: I have been looking at: Thank you in advance!
563e327b61a801306526799c	X	For your use, you can use Amazon S3. It is great and it just works perfectly! A possible cheaper option is Google. Google docs now supports all file types, so you can load the images up to a Google docs folder, and share the folder for public access. The URL's are kind of long though. There are various options for resizing etc and it's Google so obviously their API is stable and perfect. Google only charges USD5/year for 20GB. There is a full API for uploading photos, docs etc.
563e327b61a801306526799d	X	I am not sure if you would like to have a self hosted image hosting service for your apps. If yes, I would say check out ImageS3, https://github.com/images3/images3-play. An image hosting service run on top of Amazon S3 and it is like other image hosting service you have been looking at, the only different is that it is open source.
563e327b61a801306526799e	X	Thank you for your answer. This already helped me. By "bootstrapping" I meant "prefilling" the intial HTML Page that is requested by the client with data in json format on the server. This would reduce initial ajax calls to the Rest API and may speed up rendering on the client. But I think this concept is already out of the window, because we will probably go with a Single Page App on the client which receives data solely via the Rest API (no templating/view definitions on the server). I found this guide ng-newsletter.com/posts/aws-js-sdk.html which also may help me setting this up.
563e327b61a801306526799f	X	Cool, I'll update my answer. If it really helped would you mind upvoting it :) If it answered your question then you could accept it by clicking the green tick to the left.
563e327b61a80130652679a0	X	Sorry I don't have enough rep to upvote your answer, but I will accept it. Could you also elaborate on the minify/compress methods? The link you provided seems just like an online tool - I think what I need is some sort of Java App which does that automatically on the Server as soon as I upload my JavaScript/CSS files into EC2/S3.. Is there maybe an existing solution for that? Thanks for your effort!
563e327b61a80130652679a1	X	Yeah I'll add to my example above, basically if you build your application with Maven, there's a plugin you can download that I've used quite a lot that will minify a folder of resources (css/js)
563e327c61a80130652679a2	X	Our new start-up company is trying to build a mobile app with an accompanied website. We are trying to setup our application on Amazon Web Services. We have Java code running in an EC2 instance, which will store data in S3. We want clients (iOS and Web for now) to communicate with the Java Backend via a REST API. Ideally the website would be hosted under the same AWS account. The Java Code and REST API are already set up in a very basic form, but the setup of the Website is unclear, since this is new to us all. We also would like to evaluate beforehand if such a setup is even feasible. Since I am in charge of the Website i have already spend hours researching this specific setup, but i simply lack experience in cloud/backend development to come to a conclusion. Here are some questions we would like to answer: Please point me into the right direction, any comment is appreciated.
563e327c61a80130652679a3	X	Where would the HTML files and accompanied JavaScript etc. files be stored? Either on the same AWS EC2 box or a different one, just give it a static IP and link that IP to the domain you want, done. Just remember to have port 80 open as a firewall rule. How can data (images etc.) that is stored within S3 by the JAVA code be accessed from the Website directly? The files will have some url that you can link to directly in your html so it's essentially just a url. How could something like bootstrapping of data within HTML files be achieved (in JSON format preferably)? You have a number of choices here. You could potentially create some JSP files to generate the HTML and load the JSP files on access and cache them so they load up super fast. You could argue however, this is overkill and in some ways, the REST endpoint should be robust enough to handle the requests. Part of me thinks you should endeavor to use the REST API for this information so you just have to manage one endpoint, why make an extra endpoint or over engineered solution for the HTML that you then have to maintain? Build once and reuse. How could the server be set up to compress certain files like CSS or JavaScript? During the build process, run the files through a minify process. This is built into maven so you could do it automatically or by hand using something like jscompress. This Minify plugin shows how to automatically minify your resources. Consider you'll need to be using Maven though as your build tool.
563e327c61a80130652679a4	X	Unfortunately the accounts do not have any IAM users (and there is not a default one)
563e327c61a80130652679a5	X	Most likely not then. I have never seen a method that would allow access to that data from the interface
563e327c61a80130652679a6	X	Assigning the bounty to tkone even if not able to find out a way of doing it, as I am not sure by now one exists. Thanks for taking the time to respond
563e327c61a80130652679a7	X	been looking up and down through aws docs -- i see nothing that would let you do this. thanks for the tip. if i see anything in my experiences i'll surely update this.
563e327d61a80130652679a8	X	Given a set of Amazon credentials (not the username and password, just the API credentials), is there a programmatic way of finding out when that account was created? I am guessing if the user has an EBS volume, S3 object, I could ask the date it was created. Is there a better way?
563e327d61a80130652679a9	X	In the IAM service there a GetUser command. That seems to be the most relevant. If that doesn't work, then it might not be possible.
563e327e61a80130652679aa	X	Good suggestion, i will check if it's possible to implement this without an additional webserver as we won't be able to get one for this.
563e327e61a80130652679ab	X	It's totally possible, I suggested the dedicated server just to share the load. In any case even on a single server, consider using a different virtual host for the last point I mentioned
563e327e61a80130652679ac	X	I'm accepting this solution now since it seems to be the solution that scales best with the least overhead, now i just have to get it through to management.
563e327e61a80130652679ad	X	Just wanted to give more feedback: 9 Month and many meetings and discussions later and we switched several of our main pages to this solution, works flawlessly since a week while having minimal load on the server we recycled for this. There are already 12GB Images in the on-demand generated cache. Thanks again for the excellent solution.
563e327f61a80130652679ae	X	I recently set up something similar to this (dynamic image resizing, with CloudFront and S3). Images are dynamically resized on the first request, then served directly from S3 on subsequent requests. Python code is available here: github.com/mikery/res3izer
563e327f61a80130652679af	X	Even better than caching them locally... request the images via a CDN and then the CDN will cache all the generated images for you and serve them much faster than you can, with cheaper bandwidth costs. That's how we do it and it's extremely effective.
563e327f61a80130652679b0	X	+1 I absolutely second this! Why set up an expensive on-the-fly server when what you need can be achieved with household tools.
563e327f61a80130652679b1	X	@Greg Beech: it may be hard to find a CDN which will be happy to cache these kind of files.
563e328061a80130652679b2	X	@cherouvim - I would be surprised if you can't find a CDN happy to cache these kind of files. It's what you pay them for after all. We use Akamai, who do a grand job.
563e328061a80130652679b3	X	Thanks for the good suggestion but an external provider is (sadly) not an option (management won't approve it, we already tried to get something similar through)
563e328161a80130652679b4	X	nice suggestion! can this technique be used to support exact height and width ?\
563e328161a80130652679b5	X	Thanks for the suggestions and the code, i'll benchmark it and check if it could be fast enough (and i will create a PHP benchmark, too, to check if my coworkers were right with "PHP is too slow").
563e328261a80130652679b6	X	I have to discard this solution as the available budget is exactly 0 Euro (or for you US folks: exactly $0). We couldn't even get management to approve installing a test or development server (but that it now takes us twice the time to develop is no problem because developers cost nothing as they are already there...). Still, thanks for your suggestion.
563e328261a80130652679b7	X	A dedicated image server AND a CND network can work well together. You will take load away fro your main site and the images will still be delivered very fast across the globe.
563e328261a80130652679b8	X	my company has recently started to get problems with the image handling for our websites. We have several websites (adult entertainment) that display images like dvd covers, snapshots and similar. We have about 100'000 movies and for each movie we have an average of 30 snapshots + covers. Almost every image has an additional version with blurring and overlay for non-members, this results in about 50 images per movie or a total of 5 million base images. Each of the images is available in several versions, depending on where it's placed on the page (thumbnail, original, small preview, not-so-small preview, small image in the top-list, etc.) which results in more images than i cared to count. Now i had the idea to use a server to generate the images on-the-fly since it became quite clumsy to generate all the different images for all the different pages (as different pages sometimes even need different image sizes for basically the same task). Does anyone know of an image processing server that can scale down images on-the-fly so we only need to provide the original images and the web guys can just request whatever size they need? Requirements: Security is not that much of a concern as i.e. the unblurred images can already be reached by URL manipulation and more security would be nice but it's not required and frankly i stopped caring (After failing to get into my coworkers heads why (for our small reseller page) it's a bad idea to use http://example.com/view_image.php?filename=/data/images/01020304.jpg to display the images). We tried PHP scripts to do this but the performance was too slow for this many users. Thanks in advance for any suggestions you have.
563e328361a80130652679b9	X	I suggest you set up a dedicated web server to handle image resize and serve the final result. I have done something similar, although on a much smaller scale. It basically eliminates the process of checking for the cache. It works like this: EDIT: I don't think that PHP itself would slow the process much, as PHP scripting in this case is reduced to a minimum: the image scaling is done by a builtin library written in C. Whatever you do you'll have to use a library like this (GD or libmagick or so) so that's unavoidable. With my system at least you totally skip the overhead of checking the cache, thus further reducing PHP interaction. You can implement this on your existing server, so I guess it's a solution well suited for your budget.
563e328361a80130652679ba	X	Based on We tried PHP scripts to do this but the performance was too slow for this many users. I'm going to assume you weren't caching the results. I'd recommend caching the resulting images for a day or two (i.e. have your script check to see if the thumbnail has already been generated, if so use it, if it hasn't generate it on the fly). This would improve performance dramatically as I'd imagine the main/start page probably has a lot more hits than random video X, thus when viewing the main page no images have to be created as they're cached. When User Y views Movie X, they won't notice the delay as much since it just has to generate that one page. For the "On-the-fly resize" aspect - how important is bandwidth to you? I'd want to assume you're going through so much with movies that a few extra kb in images per request wouldn't do too much harm. If that's the case, you could just use larger images and set the width and height and let the browser do the scaling for you.
563e328361a80130652679bb	X	The ImageCache and Image Exact Sizes solutions from the Drupal community might do this, and like most solutions OSS use the libraries from ImageMagik There are some AMI images for Amazons EC2 service to do image scaling. It used Amazon S3 for image storage, original and scales, and could feed them through to Amazons CDN service (Cloud Front). Check on EC2 site for what's available Another option is Google. Google docs now supports all file types, so you can load the images up to a Google docs folder, and share the folder for public access. The URL's are kind of long e.g. http://lh6.ggpht.com/VMLEHAa3kSHEoRr7AchhQ6HEzHVTn1b7Mf-whpxmPlpdrRfPW216UhYdQy3pzIe4f8Q7PKXN79AD4eRqu1obC7I Add the =s paramter to scale the image, cool! e.g. for 200 pixels wide http://lh6.ggpht.com/VMLEHAa3kSHEoRr7AchhQ6HEzHVTn1b7Mf-whpxmPlpdrRfPW216UhYdQy3pzIe4f8Q7PKXN79AD4eRqu1obC7I=s200 Google only charge USD5/year for 20GB. There is a full API for uploading docs etc Other answers on SO http://stackoverflow.com/questions/236139/how-best-to-resize-images-off-server/236264
563e328361a80130652679bc	X	Ok first problem is that resizing an image with any language takes a little processing time. So how do you support thousands of clients? We'll you cache it so you only have to generate the image once. The next time someone asks for that image, check to see if it has already been generated, if it has just return that. If you have multiple app servers then you'll want to cache to a central file-system to increase your cache-hit ratio and reduce the amount of space you will need. In order to cache properly you need to use a predictable naming convention that takes into account all the different ways that you want your image displayed, i.e. use something like myimage_blurred_320x200.jpg to save a jpeg that has been blurred and resized to 300 width and 200 height, etc. Another approach is to sit your image server behind a proxy server that way all the caching logic is done automatically for you and your images are served by a fast, native web server. Your not going to be able to serve millions of resized images any other way. That's how Google and Bing maps do it, they pre-generate all the images they need for the world at different pre-set extents so they can provide adequate performance and be able to return pre-generated static images. If php is too slow you should consider using the 2D graphic libraries from Java or .NET as they are very rich and can support all your requirements. To get a flavour of the Graphics API here is a method in .NET that will resize any image to the new width or height specified. If you omit a height or width, it will resize maintaining the right aspect ratio. Note Image can be a created from a JPG, GIF, PNG or BMP:
563e328361a80130652679bd	X	In the time that this question has been asked, a few companies have sprung up to deal with this exact issue. It is not an issue that's isolated to you or your company. Many companies reach the point where they need to look for a more permanent solution for their image processing needs. Services like imgix serve as a proxy and CDN for image operations like resizing and applying overlays. By manipulating the URL, you can apply different transformations to each image. imgix serves billions of requests per day. You can also stand up services on your own and put them behind a CDN. Open source projects like imageproxy are good for this. This puts the burden of maintenance on your operations team. (Disclaimer: I work for imgix.)
563e328461a80130652679be	X	If each different image is uniquely identifiable by a single URL then I'd simply use a CDN such as AKAMAI. Let your PHP script do the job and let AKAMAI handle the load. Since this kind of business doesn't usually have budget problems, that'd be the only place I'd look at. Edit: that works only if you do find a CDN that will serve this kind of content for you.
563e328461a80130652679bf	X	My application needs import a .zip file from a brazilian FTP server. It works nice in my development machine whereas I'm using Net/Ftp Ruby native lib or using linux wget. My Problem is the connection time out in the production environment in heroku: "Connecting to ftp.xxxxxx.com.br (ftp.xxxxxx.com.br)|xxxx.xx.xxx.xxx|:21...". I have tried use wget in a Digital Ocean's virtual machine and the issue still the same. So, I get the problem is the brazilian host is refusing connection from foreign IP. Have you idea how bypass this problem? I'm going to try hire a host here in Brazil or even create a Amazon's EC2 instance in Sao Paulo region (will it get a brazilian IP?) just to host a API application who downloads the file from the FTP server and upload it to Amazon S3, where I have fully access to any file.. Have you a better solution?
563e328461a80130652679c0	X	Is this safe? Maintaining security using a pre-signed url with AWS S3 Bucket object? Another words - part 1... say I'm storing a bunch of separate individual's files in a bucket. I want to provide a link to a file for a user. Obviously, each file is uniquely but consecutively named, I don't want people to be able to change the link from 40.pdf to 30.pdf and get a different file. This URL seems to do that. part 2, and more importantly.... Is this safe or is a it dangerous method of displaying a URL in terms of the security of my bucket? Clearly, i will be giving away my "access key" here, but of course, not my "secret". Already answered 3 years ago... sorry. How secure are Amazon AWS Access keys?
563e328461a80130652679c1	X	AWS Security Credentials are used when making API calls to AWS. They consist of two components: A Signed URL is a method of granting time-limited access to an S3 object. The URL contains the Access Key and a Signature, which is a one-way hash calculated from the object, expiry time and the Secret Key. A Signed URL is safe because: However, anyone can use the URL during the valid time period. So, if somebody Tweets the URL, many people could potentially access the object until the expiry time. This potential security threat should be weighed against the benefit of serving traffic directly from Amazon S3 rather than having to run your own web servers.
563e328461a80130652679c2	X	Is there any chance config.eager_load is off? Or you're testing this with config.eager_load off? cache_classes? I'd expect to see this behavior if we aren't in production mode.
563e328461a80130652679c3	X	Our codebase doesn't say "eager_load" at all, so I'm guessing it's off, both in dev and in prod. Cache_classes is false in dev, and true in production, as you'd expect. To clarify, this works slowly in production, and fast in dev, but I believe it's the SSD drive. eager_load doesn't seem to have an effect of whether CarrierWave hits the disk for each mounted file, does it?
563e328561a80130652679c4	X	For others finding this, we're also discussing this here.
563e328561a80130652679c5	X	I have a model where I'm using Carrierwave to let users upload their avatars. I have one version for the avatar, cropped to a square called "cropped". In one of my pages, where I'm listing a few thousand users, each with their avatar, the call to generate the URL for these images is taking a long time. The problem seems to be that just by accessing to the model attribute that has the uploader mounted, Carrierwave seems to be hitting the disk to either read the file, check that it exists, or something. In other words, calling: All of these hit the HDD. In the worst case I found, for one of our clients that have a massive amount of data, the page renders in 10 seconds my dev machine, and 30 seconds on the server. This page render implies about 1200 calls to "user.avatar.cropped". The difference seems to be due to SSD vs HDD (or maybe overhead due to VM's, not sure). OS Disk Caching does kick in, though, since rendering the same page for a second time in the server takes 10 seconds, presumably since disk hits are cached. If I generate the URL "manually", instead of using CarrierWave, it's back down to 10 seconds in all machines, so it's definitely Carrierwave that's causing the slowdown. It seems like hitting the HDD shouldn't be necessary to generate the path. Is there any way to avoid this, or to work around this problem? (NOTE: I know 10 seconds is atrocious for a page anyway, we're doing stuff to solve this, but site-wide, we have lots of pages that show tons of avatars, and we're getting a slowdown due to these HDD hits, so we'd like to improve them, without having to manually generate the URLs, since the abstraction Carrierwave provides is awesome) UPDATE (due to comment below): We're not using eager_load. Cache_classes is off in dev, and on in production. (Again, dev is fast, prod is slow, but I don't think it's related to cache_classes)
563e328561a80130652679c6	X	I ran into the same issue with Carrierwave and S3. Here is my scenario, I have an API responsible of uploading avatars into S3, by default a user has an avatar that is stored in the assets folder of my API. If 10 users have a default avatar they all use the same image in order to avoir to uploads a bunch of default avatar to S3. I have created a custom method that essentially checks if an avatar has been uploaded or not. If an avatar has been uploaded we build manually the S3 path to the avatar depending on the size. Otherwise we just return the default avatar path which is stored locally in our assets folder. This works for a public amazon s3 bucket, I'm not sure what would happen with a private bucket or how to generate the expire parameters, but since you're using the file storage system it shouldn't be a problem. This instruction was the key for generating the name of the file and not hitting S3 to get the img path back. I had a time response of 2seconds before, now I'm at 300-500ms Hope this helps!
563e328861a80130652679c7	X	What is your definition of 'web service'?
563e328861a80130652679c8	X	web service means normal SOAP web services...
563e328861a80130652679c9	X	Hi Maksym, I am working on java...the way you suggest but our client may use .Net to download the file. as my understanding Amazon S3 will provide a server where they will stage the file and we can request the file by there API..is it possible to use webservice communication with Amazon S3 server to download and upload the files.?
563e328961a80130652679ca	X	Yes, it's possible, see my anwser update.
563e328961a80130652679cb	X	thanks..Maksym .. i will try the same
563e328961a80130652679cc	X	I have a requirement where I need to upload a large file (can be 10 GB) to a shared space(windows) ( say APP1) . and we have a separate application( say APP2) different network now I need to download the same file from in second application via internet. My approach is I need to create webservice to upload the document to shared space. and then expose a webservice for outer world to download the document. My point is how I can manage the huge files upload/download through webservice ? Please suggest if some one have any idea. I have flexibikity to use any third party APIs.but the application can talk only through webservices.
563e328961a80130652679cd	X	From your question it's not really clear which development platform you mean, .NET, Java, etc. Also it's important to know how interoperable your services should be, security requirements, etc. Anyway will try to come up with a couple of solutions which you might research in more detail if you found them useful. .NET It's relatively easy to built such a web service with WCF. It supports streaming which could be interoperable, reliable and secure to some extend. You can read more on this here. This approach implies you have a huge disk to store files and you have a good recovery plan for that in case it goes down or just dies. .NET, Java, etc. - cloud based There are a lot of vendors who provide cloud storage and APIs to work with it. It's an ideal solution for a quick start. They take care of data availability, redundancy, etc. All you have to do is to use their API to upload and download files and to pay them for this :) In many cases it's worth it. Personally I used to work with Amazon S3. Their API is simple to use and there's plenty of docs for that. EDIT: Amazon S3 provides a simple web-services interface that can be used to store and retrieve any amount of data, at any time, from anywhere on the web. I think you should take a look at Amazon S3 overview here. This also provides API for a number of different platforms - Java, .NET, Node.js, etc. You an find the full list here. Hope it helps!
563e328c61a80130652679ce	X	@user2939212 I understand that, but in some cases such as mine, there are so many parameters that you reach a point time when you might find that solution inevitable :)
563e328c61a80130652679cf	X	@user2939212 yeah, and moreover, you cannot name them in argv[], unless you use dash options -param1 val1 -param2 val2 etc. The process becomes cumbersome and error-prone.
563e329461a80130652679d0	X	Not done extensive testing, but Northern California seems to do the same thing as Ireland. Not sure which is the desired result.
563e329461a80130652679d1	X	Good question and an annoying difference indeed - I'm pretty sure all regions but US Standard will return the same result (i.e. a dedicated object/key for the simulated directory), see my answer for details on this.
563e329461a80130652679d2	X	To ensure the analysis is actually digging into the correct origin: How have these buckets/keys been created in the first place? Or more specifically, have these been created by a 3rd party service/tool or by yourself via the S3 API or the AWS Management Console?
563e329461a80130652679d3	X	Good point, I have keys that have been uploaded using the console, and also some created by boto. I think I will look into if they consistently behave the same way for both.
563e329461a80130652679d4	X	+1 for following up with your analysis, thanks! Quite some confusing behind the scene magic indeed ...
563e329461a80130652679d5	X	The AWS console is creating an empty file named 'bucketName/someDir/' which is quite confusing; you can delete that empty "file" from the command line and then that clears up the issue
563e329461a80130652679d6	X	Not very good solution if you have files without extension.
563e329461a80130652679d7	X	I've noticed a difference between the returns from boto's api depending on the bucket location. I have the following code: which im running against two buckets, one in us-west and one in ireland. Path in this bucket is a sub-directory, against Ireland I get the sub directory and any keys underneath, against us-west I only get the keys beneath. So Ireland gives: where as US Standard gives: Obviously, I want to be able to write the same code regardless of bucket location. Anyone know of anything I can do to work around this so I get the same predictable results. Or even if it's boto causing the problem or S3. I noticed there is a different policy for naming buckets in Ireland, do different locals have their own version of the api's? Thanks, Steve
563e329461a80130652679d8	X	Thanks to Steffen, who suggested looking at how the keys are created. With further investigation I think I've got a handle on whats happening here. My original suposition that it was linked to the bucket region was a red herring. It appears to be due to what the management console does when you manipulate keys. If you create a directory in the management console it creates a 0 byte key. This will be returned when you perform a list. If you use boto to create/upload a file then it doesn't create the folder. Interestingly, if you delete the file from within the folder (from the AWS console) then a key is created for the folder that used to contain the key. If you then upload the bey again using boto, then you have exactly the same looking structure from the UI, but infact you have a spurious additional key for the directory. This is what was happening to me, as I was testing our application I was clearing out keys and then finding different results. Worth knowing this happens. There is no indicator in the UI to show if a folder is a created one (one that will be returned as a key) or an interpreted one (based on a keys name).
563e329561a80130652679d9	X	I don't have a definite answer for your question, but can throw in some partial ones at least: Amazon S3 doesn't actually have a native concept of folders/directories, rather is a flat storage architecture comprised of buckets and objects/keys only - the directory style presentation seen in most tools for S3 (including the AWS Management Console itself) is based solely on convention, i.e. simulating a hierarchy for objects with identical prefixes - see my answer to How to specify an object expiration prefix that doesn't match the directory? for more details on this architecture, including quotes/references from the AWS documentation. I noticed there is a different policy for naming buckets in Ireland, do different locals have their own version of the api's? That's apparently the case indeed for Amazon S3 specifically, which is one of their oldest offerings, see e.g. Bucket Restrictions and Limitations: In all regions except for the US Standard region, You must use the following guidelines when naming a bucket. [...] [emphasis mine] These specifics for the US Standard region are seen in other places of the S3 documentation as well, and US Standard is an unusual construct itself compared to the otherwise clearly geographically constrained Regions: US Standard — Uses Amazon S3 servers in the United States This is the default Region. The US Standard Region automatically routes requests to facilities in Northern Virginia or the Pacific Northwest using network maps. To use this region, select US Standard as the region when creating a bucket in the console. The US Standard Region provides eventual consistency for all requests. [emphasis mine] This implicit CDN behavior is unique for this default Region of S3 (i.e. US Standard) and not seen elsewhere on any other AWS service I think. I have a faint memory of S3 actually placing a zero byte object/key into a bucket for the simulated directory/folder in more recent regions (i.e. all but US Standard), whereas the legacy solution for the US Standard region might be different, for example simply based on the established naming convention for directory separation by / and omitting a dedicated object/key for this altogether. If the analysis is correct, there is nothing you can do but maintain separate code paths for both cases, I'm afraid Good luck!
563e329561a80130652679da	X	I've had the same problem. As a work around you can filter out all the keys with a trailing '/' to eliminate the 'directory' entries.
563e329561a80130652679db	X	I'm using the fact that a "Folder" has no "." in its path. A file does. media/images will not be deleted media/images/sample.jpg will be deleted e.g. clean bucket files
563e329561a80130652679dc	X	Genius as ever - cheers Darin, that's fixed it
563e329661a80130652679dd	X	I'm using fineuploader for uploading images to an ASP.NET 4.5 Web API controller. The controller is hit ok and the image is uploaded. I get a 200 response from the API controller. I'm displaying a thumbnail after successful upload and the filename to the location within Amazon S3 is supposed to come back in the JSON response. When I try this in Chrome or IE10 it works fine. When I try this in Firefox, the responseJson that comes back is an empty object, which is displayed as a failure. I'm using jQuery 1.8.2 and fineuploader 3.2. I actually get undefined displayed on the screen because responseJson.message is undefined. Here's the js code: The response that comes back is application/json. What do I need to do to get this working properly in Firefox?
563e329661a80130652679de	X	The difference between FF and Chrome is the Accept request header being sent. Just use FireBug and Chrome developer toolbar to compare the results between the 2 browsers: FF: Chrome: So as you can see FF is not sending the correct Accept header and the Web API's content negotiation mechanism simply falls back to text/xml (because that's what the client requested). Fortunately the plugin allows you to override the request headers using the customHeader property and force it to the expected type (application/json in your case):
563e329761a80130652679df	X	Which file system is currently in use on the EBS volume?
563e329761a80130652679e0	X	The file system is ext2, which is why I don't understand why it's having a hard time. Isn't that one of the most basic types? I will take your advice and snapshot the volume. Thanks for that tip.
563e329761a80130652679e1	X	I would like to use an EBS volume with data on it that I've been working with in an Ubuntu AMI in a RedHat 6 AMI. The issue I'm having is that RedHat says that the volume does not have a valid partition table. This is the fdisk output for the unmounted volume. Disk /dev/xvdk: 901.9 GB, 901875499008 bytes 255 heads, 63 sectors/track, 109646 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Disk /dev/xvdk doesn't contain a valid partition table Interestingly, the volume isn't actually 901.9 GB but 300 GB.. I don't know if that means anything. I am very concerned about possibly erasing the data in the volume by accident. Can anyone give me some pointers for formatting the volume for RedHat without deleting its contents? I also just checked that the volume works in my Ubuntu instance and it definitely does.
563e329761a80130652679e2	X	I'm not able to advise on the partition issue as such, other than stating that you definitely neither need nor want to format it, because formatting is indeed a (potentially) destructive operation. My best guess would be that RedHat isn't able to identify the file system currently in use on the EBS volume, which must be advertized by some means accordingly. However, to ease with experimenting and gain some peace of mind, you should get acquainted with one of the major Amazon EBS features, namely to create point-in-time snapshots of volumes, which are persisted to Amazon S3: These snapshots can be used as the starting point for new Amazon EBS volumes, and protect data for long-term durability. The same snapshot can be used to instantiate as many volumes as you wish. This is detailed further down in section Amazon EBS Snapshots: Snapshots can also be used to instantiate multiple new volumes, expand the size of a volume or move volumes across Availability Zones. When a new volume is created, there is the option to create it based on an existing Amazon S3 snapshot. In that scenario, the new volume begins as an exact replica of the original volume. [...] [emphasis mine] Therefore you can (and actually should) always start experiments or configuration changes like the one you are about to perform by at least snapshotting the volume (which will allow you to create a new one from that point in time in case things go bad) or creating a new volume from that snapshot immediately for the specific task at hand. You can create snapshots and new volumes from snapshots via the AWS Management Console, as usual there are respective APIs available as well for automation purposes (see API and Command Overview) - see Creating an Amazon EBS Snapshot for details. Good luck!.
563e329761a80130652679e3	X	Check out firebase.com. It's in beta though.
563e329761a80130652679e4	X	Firebase.com looks good.
563e329761a80130652679e5	X	I'm really surprised that this is the only one out there.
563e329761a80130652679e6	X	update: I ended up using mongolab.com. It has CORS enabled which was exactly what I needed.
563e329861a80130652679e7	X	Currently the firebase has the free plan
563e329861a80130652679e8	X	Hey I'm making a small project and I would like to use a JSON style database service where I can set and get parts of the JS object. I would like this to be completely on the front end and not require any server technologies. It dosent need to be secure as its just a hobby project. Security is a bonus. I'm currently using HTML5 local storage. I'm happy to pay for this as a service.
563e329861a80130652679e9	X	https://www.firebase.com/ does not have free plan. Direction to go seems to select NoSQL db like MongoDB or CouchDB, and then search/compare for db as service providers. Quick search gives 10 Online Storage APIs That is file oriented APIs Box.net: The “Box Enabled” platform offers a choice of SOAP, REST or XML-POST APIs. Below is Fireloader, a Firefox extension which allows you to upload, and download photos, files and videos using a familiar FTP like interface. In this version Flickr, Picasa, Box.net and Youtube are supported. Cellblock: This multimedia sharing service offers a REST-based API. ref http://webapps.stackexchange.com/questions/8247/seeking-online-hosted-database-web-service-with-rest-api
563e329861a80130652679ea	X	You may also want to check out www.ttmsolutions.com/restjee. Its a lightweight server-side JSON ORM that doesn't require you to develop any server-side code and works with any DB.
563e329861a80130652679eb	X	This site is for programming questions. Backup strategies are WAY off topic.
563e329861a80130652679ec	X	No problem. Checked the FAQ and thought it were covered by "software tools commonly used by programmers" and "practical, answerable problems that are unique to the programming profession" –
563e329861a80130652679ed	X	+1 Redundant storage and data backups are separate problems. I would add that S3 buckets do support versioning and Multi-factor authentication for deleting objects. 99% of data restores I've seen were of the "Oops! I deleted this. Can you get it back?" variety.
563e329861a80130652679ee	X	S3 has a way to version files.
563e329861a80130652679ef	X	Hi AvkashChauhan, good answer IMHO. Can you just share some best practices to synch data between an S3 bucket and another with RSS? With s3synch? From an EC2 instance?
563e329961a80130652679f0	X	Say Amazon deletes your account (your credit card expires, someone logs into your account and requests it be deleted, or some other method). You lose your files.
563e329961a80130652679f1	X	Thanks. Is it not an option for me to download it manually - or save it to a harddisk in house.
563e329961a80130652679f2	X	I have just finished setting up Amazone S3 as a CDN for our website. From now on we will host tons of picture directly in the cloud. It is cool! - but leave me with a problem in regards to backup. Earlier we backup up everything by uploading the pictures to amazon once a day, but I really don't want to backup to the same place I host the files. How do I backup from Amazone S3 in the most efficient way? I have considered: Any solution I do not now of? A solution like myrepono.com for Amazone s3 would be cool. Cheers, Peter
563e329a61a80130652679f3	X	One of the main things not addressed by Amazon S3 at the moment is any concept of a history or recycle bin for files that are deleted or modified. The reason for this is not to address Amazon's vulnerability to failure of the service, but to address malicious or accidental removal of files by someone with the access credentials, and not having any recovery option. This single point of failure still is a weakspot for AWS services, and even on ones with snapshot capability (RDS, EC2, but not S3), it still is a problem because a malicious user with access could remove snapshots with the same single entry point. You have to differentiate between the following scenarios: 1) Oops, the infrastructure failed and I lost my file! (very unlikely with S3) 2) Oops, I (or a client) accidentally deleted that file on purpose but I want it back! 3) Oops, That API script on my system was supposed to only remove one file, but due to a programming error it looped through all of them and deleted all of them! 4) Oops, someone got access to my AWS account and deleted my files! 1 is very low worry based on how S3 works. 2 you can program around by making systems utilize a recycling bucket for intentional deletions. But right now 3 and 4 leave you with real and substantive data loss, and don't have a solution on S3 itself that can solve it. So you either have to hope these last two things don't happen, or you are left keeping periodic backups of S3 elsewhere, which at the moment is really cumbersome. I think the best thing would for Amazon to add some sort of deletion retention automatically as a feature, meaning that any delete of any kind could be reversed for 48 hours or something before it was gone for good.
563e329a61a80130652679f4	X	I am very much interested to know the main concern why you wouldn't believe that you still need backup solution after moving to cloud because sooner later everyone will be looking cloud for main storage and backup storage and another backup storage (if needed). I think the time is already now. My point is that when you have data stored to any Cloud storage, you can depend on storage service provider 99.999999% SLA. These cloud service replicate the data for multiple copies to meet the SLA requirement and sometime copy data to separate location to avoid problems related with a scenario when whole data center at one location completely shutdown. When data is geo-copied it actually satisfy the backup requirement itself. For example with Windows Azure Blob Storage: With Amazon S3 you already have backup storage with ability to get data based on versioning: Finally if i really want to backup my data which is already in Amazon S3, I would like to use "RRS" as backup to my S3 data because:
563e329a61a80130652679f5	X	I'm not sure why you don't want to "backup to the same place as [you] host the files" when Amazon S3 is the hosting solution. If you are worried about a single point of failure, here's what Amazon says about S3's reliability and durability: Amazon S3 provides a highly durable storage infrastructure designed for mission-critical and primary data storage. Objects are redundantly stored on multiple devices across multiple facilities in an Amazon S3 Region. To help ensure durability, Amazon S3 PUT and COPY operations synchronously store your data across multiple facilities before returning SUCCESS. Once stored, Amazon S3 maintains the durability of your objects by quickly detecting and repairing any lost redundancy. Amazon S3 also regularly verifies the integrity of data stored using checksums. If corruption is detected, it is repaired using redundant data. In addition, Amazon S3 calculates checksums on all network traffic to detect corruption of data packets when storing or retrieving data. Amazon S3’s standard storage is: Amazon S3 provides further protection via Versioning. You can use Versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. This allows you to easily recover from both unintended user actions and application failures. By default, requests will retrieve the most recently written version. Older versions of an object can be retrieved by specifying a version in the request. Storage rates apply for every version stored. If you want to increase redundancy, you can store your data in multiple S3 regions. Now, if you REALLY want to sync your files between multiple cloud storage providers, you can use CloudBerry to synchronize your data between multiple cloud providers. They support S3, Google Storage, Azure Blog Storage and Rackspace Cloud files. Pro: you pay a one-time fee for the tool. Con: You need a witness server to run CloudBerry.
563e329a61a80130652679f6	X	my stupid answer, Use a Service of AWS called "AWS Import/Export" ,now you can backup to your own HDD
563e329b61a80130652679f7	X	I am writing an application where users are required to show their photo, however as my server resources are very limited I can not let them upload it to the server. So I have three major questions: 1. How to properly validate photo URL? At least I can validate with regexp, however I need to check for file ending: 2. Security issues? XSS? Even I validate the picture at the moment of creation, hacker can replace image with malicious stuff anytime. 3. Maybe there are free asset stores with API?
563e329b61a80130652679f8	X	1. How to properly validate photo URL? You can use a plugin that validates the format of an URL or write it your self: 2. Security issues? XSS? There aren't any outstanding security issues as long as you escape the text. <%=h image_tag obj.photo_url %> is safe. Take in mind, that the user can still use a 100MB image that will slow down every visitor. 3. Maybe there are free asset stores with API? There aren't any that I know of, but rackspace cloud, amazon s3 hosting is pretty cheap. Some image upload plugins have support for these two, so you'll at least save some time.
563e329b61a80130652679f9	X	You are right, when you use S3, Content-Dispotion is set on upload. Using other methods, such as the send_file method, will allow you to set the Content-Dispoition on sending. However, send_file only works when you are sending a file from your filesystem, therefore you can not use that in conjunction with S3. I did figure out that I could set the Content-Dispotion of a file but still send it as an inline element. For example, if an image has the Content-Disposition set to "attachment" it could still be used as the src for an image tag, while download if you put the image URL in the browser.
563e329b61a80130652679fa	X	Ah, that's good to know. Sounds like it would be best to upload them all as "attachment" if they still work as inline "src" for images.
563e329c61a80130652679fb	X	I am using Amazon S3 to store and serve user content for user accounts. I need the ability to serve the files either inline (sometimes urls for images will be in blog posts, etc) or as a download. By default when uploading a file to my S3 bucket, the file has no Content-Disposition set (which is fine because it will server inline as long as the browser recognizes the file MIME), however at times I will need to set the Content-Disposition to attachment in order to download the file. Using Rails/S3 gem, is it possible to send a request to Amazon to specify that the file should be sent with the Content-Disposition set to attachment (or vise-versa) for just that request? Possibly it could use some sort of token (in reference to a token for the request, typically used for authenticated reads…just wondering if that can help me in this situation too)? Using the S3 gem, I know how to set and save the Content-Disposition for each file, but that would cause the file to always be downloaded and could not be used as an image inline (not tested). Short of having two files (one with and without the Content-Disposition='download' set), any ideas? Thanks in advance. PS I using rails Rails 2, attachment_fu and the aws-s3 gem (I can't change these because the above mentioned app is apart of a much larger, already running app and I know conflicts exist between the aws-s3 and aws_right gem)
563e329c61a80130652679fc	X	Just in case anybody stumbles upon this old post, Amazon's API now allows for changing the Content-Disposition for files stored on S3. Read the announcement here.
563e329c61a80130652679fd	X	I believe the Content-Disposition is set upon upload, so if you don't want two copies of the file each with their own Content-Disposition, one way would be to stream it from a controller using send_file http://api.rubyonrails.org/classes/ActionController/Streaming.html#method-i-send_file
563e329c61a80130652679fe	X	It doesn't do multi-part upload in parallel, which hurts its speed. It also spews error messages about S3 failing (that's what S3 does, get over it) and tries to slow down even more.
563e329c61a80130652679ff	X	AWS CLI fully saturated my connection at 7.3 MB/s and did not fail. I tried S3 Tools for a 1GB file, and it kept showing errors and uploaded at only 2-3 MB/s.
563e329c61a8013065267a00	X	What amazon s3 client do you use in linux with multipart upload feature? I have 6GB of zip files to upload and s3curl is not possible due to maximum limit of 5GB only. Thanks. James
563e329c61a8013065267a01	X	I use S3 Tools, it will automatically use the multipart upload feature for files larger than 15MB for all PUT commands: Multipart is enabled by default and kicks in for files bigger than 15MB. You can set this treshold as low as 5MB (Amazon’s limit) with —multipart-chunk-size-mb=5 or to any other value between 5 and 5120 MB Once installed and configured, just issue the following command: Alternatively, you could just use split from the command-line on your zip file: and recombine later on your filesystem using: If you choose the second option, you may want to store MD5 hashes of the files prior to upload so you can verify the integrity of the archive when it's recombined later.
563e329c61a8013065267a02	X	The boto library includes an s3 command line tool called s3put that can handle multipart upload of large files.
563e329d61a8013065267a03	X	The official AWS Command Line Interface supports multi-part upload. (It uses the boto successor botocore under the hood): The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. On top of this unified approach to all AWS APIs, it also adds a new set of simple file commands for efficient file transfers to and from Amazon S3, with characteristics similar to the well known Unix commands, e.g.: So cp would be sufficient for the use case at hand, but be sure to check out sync as well, it is particularly powerful for many frequently encountered scenarios (and sort of implies cp depending on the arguments).
563e329d61a8013065267a04	X	You could mount the S3 bucket to the filesystem.
563e329d61a8013065267a05	X	You can have a look at the FTP/Amazon S3/Glacier client CrossFTP.
563e329d61a8013065267a06	X	Personally I created python file s3upload.py with simple function to upload large files using boto and multipart upload. Now every time I need to upload large file, I just run command like this: More details and function code can be found here.
563e329d61a8013065267a07	X	Based on this sample http://aws.amazon.com/articles/0006282245644577, it is clear how to use multipart upload using the AWS iOS SDK. However, it seems that my uploads are not stitched together correctly when I try to resume an interrupted upload. I use the code below to resume an upload. Is this the correct way to set the upload id of a multipart upload? I'd appreciate any help or pointers.
563e329d61a8013065267a08	X	Your code should be robust enough to handle cases where you may need to track which parts were uploaded. Part Uploads of the multipart upload can be done in many ways (either in parallel, multithreaded manner or one after the other in sequence). Whatever the above approach may be, you can use the listParts API to determine how many parts were successfully uploaded. Since you would already have the upload ID your design must support the ability to continue from the following part upload. Another useful resource to help optimize multipart uploads: http://aws.typepad.com/aws/2010/11/amazon-s3-multipart-upload.html
563e329d61a8013065267a09	X	I have an article on the website I help maintain, that I want to share on LinkedIn, via the "Share an update" form on the site. The possible thumbnail images are being detected, and LinkedIn is receiving the correct urls for the images in question (they are served from Amazon's S3 service). Inspecting the page, I see that a call is made by the page to https://www.linkedin.com/sharing/api/url-preview and the JSON response includes a "previewImages" field (under "data"."content") which is an array of objects/dicts with the fields "url", "mediaProxyUrl", "width", "height", and "size". The "url" of my preview images is correct. Copying and pasting into the address bar brings it up. The "mediaProxyUrl" however does not load an image. The "size" field is null. Using a working reference url (an article on another site), I can see that "mediaProxyUrl" is supposed to be linkedin's url for the thumbnail, and that "size" is supposed to be the file size of the original image. So, why are my preview thumbnails blank? Is this a problem on LinkedIn's end? Is Amazon s3 the problem? I'm at a loss. P.S. I've checked my og:image and og:image:secure_url headers, they're in order.
563e329e61a8013065267a0a	X	I have an application where customers upload files like Powerpoints and Excel spreadsheets to the application through a web UI. The files then have meta data associated with them and they are stored as BLOBs in a MySQL database. The users may download these files occasionally, but not very often. The emphasis here is on archiving. Security of data is also important. If that is the case, what are the pros and cons of storing the files as BLOBs in MySQL as opposed to putting them on Amazon S3? I've never used S3 before but hear that it's popular for storing files.
563e329e61a8013065267a0b	X	The main advantage of relational databases (such as MySQL) is the elegance it permits you to query for data. BLOB columns, however, offer very little in terms of rich query semantics compared to other column types, so If that's your main use case, there's hardly any reason to use a relational database at all, it doesn't offer much above and beyond a regular filesystem or simple key-value datastore (such as s3). Dollars to bytes, s3 is likely much more cost effective. On the other hand, there are some things that a relational database can bring that would be worhtwhile. The most obvious is transactional semantics (only on the InnoDB engine, not available with MyISAM), so that you can safely know that whole groups of uploads or modifications take place consistencly. Another advantage is that you can still add metadata about your blobs (even if it's only over time, as your application improves) so you can still benefit some from the rich queries MySQL supports.
563e329e61a8013065267a0c	X	storing binary data into blob there is no true security If you are archiving the binary data, store into normal disk file If security is important, consider separate between your UI server and storage server, but is hard to archive, you can always consider to embed password / encryption into these binary files security over amazon s3
563e329e61a8013065267a0d	X	Security of data is also important. Do note that files on S3 are not stored on encrypted disks, so you may have to encrypt client-side or on your servers before sending it up to S3.
563e329e61a8013065267a0e	X	I've been storing data in S3 for years and completely love it! What I do is upload the file to S3 (where its copied multiple times by the way) and then store a reference to the file path and name into my MySQL files table. If anything else, it takes that much load off of the MySQL DB and S3 now offers AES256 bit encryption with revolving master keys so you know its secure!
563e329e61a8013065267a0f	X	On my android application I use the dropbox API. I hardcode the app key and secret. But to authenticate I need to log in using the dropbox account. But whats the point of using a app key and secret if you have to enter a username and password. Also what if you would like other people to be able to upload to your dropbox without using the accounts username and password. Can they use the app key and secret to just upload to the account without entering the accounts username/password?
563e329e61a8013065267a10	X	You are a bit confused with what are app key/secret used to do. In briefly, a pair of app key/secret is used to identify an app. Is it a valid app? Is it authorized by user? And is it out of API call limit/throttling? Therefore, only with key/secret, app has no right to access an unauthorized user's private data. I'm sorry but I have to say allowing people upload data to your own cloud is not a good idea. At least, dropbox is not for that purpose. Instead, why not try some other cloud storage service, like Amazon S3?
563e329f61a8013065267a11	X	use any file hosting (tinypic.com or imagehost.org)
563e329f61a8013065267a12	X	If the admin site manages the other site that shows the content, I'd question splitting them into two seperate sites. Usually I see setups where the main site is www.mysite.com and www.mysite.com/admin or admin.mysite.com for the administrative pages. Really makes everything alot simpler. What is the major motivation to split everything to two sites?
563e329f61a8013065267a13	X	I've got two web sites (written in c#) which are pretty common: Images should be stored outside these two sites but where and how?
563e329f61a8013065267a14	X	You could store them on a third site for static content, then both sites would link to that content. That can give you some benefits if you really need scalability as well. However, you may also store it in a shared database, or just a shared file share, that you have encapsulated logic that each sites that needs access to them uses. If you dont want to both with the storage there are many "cloud" storage servers that will host all your images for you. (like Amazon S3, Smugmug, flickr). But then you will have to build the logic in your app to upload the submitted images to your 3rd party storage provider.
563e329f61a8013065267a15	X	You can store images inside a database. There are plenty of tutorials on how to store images in a database (images in mysql php, images in mysql asp.net, etc). The language isn't important, any of the tutorials can help show how to store images inside the database itself. Then it's just a matter of language specific calling the image out to display it.
563e32a061a8013065267a16	X	Have you considered storing your images on something like Amazon S3? You could then access the images from both sites. Somehow, though, you'll need to store the URLs to the images and make them available for both sites. If both sites run off of the same database, this is fairly easy, however, if they are separate databases, then I would suggest creating some sort of API on the creation site so that the content site can easily discover which images are available to be shown.
563e32a061a8013065267a17	X	The difference is only in definition. A file is an entity inside a file system. Since e.g. S3 is not exactly a file system, they call their entities something else. You can think of them as the same.
563e32a161a8013065267a18	X	How do Walrus, S3 or any cloud storage system, take in a file and convert it to object programmatically?
563e32a161a8013065267a19	X	If you're asking how does S3 lay its objects out on disk on its internal servers, we don't know (the implementation details are not public.) What they are very likely doing is taking your object's key (bucket + path) and consulting a consistent hash that maps your object to a set of servers. The upload is directed to one of these servers (essentially at random) which stores it and enqueues future work to propagate the new object to the other servers responsible for replicating it. This replication delay is the underlying need for eventual consistency. I also heard once somewhere that Amazon uses an error-correcting encoding at the storage level to further defray bad reads.
563e32a161a8013065267a1a	X	To get the general understanding of how Cloud Object Storage systems stores the objects(Binary Files), you can read the documentation of Swift Object Storage-Openstack. Swift is similar to Amazon S3 and hence to Walrus. Swift communicates through the Proxy Server to Clients(Outside of the Cluster) and Client can store,delete the objects through the RESTful HTTP API. The server maintains the Ring-Configuration file that maintains the mapping between files and their physical location. When there is a request to upload a file, MD5 hash is calculated from the path of the file. Please find the details at: Openstack Swift Architecture Swift Documentation
563e32a161a8013065267a1b	X	I'm not familiar with the Ruby SDK, but S3 only allows you to list 1000 objects at a time, so listing 100,000 objects is going to result in at least 100 HTTP requests. If you want to check for the existence of a particular object then sending a HEAD request for that object is the best way. It sounds like you want to check that one or more files match a given prefix, can you not just adapt your existing prefix search to include the sub dir name?
563e32a161a8013065267a1c	X	hi, aws ping take 288 ms - 0.3 sec х 10000= 3000sec = 50min, i`ts very long.
563e32a161a8013065267a1d	X	AWS-SDK list ruby code: fail error: "Unable to find marker in S3 list objects response" Structure of directory ... more of 100 000 obj I want to verify that the directory(for example "1474472") was created my plan: aws-s3-list-> ruby-array->find in array (array.include?) !!!need very fast method - soon the end of the world :)
563e32a261a8013065267a1e	X	There is no such stuff as folders in Amazon S3. It is a "flat" file system. Have a look into this answer. What you really are looking for is verifying whether a given prefix ("/myshop/products/1474472", for instance) exists in your bucket. Their REST API definitely supports it, have a look into the documentation. You need to list the keys (which would be the "file names") matching a given prefix, that can be passed as parameter. You can also optimize your call by setting the max-keys parameter to 1. That way, if you receive any non-zero amount of items in the response, the bucket already contains files with names starting with the given prefix.
563e32a261a8013065267a1f	X	I have a file host website thats burning through 2gbit of bandwidth, so I need to start adding secondary media servers to store the files. What would be the best way to manage a multiple server setup, with a large amount of files? Preferably through php only. Currently, I only have around 100Gb of files... so I could get a 2nd server, mirror all content between them, and then round robin the traffic 50/50, 33/33/33, etc. But once the total amount of files grows beyond the capacity of a single server, this wont work. The idea that I had was to have a list of media servers stored in the DB with the amounts of free space left on each server. Once a file is uploaded, php will choose to which server the file is actually uploaded to, and spread out all the files evenly among the servers. Was hoping to get some more input/inspiration. Cant use any 3rd party services like Amazon. The files range from several bytes to a gigabyte. Thanks
563e32a261a8013065267a20	X	If you are doing as much data transfer as you say, it would seem whatever it is you are doing is growing quite rapidly. It might be worth your while to contact your hosting provider and see if they offer any sort of shared storage solutions via iscsi, nas, or other means. Ideally the storage would not only start out large enough to store everything you have on it, but it would also be able to dynamically grow beyond your needs. I know my hosting provider offers a solution like this. If they do not, you might consider colocating your servers somewhere that either does offer a service like that, or would allow you install your own storage server (which could be built cheaply from off the shelf components and software like Freenas or Openfiler). Once you have a centralized storage platform, you could then add web-servers to your hearts content and load balance them based on load, all while accessing the same central storage repository. Not only is this the correct way to do it, it would offer you much more redundancy and expandability in the future if you endeavor continues to grow at the pace it is currently growing. The other solutions offered using a database repository of what is stored where, would work, but it not only adds an extra layer of complexity into the fold, but an extra layer of processing between your visitors and the data they wish to access. What if you lost a hard disk, do you lose 1/3 or 1/2 of all your data? Should the heavy IO's of static content be on the same spindles as the rest of your operating system and application data?
563e32a261a8013065267a21	X	You could try MogileFS. It is a distributed file system. Has a good API for PHP. You can create categories and upload a file to that category. For each category you can define on how many servers it should be distributed. You can use the API to get a URL to that file on a random node.
563e32a261a8013065267a22	X	Your best bet is really to get your files into some sort of storage that scales. Storing files locally should only be done with good reason (they are sensitive, private, etc.) Your best bet is to move your content into the cloud. Mosso's CloudFiles or Amazon's S3 will both allow you to store an almost infinite amount of files. All your content is then accessible through an API. If you want, you can then use MySQL to track meta-data for easy searching, and let the service handle the actual storage of the files.
563e32a261a8013065267a23	X	i think your own idea is not the worst one. get a bunch of servers, and for every file store which server(s) it's on. if new files are uploaded, use most-free-space first*. every server handles it's own delivery (instead of piping through the main server). pros: use multiple servers for a single file. e.g. for cutekitten.jpg: filepath="server1\cutekitten.jpg;server2\cutekitten.jpg", and then choose the server depending on the server load (or randomly, or alternating, ...) if you're careful you may be able to move around files automatically depending on the current load. so if your cute-kitten image gets reddited/slashdotted hard, move it to the server with the lowest load and update the entry. you could do this with a cron-job. just log the downloads for the last xx minutes. try some formular like (downloads-per-minute*filesize*(product of serverloads)) for weighting. pick tresholds for increasing/decreasing the number of servers those files are distributed to. if you add a new server, it's relativley painless (just add the address to the server pool) cons: homebrew solutions are always risky your load distribution algorithm must be well tested, otherwise bad things could happen (everything mirrored everywhere) constantly moving files around for balancing adds additional server load * or use a mixed weighting algorithm: free-space, server-load, file-popularity disclaimer: never been in the situation myself, just guessing.
563e32a361a8013065267a24	X	Thanks, I know about the existence of OGM, but you are correct, I should be more specific. I will create a new post with more detailed information about my problem. Thank you for your time! karma++!
563e32a361a8013065267a25	X	After checking Hibernate and discovering that Hibernate support relational databases (Like MySQL) I would like to know if it support Amazon RDS. I am assuming that it does because of the ORM thing, but since I am very newbie I decided to come here and ask to the people who know. Assuming that Hibernate can read from my MySQL database, I would also like to know if there is a way for it to actually support key-value database, like Amazon S3. I searched but I simply couldn't find anything else. Finally, can someone let me know the level of support that Hibernate has for MySQL joins? I would like to know if it supports them and how much of it is supported.
563e32a361a8013065267a26	X	if there is a way for it to actually support key-value database, like Amazon S3 There's a separate project for this, called Hibernate OGM. It aims to support JPA for NoSQL databases. Check it out. can someone let me know the level of support that Hibernate has for MySQL joins? I would like to know if it supports them and how much of it is supported Joins are definitely supported through Criteria API, and definitely in HQL. You need to be more specific on what you really want to achieve in order to get a more complete answer.
563e32a361a8013065267a27	X	How can I create that kind of server ? In Visual Studio, choose File => New Project. Then write the code.
563e32a361a8013065267a28	X	what do you mean ?
563e32a361a8013065267a29	X	This question is way too broad and prone for opinion-based answers.
563e32a361a8013065267a2a	X	what do you mean ? Roll up your sleeves and get to it
563e32a361a8013065267a2b	X	How can I "let the cloud client deal with it" ?
563e32a361a8013065267a2c	X	@user2992413 did you ever use dropbox or google drive? When you install their client application it creates local folder and automatically synchronizes it with server. You just need to copy the file. In case it's too big and you want to make sure that it has been uploaded before your user shuts down the pc, you might be needing to add some code to determine upload status.
563e32a461a8013065267a2d	X	But I want the program to do it by itself
563e32a461a8013065267a2e	X	then use API dropbox.com/developers/core/docs#files_put with 'HttpClient' msdn.microsoft.com/en-us/library/…
563e32a461a8013065267a2f	X	I made a program that saves information on .dat files. On each time the program is closed the old information is replaced by the new information. When I open the program, it reads the information from the dat files. I want to create a server that I will be able to upload the .dat files to it and on each Form_Close the old information will be deleted from the sever and the new information will be uploaded to the server. When I open the program, I want it to delete the .dat files on the computer and replace them by the information from the server. What is the best choise of service for that kind of problem ? I thought maybe I should use Google Drive. But I don't know if it's a good choise. Thanks for helping.
563e32a461a8013065267a30	X	You have tons of options here, which one is better depends on your project and goals, but all of them will work.
563e32a461a8013065267a31	X	My client has requested for his database to be stored such that he can access via the Android and iOS versions of his mobile software. Is Amazon Web Services a good option for this? I'm looking into something that is quick, yet scalable. Thanks for your advice. Pier.
563e32a561a8013065267a32	X	Just to answer this, I implemented it eventually using Amazon SimpleDB and S3, the API is pretty easier to get into.
563e32a561a8013065267a33	X	I'm using the aws-sdk gem and I'm trying to basically upload a really large file (it takes 2 days). The file is uploaded in chunks but sometimes the script will crash and I would like to resume uploading (the next chunk). During uploading we want to close close the multipart upload (so we can access the s3 data that has been uploaded so far). Is it possible to add a part after the multipart upload is closed? (say the next day) basically resuming the upload?
563e32a661a8013065267a34	X	Is it possible to add a part after the multipart upload is closed? (say the next day) basically resuming the upload? Not as such, but you can simulate the affect you desire. Uploading Objects Using Multipart Upload API allows you to upload a single object as a set of parts: Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. [emphasis mine] This is further detailed in Complete Multipart Upload: You first initiate the multipart upload and then upload all parts using the Upload Parts operation (see Upload Part). [...] Upon receiving this request, Amazon S3 concatenates all the parts in ascending order by part number to create a new object. [...] You must ensure the parts list is complete, this operation concatenates the parts you provide in the list. [...] [emphasis mine] That is, the upload operation is finalized here and cannot be resumed by uploading another part. (Technically speaking the upload ID required for any operation on an initiated multipart upload is not available/valid anymore). You can simple initiate a new multipart upload and upload your previously uploaded S3 object as the first part of this new multipart object by means of the Upload Part - Copy operation, which Uploads a part by copying data from an existing object as data source.
563e32a661a8013065267a35	X	Champion! I upvote this,because it is the thing i was searahing for! With the use of httpwebrequest from c# you can EASY upload an image and get the link to it :) thnx!
563e32a661a8013065267a36	X	Hi! I'm trying to use deviantsart.com API but I keep getting a XMLHttpRequest cannot load http://deviantsart.com/. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://localhost:8080' is therefore not allowed access. I'm doing a POST using jQuery and image data gotten from a canvas. Do you know what may be wrong? Thanks!
563e32a661a8013065267a37	X	check out enable-cors.org/index.html
563e32a661a8013065267a38	X	What is the format you have to send the image? a byte array?
563e32a661a8013065267a39	X	Any POST with file is acceptable, i.e. via curl cli: ``` curl -X POST deviantsart.com -F my_file=@/path/to/file.jpg ```
563e32a661a8013065267a3a	X	Interesting... just got a drive-by downvote on this 4 year old question with no comment stating just what might be wrong with it.
563e32a661a8013065267a3b	X	...You're on 70k(!) and you bother to remark on one downvote? My goodness. I'm on 14k on my best SE site, and even I don't complain any more.
563e32a761a8013065267a3c	X	@NickWiggill: If someone found a problem with this answer, they should share what that problem is. It's not a question of reputation.
563e32a761a8013065267a3d	X	...And yet as the rules of site have it, nobody is under any obligation to. Just like numerous other flaws in the system, such as upvote leads on early answers, etc. etc. Call me a realist: It's not going to change. Some people simply troll, that's how they roll. P.S. Your answer looked fine to me, but it's probably the fact that you said "I have not used that particular service" that garnered the unwanted attention.
563e32a761a8013065267a3e	X	@NickWiggill: No obligation certainly, but like the FAQ used to say If you see misinformation, vote it down. Add comments indicating what, specifically, is wrong. Provide better answers of your own. Best of all edit and improve the existing questions and answers! Words to live by. PS - Almost all answers got down-voted, without comment.
563e32a761a8013065267a3f	X	Flickr is somewhat strict as it comes to commercial usage of their service and they will remove all your images once they discovery they are used to host images from a service that e.g. has ads.
563e32a761a8013065267a40	X	I've been considering image hosting services for a project but am starting to wonder if that's just too complicated for my target audience as they'd have to upload all their images to the hosting service and then "attach" the images to the CSS file using the links the hosting service provides them. While that's a fairly simple process for us developers, I'm thinking that might be a large barrier to getting user buy-in for this feature. I could simplify by hosting and serving the images myself but I'm worried about potential scalability issues that could present which I don't have the hardware or bandwidth to handle at the present time. My thought is that I could have users upload their images and CSS to the server in a single zip file to the web server which could then extract the files from the zip, push the images on to an image hosting service, programmatically get the corresponding URL from the service and update the CSS accordingly before attaching it to the user's display profile. This approach could kill both birds with one stone, I wouldn't have to worry about the bandwidth issues caused by serving potentially large images on every profile request and the user doesn't have to go through the headache of needing to set up an account on an image hosting service. Does anyone know of any image hosting services that I can programmatically integrate with that has good reliability and performance that could assist me with this conundrum? Thanks in advance
563e32a761a8013065267a41	X	Review the Picasa Web Albums Data API: If you've signed up for Google+ then photos up to 2048x2048 pixels and videos up to 15 minutes won't count towards your free storage.
563e32a761a8013065267a42	X	http://deviantsart.com has a public and easy to use API just HTTP POST the image to their domain and you will get a json with the url
563e32a861a8013065267a43	X	I used https://cloudinary.com/ and found it pretty neat!
563e32a861a8013065267a44	X	You might consider Amazon CloudFront. I have not used that particular service, but I have used Amazon EC2 and S3 extensively and am quite happy. UPDATE: I recently used CloudFront for a video hosting project and found it quite simple to setup and use.
563e32aa61a8013065267a45	X	Check out Flickr's API: http://www.flickr.com/services/api/
563e32ab61a8013065267a46	X	Imgur has an API. From the "Overview": Imgur's API exposes the entire Imgur infrastructure via a standardized programmatic interface. Using Imgur's API, you can do just about anything you can do on imgur.com, while using your programming language of choice. The Imgur API is a RESTful API based on HTTP requests and XML or JSON(P) responses. If you're familiar with the APIs of Twitter, Amazon's S3, del.icio.us, or a host of other web services, you'll feel right at home. This version of the API, version 3, uses OAuth 2.0. This means that all requests will need to be encrypted and sent via SSL/TLS to https://. It also means that you need to register your application, even if you aren't allowing users to login.
563e32ac61a8013065267a47	X	What!? In the name of Trident, I demand the long story!
563e32ac61a8013065267a48	X	I am serving files from Amazon S3 now, and I generate a secure URL using the API. Works great everywhere except for people downloading .exe files in IE. I have tested this on IE 8 and 7. If running a local webserver you can test by putting notepad.exe in your web root. Go to http://localhost/notepad.exe (or equivalent) Now try http://localhost/notepad.exe? It should save the file as notepad, without extension. Is this a 'feature' because googling it is coming up with nothing. Thanks to the whole issue of IE extensions, you can't search for anything on file extensions. Also, if the file has multiple periods in the name, it sometimes gets a [1] or [] added to the end. Any ideas? Docs on this terrible behavior? It seems like it must be a security feature, but I have yet to find an option to disable it. And as always, thank you. Tim
563e32ac61a8013065267a49	X	There's a long story here, but the simple workaround is to do this: http://www.example.com/dl/test.exe?mysecret=12321412&FixForIE=.exe As for the trailing [1] or whatnot, no, there's not really anything you can do about that if the user happens to have downloaded from that URL before.
563e32ac61a8013065267a4a	X	We had the same problem when serving files from S3. Turns out you need to set the content-disposition correctly for IE to handle the files correctly. Namely, the HTTP header Content-Disposition: attachment; filename="text.exe" This article describes in a little more detail: http://www.jtricks.com/bits/content_disposition.html
563e32ad61a8013065267a4b	X	I have a couple of questions, and the first one involves the integration of a Google or Bing map. I am trying to get multiple markers to appear on a map based on the results of a user-submitted property search form. When the user searches for properties from any combination of available criteria (Address, city, Zip code, number of garages, etc.), they are taken to a results page that shows ten matches per page. The map needs to be able to mark the location of those properties as they are being viewed on the results page. How can this be accomplished in the scripting of the map and/or the search form? My second question involves storing images on a database. We need to download a large number of images from a listing server, but unfortunately our server is not large enough to support easily tens of thousands of image files. How can the images be stored on a relational database with PHP and SQL queries? How can SQL be used to convert the actual images to data for easier storage? Thank you for any answers you have!
563e32ad61a8013065267a4c	X	The basic workflow would be to query your database in a loop and output the address information together with their latitude and longitude to your HTML page. Then you would simply add multiple markers to your map and style the results, markers and infobubbles to your needs. A simple example on how to add a single marker can be found in the Google Maps API Documentation. Are you sure you need to store the images itself in a database? Perhaps it is also possible to store them on a filesystem or on another location like Amazon S3. You may want to read this posting for a detailed discussion.
563e32ad61a8013065267a4d	X	Is there a way to bulk upload images to my cloudinary account? I am looking to import 100 images of 3MB each at a time. Thank you.
563e32ad61a8013065267a4e	X	You can use Cloudinary's upload API to upload images one by one. Here is a sample upload code in Python. If your images are already in a public location, you can specify the remote HTTP URL as the file parameter instead of sending the actual image's data. This allows much faster uploading. If your images are in an Amazon S3 bucket, images can be fetched by Cloudinary directly from S3 for reaching even higher upload performance. You can also run your upload code using multiple processes in parallel for quickly uploading multiple files simultaneously. In our Ruby on Rails client library we included a migration tool. Currently there is no dedicated API method of Cloudinary for performing bulk upload of images.
563e32ad61a8013065267a4f	X	Easiest way is to use the remote API - and Just pass the url reference to the account and Cloudinary will connect to the image and download it into your account. http://cloudinary.com/documentation/upload_images#remote_upload
563e32ad61a8013065267a50	X	This is for business use, We are testing DRDB on GFS, looking into Gluster,but that requires client pieces,We've played around with ZFS on Solaris (and we are looking at AVS for availability), but Solaris keeps having small problems to be over come. We have also started looking at windows DFS.
563e32ae61a8013065267a51	X	We are planning to scale up over the next few years to 5-10 TB, but nothing huge. It is a production facility, we could spend 15-30 minutes switching over if we had too, but we want to minimize data loss. Our process are almost 24 x 6, so want to minimize the 3 am calls, would prefer HA
563e32ae61a8013065267a52	X	There needs to be a sort of badge out there for this sort of post.
563e32ae61a8013065267a53	X	S3 does not have particularly fantastic availability. It is great in many ways, but does not fit the "high availability" requirement the OP is asking for.
563e32ae61a8013065267a54	X	I would like to make 2 TB or so available via NFS and CIFS. I am looking for a 2 (or more) server solution for high availability and the ability to load balance across the servers if possible. Any suggestions for clustering or high availability solutions? This is business use, planning on growing to 5-10 TB over next few years. Our facility is almost 24 hours a day, six days a week. We could have 15-30 minutes of downtime, but we want to minimize data loss. I want to minimize 3 AM calls. We are currently running one server with ZFS on Solaris and we are looking at AVS for the HA part, but we have had minor issues with Solaris (CIFS implementation doesn't work with Vista, etc) that have held us up. We have started looking at We are looking for a "black box" that serves up data. We currently snapshot the data in ZFS and send the snapshot over the net to a remote datacenter for offsite backup. Our original plan was to have a 2nd machine and rsync every 10 - 15 min. The issue on a failure would be that ongoing production processes would lose 15 minutes of data and be left "in the middle". They would almost be easier to start from the beginning than to figure out where to pickup in the middle. That is what drove us to look at HA solutions.
563e32ae61a8013065267a55	X	I've recently deployed hanfs using DRBD as the backend, in my situation, I'm running active/standby mode, but I've tested it successfully using OCFS2 in primary/primary mode too. There unfortunately isn't much documentation out there on how best to achieve this, most that exists is barely useful at best. If you do go along the drbd route, I highly recommend joining the drbd mailing list, and reading all of the documentation. Here's my ha/drbd setup and script I wrote to handle ha's failures:   DRBD8 is required - this is provided by drbd8-utils and drbd8-source. Once these are installed (I believe they're provided by backports), you can use module-assistant to install it - m-a a-i drbd8. Either depmod -a or reboot at this point, if you depmod -a, you'll need to modprobe drbd. You'll require a backend partition to use for drbd, do not make this partition LVM, or you'll hit all sorts of problems. Do not put LVM on the drbd device or you'll hit all sorts of problems. Hanfs1: We must now perform an initial synchronization of data - obviously, if this is a brand new drbd cluster, it doesn't matter which node you choose. Once done, you'll need to mkfs.yourchoiceoffilesystem on your drbd device - the device in our config above is /dev/drbd1. http://www.drbd.org/users-guide/p-work.html is a useful document to read while working with drbd. Heartbeat Install heartbeat2. (Pretty simple, apt-get install heartbeat2). /etc/ha.d/ha.cf on each machine should consist of: hanfs1: I wrote a wrapper script to deal with the idiosyncracies caused by nfs and drbd in a failover scenario. This script should exist within /etc/ha.d/resources.d/ on each machine. Then it's just a case of starting up heartbeat on both machines and issuing hb_takeover on one of them. You can test that it's working by making sure the one you issued the takeover on is primary - check /proc/drbd, that the device is mounted correctly, and that you can access nfs. -- Best of luck man. Setting it up from the ground up was, for me, an extremely painful experience.
563e32b061a8013065267a56	X	These days 2TB fits in one machine, so you've got options, from simple to complex. These all presume linux servers: There are also plenty of commercial solutions, but 2TB is a bit small for most of them these days. You haven't mentioned your application yet, but if hot failover isn't necessary, and all you really want is something that will stand up to losing a disk or two, find a NAS that support RAID-5, at least 4 drives, and hotswap and you should be good to go.
563e32b061a8013065267a57	X	I would recommend NAS Storage. (Network Attached Storage). HP has some nice ones you can choose from. http://h18006.www1.hp.com/storage/aiostorage.html as well as Clustered versions: http://h18006.www1.hp.com/storage/software/clusteredfs/index.html?jumpid=reg_R1002_USEN
563e32b161a8013065267a58	X	Are you looking for an "enterprise" solution or a "home" solution? It is hard to tell from your question, because 2TB is very small for an enterprise and a little on the high end for a home user (especially two servers). Could you clarify the need so we can discuss tradeoffs?
563e32b161a8013065267a59	X	There's two ways to go at this. The first is to just go buy a SAN or a NAS from Dell or HP and throw money at the problem. Modern storage hardware just makes all of this easy to do, saving your expertise for more core problems. If you want to roll your own, take a look at using Linux with DRBD. http://www.drbd.org/ DRBD allows you to create networked block devices. Think RAID 1 across two servers instead of just two disks. DRBD deployments are usually done using Heartbeat for failover in case one system dies. I'm not sure about load balancing, but you might investigate and see if LVS can be used to load balance across your DRBD hosts: http://www.linuxvirtualserver.org/ To conclude, let me just reiterate that you're probably going to save yourself a lot of time in the long run just forking out the money for a NAS.
563e32b161a8013065267a5a	X	I assume from the body of your question is you're a business user? I purchased a 6TB RAID 5 unit from Silicon Mechanics and have it NAS attached and my engineer installed NFS on our servers. Backups performed via rsync to another large capacity NAS.
563e32b161a8013065267a5b	X	Your best bet maybe to work with experts who do this sort of thing for a living. These guys are actually in our office complex...I've had a chance to work with them on a similar project I was lead on. http://www.deltasquare.com/About
563e32b161a8013065267a5c	X	Have a look at Amazon Simple Storage Service (Amazon S3) http://www.amazon.com/S3-AWS-home-page-Money/b/ref=sc_fe_l_2?ie=UTF8&node=16427261&no=3435361&me=A36L942TSJ2AJA -- This may be of interest re. High Availability Dear AWS Customer: Many of you have asked us to let you know ahead of time about features and services that are currently under development so that you can better plan for how that functionality might integrate with your applications. To that end, we are excited to share some early details with you about a new offering we have under development here at AWS -- a content delivery service. This new service will provide you a high performance method of distributing content to end users, giving your customers low latency and high data transfer rates when they access your objects. The initial release will help developers and businesses who need to deliver popular, publicly readable content over HTTP connections. Our goal is to create a content delivery service that: Lets developers and businesses get started easily - there are no minimum fees and no commitments. You will only pay for what you actually use. Is simple and easy to use - a single, simple API call is all that is needed to get started delivering your content. Works seamlessly with Amazon S3 - this gives you durable storage for the original, definitive versions of your files while making the content delivery service easier to use. Has a global presence - we use a global network of edge locations on three continents to deliver your content from the most appropriate location. You'll start by storing the original version of your objects in Amazon S3, making sure they are publicly readable. Then, you'll make a simple API call to register your bucket with the new content delivery service. This API call will return a new domain name for you to include in your web pages or application. When clients request an object using this domain name, they will be automatically routed to the nearest edge location for high performance delivery of your content. It's that simple. We're currently working with a small group of private beta customers, and expect to have this service widely available before the end of the year. If you'd like to be notified when we launch, please let us know by clicking here. Sincerely, The Amazon Web Services Team
563e32b161a8013065267a5d	X	May I suggest you visit the F5 site and check out http://www.f5.com/solutions/virtualization/file/
563e32b261a8013065267a5e	X	According to MSDN, an azure service can conatins any number of worker roles. According to my knowledge a worker role can be recycled at any time by Windows Azure Fabric. If it is the true, then: But i want to make a service which conatains client data and do not want to use Azure storage service. How I can accomplish this?
563e32b261a8013065267a5f	X	The velocity (whatever it is called) component of AppFabric is a distributed cache and can be used in these situations.
563e32b261a8013065267a60	X	Azure's web and compute roles are stateless means all its local data is volatile and if you want to maintain the state you need to use some external resource to maintain that state and logic in your app to handle that. For simplicity you can use Azure drive but again internally its a blob storage.
563e32b261a8013065267a61	X	You can write to local storage on the worker role by using the standard file IO APIs - but this will be erased upon instance shutdown. You could also use SQL Azure, or post your data off to another storage service by HTTP (e.g. Amazon S3, or your own server). However, this is likely to have performance implications. Depending on how much data you'll be storing, how frequently, and how big it is, you might be better off with Azure Storage! Why don't you want to use Azure Storage?
563e32b261a8013065267a62	X	If the data could be stored in Azure you have a good number of choices: Azure distributed cache, SQL Azure, blob, table, queue, or Azure Drive. It sounds like you need persistence, but can't use any of these Azure storage mechanisms. If data security is the problem, could you encrypt/hashing the data? Understanding why would be useful. One alternative might be not persist at all, by chaining/nesting synchronous web service calls together, thus achieving reliable messaging. Another might be to use Azure Connect to domain join Azure compute resource to your local data centre (if you have one), and use you on-premise storage.
563e32b261a8013065267a63	X	Have you tried using the list of CommonPrefixes returned from ListBucket? docs.aws.amazon.com/AmazonS3/latest/dev/…
563e32b361a8013065267a64	X	Originally I couldn't find the common prefixes anywhere, but maybe I was overlooking that during my object dumps in the wrong places. I just found them now. Thanks for the help :)
563e32b361a8013065267a65	X	Given my S3 bucket that contains images in a structure like so: where root is my bucket, and there are no other files in my root, just those folders (objects), how do I retrieve a list of just those objects? I am familiar with using the Delimiter and Prefix in the ListObjects call. If I do the following, I get no results: If I don't use a Delimiter, I get everything, obviously. I cannot use a prefix because the objects I desire are root-level. Otherwise, I have no problem using the prefix to say, list just the files in 'portraits/' From my searches, I've only managed to find solutions from previous years that only apply to the aws php sdk v1 or v2, and I have had no luck in trying those (v3 is quite different) Any suggestions? I feel like I'm missing something simple, but searching through the documentation, I can't find anything to help me. As a last resort, I'll just have to stick with manually declaring an array But that isn't ideal in the case where I want to add more categories in the future, and not have to worry about adding another category manually. Any help would be greatly appreciated :) Edit - Solution I must have been looking in the wrong places during my object dumps, but eventually saw the Common Prefixes in the returned result from a ListObjects call with a delimiter of '/', like so:
563e32b361a8013065267a66	X	Directories do not actually exist in Amazon S3. However, the Management Console allows the creation of folders, and paths are supported to give the illusion of directories. For example, object bar.jpg stored in the foo directory has a path of /foo/bar.jpg. The trick is that the object is actually called foo/bar.jpg rather than just bar.jpg. Most users wouldn't even notice the difference. From the API, the ability to list directories is provided via the concept of CommonPrefixes, which look the same as directory paths and consist of the portion of object names ('keys') before the final slash. See: Listing Keys Hierarchically Using a Prefix and Delimiter
563e32b361a8013065267a67	X	Wonderful! This is the solution for direct S3 uploads. Thanks Simon!
563e32b461a8013065267a68	X	how do i force it to substitute the original file one i try to reupload a new file?? right now its creating name_1, name_2 etc
563e32cb61a8013065267a69	X	I am using this file storage engine to store files to Amazon S3 when they are uploaded: http://code.welldev.org/django-storages/wiki/Home It takes quite a long time to upload because the file must first be uploaded from client to web server, and then web server to Amazon S3 before a response is returned to the client. I would like to make the process of sending the file to S3 asynchronous, so the response can be returned to the user much faster. What is the best way to do this with the file storage engine? Thanks for your advice!
563e32cb61a8013065267a6a	X	I've taken another approach to this problem. My models have 2 file fields, one uses the standard file storage backend and the other one uses the s3 file storage backend. When the user uploads a file it get's stored localy. I have a management command in my application that uploads all the localy stored files to s3 and updates the models. So when a request comes for the file I check to see if the model object uses the s3 storage field, if so I send a redirect to the correct url on s3, if not I send a redirect so that nginx can serve the file from disk. This management command can ofcourse be triggered by any event a cronjob or whatever.
563e32cb61a8013065267a6b	X	It's possible to have your users upload files directly to S3 from their browser using a special form (with an encrypted policy document in a hidden field). They will be redirected back to your application once the upload completes. More information here: http://developer.amazonwebservices.com/connect/entry.jspa?externalID=1434
563e32cc61a8013065267a6c	X	There is an app for that :-) https://github.com/jezdez/django-queued-storage It does exactly what you need - and much more, because you can set any "local" storage and any "remote" storage. This app will store your file in fast "local" storage (for example MogileFS storage) and then using Celery (django-celery), will attempt asynchronous uploading to the "remote" storage. Few remarks: The tricky thing is - you can setup it to copy&upload, or to upload&delete strategy, that will delete local file once it is uploaded. Second tricky thing - it will serve file from "local" storage until it is not uploaded. It also can be configured to make number of retries on uploads failures. Installation & usage is also very simple and straightforward: append to INSTALLED_APPS: in models.py:
563e32cc61a8013065267a6d	X	You could decouple the process: [*: In case you have only a shared hosting you could possibly build some solution which uses an hidden Iframe in the users browser to start a script which then uploads the file to S3]
563e32cc61a8013065267a6e	X	You can directly upload media to the s3 server without using your web application server. See the following references: Amazon API Reference : http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?UsingHTTPPOST.html A django implementation : https://github.com/sbc/django-uploadify-s3
563e32cc61a8013065267a6f	X	As some of the answers here suggest uploading directly to S3, here's a Django S3 Mixin using plupload: https://github.com/burgalon/plupload-s3mixin
563e32cc61a8013065267a70	X	Is the intention to display them in a browser?
563e32cc61a8013065267a71	X	yes, the intention is to display the objects
563e32cc61a8013065267a72	X	For a while I have been building a program that decompiles Adobe flash .swf files. I have stored them in aws s3, and have keys stored in a local file that helps me locate them. Essentially I am using them as a database of sorts. An example of the keys I have are: In the above case these are jpg files that are stored. I search through my keys and find those that I want. For example I might want an ad hosted on amazon that is a jpg. When I run something like this in my Api Flask file: I would like to have the page .../getToday to display each object. I have the following types of files: .as (actionscript) .svg .swf .jpg I would like them to each render correctly. I assume I may need to check each key and then display each type differently. So when I go to .../getToday I would like to see all of the objects in a visual form. When I am using jsonify I am getting the following error: ValueError: dictionary update sequence element #0 has length 1; 2 is required But what can I use that isn't jsonify (just text) to see the images render?
563e32d161a8013065267a73	X	Sigh...this has been asked and answered about 50 kajillion gazillion times on this site...
563e32d161a8013065267a74	X	I've tried searching for this, Perhaps you could link me too a previous question?
563e32d261a8013065267a75	X	what query you tried to search for?
563e32d261a8013065267a76	X	stackoverflow.com/questions/1257488/…
563e32d261a8013065267a77	X	You know, it's even no need to bother with search. I tried to start another question with your title and got a dozen suggestions immediately. Didn't you notice it?
563e32d261a8013065267a78	X	Databases work well with data files?
563e32d261a8013065267a79	X	@Nicklamort -- yes that is what the BLOB SQL datatype is (Binary Large OBject) here is a link with an interesting description: [link]searchsqlserver.techtarget.com/definition/BLOB
563e32d261a8013065267a7a	X	Ah, I see. Nice man +1 =P
563e32d261a8013065267a7b	X	i always like downvotes without comment.
563e32d361a8013065267a7c	X	I am curious also about CSS and js files. pdf, swf and, say, xls are counted too. Is it good to store just whole site contents in a database to make filesystem backups really unnecessary? Are them filesystem backups that evil?
563e32d361a8013065267a7d	X	its another thing you need to take care of, monitor, program and maintain. if you just have IMAGES (as stated in the question) and there are not much (as stated as condition in my answer) it MAY be EASIER. i never used the word IS and gave a valid example. imagine you have a website with big articles that is only text based. and now you want a little image to show up as a preview on facebook. do you really want to set up a filesytem solution for that with all that comes along with it?
563e32d361a8013065267a7e	X	So, these poor pdf's illustrating some articles are doomed to be unsafe. What a pity...
563e32d361a8013065267a7f	X	umm another one file type just came to my mind. php files. let's say it's just ordinal hosting with no cvs to deal with sources. is it good idea to store source files in a database too? just to make sure that no filesystem backup needed ever? ugh, and web-server configs! And php binaries! Whoa, it's really amazing idea to store them all in a database!
563e32d361a8013065267a80	X	Possible Duplicate: MySQL - storing images in a database?? I'm working on a profile system for my webpage, However I was wondering what the best way to store images are. The first method I have read about is using BLOB in mysql. The second would be how I planned to do it in the first place, First i would get the image from a upload script, then giving the picture a id (md5) then renaming and move the picture to a folder named "md5".jpg I was wondering what the best option is.
563e32d361a8013065267a81	X	I have seen this question pop up before, and the generally accepted answer is to store your images in a folder, and then store the URL in the database. I think it is technically possible to store an image in a database using a BLOB, but from the documentation and consensus that I've read online, this is not a ideal scenario. Databases tend to work well with Numbers, Strings and data files. Here are some links on this site for the question you are asking: Storing images on a database Storing Images in DB - Yea or Nay?
563e32d361a8013065267a82	X	From Choosing data type for MySQL? MySQL is incapable of working with any data that is larger than max_allowed_packet (default: 1M) in size, unless you construct complicated and memory intense workarounds at the server side. This further restricts what can be done with TEXT/BLOB-like types, and generally makes the LARGETEXT/LARGEBLOB type useless in a default configuration. So careful with your blobs I like just storing the image on the HDD and a path/id in the DB. The advantages of storing in a db are very small, and it can be slower.
563e32d361a8013065267a83	X	both are good. but i have always opted to using the actual file with giving the image an id. This is because your server and php has to do more work for a blob.
563e32d361a8013065267a84	X	The best idea is probably to store the actual file on the web/file server and only store the link to that file in the database. This keeps your database server from getting bogged down with unnecessary busy work.
563e32d361a8013065267a85	X	Check out http://transloadit.com, it's an service for uploading and storing (Amazon Cloud) images.
563e32d361a8013065267a86	X	database storing can have a some advantages that are often forgotten. you can backup your images along with your database, no filesystem backups needed for this. it may be easier to scale when your site gets heavy traffic of course its not suitable if you have a lot of images. like user contributed content etc. but if you have a little newspaper site with not many articles online, you can perfectly store thumbnails in a database and build your system upon replication slaves. however... honestly: store your images on amazon s3. simple api, redundant, highly available and loadbalanced storage for cheap.
563e32d361a8013065267a87	X	I'd recommend saving them as regular static files. Then store their file names within your database. You can pull out the matching file name and either file_get_contents the image data or link directly to the real image file. Don't store the image data itself within the DB.
563e32d361a8013065267a88	X	I would like to use node.js to download files from s3. The uploaded files were decrypted using java AmazonS3Encryption client. I could not find however equal api for node.js amazon client. The only possibly I see is downloading the files and decrypting them locally. Any ideas?
563e32d461a8013065267a89	X	If you're going to downvote, can you explain why?
563e32d461a8013065267a8a	X	I'd like to use Google Cloud Storage to pull down some files for a project I'm building automatically through Docker. To do this, I'd like to do the authentication entirely through commands without opening browser windows (something like authenticating with a client_id and secret as happens with Amazon's S3). Is this possible? Is there any way to programmatically authenticate for Google Cloud Storage, through gsutil or the API?
563e32d461a8013065267a8b	X	You can use Service accounts for your application. For more information check the following link
563e32d461a8013065267a8c	X	You say you also tried md5(key,true) but did you try base64_encode(md5(key,true)) ...?
563e32d461a8013065267a8d	X	@Michael-sqlbot Yes sir
563e32d461a8013065267a8e	X	I think my issue is that my key is not actually 256 bit. I will attempt to use openssl_random_pseudo_byte to generate 256 bit, and I suspect that it will output 64 hex string
563e32d461a8013065267a8f	X	Actually, I suspect that your key is 256 bits until you base_64 encode it, at which point it will be more. I have coded extensively against the REST API but am generally unfamiliar with the PHP SDK... rethinking, now, I suspect your error is base_64 encoding either of those values. The SDK probably handles that for you, in which case, you're double-encoding. Try SSECustomerKey => 'raw key string with 32 chars', SSECustomerKeyMD5 => md5(key,true). The "true" as I understand it returns the binary md5, not hex, which is probably what you need here, since docs mention 128 bits for the md5.
563e32d461a8013065267a90	X	See github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/…
563e32d461a8013065267a91	X	Actually, the SDK will also set the SSECustomerKeyMD5 parameter for you if you don't provide it. See github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/…
563e32d461a8013065267a92	X	Nice. Thanks, @JeremyLindblom.
563e32d461a8013065267a93	X	the only post which helped me ... though there is no other post regarding to that topic :)
563e32d461a8013065267a94	X	how to create string_of_exactly_32_bytes with php ?
563e32d461a8013065267a95	X	@Richerdfuld that string is your encryption key. It has to be exactly 32 bytes.
563e32d461a8013065267a96	X	I am attempting to upload an object to S3 using the customer provided encryption key. http://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html My code looks like: The error I am getting says AWS Error Message: The calculated MD5 hash of the key did not match the hash that was provided What am I doing wrong? My key 48wk86271sDb23pY23zT5rZJ7q55R7eE is 256 bits. I've also tried using base64_encode(md5(key, true)). Thanks in advance
563e32d461a8013065267a97	X	The REST API documentation specifies that both the customer key and customer key MD5 be sent base-64 encoded... x-amz-server-side​-encryption​-customer-key Use this header to provide the 256-bit, base64-encoded encryption key for Amazon S3 to use to encrypt or decrypt your data. x-amz-server-side​-encryption​-customer-key-MD5 Use this header to provide the base64-encoded 128-bit MD5 digest of the encryption key according to RFC 1321. Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error. ...however, the PHP SDK handles both encoding steps for you, so the arguments should be passed without any encoding. Of course, you'd probably want that 32 byte key string in a variable rather than copypasting the same literal string in the code twice. The second argument "true" to md5() specifies that the binary md5 hash is to be returned, as expected by the SDK, instead of the hex-encoded variant that would be returned by default. Remember that when using customer-provided encryption keys, if you lose the key, you lose the data. S3 does not store the key, and without the key, fetching the stored object is not possible.
563e32d561a8013065267a98	X	See my edit about HTTPS. I am planning to retrieve it as it is required and replacing them dynamically with HTTPS AmazonS3 URLs.
563e32d561a8013065267a99	X	See my edit about HTTPS. I wish I could use directly the URL that the API gives me but I can't as it is because it's HTTP. I think you're right, caching is not a good idea for this.
563e32d561a8013065267a9a	X	The Twitter API returns this value for the Twitter account 'image_url': http://a1.twimg.com/profile_images/75075164/twitter_bird_profile_bigger.png In my Twitter client webapp, I am considering hotlinking the HTTPS version of avatars which is hosted on Amazon S3 : https://s3.amazonaws.com/twitter_production/profile_images/75075164/twitter_bird_profile_bigger.png Any best practices which would discourage me from doing this ? Do 3rd party Twitter client applications typically host their own copies of avatars ? EDIT: To clarify, I need to use HTTPS for images because my webapp will use a HTTPS connection and I don't want my users to get security warnings from their browser about the page containing some content which is not authenticated. For example, Firefox is known to complain about mixed http/https content. My problem is to figure out whether or not hotlinking the https URLs is forbidden by Twitter, since these URLs are not "public" from their API. I got them by analyzing their web client HTML source when connected to my Twitter account in HTTPS.
563e32d561a8013065267a9b	X	Are you thinking of storing the image URL in your application or retrieving it for the user as it is required? If its the latter option then I don't see an issue with hot-linking the images. If you are storing the location of the image url in your own system then I see you having broken links whenever the images change (I'm sure they will change the URLs at some point in the future). Edit Ok, now i see your dilemma. I've looked through the API docs and there doesnt seem to be too much in terms of being able to get images served in HTTPS or getting the URL of the Amazon S3 image. You could possibly write a handler on your own server that would essentially cache & re-serve the HTTP image as HTTPS however thats a bit of un-neccesary load on your servers. Short of that I haven't come across a better solution. GL
563e32d561a8013065267a9c	X	the things seems updated since that. Please check: https://dev.twitter.com/docs/user-profile-images-and-banners The SSL-enabled path template for a profile image is indicated in the profile_image_url_https. The table above demonstrates how to apply the same variant selection techniques to SSL-based images.
563e32d561a8013065267a9d	X	Why would you want to copy the image to your own webspace? This will increase your bandwidth cost and you get cache consistency issues. Use the URL that the API gives you. I can see that you may want to cache the URL that the API returns for some time in order to reduce the amount of API calls. If you are writing something like an iPhone app, it makes sense to cache the image locally (on the phone), in order to avoid web traffic altogether, but replacing one URL with another URL should not make a difference (assuming that the Twitter image server works reliably). Why do you want HTTPS?
563e32d561a8013065267a9e	X	I'm afraid the mail app cannot read your file, have you tried to change the mode in the openFileOutput to MODE_WORLD_READABLE?
563e32d561a8013065267a9f	X	Well it says that it was deprecated in API 17, but it seems to work, now Mail can read it and send it. However, by changing it to MODE_WORLD_READABLE, It's lost the APPEND quality, which is pretty important... Is there any way to keep both?
563e32d561a8013065267aa0	X	Yeah actually it's not a good move because a security hole, but it lead me to an answer, please wait I will write an answer for you :)
563e32d661a8013065267aa1	X	Append and world readable should not be exclusive, but some mail clients won't take an attachment from another app's internal storage even if it is readable to them (unless you trick them). So it's best to either use external storage, or the modern "android way" with a content provider.
563e32d661a8013065267aa2	X	Nice Explanation. Can u provide some Sample code for this?? "Attach Audio file and send to some other user of my App"
563e32d661a8013065267aa3	X	Before anything else, I have actually read through several threads regarding sending attachments on Android. That said, I haven't found a solution to my problem. My app is relatively simple, user types numbers, they get saved to "values.csv" using openFileOutput(filename, Context.MODE_APPEND);. Now, here's the code I'm using to attach the file to an email and send (I got it from one of the other file threads.) This opens my email client, which does everything right except attach the file, showing me a toast notification saying "file doesn't exist." Am I missing something? I've already added the permissions for reading and writing to external storage, by the way. Any and all help would be much appreciated. EDIT: I can use the File Explorer module in DDMS and navigate to /data/data/com.example.myapp/files/, where my values.csv is located, and copy it to my computer, so the file DOES exist. Must be a problem with my code.
563e32d661a8013065267aa4	X	So another way to attach the email except using MODE_WORLD_READABLE is using ContentProvider. There is a nice tutorial to do this in here but I will explain it to you shortly too and then you can read that tutorial after that. So first you need to create a ContentProvider that provides access to the files from the application’s internal cache. and then write a temporary file And then pass it to the email Intent and don't forget to add this to the manifest
563e32d661a8013065267aa5	X	I see the answers and of course the question. You asked how you can send a file from android, but in your code example you are talking about sending by email. Those are two different things at all... For sending by email you already have answers so I will not enter it at all. For sending files from android is something else: you can create a socket connection to your server and send it. you have of course to write both client and server side. An example can be found here : Sending file over socket... you can use an ftp server and upload it to there. you can use any server. example : It's using apache ftp client library (open source) you can use a http request and make it a post (this method is preferred only for small files) example : Sending file with POST over HTTP you can also use dropbox api or amazon s3 sdk's so you don't care about any connections issues and retries and so on and at the end you have a link to the file and pass over the link. a. DropBox API : documentation b. Amazon S3 SDK API : documentation c. Google Drive API : documentation The advantages in working with Google Drive. regards
563e32d661a8013065267aa6	X	Great idea, shortens the url considerably too!
563e32d661a8013065267aa7	X	Warning! Base64 is a very recognisable encoding. Any tech savvy user will be able to find it out and forge new ones. You must sanitize it (look for .. in path for exemple). You better crypt it with some xor algorithm before encoding it in base64.
563e32d661a8013065267aa8	X	@Nicolas Or even easier just put to a uid in the querystring that relates to the database row
563e32d661a8013065267aa9	X	Thanks mark, some interesting thoughts there. You ought to start a blog! Point #2 and #4 are features of any cdn, and the point behind this querystring -> filename operation is so that images can be stored on the local filesystem rather than retrieved from the database every time (doing it on a first-run-basis is fine), so thats point #3. I think where Amazon excells are in its (international) distribution, pricing model and api (although i havent had personal experience of this.
563e32d661a8013065267aaa	X	I have a handler (c#/asp.net) that pulls images from a database. The current format looks like foo.com/_image.ashx?querystring1&querystring2 The querystring keys are things like id, width, zoom level etc. I can either munge this up into a filename (i.e. foo.com/id__999_w__128__h__200.jpg), or a whole url structure (i.e. foo.com/id/999/w/128/h/200/image.jpg). Im interested to see what other people would do given this situation, as there dont seem to be any articles or discussions of this practice on the net.
563e32d661a8013065267aab	X	The easiest solution would be to base64 the filename, then you can pretty much guarantee the filename will be correct each time, and not tampered with or broken.
563e32d661a8013065267aac	X	Best practices are a tricky thing, a lot of the choices you'll want to make depend on your situation. Today a good way to handle this, is to load the images into a server like Amazon S3 instead of storing them into your database. Amazon S3 has an api that you can program to, so the application that you've already written that is storing the images into the database can be tweaked to upload them into Amazon S3 instead. Then you will store the url to the image in your database. This will solve a number of problems. 1. Images stored on Amazon S3 can be exposed on Amazon CloudFront which is a CDN and will help you to route the image data to your end users in the fast possible route over the internet. 2. You can apply expires headers to these images so they cache on the client browser and the user will get optimum performance while using your website. In your curren mechanism you would have to add these headers programmatically. 3. Pulling blob data in and out of databases has never been the most performant operation against a database so you would be able to get rid of this code completely. 4. You can apply a cookie-free domain name to these images so that needless cookie passing is prevented and will also have a slight performance increase for your users. For example you might host your website on www.mysite.com, for your images you could assign the name images.mysitestatic.com, and this way you know that you don't have any cookies on the mysitestatic.com domain name that your user would need to upload and download on each request. There are many advantages to hosting you images on a server as opposed to data pass through like your _images.ashx. As far as the urls you presented above, they both are pretty much the same. Unless you are looking to get seo value out of them it doesn't really matter. The url depends on your goals for performance/caching/seo, etc. Hope this helps. Mark
563e32d661a8013065267aad	X	Sorry, I'm new to this but I wanted to comment on the answer above. I don't understand what you gain by BASE64 encoding the url? It won't make the url shorted, the base64 encoded string will usually be about 33% longer. Here is an example: C:\images\myimage.png Base64 encoded: QzpcaW1hZ2VzXG15aW1hZ2UucG5n If you wanted to keep the url short, and you wanted to make sure the filename is correct per the solution above, you should just use the filename itself. The base64 encoded version isn't offering you any benefit. Although, I'm not sure this helps you because its not related to you database solution at all. unless you were to store "images/myimage.png" in a field in your database and have your url look as follows: foo.com/images/image.jpg?h=5&w=10 With that format, the name of the resource being requested, is separated from the representation of the image at a height of 5 and width of 10.
563e32d661a8013065267aae	X	It doesn't really mater even if one is 100x slower than the other it is insignificant compared with the IO speed of the disc, chose the versions where it have less lines of code making it much faster to understand and maintain by other programmers.
563e32d761a8013065267aaf	X	When uploading a file to S3 using the TransportUtility class, there is an option to either use FilePath or an input stream. I'm using multi-part uploads. I'm uploading a variety of things, of which some are files on disk and others are raw streams. I'm currently using the InputStream variety for everything, which works OK, but I'm wondering if I should specialize the method further. For the files on disk, I'm basically using File.OpenRead and passing that stream to the InputStream of the transfer request. Are there any performance gains or otherwise to prefer the FilePath method over the InputStream one where the input is known to be a file. In short: Is this the same thing As: Or are there any significant difference between the two? I know if files are large or not, and could prefer one method or another based on that. Edit: I've also done some decompiling of the S3Client, and there does indeed seem to be some difference in regards to the concurrency level of the transfer, as found in MultipartUploadCommand.cs
563e32d761a8013065267ab0	X	From the TransferUtility documentation: When uploading large files by specifying file paths instead of a stream, TransferUtility uses multiple threads to upload multiple parts of a single upload at once. When dealing with large content sizes and high bandwidth, this can increase throughput significantly. Which tells that using the file paths will use the MultiPart upload, but using the stream wont. But when I read through this Upload Method (stream, bucketName, key): Uploads the contents of the specified stream. For large uploads, the file will be divided and uploaded in parts using Amazon S3's multipart API. The parts will be reassembled as one object in Amazon S3. Which means that MultiPart is used on Streams as well. Amazon recommend to use MultiPart upload if the file size is larger than 100MB http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Using multipart upload provides the following advantages: Improved throughput—You can upload parts in parallel to improve throughput. Quick recovery from any network issues—Smaller part size minimizes the impact of restarting a failed upload due to a network error. Pause and resume object uploads—You can upload object parts over time. Once you initiate a multipart upload there is no expiry; you must explicitly complete or abort the multipart upload. Begin an upload before you know the final object size—You can upload an object as you are creating it. So based on Amazon S3 there is no different between using Stream or File Path, but It might make a slightly performance difference based on your code and OS.
563e32d761a8013065267ab1	X	This question may be a bit subjective but I think will offer some valuable concrete information and solutions to proxying to heroku and debugging latency issues. I have an app built using Sinatra/Mongo that exposes a REST API at api.example.com. It's on Heroku Cedar. Typically I serve static files through nginx at www and proxy requests to /api through to the api subdomain to avoid cross-domain browser complaints. I have a rackspace cloud instance so I put the front-end there temporarily on nginx and setup the proxy. Now latency is horrible when proxying, every 3 or 4 requests it takes longer than 1 minute, otherwise ~150ms. When going directly to the API (browser to api.example.com) average latency is ~40ms. While I know the setup isn't ideal I didn't expect it to be that bad. I assume this is in part due to proxying from rackspace - server may well be on the west coast - to heroku on amazon ec2 east. My thought at the moment is that getting an amazon ec2 instance and proxying that to my heroku app would alleviate the problem, but I'd like to verify this somehow rather than guessing blindly (it's also more expensive). Is there any reasonable way to determine where the long latency is coming from? Also, any other suggestions as to how to structure this application? I know I can serve static files on Heroku, but I don't like the idea of my API serving my front-end, would rather these be able to scale independently of one another.
563e32d761a8013065267ab2	X	Since you're using Heroku to run your API, what I'd suggest is putting your static files into an Amazon S3 bucket, something named 'myapp-static', and then using Amazon Cloudfront to proxy your static files via a DNS CNAME record (static.myapp.com). What's good about using S3 over Rackspace is that: What's good about using Cloudfront is that it will cache your static files as long as you want (reducing multiple HTTP requests), and serve files from an endpoint closest to the user. EG: If a user in California makes an API request and gets a static file from you, it will be served from them from the AWS California servers as opposed to your East Coast Heroku instances. Lastly, what you'll do on your application end is send the user a LINK to your static asset (eg: http://static.myapp.com/images/background.png) in your REST API, this way the client is responsible for downloading the content directly, and will be able to download the asset as fast as possible.
563e32d761a8013065267ab3	X	Thanks! Azure has the best Silverlight integration so I'm going with that.
563e32d861a8013065267ab4	X	I'm working on a Silverlight app that would allow a user to upload a few gigs of files to a hypothetical cloud based file store, then allow the user to view some data about those files later (more functionality than a file store). Ideally I'd like to use a free, per-user store such as SkyDrive but I can't seem to find an API for that service (and read elsewhere on stack overflow that programmatic access violates their TOS). Do any services fit this bill? I've heard of Amazon S3 but I understand that'll cost some money - is anything free? EDIT: Could Mesh be an option? http://stackoverflow.com/questions/1061926/what-is-livemesh-object-and-its-connection-with-silverlight-3-0
563e32da61a8013065267ab5	X	You could look at using Azure as it offers a blob and table storage cloud infrastrucutre and will happily run silverlight applications in an azure web role. Currently there is no cost but this will change once it RTW's. More info at http://www.azure.com/
563e32da61a8013065267ab6	X	AFAIK, nothing in this world is free when you're dealing with gigabytes of storage, plus the bandwith to put them in the cloud. Amazon S3 is quite reasonable on its pricing.
563e32db61a8013065267ab7	X	Appreciate your comments for pre-signed url we don't need to pass secret password as it will be encrypted in the url. We have to just upload the image. If I use the same URL in postman it was prefect.
563e32db61a8013065267ab8	X	Sounds like a platform bug to me then. One more "stupid" question, you've checked that the image data is actually populated and being passed with the method call and thence with the HTTP request? And that sniffing the request with Fiddler (or equivalent) finds everything sent over the wire matches your test request?
563e32dc61a8013065267ab9	X	I am trying to upload an image from my iPhone app to S3 using pre-signed url. AWS ended up with no answer. Step 1: iPhone send a request to server to GET S3 link to upload an image Step 2: Using "signed_request" value I am trying to upload an image to S3 using method "PUT" //Response from server //DATA i receive
563e32dc61a8013065267aba	X	I am unable to find an error in your code; however, as the response states, your SignatureDoesNotMatch the expected value. No secret password, no access to the secret club. Start with the basics and then narrow in on the details:
563e32dd61a8013065267abb	X	Does Google App Storage provide storage and database access in a centralizes public server environment just as the others do?
563e32dd61a8013065267abc	X	code.google.com/appengine/whyappengine.html - "App Engine will always be free to get started, and you can purchase more computing resources, paying only for what you actually use. Detailed pricing for usage that has exceeded the free quota of 500 MB of storage and around 5M pageviews per month is available"
563e32de61a8013065267abd	X	This seems to me like it's a web server to store/serve an application. I just need to store/serve data, like a cloud database (SC3, Google Storage, etc.) Am I correct?
563e32de61a8013065267abe	X	I'm not entirely clear on the use case. It's not impossible to implement an architecture that stores directly to something like S3 from an android device, but typically you'd want to provide other service, like authentication to the storage mechanism, which you couldn't do without a web server.
563e32de61a8013065267abf	X	So for my Android app I need a centralized server so multiple phones can use it. This server is only going to hold text. Just coordinates and a string per request. I looked into Amazon S3 and Google Storage. These store "buckets" of info through a RESTful API. So basically, I can't put/get info into these things as I would into a database. I'm going to have to send/receive a text file correct? Would it make more sense (both financially and technically), to buy my own web server so I can just create a MySQL DB or something and do it this way, or would the bucket thing still be the best way to go (cloud storage). Thanks!
563e32de61a8013065267ac0	X	The storage services tend to make the most sense when the data is read/served many multiples of times more then it is written/stored. The situation you describes sounds like a good use case for one of the hosted server options (EC2, Rackspace, etc.) but you may also want to look into Google App Engine since it provides you some free service before you incur any cost. That will let you develop an app without requiring any investment, especially since you don't yet know what the future usage rate of your app will be. In any of these environments you could create a web service that allows clients to connect via http / REST to store information.
563e32de61a8013065267ac1	X	What specific line is associated with the error?
563e32df61a8013065267ac2	X	Thanks for the speedy reply Ray! Have updated with the error above
563e32df61a8013065267ac3	X	You'll need to point at the specific line in your example code above that is failing. You're using a custom build, so I have no idea what line 212 looks like. Or, you can look up that line and post it here (with context).
563e32df61a8013065267ac4	X	When you update your question again, please be sure to include the full stack trace along with any line that references your own code.
563e32e061a8013065267ac5	X	It looks like it's this line: ExifLocation.loadFromFile(file, function(err, exifLocation, index) { Line 212 in my build is: window.console[level](message);
563e32e061a8013065267ac6	X	I'm using Fine Uploader to upload direct to Amazon S3. All is working fine, but I want to use a third party script to access the exif gps data in the image. I've found a script to do it (https://github.com/mattcg/exiflocation.git) which requires a file field in the example script. Is there a way I can pass the local file on to this script using the Fine Uploader API? Here's my callback script: Console logging the file outputs a file object. When I run this I get: Caught exception in 'onSubmit' callback - undefined is not a function Any ideas? Here's the complete error from the console: [Fine Uploader 5.0.8] Caught exception in 'onSubmit' callback - undefined is not a function custom.fineuploader-5.0.8.js:212
563e32e061a8013065267ac7	X	You're quite right Ray - my mistake. It's been a long day! Thanks for your help. The function is called using ExifLocation.prototype.loadFromFile. Thanks for your help!
563e32e161a8013065267ac8	X	did you figure this out? I'm having the same issue
563e32e261a8013065267ac9	X	I'm building a tool that uses javascript & canvas to take screenshots of a video and it all works fine, as long as the video is on the same domain as my code. Now my client is asking if it is possible to create some kind of API so that users could include an iframe from domain X, but insert their own video's from domain Y. I've managed to do this, It works on Chrome, Firefox, BUT on IE10 & safari I keep getting the common "Uncaught SecurityError: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The canvas has been tainted by cross-origin data."-error. I've been using a video that was hosted on Amazon S3, with the following CORS-configuration: And I've added the crossOrigin="anonymous" attribute to the video-element. According to Caniuse.com CORS should be working fine on those browsers, so what is causing these issues on IE10 & Safari?
563e32e261a8013065267aca	X	Why have the upload pass-through the play server at all and not do a direct upload to S3 using a CORS configuration?
563e32e261a8013065267acb	X	@LimbSoup I am creating a service that sort of manages the uploads. In order to keep track of all of that I need to do post-processing once the file is uploaded. Wouldn't it make more sense to use my server as a passthrough as opposed to making separate calls to upload the file from the client and then inform my server about it afterwards?
563e32e261a8013065267acc	X	It seems like a waste of resources to have two IO streams that can be avoided by having users upload directly to S3 with a signed form. To do this I have the client upload the file, then confirm to the server when the upload is finished. The server can then find the file on S3 and change it's properties from there if necessary. Downloads work the same way using signed URLs.
563e32e261a8013065267acd	X	I am using Play Framework version 2.2 and I am trying to get the mime type of a partially uploaded file so I can do a direct upload to an Amazon S3 instance. What is the best practice for doing this? I am currently using FlowJS but it doesn't look like they have anything in particular for dealing with mime types. Additionally, I plan on making mobile apps that will use the same API so it would be best if it was on the server side and not the client side. The only solution I can think of is parsing the extension and mapping that to a mime type, but that sounds like a hacky way to do it.
563e32e361a8013065267ace	X	perhaps do a partial get, and write each block to GCS, then stitch them all together?
563e32e361a8013065267acf	X	@PaulCollingwood: thank you for the suggestion, it worked! :) See my answer below.
563e32e361a8013065267ad0	X	I'm curious if you can avoid needing the larger instance if you write each chunk immediately to GCS (in the while loop) instead of writing them all at the end.
563e32e361a8013065267ad1	X	This is pretty cool. I wonder if you could do this in parallel? Once you have the length from the HEAD request you could start tasks in a queue and allow a couple of them in parallel. You might even combine with resumable uploads (cloud.google.com/storage/docs/concepts-techniques#resumable) so you wouldn't need to stitch the files at the end.
563e32e361a8013065267ad2	X	@Kekito: your suggestion worked, thank you!
563e32e461a8013065267ad3	X	I'm writing an API that needs to ingest HD videos (at least 100MB). I only have access to the videos through an HTTP XML feed, so I can only pull the videos (with a GET) once I have the video's URL. The plan is to store the videos in GCS. But I'm running into the 32MB-per-request limit in AppEngine before I can upload/write to GCS. Is there a GAE-way around these two limitations: I know of Amazon S3, if I must go outside of Google Cloud products, but I don't know if that can be configured to pull in large data. Thank you.
563e32e461a8013065267ad4	X	Following Paul Collingwood's advice, I came up with the following. I decided not to write chunks to GCS and then stitch them back together. Instead I chose to do it all in-memory, but I might change that depending on resource costs (had to run an F4@512MB to avoid exceeding an F2's 256MB soft limit). Which looks something like this in the logs: Update, 6/May/15 Along the lines of Kekito's suggestion, I moved the GCS write into the loop, keeping the file handle open for the entire duration. Following the advice here, I used top to monitor the Python processes running the GAE local dev server, started an upload, and recorded the memory footprints between download-and-upload cycles. I also experimented with changing how big a chunk is processed at a time: dropping the chunk size from 30 MB to 20 MB reduced the max memory usage by ~50 MB. In the following chart a 560 MB file is being ingested, and I'm trying to track:  The 20-MB-Chunk-Test maxes out at 230 MB while the 30-MB-Chunk-Test maxes out at 281 MB. So, I could run an instance at only 256 MB, but will probably feel better running at 512 MB. I might also try a smaller chunk size.
563e32e461a8013065267ad5	X	Try to specify full path to 'assets/cacert.pem'
563e32e461a8013065267ad6	X	yes. in the $this->curl->ssl function it will turn it into realpath "C:/wamp/www/assets/cacert.pem"
563e32e461a8013065267ad7	X	sorry because is self signed certification the name should be AIMS-BSN-WEB01.crt instead of cacert.pem
563e32e561a8013065267ad8	X	Im using codeigniter-curl extension (https://github.com/philsturgeon/codeigniter-curl) to call API that return Json format data. a simple code it will return the result etc everything seem ok until the URL changed to HTTPS. In order to call SSL url (self signed certificate), i added these few line above my $this->curl->simple_get.... code. the cacert.pem i saved from the firefox certificate viewer and place it in my web directory. reference: http://unitstep.net/blog/2009/05/05/using-curl-in-php-to-access-https-ssltls-protected-sites/ and i get this error. i've searched the answer for a day long, changed the php.ini setting Amazon S3 on wamp localhost SSL error put false for CURLOPT_SSL_VERIFYPEER. it return SSL: certificate subject name 'AIMS-BSN-WEB01' does not match target host name 'api.example.com' all these are not working.
563e32e561a8013065267ad9	X	ok. i manage to solved it by skipping to verify the peer or host of the certification - PHP CURL CURLOPT_SSL_VERIFYPEER ignored and instead of putting FALSE in CURLOPT_SSL_VERIFYPEER i use 0. now it works.
563e32e561a8013065267ada	X	Why not email them and find out?
563e32e561a8013065267adb	X	Right, like they'd tell me. :)
563e32e661a8013065267adc	X	Awesome, just as I thought. Thanks heaps for the detailed answer, wish I could +100 you! :)
563e32e661a8013065267add	X	I wonder how apps like these generate screenshots for different browsers. Are they using EC2 instances to run various browsers and generate and store (Amazon S3?) screenshots?
563e32e661a8013065267ade	X	Are they using EC2 instances to run various browsers and generate and store (Amazon S3?) screenshots? That's apparently exactly what they are doing - as far as I know this is not officially documented in detail indeed, but one can deduce it to some extent from the following information. 1) Their Feature Tour -> Email Tests answers How does it work? as follows: You send us a copy of your email design, either by uploading the HTML or sending us a test email. Within a couple of minutes you'll see screenshots of your email as it's rendered by all the different email clients. Made a change? One click starts a re-test. This is exactly what one would expect, i.e. they are apparently running a test harness which exercises all supported email clients (and dito for browsers) after a new test is scheduled via a queue. This requires a decent amount of automation around all these clients; while some may nowadays offer a dedicated automation API/component to allow rendering without running the full application, I expect this to be a fairly complicated process all in all still, likely requiring external UI automation, which used to be brittle and slow (OS support for this improved in recent years though). 2) Litmus has fortunately participated in an AWS Case Study in February 2010 (updated in April 2011), which confirms their infrastructure to be (meanwhile) running on AWS (specifically Amazon EC2 and Amazon S3) and provides additional insight: Initially, Litmus was hosted on a combination of in-house hardware and dedicated servers. The company grew quickly, and soon they outgrew their hardware. [...] Paul Farnell tells us about the process, “We looked for solutions that would meet our needs of scalability and cost. We chose Amazon S3 because there was nothing else like it when we first started. For Amazon EC2 we initially trialed a competitor to Amazon, but found it to be tremendously unreliable. [emphasis mine] Furthermore (as of April 2011) Litmus uses Amazon S3 to store over 6TB of customers’ images and Amazon EC2 for running customers’ tests: When we first started out we stored the images on our own hardware, but as we grew we realized this was quickly going to become a headache. By using S3 we were able to focus on improving our product, not worrying about scaling up our storage. We also use Amazon EC2 to run the automated email tests for our customers; we currently have 400 EC2 servers. By using EC2 we’re able to add more servers to our grid during the busy periods of the day, and remove them during quieter periods. Finally, they are using Spot Instances [to] gain significant EC2 cost savings: Specifically, we have a queue-based architecture where a worker node will pull a job from the queue and then process it. As worker nodes appear after a Spot bid is accepted, they can just take jobs off of the queue. [emphasis mine]
563e32e661a8013065267adf	X	It seems unlikely that there is a hard limit, or it would be documented. You must be running up against some capacity constraint. Amazon monitors their services, so it wouldn't surprise me if they were working to handle the higher load, now that they see users hitting it.
563e32e661a8013065267ae0	X	I'm using AWS to run some data processing. I have 400 spot instances in EC2 with 4 processes each, all of them writing to a single bucket in S3. I've started to get a (apparently uncommon) error saying: 503: Slow Down Does anyone know what the actual request limit is for an S3 bucket? I cannot find any AWS documentation on it. Thank you!
563e32e661a8013065267ae1	X	From what I've read, Slow Down is a very infrequent error. However, after posting this question I received an email from AWS that said the had capped my LIST requests to 10 requests per second because I had too many going to a specific bucket. I had been using a custom queuing script for the project I am working on, which relied on LIST requests to determine the next item to process. After running into this problem I switched to AWS SQS, which was a lot simpler to implement than I'd thought it would be. No more custom queue, no more massive amount of LIST requests. Thanks for the answers!
563e32e661a8013065267ae2	X	AWS documents 503 as a result of temporary error. It does not reflect a specific limit. According to "Best Practices for Using Amazon S3" section on handling errors (http://aws.amazon.com/articles/1904/): 500-series errors indicate that a request didn't succeed, but may be retried. Though infrequent, these errors are to be expected as part of normal interaction with the service and should be explicitly handled with an exponential backoff algorithm (ideally one that utilizes jitter). One such algorithm can be found at http://en.wikipedia.org/wiki/Truncated_binary_exponential_backoff. Particularly if you suddenly begin executing hundreds of PUTs per second into a single bucket, you may find that some requests return a 503 "Slow Down" error while the service works to repartition the load. As with all 500 series errors, these should be handled with exponential backoff. While less detailed, the S3 Error responses documentation does include 503 Slow Down (http://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html).
563e32e761a8013065267ae3	X	To add to what James said, there are some internals about S3 partitioning that have been discussed and can be used to mitigate this in the future because exponential backoff is required. See here: http://aws.typepad.com/aws/2012/03/amazon-s3-performance-tips-tricks-seattle-hiring-event.html Briefly, don't store everything with the same prefix or there is a higher likelihood you will have these errors.Find some way to make the very first character in the prefix be as random as possible to avoid hotspots in S3's internal partitioning.
563e32eb61a8013065267ae4	X	Yes, your script will be similar to that. You've asked several fairly broad questions. What have you tried so far? What stumbling blocks have you reached?
563e32eb61a8013065267ae5	X	To be honest - right now I am still in the planning phase. I wrote a script that overlays text on top of images (similar to the one in the link). Do you have any experience implementing something like this?
563e32eb61a8013065267ae6	X	Currently our application is restricted to text, images and audio mime types. I have been given the responsibility of adding a new feature - overlaying text on images (similar to snapchat). The image editing will be done on the server side (my task) and the text would be provided by the client using a jquery. I think that the ImageFont module of Python's Imaging Library (PIL) can be used to do this. So my question - how would I go about implementing this? Will the script be similar to this - http://python-catalin.blogspot.com/2010/06/add-text-on-image-with-pil-module.html ? Currently we are using amazon s3 for our object store. If I write the script in our DB API, Will it be major performance issue (the ultimate goal is to have a separate service for image processing)? I am quite new to Python and PIL so any help would be much appreciated.
563e32eb61a8013065267ae7	X	did you implement proper protocol method that RestKit calls when response is available? does it get called? can you sniff your traffic with wireshark to verify that request gets to S3 and that proper 200 is returned?
563e32eb61a8013065267ae8	X	What is your question? The code you have displayed looks fine
563e32ec61a8013065267ae9	X	@IvorPrebeg the delegate doesnt get called, My implementation has the protocol working for the normal RKCLient. this code is actually withing that part, It will send a request to a very specific url which is totally different from the rkclient base url which is why i need to send it this way. It never gets called and if i print the URL property i receive Null.
563e32ec61a8013065267aea	X	@PauldeLange please see my edit
563e32ec61a8013065267aeb	X	@PauldeLange if you put this as an answer ill accept it, there was an extra \n at the end of my url.
563e32ec61a8013065267aec	X	I am having trouble using the RestKit api to create a simple RKRequest. All i need is to make a PUT request with a provided url (which is a presigned amazon s3 url) and then attach some NSData to this request. I also want to set the delegate of this request. I have tried many ways but none seem to work. Any help will be greatly appreciated. Edit: The problem is that the delegate never gets called, it does get called for the RKCLient sent requests but not for this specific one. When I NSLOG the url from the uploadpicture request i get NULL and the message i recieve is: E restkit.network:RKRequest.m:623 Failed to send request to (null) due to connection timeout. Timeout interval = 0.000000
563e32ec61a8013065267aed	X	I'm having this issue at the moment - were you able to resolve yours?
563e32ec61a8013065267aee	X	I posted what I ended up doing as an answer below. Let me know if you have any questions!
563e32ec61a8013065267aef	X	I am still learning a lot about Rails and Android development, so forgive me if my question is a bit unclear. Fundamentally, what I'd like to do is uploading photos to my rails app using an android app. I have a Rails app that uses Carrierwave and Amazon S3 for image uploading. I'm writing a companion Android application app that can be used for updating entries on the site and for uploading photos. I created a REST API for the rails app so that I can perform http post / get / and delete requests using the Android app, which is working for updating text entries. But I'm unsure how to approach doing image uploading since when I look at the POST parameters in my Rails logs, it includes a lot of CarrierWave specific actions (such as @headers, @content_type, file, etc.). Can anyone recommend a way for me to get started? Many thanks!
563e32ec61a8013065267af0	X	I ended up piecing together code snippets and getting something that worked. I had to send the image in a multipart entity: The asynctask requires a filepath and filename. In my app, I allowed users to pick images from the gallery. I then retrieve the filepath and filename like this: Hope that helps!
563e32ed61a8013065267af1	X	I had a problem with the args[] being generated in Java. Where my command had been this: /home/hadoop/.versions/hive-0.7.1/bin/hive '-f' 's3://anet-emr/scripts/admin.q' '-d rawDataLocation=s3://anet-emr/raw -d year=2010 -d cycle=1' it should have been this /home/hadoop/.versions/hive-0.7.1/bin/hive '-f' 's3://anet-emr/scripts/admin.q' '-d' 'rawDataLocation=s3://anet-emr/raw' '-d' 'year=2010' '-d' 'cycle=1'
563e32ed61a8013065267af2	X	I have been developing a data processing application using Amazon Elastic MapReduce and Hive. Now that my Hive scripts work when I SSH and run them using the Interactive Mode Job Flow, I'm trying to create a Job Flow using the AWS Java API. Using http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/calling-emr-with-java-sdk.html as my starting point, I create a step config like this I assumed/hope that scriptPath can be an s3 url to my Hive script, like: s3://bucketName/hive-script. The only documentation I've found talks about using a script from the master node's file system. But if the master node is an instance started for the sake of this Job Flow, I don't understand how I can get any script (Hive or otherwise) onto the file system. When I try my idea (passing an s3 location to the stepFactory method), the runScript step fails. I've checked the logs via the AWS console. The stdout logs end with 2012-11-19 19:28:33 GMT - ERROR Error executing cmd: /home/hadoop/.versions/hive-0.7.1/bin/hive '-f' 's3://anet-emr/scripts/admin.q' '-d rawDataLocation=s3://anet-emr/raw -d year=2010 -d cycle=1' The stderr logs end with java.lang.NoSuchMethodError: org.apache.commons.cli.CommandLine.getOptionProperties(Ljava/lang/String;)Ljava/util/Properties; at org.apache.hadoop.hive.cli.OptionsProcessor.process_stage1(OptionsProcessor.java:115) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:399) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:155) at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79) at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68) and the Controller log has 2012-11-19T19:28:27.406Z INFO Executing /usr/lib/jvm/java-6-sun/bin/java -cp /home/hadoop/conf:/usr/lib/jvm/java-6-sun/lib/tools.jar:/home/hadoop:/home/hadoop/hadoop-0.18-core.jar:/home/hadoop/hadoop-0.18-tools.jar:/home/hadoop/lib/:/home/hadoop/lib/jetty-ext/ -Xmx1000m -Dhadoop.log.dir=/mnt/var/log/hadoop/steps/3 -Dhadoop.log.file=syslog -Dhadoop.home.dir=/home/hadoop -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,DRFA -Djava.io.tmpdir=/mnt/var/lib/hadoop/steps/3/tmp -Djava.library.path=/home/hadoop/lib/native/Linux-i386-32 org.apache.hadoop.mapred.JobShell /mnt/var/lib/hadoop/steps/3/script-runner.jar s3://us-east-1.elasticmapreduce/libs/hive/hive-script --base-path s3://us-east-1.elasticmapreduce/libs/hive/ --hive-versions latest --run-hive-script --args -f s3://anet-emr/scripts/admin.q -d rawDataLocation=s3://anet-emr/raw -d year=2010 -d cycle=1 2012-11-19T19:28:34.143Z INFO Execution ended with ret val 255 2012-11-19T19:28:34.143Z WARN Step failed with bad retval The problem seems to lie with the arguments I'm passing via Amazon's API to Hive's call to the Apache CLI library... I've tried passing a single string with "-d arg1=val1 -d arg2=val2", I've tried "-d,arg1=val1 etc.." and I've tried various ways of chopping up into String arrays - ie { "-d", "arg1=val1" ...}. Can't find any documentation of the proper way to do this! Any help appreciated, thank you Coleman
563e32ed61a8013065267af3	X	Hi this code works for me: Hope this helps :)
563e32ed61a8013065267af4	X	I answered this before, the key part was source.sourceBuffers[0].timestampOffset = duration. However, that no longer works in newer versions of Chrome/Firefox. Even Eric Bidelman's Demo, which everyone seems to use at first, isn't fully working now. Do you have any code that works with a single video? Creating a playlist is tough when I can't even find a basic demo that works any longer.
563e32ed61a8013065267af5	X	I don't have any working code beyond that demo, however the demo that you pointed out does seem to work for me.
563e32ed61a8013065267af6	X	Does the video in that demo actually play the full 6 seconds of the clip? For me, the video stops at 04.19 in Chrome and 05.08 in Firefox with the mediasource.enabled flag, but it used to go the full 6 seconds.
563e32ed61a8013065267af7	X	Ah, no, you're right. Maybe we should abandon this idea until browsers/stdanards are updated. What are your thoughts?
563e32ed61a8013065267af8	X	Yeah, unfortunately I had to table my mediasource plans. I have a hacky fallback where I load the second video in a separate <video> element set to display: none, then toggle display on both and start playing the second when the first finishes. Good enough for our needs right now, but not ideal because of lack of controls for seeking through the "full video" and possible stuttering between clips on slow devices.
563e32ed61a8013065267af9	X	I am trying to achieve a reliable, gapless video playlist in HTML5/JS. To do this, I want to buffer a dynamic playlist in memory and then send the buffer to an HTML5 video element. I have a choice in how the videos are encoded, and this only has to work on the Chrome browser, so I'm thinking of using webm videos and MediaSource extensions. The video files will be stored on Amazon S3 and delivered with CloudFront. I've seen the following example of the MediaSource API. The key difference is that instead of reading chunks of a file, I'm reading in lots of files. http://bluishcoder.co.nz/2013/08/20/progress-towards-media-source-extensions-in-firefox.html How can this be adapted to work with multiple files rather than chunks of a file?
563e32ed61a8013065267afa	X	We ended up writing this Javascript library to handle video playback: https://github.com/jameshadley/LifemirrorPlayer/blob/master/LifemirrorPlayer.js It doesn't use the MediaSourceAPI but it works surprisingly well.
563e32ee61a8013065267afb	X	The answer you provided cannot explain why there is no native public cloud storage service in market.
563e32ee61a8013065267afc	X	I think object storage has a lot to do with scale, that is why is is becoming so popular(Google File System, Amazon S3).
563e32ee61a8013065267afd	X	The nature of object storage allows it to be implemented at HyperScale using distributed architectures.
563e32ee61a8013065267afe	X	Big thanks for your describe. But I know and I read amazons docs. I know about openstack swift, also I know about RADOS object store. But I still dont understand difference between object storage and file storage. Because in all case we save files to file system, and both can be retrieved via url. Both have metadata (for 'object storage' we can add more attributes, thats all difference???). And no one cant give me simple example of difference. How for instance to store image like object using some programming language (for example python, java, php)?
563e32ee61a8013065267aff	X	You say: "I use both Rackspace Cloud files and Amazon S3 (in addition to EBS and Glacier) for backing up, storing, and archiving data." You can do all that (backing up, storing, and archiving data) even if amazon just store your files. I think "object storage" just new marketing word. Because, nobody can givee really usefull example.
563e32ee61a8013065267b00	X	You are correct in that "object storage" is a popular industry and marketing term, some will say its new, yet its been around for at least a decade (e.g. EMC Centera among others). When I backup up files to S3 or Rackspace, the software tool maps the files into objects that are then saved within those repositories. If I go to S3 or Rackspace and look at my accounts, I dont see files/folders per say, rather I see buckets with objects, in those objects are the streams for the backup sets. Did you look at the preso I mentioned?
563e32ee61a8013065267b01	X	You say you know about RADOS, S3, OpenStack Swift, etc having read the docs, on one hand having looked at all of those and more, I can see where it is easy to come to the conclusion of what is the difference. On the other hand, I would also think that you would be able to start to see the difference? Having said that, and industry and marketing hype aside, as well as API access vs. file name access, the lines between the two can be blurred as many scaleout filesystems are themselves object based designs (e.g. Lustre and others). Hence there are object access, and object architectures.
563e32ef61a8013065267b02	X	Thank you again for your kindly reply. How can I understand from here: managedview.emc.com/2012/09/… we can download file (as they say 'object') knowing only IDs (which of course, already in metadata)? Then how system find file by ID (I just want to know thats programming language work or OS)?
563e32ef61a8013065267b03	X	Thank you. But i dont exactly agree with your explanation. In both case we need database to save file location. Then why I need metadata? if I can just save it also in database? And why just PUT API, I also can use POST.
563e32ef61a8013065267b04	X	File systems only have a limited set of metadata (access time, modification time, etc.). If you want to add additional metadata, object storage provides the ability to add additional metadata. With a file system, there is no database whereas in case of object storage there is. Finally, reg the API, you are correct that it could be a PUT or POST. I was just providing an example. All great questions and hope this clarifies everything. Ask more if you need to.
563e32ef61a8013065267b05	X	Thank you! But I already know about that article. I need real example if that possible.
563e32ef61a8013065267b06	X	Could someone explain what difference between Object Storage and File Storage is please? I read about Object Storage on wiki, also I read http://www.dell.com/downloads/global/products/pvaul/en/object-storage-overview.pdf, also I read amazons docs(S3), openstack swift and etc. But could someone give me an example to understand better? All the difference is only that for 'object storage' objects we add more metadata? For example how to store image like object using some programming language (for example python)? Thanks.
563e32ef61a8013065267b07	X	IMO, Object storage has nothing to do with scale because someone could build a FS which is capable of storing a huge number of files, even in a single directory. It is also not about the access methods. HTTP access to data in filesystems has been available in many well known NAS systems. Storage/Access by OID is a way to handle data without bothering about naming it. It could be done on files too. I believe there is an NFS protocol extension that allows this. I would muster this: Object storage is a (new/different) ''object centric'' way of thinking of data, its access and management. Think about these points: What are snapshots today? They are point in time copies of a volume. When a snapshot is taken, all files in the volume are snapped too. Whether all of them like it or not, whether all of them need it or not. A lot of space can get used(wasted?) for a complete volume snapshot while only a few files needed to be snapped. In an object storage system, you will rarely see snapshots of volumes, objects will be snapshot-ed, perhaps automatically. This is object versioning. All objects need not be versioned, each individual object can tell if it is versioned. How are files/volumes protected from a disaster? Typically, in a Disaster Recovery(DR) setup, entire volumes/volume-sets are setup for replication to a DR site. Again, this does not bother whether individual files want to be replicated or not. The unit of disaster protection is the volume. Files are small fry. In an object storage system, DR is not volume centric. Object metadata can decide how many copies should exist and where(geo locations/fault domains). Similarly for other features: Tiering - Objects placed in storage tiers/classes based on its metadata independent of other unrelated objects. Life - Objects move between tiers, change the number of copies, etc, individually, instead of as a group. Authentication - Individual objects can get authenticated from different authentication domains if required. As you can see, the change in thinking is that in an object store, everything is about an object. Contrast this with the traditional way of thinking about and management and access larger containers like volumes(containing files) is not object storage. The features above and their object-centric-ness fits well with the requirements of unstructured data and hence the interest. If a storage system is object(or file) centric instead of volume centric in its thinking, (irrespective of the access protocol or the scale,) it is an object storage system.
563e32ef61a8013065267b08	X	The simple answer is that object accessed storage systems or services utilize APIs and other object access methods for storing, retrieving and looking up data as opposed to traditional file or NAS. For example with file or NAS, you access storage using NFS (Network File System) or CIFS (e.g. windows file share) aka SMB aka SAMBA where the file has a name/handle with associated meta data determined by the file system. The meta data includes info about create, access, modified and other dates, permissions, security, application or file type, or other attributes. Files are limited by the file system in terms of their size, as well as the number of files per file system. Likewise, file systems are limited by their total or aggregate size in terms of space capacity and the number of files in the filesystem. Object access is different in that while file or NAS front-end or gateways or plugins are available for many solutions or services, primary access is via an API where an object can be of arbitrary size (up to the maximum of the object system) along with variable sized meta data (depends on the object system/service implementation). With most object storage systems/services you can specify anywhere from a few Kbytes of user defined meta data or GBytes. What would you use GBytes of meta data for? How about in addition to normal info, adding more data for policies, managements, where other copies are located, thumbnails or small previews of videos, audio, etc. Some examples of object access APIs or interfaces include Amazon Web Services (AWS) simple storage services (S3) or other HTTP and REST based ones, SNIA CDMI. Different solutions will also support IOS (e.g. iphone/ipad) access, SOAP, Torrent, WebDav, JSON, XAM among others plus NFS/CIFS. In addition many of the object storage systems or services support programmatic bindings for python among others. The APIs allow you to essentially open a stream and then get or put, list and other functions supported by the API/system to determine how you will use it. For example, I use both Rackspace Cloud files and Amazon S3 (in addition to EBS and Glacier) for backing up, storing, and archiving data. I can access the objects stored via a web browser or tools including Jungle disk (JD) which is what I backup and synchronize files with. JD handles the object management and moves data to both Rackspace as well as Amazon for me. If I were inclined, I could also do some programming using the APIs and then directly access either of those sites supplying my security credentials to do things with my stored objects. Here is a link to object and cloud storage primer from a session I did in Holland last year that has some simple examples of objects and access. http://storageio.com/DownloadItems/Nijkerk_Nov2012/SIO_IndustryTrends_CloudObjectStorage.pdf Using the programmatic binding, you would define your data structures or objects in your program and then use the APIs or calls for storing, retrieving, listing of data, meta data access etc. If there is a particular object storage system, software or service that you are looking to work with or need to know how to program to, go to their site and you should find their SDK or API info with examples. With objects, once you create your initial bucket or container on a service or with a product/system, you then simply create and store additional objects as you go. Here is a link as an example to AWS S3 API/programming: http://docs.aws.amazon.com/AmazonS3/latest/API/IntroductionAPI.html In theory object storage systems are talked about has having unlimited numbers of objects, or object size, in reality, most systems, solutions, software or services are limited by what they have either tested or currently support, which can be billions of objects, with objects sizes of 5GByte or larger. Pay attention to the limits on specific services or products as to what is actually tested, supported vs. what is architecturally possible or what is implemented on webex or powerpoint. Again its very service and product/service/software dependent as to the number of objects, size of the objects, size of meta data, and amount of data that can be moved in/out via their APIs. However, it is generally safe to assume that object storage can be much more scalable (depending on implementation) than file systems (without using global name space, federation, file virtualization or other techniques). Also in my book Cloud and Virtual Data Storage Networking (CRC Press) that is Intel Recommended Reading, you will find more information about cloud and object storage. I will be adding more related material to www.objectstorage.us soon. Cheers gs
563e32ef61a8013065267b09	X	There are some very fundamental differences between File Storage and Object Storage. File storage presents itself as a file system hierarchy with directories, sub-directories and files. It is great and works beautifully when the number of files is not very large. It also works well when you know exactly where your files are stored. Object storage, on the other hand, typically presents itself via. a RESTful API. There is no concept of a file system. Instead, an application would save a object (files + additional metadata) to the object store via. the PUT API and the object storage would save the object somewhere in the system. The object storage platform would give the application a unique key (analogous to a valet ticket) for that object which the application would store in the application database. If an application wanted to fetch that object, all they would need to do is give the key as part of the GET API and the object would be fetched by the object storage. Hope this is now clear.
563e32ef61a8013065267b0a	X	This link explains the differences between the two: http://www.dell.com/downloads/global/products/pvaul/en/object-storage-overview.pdf
563e32ef61a8013065267b0b	X	I think the white paper explains the idea of object storage quite well. I am not aware of any standard way to use object storage devices (in the sense of a SCSI OSD) from a user application. Object storage is in use in some large scale storage products like the storage appliances of Panasas. However, these appliances then export a file system to the end user. It is IMHO fair to say that the T10 OSD idea never really caught momentum. Related ideas to the OSD standard can be found in cloud storage systems like S3 and RADOS.
563e32ef61a8013065267b0c	X	OpenStack Swift is made for Object Storage & is production at HPcloud, Rackspace, Internap, Korea Telecom, Disney and many others across globe. You can find the release note with the havana release for Swift here You can contribute as developer to the project following the developer doc You can also read detailed explanation from here. Hope it gives a good idea about what is Object Storage & how OpenStack Swift makes most of it & is widely adopted in production for more than 2 years.
563e32ef61a8013065267b0d	X	Oh I wish I could down vote some answers and up vote others with an account. The one with the most votes, as of this writing, doesn't even explain anything about the differences. There are some very fundamental differences between File Storage and Object Storage. File storage presents itself as a file system hierarchy with directories, sub-directories and files. It is great and works beautifully when the number of files is not very large. It also works well when you know exactly where your files are stored. Object storage, on the other hand, typically presents itself via. a RESTful API. There is no concept of a file system. Instead, an application would save a object (files + additional metadata) to the object store via. the PUT API and the object storage would save the object somewhere in the system. The object storage platform would give the application a unique key (analogous to a valet ticket) for that object which the application would store in the application database. If an application wanted to fetch that object, all they would need to do is give the key as part of the GET API and the object would be fetched by the object storage. Hope this is now clear. This explained a large portion of it; but you argued about the meta data. The following is from what I have been reading these last two days, and since this hasn't been resolved, I will post. Object storage has no sense of folders, or any kind of organization structure which makes it easy for a human to organize. File Storage, of course, does have all those folders that make it so easy for a human to organize and shuffle through...In a server environment with the number of files in a scale that is astronomical, folders are just a waste of space and time. Databases you say? Well he's not talking about the Object storage itself, he is saying your http service (php, webmail, etc) has the unique ID in its database to reference a file that may have a human recognizable name. Metadata, well where is this file stored you say? That's what the metadata is for. You single file is split up into a bunch of small pieces and spread out of geographic location, servers, and hard drives. These small pieces also contain more data, they contain parity information for the other pieces of data, or maybe even outright duplication. The metadata is used to locate every piece of data for that file over different geographic locations, data centres, servers and hard drives as well as being used to restore any destroyed pieces from hardware failure. It does this automatically. It will even fluidly move these pieces around to have a better spread. It will even recreate a piece that is gone and store it on a new good hard drive. This maybe a simple explanation; but I think it might help you better understand. I believe file storage can do the same thing with the metadata; but file storage is storage that you can organize as a human (folders, hierarchy and such) whereas object storage has no hierarchy, no folders, just a flat storage container.
563e32f061a8013065267b0e	X	Most companies with object based solutions have a mix of block/file/object storage chosen based on performance/cost reqs. From a use case perspective: Ultimately object storage was created to address unstructured data which is growing explosively, far quicker than structured data. For example, if a database is structured data, unstructured would be a word doc or PDF. How do you search 1 billion PDFs in a file system? (if it could even store that many in the first place). How quickly could you search just the metadata of 1 billion files? Object storage is currently used more for long term or archival, cheap and deep storage, that keeps track of more detail of what that data is. This metadata becomes very powerful when searching or mining very large data sets. Sometimes you can get what you need from the metadata without even accessing the data itself. Object storage solutions can typically replicate automatically with geographic failover built-in. The problem is that application would have to be re-written to use object access methods rather than file hierarchy (which is simpler from a app dev perspective). It's really a change in the philosophy of data storage, and storing more actionable information about that data from a management standpoint as well as usage. Quick example might be an MRI scan image. On Filesystem you have owner/creation date, but not much else. If it were an object, all of the information surrounding the MRI could be stored along with it in metadata, like patient name, MRI center location, the requesting Dr., insurance carrier, etc. Block/file are more well suited for local access or OTLP where performance is more important than retention and cost. For example, you would not want to wait minutes for a Word doc to open, but you could wait a few minutes for a data mining/business intelligence process to complete. Another example would be a legal search where you have to search everything from 5 years ago to present. With retention policies in place to decrease the active data set and cost, how would you even do that without restoring from tape? Object storage is a great solution for replacing long term archival methods like tape. Setting up replication and failover for block and file can get very expensive in the enterprise and usually requires very expensive software and services. Note: At the lower level, object storage access happens via the RESTful API which is more like a web request than accessing a file at the end of a path.
563e32f061a8013065267b0f	X	Actually you can mount an bucket/container and access the objects or subfolders (and their objects) from Linux. For example, I have s3fs installed on Ubuntu that I have setup a mount point to one of my S3 buckets and able to do regular cp, ls and other functions just as though it were another filesystem. The key is getting the software tool of which there are plenty that allows you to map a bucket/container and present it as mount point. There are also software tools that allow you to access S3 and other buckets/containers via iSCSI in addition to as NAS.
563e32f061a8013065267b10	X	Thanks! A rough estimate is that with the current application I use for transfer I would end up with around 1-1.5 milj transactions per month if I transfer 1 16GB file per day. Wich is not that much cost wise even if it was 10 or 50 times that. I think I understand it better now. Thanks!
563e32f061a8013065267b11	X	Storage is so cheap these days that often theactual discussion about cost optimisations (using everybody's hourly rate) is more expensive than a years' worth of optimisation! Not off-topic though - developers need to know how much things will cost, so it is a good and valid question.
563e32f061a8013065267b12	X	we are considering using Azure blob storage as storage for our backups. But we are not sure of what the transaction price would mean for us in reality. (they charge cost per storage volume and cost per transactions) For example if I transfer one 16 GB file to the storage every day (and deleting so I in the end always keep 10 versions). Does that only mean 1 transaction per day (+ maybe a few for listing and such) or is a transaction like per packet of some size so that it will cost me loads each day. or what does the transaction mean?
563e32f061a8013065267b13	X	Be careful, it may not be as simple as you think. Firstly, it depends on if you are using page or block blobs. It also depends on what library you are using to upload the blob. For block blobs, the storage client has a default value of the maximum size of the block being uploaded (32MB) and will split the file into n blocks - each block will be a transaction (see Understanding Block Blobs and Page Blobs. You will also need to consider retries, and as you point out, listing, deleting etc. I suggest you look closely at how you are backing up and find the size of the blocks - then do the calculations. Then do some controlled trails in an isolated account and see if you can reconcile the billing transactions against your estimate.
563e32f061a8013065267b14	X	Do take a look at this blog post from Storage team about billing: http://blogs.msdn.com/b/windowsazurestorage/archive/2010/07/09/understanding-windows-azure-storage-billing-bandwidth-transactions-and-capacity.aspx To summarize, you're charged for 3 things in Windows Azure Storage: I also built a simple calculator which would give you a rough idea about your Windows Azure Blob Storage bill. You can use this calculator here: http://gauravmantri.com/2012/09/03/simple-calculator-for-comparing-windows-azure-blob-storage-and-amazon-s3-pricing/. It was basically built to compare Amazon S3 costs and Windows Azure Blob Storage costs but can be used for just Windows Azure Blob Storage as well.
563e32f061a8013065267b15	X	I'd say your first guess : 1 transaction per day, based on their explanation : Transactions – Each individual Blob, Table and Queue REST request to the storage service is considered as a potential transaction for billing. Applications can then control their transaction costs by controlling how often and how many requests they send to the storage service. We analyze each request received and then classify it as billable or not billable based upon our ability to process the request and the request’s outcome. Quoted from here. But the best thing to do would be to go for the trial. I think Azure is free for a certain time, that would allow you to see how many requests are really going.
563e32f161a8013065267b16	X	As I understand, Store Kit API will handle storage and retrieval of history related to "Non-Consumables" products, while for "Consumables" and "Subscriptions" you have to have your own server/backend. What are the best hosted existing solutions for that kind of server (backend)? Inexpensive (or free? :-) and reliable? Thanks!
563e32f161a8013065267b17	X	There is not any service (that I know of) that includes the needed functionality by default. There is more here than just hosting the files. You need to use the included functionality to verify the receipt that is sent from the StoreKit API to your server (to ensure that it is a valid receipt): http://developer.apple.com/library/ios/#documentation/NetworkingInternet/Conceptual/StoreKitGuide/VerifyingStoreReceipts/VerifyingStoreReceipts.html So you would need to have a web application that managed this process as well as hosting the actual consumable content (in a way that made it inaccessible from the outside). You might want to look at a Java application hosted on a solution like Elastic Beanstalk with a connection to a protected Amazon S3 account. If this is too complex, you could also create a simple PHP application that could be run from most any web server that could also do this functionality and manage access to the S3 account's files.
563e32f161a8013065267b18	X	Usually best practice questions are not well received in SO, so it'd be better you you shared at least a bit of code to show what you've tried so far. Bu anyway, take a look at the picasso library, it's an elegant way of solving your problem.
563e32f161a8013065267b19	X	you can upload and save the image in either web or file system and save the corresponding path in sqllite db. to change the picture by you, you can save it in filesystem of your server where the app can connect and you can change them there. download would be just reading the images/buffer from the path stored in sqlite db during upload.
563e32f161a8013065267b1a	X	I will write to you later. I just finished a view that load image and save it to cache memory.
563e32f161a8013065267b1b	X	@EdsonMenegatti thank you, the picasso library looks definitly like a solution to a part of my problem!
563e32f161a8013065267b1c	X	@ssh thank you, i would really appreciate that!
563e32f161a8013065267b1d	X	yeah, seems like Picasso library is an absolutely must have for this problem!
563e32f261a8013065267b1e	X	thank you for your input, the links seems to contain a lot of useful information for my problem!
563e32f261a8013065267b1f	X	thank you for your help. Do you also know an option for storing images online which is for free? (if there is one...)
563e32f261a8013065267b20	X	thank you for the answer! could you be more specific on how it works to make such kind of API and where to host this flat file? GCM sounds interesting, that is definitely an option
563e32f261a8013065267b21	X	Well you would need to get a server to host it on, depending on the amount of traffic you generate you could perhaps find a free one. Putting a lets say json file on a server shouldn't be too expensive. Alternatively you could write a regular API in a language you are proficient with, but it's probably overkill for such a simple case.
563e32f261a8013065267b22	X	How would you approach this problem: My app should download different packages of pictures (containing up to 300 pngs, each about 20 kb) and store the pictures on the phone, so i can display them. I want to upload the pictures somewhere online, so I can change them every time and the user can enjoy the newest pictures. (I upload the pictures not with the app) I read that storing them in a sqlite db isn't the best option. At the moment I am storing the pictures in the app, but then I don't know how I can upload and replace pictures on all apps immediately without the need of updating the whole app. I don't need code or stuff, so don't waste your precious time on that, just some general hints where and how you would store the pictures online, and how android can download the pictures easily.
563e32f261a8013065267b23	X	Take a look at the Glide or Picasso libraries. Those are super easy to use for thread-safe downloading of images. Personally, I just fetch/store the images on imgur. If you want to upload a dedicated databse, you'll have to set one up. Some common ones are Amazon, Google, etc. There are tons.
563e32f261a8013065267b24	X	Have a look at this answer. In this answer Picasso library is used to handle image download. Picasso gets rid of a lot of coding and testing to handle image download. In a project that I am working on, we use Amazon S3 to store our pictures, it's very reliable and is one of the goto solutions right now. From what I heard Snapchat and some other big firms use S3 to store their picture! It's also very cheap, plus I believe they have free hosting to a certain degree. This is their API guide for android. We use a service called File Picker to handle upload and download from amazonS3, it reduces a lot of work, but I don't think it's a free service.
563e32f261a8013065267b25	X	You can use Picasso for downloading images from network in Android. For storing images Amazon S3 or Google cloud storage can be your options.
563e32f261a8013065267b26	X	Not sure if downloading packages is better than downloading individual pictures (archiving won't save you much space). As for your question, you can make some kind of API you will query from your app, even a flat file hosted somewhere with changing content would work. Your app could check it periodically for the new address to download pictures from (assuming it will change). another way is using push messages - sending out a push through GCM that your apps will receive that will notify them about new content available. It would even work when the app is closed.
563e32f361a8013065267b27	X	Can you catch the exception and retry? The 2nd request should b OK
563e32f361a8013065267b28	X	When you call connect, do you receive onConnected() callback? There shouldn't be any need to catch an exception in the app and try again.
563e32f361a8013065267b29	X	I need to track down the device that is presenting with the error and debug whether I'm getting the onConnected() callback. I will update after I test that.
563e32f361a8013065267b2a	X	Just remember that until you receive onConnected, you should not try to launch your app
563e32f361a8013065267b2b	X	We don't try to launch the app until we receive the onConnected callback.
563e32f361a8013065267b2c	X	Interestingly enough, the Chromecasts could not be seen when the time was 2 hours off. It did not reproduce the issue that I described.
563e32f361a8013065267b2d	X	I work on a Chromecast app that has been out on the market for a little while. We started to receive reports that people cannot connect to the Chromecast on the first try after it has been booted up. We could not reproduce this until recently (and only one device happens to exhibit this behavior) The Chromecast icon appears that it is connected, but the app never launches. Eventually the Chromecast icon shows that it is disconnected. I grabbed a logcat from this device. This appears to be all in the Chromecast API in Google Play Services. We use Amazon S3 to host and use their SSL certificate. It seems odd that after this initial error, the devices do connect. I haven't been able to wrap my brain around this one. Our app is only available on Android devices, and not all devices exhibit this behavior.
563e32f361a8013065267b2e	X	The "CertificateNotYetValidException" exception usually happens when the client's clock is off. Perhaps the users that encountered this issue have their current date set in the past. You can reproduce this by changing your device's current date to be in the past.
563e32f461a8013065267b2f	X	And that traffic between Windows Azure and Amazon datacenters is charged for (bandwidth in/out).
563e32f461a8013065267b30	X	I'm currently looking at Windows Azure to host an ElasticSearch implementation. Loading the application and running it under Java is not that difficult. Currently, ElasticSearch only supports Amazon's S3 when it comes to cloud storage. As a result, I've made a request to add support for Azure Blob Storage in ElasticSearch. Right after I made the request, it occured to me that while I can host ElasticSearch in Azure, I can create an Amazon S3 account and then have the instance running in Azure connect to the S3 account for storage. However, I do have concerns about the speed between the two. While I am sure both Azure Storage and Amazon's S3 are both optimized for really fast speeds, I have a nagging feeling that storage systems are really optimized when accessed from their respective computing clusters. That said, is there any definitive information on this? It makes sense, but I'm looking for specific confirmation or denial.
563e32f461a8013065267b31	X	It's not so much a matter of optimization of the Azure storage API for Azure Roles, but simply a matter of physical co-location and network distance / number of hops. You can (and should) specify that your Azure storage service resides in the same data center as the Azure roles that will be using that storage service. You can expect network bandwidth to be greatest and latency to be lowest between an Azure role and an Azure storage residing in the same data center. Bandwidth will be lower and latency higher when your Azure role connects to anything outside of its own data center - be that Azure storage in another data center, or Amazon S3 storage in another data center. Besides performance, also keep in mind that you pay for all data traffic in and out of the Azure data center for your services. Having your Azure role accessing data on Amazon S3 or in another Azure data center will take a bite out of your bandwidth quota, whereas accessing Azure storage within the same data center costs you nothing, no matter how much traffic you use between your role and your Azure storage.
563e32f461a8013065267b32	X	Is it possible to delete a folder with the SDK that still contains content? It is possible to do in the amazon console but doesn't appear to work with the php sdk.
563e32f561a8013065267b33	X	Not Sure What you are asking for , if you want to delete an object completely here is the code for that using Amazon SDK for PHP, If you want to delete a specified version of object of S3 then the code here does that You can find still more info on API calls here AMAZON SDK FOR PHP , API's FOR S3
563e32f561a8013065267b34	X	You can use delete_bucket(). There's a parameter that allows you to "force" delete the bucket and all of its contents. There are also delete_all_objects() and delete_all_object_versions().
563e32f561a8013065267b35	X	Are skip/take properly pagination? How this implemented? Fullscan?
563e32f561a8013065267b36	X	1) Client Access: Is there anyway to perform CRUD operations on DynamoDB using client side JavaScript (REST/Ajax/jQuery)? I know Amazon has support for .NET and Java. 2) Server Access: Is there any way we can access DynamoDB using server side JavaScript (Node.js) without having to install Java/.NET on the server?
563e32f561a8013065267b37	X	Update 2012-12-05 There is now an official AWS SDK for Node.js, see the introductory post AWS SDK for Node.js - Now Available in Preview Form for details, here are the initially supported services: The SDK supports Amazon S3, Amazon EC2, Amazon DynamoDB, and the Amazon Simple Workflow Service, with support for additional services on the drawing board. [emphasis mine] Update 2012-02-27 Wantworthy has implemented a Node.js module for accessing Amazon DynamoDB a week after its launch date, thus covering 2) as well, see dynode: Dynode is designed to be a simple and easy way to work with Amazon's DynamoDB service. Amazon's http api is complicated and non obvious how to interact with it. This client aims to offer a simplified more obvious way of working with DynamoDB, but without getting in your way or limiting what you can do with DynamoDB. Update 2012-02-11 Peng Xie has implemented a Node.js module for accessing Amazon DynamoDB at its launch date basically, thus covering 2) already, see dynamoDB: DynamoDB uses JSON for communication. [...] This module wraps up the request and takes care of authentication. The user will be responsible for crafting the request and consuming the result. Unfortunately there is no official/complete JavaScript SDK for AWS as of today (see AWS Software Development Kits and boto [Python] for the available offerings). Fortunately decent coverage for several AWS services in JavaScript is provided by the Node.js library aws-lib already though, which would be a good starting point for adding DynamoDB accordingly. An as of today unresolved feature request to Add support for DynamoDB has been filed already as well. Further, AWS forum user gmlvsk3 has recently implemented dedicated JavaScript interface for DynamoDB, but supposedly you need [a] Java runtime to run it, because it is based on the Mozilla Rhino JavaScript engine - I haven't reviewed the code in detail yet (at first sight it looks a bit immature though in comparison to e.g. aws-lib, but may cover your needs regardless of course), so you should check it out yourself. Finally, you can implement JavaScript HTTP Requests to Amazon DynamoDB yourself of course (see the API Reference for Amazon DynamoDB for details): If you don't use one of the AWS SDKs, you can perform Amazon DynamoDB operations over HTTP using the POST request method. The POST method requires you to specify the operation in the header of the request and provide the data for the operation in JSON format in the body of the request.
563e32f561a8013065267b38	X	I created a module called Dino to make it easier to work with the AWS SDK in web applications. You can use something like Restify to expose your data to jQuery via a REST interface. Suppose you wanted to display pages of blog posts for a user. Using Dino and Restify, you would do the following:
563e32f561a8013065267b39	X	Regarding 1), there is now the AWS SDK for JavaScript in the Browser that allows you to access services including DynamoDB.
563e32f561a8013065267b3a	X	as for 2) we've been working as well since DDB launch date. One of its key features are simplicity/performance and how close it is (retry behavior, etc) to Amazon official Java/PHP libraries: https://github.com/teleportd/node-dynamodb It's successfully used in production at various places with 100+ write/s (at teleportd). Additionally we're working on a a mocked version to enable efficient testing of the library's client code.
563e32f661a8013065267b3b	X	Can you elaborate more on what you're planning to do? This is an interesting question but a bit broad. I wrote a (non distributed) NoSQL database in Go so I can share some insight.
563e32f661a8013065267b3c	X	Basically, my current plan is building a distributed NoSQL, might be a Key-value store, that is designed for high reliability, also designed for high write performance. It should be able to be the backend storage for hadoop MapReduce tasks, so we can run "hadoop jar foo.jar gonosql://some/key/prefix" to use it.
563e32f661a8013065267b3d	X	I would like to build a distributed NoSQL database or key-value store using golang, to learn golang and practice distribute system knowledge I've learnt from school. The target use case I can think of is running MapReduce on top of it, and implement a HDFS-compatible "filesystem" to expose the data to Hadoop, similar to running Hadoop on Ceph and Amazon S3. My question is, what difficulties should I expect to integrate such an NoSQl database with Hadoop? Or integrate with other languages (e.g., providing Ruby/Python/Node.js/C++ APIs?) if I use golang to build the system.
563e32f661a8013065267b3e	X	Ok, I'm not much of a Hadoop user so I'll give you some more general lessons learned about the issues you'll face: Protocol. If you're going with REST Go will be fine, but expect to find some gotchas in the default HTTP library's defaults (not expiring idle keepalive connections, not necessarily knowing when a reader has closed a stream). But if you want something more compact, know that: a. the Thrift implementation for Go, last I checked, was lacking and relatively slow. b. Go has great support for RPC but it might not play well with other languages. So you might want to check out protobuf, or work on top the redis protocol or something like that. GC. Go's GC is very simplistic (STW, not generational, etc). If you plan on heavy memory caching in the orders of multiple Gs, expect GC pauses all over the place. There are techniques to reduce GC pressure but the straight forward Go idioms aren't usually optimized for that. mmap'ing in Go is not straightforward, so it will be a bit of a struggle if you want to leverage that. Besides slices, lists and maps, you won't have a lot of built in data structures to work with, like a Set type. There are tons of good implementations of them out there, but you'll have to do some digging up. Take the time to learn concurrency patterns and interface patterns in Go. It's a bit different than other languages, and as a rule of thumb, if you find yourself struggling with a pattern from other languages, you're probably doing it wrong. A good talk about Go concurrency is this one IMHO http://www.youtube.com/watch?v=QDDwwePbDtw A few projects you might want to have a look at: Groupcache - a distributed key/value cache written in Go by Brad Fitzpatrick for Google's own use. It's a great implementation of a simple yet super robust distributed system in Go. https://github.com/golang/groupcache and check out Brad's presentation about it: http://talks.golang.org/2013/oscon-dl.slide InfluxDB which includes a Go based version of the great Raft algorithm: https://github.com/influxdb/influxdb My own humble (pretty dead) project, a redis compliant database that's based on a plugin architecture. My Go has improved since, but it has some nice parts, and it includes a pretty fast server for the redis protocol. https://bitbucket.org/dvirsky/boilerdb
563e32f661a8013065267b3f	X	I want to enable SSE-S3 on Amazon S3. I click properties and check the encryption box for AES-256. It says encrypting, then done. But I can still read the files without providing a key, and when I check properties again, it shows the radio buttons unchecked. Did I do this correctly? Is it encrypted? So confusing. 
563e32f661a8013065267b40	X	You're looking at a view of a bucket in the S3 console that shows more than one file, or shows only one file but that file isn't selected. The radio buttons allow you to set all items you select to the values you select in the radio buttons, but the radio buttons remain blank whenever multiple files are shown, because they're only there to let you make a change -- not to show you the values of existing object. Click on an individual file and view its properties and you'll see that the file is stored with server-side-encryption = AES256. Yes, you can download the file without needing to decrypt it, because this feature is server-side encryption of data at rest -- the files are encrypted by S3 prior to storage on the physical media that S3 runs on. This is often done for compliance purposes, where regulatory restrictions or other contractual obligations require that data to be encrypted at rest. The encryption keys are stored, separately from the object by S3, and are managed by S3. In fact, the encryption keys are actually stored, encrypted, by S3. (They generate a key for each object, and store that key in an encrypted form, using a master key). Decryption of the encrypted data requires no effort on your part. When you GET an encrypted object, we fetch and decrypt the key, and then use it to decrypt your data. https://aws.amazon.com/blogs/aws/new-amazon-s3-server-side-encryption/ For data in transit, S3 encrypts that whenever you use HTTPS. Different than the feature that's available in the console, S3 also supports server-side AES-256 encryption with keys you manage. In this scenario, called SSE-C, you still aren't responsible for the actual encryption/decryption, because S3 still does that for you. The difference is that S3 doesn't store the key, and you have to present the key to S3 with a GET request in order for S3 to fetch the object, decrypt it, and return it to you. If you don't provide the correct key, S3 won't bother to return the object -- not even in encrypted form. S3 knows whether you've sent the right key with a GET request, because S3 stores a salted HMAC of the key along with the object, for validation of the key you send when you try to fetch the object, later. This capability -- where you manage your own keys -- requires HTTPS (otherwise you'd be sending your encryption key accross the Internet unencrypted) and is only accessible through the API, not the console. You cannot use the Amazon S3 console to upload an object and request SSE-C. You also cannot use the console to update (for example, change the storage class or add metadata) an existing object stored using SSE-C. http://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html And, of course, this method -- with customer-managed keys -- is particularly dangerous if you don't have a solid key-management infrastructure, because if you lose the key you used to upload a file, that file is, for all practical purposes, lost.
563e32f761a8013065267b41	X	Saying "by use of parameters" is vague. Try doing this in your PHP script: var_dump($_POST); var_dump($_GET); var_dump($_FILES); and see where the image comes in.
563e32f761a8013065267b42	X	Or does it pass the image back via JavaScript? If so, does it pass the binary string of the image back, or does it pass a URL to the image? If it is passed back to JavaScript (which is executing on the client-side), then you will need an AJAX call to pass the image to your PHP script, which can then save it to a file.
563e32f761a8013065267b43	X	The image is coming back in a binary string via POST method
563e32f761a8013065267b44	X	Thanks much Travesty3. All images will be in png format. How would I add this extension to the file name after it is on my server?
563e32f761a8013065267b45	X	I'm getting these errors: Warning: fopen() [function.fopen]: Filename cannot be empty in ../test/target.php on line 4 and this one: Warning: fclose(): supplied argument is not a valid stream resource in ../test/test.php on line 6
563e32f761a8013065267b46	X	@user1322707: You need to set the value of $imageName before doing this. You must specify the absolute path to the folder and give the file a name, ending with .png. It completely depends on where on your server that you want to save the file. If you are using a Linux server, you might be able to use something like $imageName = "/var/www/uploadedPictures/". time() .".png";. You can name the file whatever you want, but you want to give it some sort of unique name to make sure that subsequent uploads don't overwrite it.
563e32f761a8013065267b47	X	Thanks again Travesty3. We're close! An image file is being saved, but when attempting to open it, it's empty. Also, I get the following error, pointing to fclose($outputFile);. Here is the error: Warning: fclose(): supplied argument is not a valid stream resource in /home/path/domain.com/test.php on line 8 where test.php is the file with the code in it.
563e32f761a8013065267b48	X	@user1322707: My fault. Copy and paste error in my code. Use fclose($handle);.
563e32f761a8013065267b49	X	I am using the API of an image editing website (pixlr.com) for use by members of my site. I open Pixlr.com in an iframe where they can create an image and upon SAVE, pixlr sends the image file by use of parameters. I want to save these image files (unique for each member) in a folder on my server (or on Amazon's S3 image server), using PHP. How do I receive their parameters ("image") of the image file and store them on my/Amazon's image server?
563e32f761a8013065267b4a	X	If the image is sent to your PHP script via POST, then you should be able to do something like this: Where $imageName is the absolute path and filename of the image where you want to save it (make sure you Apache user has write permissions to that directory). Depending on the picture's encoding you may need to figure out which extension to save it with (ie .jpg, .bmp, .png, etc).   Looks like they are sending the image via $_FILES. Try this:
563e32f861a8013065267b4b	X	What database are you using?
563e32f861a8013065267b4c	X	I was going to up vote this comment until you stated that you think it is better to put the file in the database :-\ if you have lots of images stored in the database, it can cause serious performance issues.
563e32f861a8013065267b4d	X	I understand there is a limitation. but by my own experience, the sql server handles it pretty well... :-)
563e32f861a8013065267b4e	X	Andrew, see my comment re: FILESTREAM storage - that addresses the performance concern
563e32f861a8013065267b4f	X	If you are in a load balanced environment though, you will want to either have a server designated as a resource server, or create some sort of duplication mechinism between the servers.
563e32f861a8013065267b50	X	What would be the best method to implement the following scenario: The web site calls for a image gallery that has both private and public images to be stored. I've heard that you can either store them in a file hierarchy or a database. In a file hierarchy setup how would prevent direct access to the image. In a database setup access to the images would only be possible via the web page view. What would be a effective solution to pursue? [Edit] Thanks all for the responses. I decided that the database route is the best option for this application since I do not have direct access to the server. Confined to a webroot folder. All the responses were most appreciated.
563e32f861a8013065267b51	X	Having used both methods I'd say go with the database. If you store them on the filestore and they need protecting then you'd have to store them outside the web-root and then use a handler (like John mentions) to retrieve them, anyway. It's as easy to write a handler to stream them direct from database and you get a few advantages: The disadvantage is that of performance, but you can use caching etc. to help with that. You can also use FILESTREAM storeage in SQL Server 2008 (and 05?) which means you get filesystem performance but via the DB: "FILESTREAM integrates the SQL Server Database Engine with an NTFS file system by storing varbinary(max) binary large object (BLOB) data as files on the file system. Transact-SQL statements can insert, update, query, search, and back up FILESTREAM data. Win32 file system interfaces provide streaming access to the data. FILESTREAM uses the NT system cache for caching file data. This helps reduce any effect that FILESTREAM data might have on Database Engine performance. The SQL Server buffer pool is not used; therefore, this memory is available for query processing."
563e32f861a8013065267b52	X	Using file hierarchy, you can put the files out of the website file folder, for example, suppose the web folder is c:/inetpub/wwwroot/somesite, put the file under c:/images/, so that the web users won't be able to access the image files. but you cannot use the direct link in your website neither, you need to create some procedure to read the file, return the stream. personally I think it's better to put the file in the database, still create some procedure to retrieve the binary image data and return to wherever it needed.
563e32f961a8013065267b53	X	In reality both scenarios are very similar, so it's up to you... Databases weren't designed to serve files, but if the size isn't really a concern for you, I don't see a problem with doing it. To answer your question about direct access, you'd setup the file images the same way you would for the database: You'd use some sort of page (probably a .ashx handler) that serves the images, allowing you a layer of logic between the user and image to determine whether or not they should have access to it. The actual directory the images are located in would then need to either a) not be part of the directory structure in IIS or b) if it is part of IIS, only allow windows authenticated access, and only allow the account the application process is running under access to the directory.
563e32f961a8013065267b54	X	If you're using IIS7, since .net jumps in the pipeline early I believe you can protect jpg files as well, just by using a role manager and applying roles to file system folders. If you're using IIS6, I've done something similar to the answer by John, where I store the actual file outside of the wwwroot, and use a handler to decide if the user has the correct credentials to view the image. I would avoid the database unless you have a strong reason to do this - and I don't think a photo gallery is one of them.
563e32f961a8013065267b55	X	Neither. Amazon S3 offers a very simple API for accepting uploads. You can use SimpleDB or your SQL database to track the URLs and permissions. Set the entire S3 bucket to private, and authenticate to it using your AWS key on the ASP.NET server. Very little code is required to upload to S3, and very little more would be required to perform bookeeping in SQL. Once they're in S3, grab the image resizer library and the S3 Reader plugin and you can have your entire system running in under an hour. And - it will scale properly. No disk or database space limits. Ever. You can implement authorization using the AuthorizeImage event of the Image Resizer library. Just throw an AccessDeniedException if access isn't allowed for the current user. If you want to tune performance a bit mare, add both the DiskCache and CloudFront plugins. CloudFront can edge-cache the public images (inexpensively), and DiskCache will handle the private images, serving them at static-file speeds.
563e32fa61a8013065267b56	X	
563e32fa61a8013065267b57	X	Requests: There's a Drive API daily quota of 500,000 requests/day ("courtesy limit") which you can find from your API dashboard (https://code.google.com/apis/console/) Bandwidth: Google Apps for Business and Education have published bandwidth limits although I don't know if these are the same as for the Drive API: http://support.google.com/a/bin/answer.py?hl=en&answer=1071518 Compared to Amazon's S3 / CloudFront? I'm not convinced that it's a like for like comparison. Drive would have to be compared to DropBox, Box.com, SkyDrive, iCloud and the rest. Google Cloud Storage enables application developers to store their data on Google’s infrastructure with very high reliability, performance and availability. If you’re looking to store personal data in the cloud, consider using Google Drive instead. Or, if you are are an app developer who wants to integrate with Google Drive, you can do so using the Google Drive SDK. https://developers.google.com/storage/docs/getting-started
563e32fa61a8013065267b58	X	This post on the AWS blog suggests otherwise: java.awsblog.com/post/Tx2Q9SGR6OKSVYX/Amazon-S3-TransferManager
563e32fa61a8013065267b59	X	@DGolberg no, it's not, that blog post doesn't claim otherwise and besides I've got proof after several hours of checking out the source code and doing profiling. The API of TransferManager is non-blocking, in the sense that it's offloading work to a configured thread-pool, but the threads in that thread-pool are blocking and hence unavailable for doing anything else. I'll probably write an article about it. Thanks for the down-vote, wasn't necessary.
563e32fa61a8013065267b5a	X	@AlexandruNedelcu Indeed I believe you are correct, it's asynchronous in the sense that if you are using it then you can continue doing other things in whatever thread you made the API call, but the underlying implementation just makes another thread to do the work. Have an upvote =D
563e32fb61a8013065267b5b	X	@AlexandruNedelcu I guess I interpreted your question incorrectly; I apologize for that. Use of the transfer manager on the current thread is asynchronous, but no, the underlying threads are not, as you've stated. I need to remember to stop trying to answer questions when hindered by a lack of sleep...
563e32fb61a8013065267b5c	X	@DGolberg that's OK, I asked myself this question because I was wondering in an application whether I should use the global and limited thread-pool that the app has configured for CPU-bound tasks or not. It would have been cool for efficiency reasons, but we've got a thread-pool meant for blocking I/O stuff anyway and the TransferManager is still pretty cool because it can batch multiple uploads together.
563e32fb61a8013065267b5d	X	I've been reading about TransferManager in the Amazon's AWS SDK for doing S3 uploads, the provided API allows for non-blocking usage, however it's unclear to me if the underlying implementation actually does asynchronous I/O. I did some reading on the source-code of TransferManager and I cannot understand if the threads in the provided ExecutorService are being blocked or not. My problem is that if this manager actually does asynchronous I/O without blocking that executor, then I could use the application's global thread-pool that is meant for CPU-bound stuff. So is this actually doing asynchronous I/O or not?
563e32fb61a8013065267b5e	X	After profiling and trying to understand the SDK's source-code I have come to the conclusion that yes, TransferManager does not work asynchronously, because it piggybacks on AmazonS3Client.putObject and such calls, while not blocking the threads per se, go in a loop until the http requests are finished, thus preventing progress in processing the thread-pool's queue.
563e32fb61a8013065267b5f	X	This sounds to me the best usecase describe so far and moreover your comment on the other answer confirms what I kind of think/feel : multi module provides value for Multipackaging or huge project.
563e32fb61a8013065267b60	X	I'm by no means a fanboi of maven but there are some cases where it has "just worked" and this seems to be one of its strengths to me. Glad the answer helped.
563e32fb61a8013065267b61	X	I am actually plotting to use this answer to demonstrate in which cases the multi module should not be used.
563e32fc61a8013065267b62	X	I know this is old, but I was wondering what the benefit of including the api / impl with the webapp was? I guess this just furthers the original question for me - I see that maybe the impl should be modularized with the API, but why things like the webapp and the other consumers of the API?
563e32fc61a8013065267b63	X	@BrandonV So you are thinking that rather than having the webapp be a module within the overall build it would be a project of its own with a dependency on the project that contains the api/impl modules? I can see that as a totally valid approach. In my case I didn't see or experience any pain with it being a single project with multiple modules. Since it was all so closely related I didn't want to take the time to separate it all out into a separate project and found it easier to just make modules within my own project.
563e32fc61a8013065267b64	X	The build order seems a valid argument to me but can you maybe describe why you would need 30 (!) submodules ?
563e32fc61a8013065267b65	X	I have worked in a situation where we had 12-15 teams working on a single project. We had a main project that had around 30 modules. It allowed each team to have ownership of their area (2-4 modules) and provided clear separation of responsibilities among teams. It wasn't perfect, but it wasn't as bad as having 50 developers all mucking around in the same packages all the time (which I've also experienced at another time)
563e32fc61a8013065267b66	X	I have some years of experience with maven projects, even with multi modules ones (which has made me hate the multi modules feature of maven (so the disclaimer is now done)) and even if I really like maven there is something I cannot get a clear answer about : What is a typical usecase of a multi module maven project ? What is the added value of such a structure compared to simple dependencies and parent pom ? I have seen a lot of configuration of multi module projects but all of them could have clearly been addressed by creating a simple structure of dependency library living their own life as deliverables (even with a parent pom, as a separate deliverable : factorising depedencies and configuration) and I haven't found any usecase where I can clearly see an added value of the multi module structure. I have always found that this kind of structure brings an overkilling complexity with no real benefit : where am I missing something ? (to be truly honest, I can get that some ear can benefit from this kind of structure but except from that particular usecase, any other real use and benefit ?)
563e32fc61a8013065267b67	X	Here's a real life case. I have a multi-module project (and to your rant... I haven't seen any complications with it.) The end result is a webapp but I have different modules for api, impl, and webapp. 12 months after creating the project I find that I have to integrate with Amazon S3 using a stand-alone process run from a jar. I add a new module which depends on api/impl and write my code for the integration in the new module. I use the assembly plugin (or something like it) to create a runnable jar and now I have a war I can deploy in tomcat and a process I can deploy on another server. I have no web classes in my S3 integration process and I have no Amazon dependencies in my webapp but I can share all the stuff in api and impl. 3 months after that we decide to create a REST webapp. We want to do it as a separate app instead of just new URL mappings in the existing webapp. Simple. One more module, another webapp created as the result of the maven build with no special tinkering. Business logic is shared easily between webapp and rest-webapp and I can deploy them as needed.
563e32fc61a8013065267b68	X	The major benefit of multi modules are I already worked in a project with about 30 submodules. Sometimes, you need to change something in more than module, and running one single command and being sure that everything that need to be compiled is compiled in the correct order is a must. EDIT Why 30 submodules ? Huge framework with lot's a features, lot's of developers, separation of features on a module base. It's a real life use case and the separation of the code into module was really meaningful.
563e32fc61a8013065267b69	X	I think you are correct in that most project that use multi modules, actually don't need them. At where I work we use multimodule projects (and I think that for a good reason). We have something similar to a service oriented architecture, so each application I agree that putting that implementation and war module in the same actual module would be ok, but the (arguably) benefit of this is that is very clear division between the classes that solve the problem and how the application communicates with the external world. In previous projects that involved just a web application, I've tried to put everything in the same module, as it made testing easier, given the modules I was using.
563e32fd61a8013065267b6a	X	I am trying to upload multiple files asynchronously on Amazon S3 using the .NET SDK. Any examples to get me started will be greatly appreciated. Thanks in advance.
563e32fd61a8013065267b6b	X	The Amazon S3 and AWS SDK for .NET functionality you are looking for is Using the High-Level .NET API for Multipart Upload: The AWS SDK for .NET exposes a high-level API that simplifies multipart upload (see Uploading Objects Using Multipart Upload API). You can upload data from a file, directory, or a stream. [...] You can optionally set advanced options such as the part size you want to use for the multipart upload, number of threads you want to use when uploading the parts concurrently, optional file metadata, the storage class (STANDARD or REDUCED_REDUNDANCY), or ACL. The high-level API provides the TransferUtilityUploadRequest class to set these advanced options. [emphasis mine] An example snippet is provided in Upload a Directory:
563e32fd61a8013065267b6c	X	I have many data files (let's call them input_files) that are stored in Amazon S3. I would like to start about 15 independent Amazon EC2 linux instances. These instances should load the input_files (that are stored in S3) and process them independently. I'd like all the 15 independent Amazon EC2 linux instances to write to the same output file. Upon completion, this output file will be saved in S3. Two questions: (1) Is it possible for Amazon EC2 linux instances to connect to S3 and read data from it? (2) How can I arrange that all the 15 independent Amazon EC2 linux instances would write to the same output file? Can I have this file in S3, and all instances will write to it?
563e32fd61a8013065267b6d	X	(1) Yes. You can access S3 from anywhere on the internet using the S3 public API (2) You are describing a database it seems. S3 is simply a file store, you don't write to files on S3 - you save files to S3. Maybe you should look into some type of database instead.
563e32fd61a8013065267b6e	X	I suggest you to take a look at this : http://docs.aws.amazon.com/IAM/latest/UserGuide/role-usecase-ec2app.html Imagine that you are an administrator who manages your organization's AWS resources. Developers in your organization have applications that run on Amazon EC2 instances. These applications require access to other AWS resources—for example, making updates to Amazon S3 buckets. Applications that run on an Amazon EC2 instance must sign their AWS API requests with AWS credentials. One way to do this is for developers to pass their AWS credentials to the Amazon EC2 instance, allowing applications to use the credentials to sign requests. However, when AWS credentials are rotated, developers have to update each Amazon EC2 instance that uses their credentials. and to see how to do this with python: https://groups.google.com/forum/?fromgroups=#!topic/boto-users/RPoFskVw1gc The basic procedure is as follows: First, you have to create a JSON policy document that represents what services and resources the IAM role should have access to. for example, this policy grants all S3 actions for the bucket "my_bucket". You can use whatever policy is appropriate for your application. BUCKET_POLICY = """{ "Statement":[{ Next, you need to create an Instance Profile in IAM. import boto c = boto.connect_iam() instance_profile = c.create_instance_profile('myinstanceprofile') Once you have the instance profile, you need to create the role, add the role to the instance profile and associate the policy with the role. role = c.create_role('myrole') c.add_role_to_instance_profile('myinstanceprofile', 'myrole') c.put_role_policy('myrole', 'mypolicy', BUCKET_POLICY) Now, you can use that instance profile when you launch an instance: ec2 = boto.connect_ec2() ec2.run_instances('ami-xxxxxxx', ..., instance_profile_name='myinstanceprofile') And the new instance should have the appropriate role and credentials associated with it once it is launched. there are same tutorials for Java, Ruby, ... Amazon website. you can refer to first url to see other tutorials.
563e32fd61a8013065267b6f	X	Thanks for your detailed reply Steffen :-) Yes, you are right - Properties window in VS does show the right region.
563e32fd61a8013065267b70	X	If I create an S3 bucket as follows: As you see I have clearly specified to create my bucket in EU region, but when I go to AWS explorer, I can see my bucket available in all the regions. What is the point of specifying bucket region if my bucket is always replicated in all the regions? Can anyone please clarify? Thank you!
563e32fd61a8013065267b71	X	Presumably you are referring to the Amazon S3 node within the AWS Explorer view of the AWS Toolkit for Eclipse or the AWS Toolkit for Microsoft Visual Studio? Amazon S3 is unique amongst the AWS services concerning its region handling in various ways (likely just a legacy issue due to it being one of the early offerings), which is also reflected in the AWS Management Console 'til this day: The major design aspect relevant here is that an S3 bucket name must be globally unique, no matter in which region you create it. That's probably why AWS has decided to show all buckets in a a single view rather than separated by region like all other services, which can admittedly be very confusing (and gets unwieldy as well with a growing number of buckets). However, the bucket still gets created in the region you specified, it just isn't obvious due to the unified presentation within the AWS Explorers and the AWS Management Console. You can see a bucket's region by opening its properties view (via the Properties context menu in the toolkits resp. the Properties button in the console). I just realized that the AWS Toolkit for Eclipse surprisingly lacks such a properties window for S3 buckets actually, which isn't only severely limiting its functionality (because you can't change advanced buckets options for example), but must be considered a notable usability bug in the light of your question. It's pretty puzzling actually, given the available view estate and the utterly simple API operation needed for this - I'm usually using both Visual Studio and Eclipse on a daily basis and have long switched to the AWS Toolkit for Microsoft Visual Studio due to its perceived performance benefits and greater and deeper service coverage to begin with, but wasn't aware of this really surprising omission yet. Accordingly, you'll need to resort to the AWS Management Console or the AWS Toolkit for Microsoft Visual Studio to visually inspect/verify your region for the time being.
563e32fe61a8013065267b72	X	Would you like to share us your final decision on it?
563e32fe61a8013065267b73	X	I was disappointed by glusterfs performance / reliability under heavy IO loads.
563e32fe61a8013065267b74	X	Can you please share what "heavy IO loads" mean? how many IOPS?
563e32fe61a8013065267b75	X	What happens if a node falls out? I'm curious about a "gluster" like setup, where the cluster can contribute data (for redundancy, or for additional storage, at the server's choice), and disconnect whenever it wants without destroying the "raid array".
563e32fe61a8013065267b76	X	Having used it extensively, I would describe the POSIX filesystem layer of ceph as experimental and horribly buggy, FYI.
563e32fe61a8013065267b77	X	@PaulWheeler: I concur. what i wanted to note is that other non-fs-like layers (RADOS, rdb) are getting quite reliable. For POSIX compatibility, it seems MooseFS is much better. I'd love to see ceph-fs mature, since rdb is quite desirable to have in the same cluster...
563e32fe61a8013065267b78	X	Isn't this a duplicate?
563e32ff61a8013065267b79	X	@dpavlin - does it matter if it's a duplicate? Yes, the answerer shouldn't have added it since it was already there, but downvoting just because it's a duplicate seems wrong
563e32ff61a8013065267b7a	X	Glusterfs is fat, eats lots of memory during high IO load, and very slow.
563e32ff61a8013065267b7b	X	Agreed with correction: MooseFS is now proprietary so its successor LizardFS is the best IMHO.
563e32ff61a8013065267b7c	X	@Onlyjob - MooseFS is no longer proprietary
563e32ff61a8013065267b7d	X	Technically speaking. But it does not have public VCS nor bug tracker. What if author take down source archive and provide it by request again? LizardFS already has community behind it and (unlike MooseFS) LizardFS will be in Debian soon. LizardFS is unrestricted (i.e. no "community edition" etc.).
563e32ff61a8013065267b7e	X	fhghfs, from the people who gave us the mp3 patent?
563e32ff61a8013065267b7f	X	Experience confirms such claim.
563e32ff61a8013065267b80	X	FhGFS is a proprietary software without sources. Don't waste everyone's time please. -1.
563e32ff61a8013065267b81	X	I have a lot of spare intel linux servers laying around (hundreds) and want to use them for a distributed file system in a web hosting and file sharing environment. This isn't for a HPC application, so high performance isn't critical. The main requirement is high availability, if one server goes offline, the data stored on it's hard drives is still available from other nodes. It must run over TCP/IP and provide standard POSIX file permissions. I've looked at the following: Lustre (http://wiki.lustre.org/index.php?title=Main_Page): Comes really close, but it doesn't provide redundancy for data on a node. You must make the data HA using RAID or DRBD. Supported by Sun and Open Source, so it should be around for a while gfarm (http://datafarm.apgrid.org/): Looks like it provides the redundancy but at the cost of complexity and maintainability. Not as well supported as Lustre. Does anyone have any experience with these or any other systems that might work?
563e32ff61a8013065267b82	X	check also GlusterFS Edit (Aug-2012): Ceph is finally getting ready. Recently the authors formed Inktank, an independent company to sell commercial support for it. According to some presentaions, the mountable POSIX-compliant filesystem is the uppermost layer and not really tested yet, but the lower layers are being used in production for some time now. The interesting part is the RADOS layer, which presents an object-based storage with both a 'native' access via the librados library (available for several languages) and an Amazon S3-compatible RESP API. Either one makes it more than adequate for adding massive storage to a web service. This video is a good description of the philosophy, architecture, capabilities and current status.
563e330061a8013065267b83	X	Gluster is getting quite a lot of press at the moment: http://www.gluster.org/
563e330061a8013065267b84	X	In my opinion, the best file system for Linux is MooseFS , it's quite new, but I had an opportunity to compare it with Ceph and Lustre and I say for sure that MooseFS is the best one.
563e330061a8013065267b85	X	If not someone forces you to use it, I would also highly recommend using anything else than Lustre. From what I hear from others and what also gave myself nightmares for quite some time is the fact that Lustre quite easily breaks down in all kinds of situations. And if only a single client in the system breaks down, it puts itself into an endless do_nothing_loop mode typically while holding some important global lock - so the next time another client tries to access the same information, it will also hang. Thus, you often end up rebooting the whole cluster, which I guess is something you would try to avoid normally ;) Modern parallel file systems like FhGFS (http://www.fhgfs.com) are way more robust here and also allow you to do nice things like running server and client components on the same machines (though built-in HA features are still under development, as someone from their team told me, but their implementation is going to be pretty awesome from what I've heard).
563e330061a8013065267b86	X	Lustre has been working for us. It's not perfect but it's the only thing we have tried that has not broken down over load. We still get LBUGS from time to time and dealing with 100TB + file systems is never easy but the Lustre system has worked and increased both performance and availability.
563e330061a8013065267b87	X	Ceph looks to be a promising new-ish entry into the arena. The site claims it's not ready for production use yet though.
563e330061a8013065267b88	X	I read a lot about distributed filesystems and I think FhGFS is the best. http://www.fhgfs.com/ It worth a try. See more about it at: http://www.fhgfs.com/wiki/
563e330161a8013065267b89	X	Thanks for the answer, but the problem with Zencoder thumbnails is that you can only generate one along with transcoding a video--you can't just generate a thumbnail alone later on. I'll look at Transloadit but it's not really reasonable for us to switch services right now. I'd really appreciate an answer that uses only scripts on my own server to handle this.
563e330161a8013065267b8a	X	I have videos hosted on Amazon S3. I encode them with Zencoder and store a thumbnail for the video then using Zencoder. However, I need a way to generate thumbnails at certain points in the video (i.e. 00:00:03, 00:10:32, 01:40:18) and store them either on S3 or my server. ffmpeg allows remote thumbnailing, however it takes a very long time (sometimes several minutes) to get a thumbnail from the middle of a file--I believe this is because it downloads the entire file up to that point to get the thumbnail. My plan is to somehow download the header of the video file via HTTP byte-range request, guesstimate the byte range where I should be looking for the thumbnail, download about a second of video from that part of the file via HTTP byte-range request, then save the header and tiny video locally. I pull the thumbnail from that using ffmpeg and delete the temporary video. I have no idea on how exactly this would work (I believe the H.264 MP4 files I'm working with have a dynamic length header, for another issue). Any suggestions or better ideas? Edit: To clarify, Zencoder thumbnailing is great, but they only allow thumbnail creation in combination with transcoding. I don't want to transcode my video every time I create a new thumbnail, so I need to do this on my own without Zencoder.
563e330161a8013065267b8b	X	As expected, a quick search through the Zencoder documentation reveals similar functionality to be available there as well, please check their API reference for Thumbnails: And (similar to Transloadit), Zencoder seems to support to upload and download files from your Amazon S3 bucket as well, see Using Zencoder with S3 for details. Good luck! Since you are using a cloud encoding service anyway, I'm going to take "Any suggestions or better ideas?" literally here and recommend to check out Transloadit eventually, insofar their offering includes your desired functionality (I'd actually expect this to be available from Zencoder as well Zencoder offers similar functionality indeed, see update above) - there are several demos for Thumbnail extraction from videos, e.g. Extract 8 thumbnails from an encoded video: This is the simplest demo to extract thumbnails from a video encoding. By default it extracts 8 thumbnails at equal time intervals each having the same dimensions as the video. » See full documentation The offset parameter of the /video/thumbs robot allows you to specify the thumbnail position more fine grained in either seconds of the file duration or respective percentage values instead. Transloadit supports Storing files in Amazon S3 as well, see e.g. the demo Encode a video, extract 8 thumbnails and store everything in your S3 bucket for a combined solution addressing your use case.
563e330261a8013065267b8c	X	And what are the "clients" here?
563e330261a8013065267b8d	X	And yes, I have an idea -- in fact it is a project I will be working on in a few days' time: a FileSystem implementation over S3; I already have a working implementation over DropBox.
563e330261a8013065267b8e	X	Why do you want to 'shield' S3? Are you concerned about introducing a security hole? With a proper policy, you allow users to upload, without exposing any other files. Solution may depend on what you are really trying to accomplish.
563e330261a8013065267b8f	X	The clients are a javascript app and mobile apps. I think I didn't express myself correctly. The "shielding" part is not for security, I simply want to be able to reliably record data about a file, such as knowing who uploaded it, its size and so on. By giving clients access directly to S3, I can't know who uploaded file X or Y, right? ( users may sign up for free with email, using IAM is not feasible )
563e330261a8013065267b90	X	Awesome, exactly the functionality I was looking for. Only have to figure out a way for native mobile apps to do this, but thanks a lot!
563e330261a8013065267b91	X	If you're using a mobile app, the better method would be to generate temporary credentials via Security Token Service (STS) and use the native mobile SDK to upload files to S3. See: docs.aws.amazon.com/STS/latest/UsingSTS/STSUseCases.html
563e330261a8013065267b92	X	That looks like a better solution, reading its docs right now. Thanks again !
563e330261a8013065267b93	X	I am trying to figure out the simplest method for allowing clients to upload media (photos and video) to my S3 bucket, without giving them direct access, or using pre-signed URLs. The idea is that I don't want any kind of media processing to occur, the only thing that I am interested in is to shield the S3 bucket from direct contact with the clients, and record information about the files being uploaded (such as size, type etc.). Do you have any ideas on how this architecture might be implemeted in a simple way?
563e330261a8013065267b94	X	To upload a file from a mobile application to Amazon S3: The temporary credentials can be granted a limited set of permissions (eg upload to a specific bucket and path) and are valid only for a limited duration, up to one hour. This is good security practice because no permanent credentials are kept on the mobile device. Use a browser-based upload via an HTML form. This allows a form in an HTML page to securely upload directly the Amazon S3 -- even to private folders. It uses a signed policy to define the permitted action (eg upload to a specific location, up to a certain file size, using a particular permission set). The form can be static -- no need to recalculate signatures for every individual file to be uploaded. See: Authenticating Requests in Browser-Based Uploads Using POST
563e330361a8013065267b95	X	I am struggling with the same question. Authentication seems to be the elephant in the REST-room...
563e330361a8013065267b96	X	jbandi, this question is more about securing the traffic rather than authentication (if I am reading this right). If you have a specific question about how to handle authentication in RESTful/Web APIs, I wouldn't mind answering it. Once its laid out, the authentication/session management options are identical to traditional web applications.
563e330361a8013065267b97	X	Great answer for a typical web app. But I believe OP is asking about the options for authentication for a REST API server. Session management is not required for example if your REST API uses HTTP Basic Auth/Digest Auth.
563e330361a8013065267b98	X	@jemeshsu they are one and the same. There is little difference between creating a REST API and a traditional web application in terms of session management, authentication/authorization, and transmission encryption.
563e330361a8013065267b99	X	I have to lay out a plan to develop a RESTful API (Python/Flask) that could be used by our future web app (Angularjs) and mobile apps (iOS/Android). I have been researching for three days and have come across several scenarios: Using HTTPS is one way on top of the methods below to keep it safer. But https is slower, which could mean we need faster and more expensive servers. How to keep the private key “secure” in a pure HTML5 app ? You are exactly right; in a pure HTML5 (JS/CSS/HTML) app, there is no protecting the key. You would do all communication over HTTPS in which case you wouldn’t need a key since you could safely identify a client using a standard API_KEY or some other friendly identifier without the need or complexity of an HMAC. So in other words there is even no point of using the method for an web app in first place. And honestly I don't understand how this should work on the mobile device either. A user downloads our app and how do I send the private key from the iphone to the server? The moment I transferred it, it will be compromised. The more I am researching the more indecisive I am getting. I was hoping to ask some pros who have done this previously and could share their experience. Many Thanks
563e330361a8013065267b9a	X	You seem to be confusing/merging two different concepts together. We start of talking about encrypting traffic (HTTPS) and then we start talking about different ways to manage authenticated sessions. In a secure application these are not mutually exclusive tasks. There also seem to potentially be a misunderstanding how session management can impact authentication. Based on that I will provide a primer on web application/web api session management, authentication, and encryption. Session Management HTTP transactions are stateless by default. HTTP does not specify any method to let your application know that a HTTP request has been sent from a specific user (authenticated or not). For robust web applications, this is not acceptable. We need a way to associate requests and data made across multiple requests. To do this, on initial request to the server a user needs to be assigned a "session". Generally sessions have some kind of unique id that is sent to the client. The client sends that session id with every request and the server uses the session id sent in every request to properly prepare a response for the user. It is important to remember that a 'session id' can be called many other things. Some examples of those are: session token, token, etc. For consistency I will use 'session id' for the rest of this response. Each HTTP request from the client needs to include the session id; this can be done in many ways. Popular examples are: Most web application frameworks use cookies. However application that rely on JavaScript and single page designs may opt to use a HTTP header/store it in some other location that is observable by the server. It is very important to remember that the HTTP response that notifies the client of their session id and the client's requests that contain the session id are completely plain text and 100% unsafe. To battle that, all HTTP traffic needs to be encrypted; that is where HTTPS comes in. It is also important to point out we have not talked about linking a session to a specific user in our system. Session management is just associating data to a specific client accessing our system. The client can be in both authenticated and unauthenticated states, but in both states they generally have a session. Authentication Authentication is where we link a session to a specific user in our system. This is generally handled by a login process where a user supplies credentials, those credentials are verified, and then we link a session to a specific user record in our system. The user is in turn associated with privileges for fine grained access control via access control lists and access control entries (ACL and ACE). This is generally referred to as "Authorization". Most system always have both Authentication and Authorization. In some simple systems all authenticated users are equals in which case you won't have authorization past simple authentication. Further information on this is out of scope for this question, but consider reading about ACE/ACL. A specific session can be flagged as representing an authenticated user in different ways. Either option is fine. It generally comes down to the technology you are working in and what they offer by default. A client generally initiates the authentication process. This can be done by sending credentials to a specific url (e.g. yoursite.com/api/login). However if we want to be 'RESTful' we generally would referencing a resource by some noun and doing the action of 'create'. This could be done by requiring a POST of the credentials to yoursite.com/api/authenticatedSession/. Where the idea would be to create an authenticated session. Most sites just POST the credentials to /api/login or the like. This is a departure from "true" or "pure" RESTful ideals, but most people find this a simpler concept rather than thinking of it as "creating an authenticated session". Encryption HTTPS is used to encrypt HTTP traffic between a client and server. On a system that relies on authenticated and unauthenticated users, all traffic that relies on a user being authenticated needs to be encrypted via HTTPS; there is no way around this. The reason for this is that if you authenticate a user, share a secret with them (their session id, etc) and then begin to parade that secret in plain HTTP their session can be hijacked by man-in-the-middle attacks. A hacker will wait for for the traffic to go through an observed network and steal the secret (since its plain text over HTTP) and then initiate a connection to your server pretending to be the original client. One way people combat this is by associating the requests remote IP address to an authenticated session. This is ineffective alone as any hacker will be able to spoof their requests remote IP address in their fake requests and then observe the responses your sever is sending back. Most would argue that this is not even worth implementing unless you are tracking historical data and using it to identify a specific user's login patterns (like Google does). If you need to split up your site between HTTP and HTTPS sections, it is imperative that the HTTP traffic does not send or receive the session id or any token used to manage the authentication status of a user. It is also important that you do not send sensitive application data within non-HTTPs requests/responses. The only way to secure data within web applications/APIs is to encrypt your traffic. Basic-Http-Auth This is a method for authenticating by web resource only. Basic authentication authenticates uses by resource identified by URL. This was most popularly implemented by Apache HTTP Web Server with the use of .htaccess based directory/location authentication. Credentials have to be sent with each request; clients generally handled this transparently for users. Basic authentication can be used by other systems as a mode of authentication. However, the systems that utilize Basic-Http-Auth are providing authentication and session management, not the Basic-Http-Auth itself. Digest-Auth This is exactly the same as Basic-Http-Auth with the addition of some simple MD5 digesting. This digesting should not be relied upon instead of using encryption. OAuth OAuth just lets you have an external service validate credentials. After that it is up to you to manage/work with the result of authentication request to your OAuth provider. Gangster Handshake / Custom HTTP header "Custom HTTP header" is a type of "Gangster Handshakes"; as such I will use the same section to discuss them. The only difference is that a "Custom HTTP header" is specifying where the hanshake (session id, token, user authentication toke, etc) will be stored (i.e. in a HTTP header). It is important to note that these do not specify how authentication will be handled, nor do they specify how session management will be handled. They essentially describe how and where session ids/authentication tokens will be stored. Authentication would need to be handled by your application or via a third party (e.g. OAuth). Session management will still need to be implemented as well. The interesting thing is you can choose the merge the two if you wish. ...I highly suggest you make sure that you understand that a robust web application that is secure needs the following: Authorization relies upon Authentication. Authentication relies upon Session Management and Encryption makes sure the session isn't hijacked and that the credentials are not intercepted. Flask-Login I think you should look into flask-login as a way to avoid re-implementing the wheel. I have personally never used it (I use pyramid for web applications in python). However, I have seen it mentioned before in web application/python boards. It handles both authentication and session management. Throw your web api/application through HTTPS and you have all three (Encryption, Session Management, and User Authentication). If you do not / can not use flask-login, be prepared to write your own, but do research first on how to create secure authentication mechanisms. If at all possible, if you do not understand how to write an authentication procedure please do not attempt it without first learning how hackers use pattern based attacks, timing attacks, etc. Please Encrypt Your Traffic ...move past the idea that you can avoid using HTTPS with some "clever" token use. Move past the idea that you should avoid using HTTPS/encryption because "its slow", process intensive, etc. It is process intensive because it is an encryption algorithm. The need to ensure the safety of your user's data and your applications data should always be your highest priority. You do not want to go through the horror of notifying your users that their data was compromised.
563e330361a8013065267b9b	X	The https it is slower, but not a not. Only the handshaking is slower. For us the biggest problem it is to upkeep the key pair on server-mobiles side and the rights. We have implemented a message digest too. The problem it is: is hard to set up the php-android-ios version properly. After this is done ( a parameter need to changes what is suggesting Google at first results only at android side) the problem will be with low-end devices: to much CPU usage, slow on decrypt-encrypt process, a lot slower than https, especially when you need to transform 10kb String(can take several minutes). If I don't transfer Nasa data to Hamas, than I would go with a very simple encryption over simple HTTP: like invert the bits or so...
563e330461a8013065267b9c	X	Is that your real private key you are showing us here?
563e330461a8013065267b9d	X	NO, that is not my real private key... do you have any sample for me?
563e330461a8013065267b9e	X	trying Fiddler first, i'll be back and report my result..
563e330461a8013065267b9f	X	Alternatively, TIdHTTP can give you access to the XML in your code. A 403 reply will cause an EIdHTTPProtocolException exception to be raised. The XML will be in the EIdHTTPProtocolException.ErrorMessage property.
563e330461a8013065267ba0	X	Also, you can use Indy's own TIdLog... components, such as TIdLogFile or TIdLogEvent, to see exactly what TIdHTTP is sending and receiving at the socket layer.
563e330461a8013065267ba1	X	Awesome to learn about this - so you would hardly need Fiddler!
563e330461a8013065267ba2	X	I do not know my code is right or wrong. when i try to run a program error occurs 403.. can any body trace my error??
563e330561a8013065267ba3	X	Amazon actually sends back a XML document that precisely describes why you got the 403 error. The easiest way to see the message would be to use Fiddler and set up your Indy HTTP to use 127.0.0.1 as a proxy. That way all your traffic goes through Fiddler and you'll see both what you sent and what Amazon returned. When I implemented my REST API to work with the Amazon S3 service I had some problems figuring out the "Canonical Headers" that need to be signed. Happily the Amazon API sends you back the text they're signing to test your signature, so you can compare that byte-by-byte and figure out if you're doing it wrong. Failure to prepare those "canonical headers" exactly as they're preparing those headers will obviously result in an 403. For example the line separator Amazon is using is LINEFEED (#10). Since you're putting your headers in a TMemo, you're going to get the Windows-style CRLF separator. That alone is enough for your code to fail. An other thing I had problems with was sending the extra headers with my Indy requests. I was following the on-line API samples, looking at what I'm supposed to send and what Amazon is supposed to answer. Fiddler was the only way to actually test and see what I'm sending, as opposed to what I thought I was sending. For example I mistakenly used TIdHttp.Request.RawHeaders to write my custom headers, but those headers get flushed while the Request is prepared. I was supposed to write my headers to TIdHttp.Request.CustomHeaders - but without Fiddler's help I wouldn't know I'm not actually sending my headers. My code looked just fine.
563e330561a8013065267ba4	X	Did you fix the FileNotFoundException?
563e330561a8013065267ba5	X	Yes, my answer helped me as a workaround.
563e330561a8013065267ba6	X	Hi, I have managed to introduce the URI of the file into the distributed cache. However, when I try to read it from the mapper, a file not found exception occurs. I am working on Amazon EMR and S3, and, at the moment, I am using the new Hadoop API (2.4.0). I have checked the file location and everything seems to be in place (other s3 files have been used without problems).
563e330561a8013065267ba7	X	I am actually working with the SDK provided by amazon for Elastic MapReduce, so I am not using the command line at all. However, I appreciate your answer, I will look forward to it.
563e330561a8013065267ba8	X	I am trying to use Hadoop in java with multiple input files. At the moment I have two files, a big one to process and a smaller one that serves as a sort of index. My problem is that I need to maintain the whole index file unsplitted while the big file is distributed to each mapper. Is there any way provided by the Hadoop API to make such thing? In case if have not expressed myself correctly, here is a link to a picture that represents what I am trying to achieve: picture Update: Following the instructions provided by Santiago, I am now able to insert a file (or the URI, at least) from Amazon's S3 into the distributed cache like this: However, when the mapper tries to read it a 'file not found' exception occurs, which seems odd to me. I have checked the S3 location and everything seems to be fine. I have used other S3 locations to introduce the input and output file. Error (note the single slash after the s3:) FileNotFoundException: s3:/myBucket/input/index.txt (No such file or directory) The following is the code I use to read the file from the distributed cache: I am using Amazon's EMR, S3 and the version 2.4.0 of Hadoop.
563e330561a8013065267ba9	X	You could push the index file to the distributed cache, and it will be copied to the nodes before the mapper is executed. See this SO thread.
563e330561a8013065267baa	X	As mentioned above, add your index file to the Distributed Cache and then access the same in your mapper. Behind the scenes. Hadoop framework will ensure that the index file will be sent to all the task trackers before any task is executed and will be available for your processing. In this case, data is transferred only once and will be available for all the tasks related your job. However, instead of add the index file to the Distributed Cache in your mapper code, make your driver code to implement ToolRunner interface and override the run method. This provides the flexibility of passing the index file to Distributed Cache through the command prompt while submitting the job If you are using ToolRunner, you can add files to the Distributed Cache directly from the command line when you run the job. No need to copy the file to HDFS first. Use the -files option to add files You can access the files in your Mapper or Reducer code as below:
563e330661a8013065267bab	X	Here's what helped me to solve the problem. Since I am using Amazon's EMR with S3, I have needed to change the syntax a bit, as stated on the following site. It was necessary to add the name the system was going to use to read the file from the cache, as follows: job.addCacheFile(new URI("s3://myBucket/input/index.txt" + "#index.txt")); This way, the program understands that the file introduced into the cache is named just index.txt. I also have needed to change the syntax to read the file from the cache. Instead of reading the entire path stored on the distributed cache, only the filename has to be used, as follows:
563e330661a8013065267bac	X	Why aren't you using the official PHP SDK for all of this work? You're re-inventing the wheel here.
563e330661a8013065267bad	X	I'm using below library for codeigniter github.com/psugand/CodeIgniter-S3. Amazon PHP SDK consist of core PHP. Is there any library which supports codeigniter and givesme session token? please help me
563e330661a8013065267bae	X	I'm new to AWS can anyone please help me how to generate session token using STS API to upload files to S3 Brief: I went through AWS documentation and researched on Google I have found below library for codeigniter to upload files to S3 https://github.com/psugand/CodeIgniter-S3 It is working fine and I'm able to upload files using my Access ID and secret key. But our requirement is to generate get temporary credentials from Amazon and send to iOS developers so that they can upload files directly to S3. I found below link on Amazon documentation where I need to follow 4 Tasks to get the temporary credentials. http://docs.aws.amazon.com/general/latest/gr/sigv4-create-canonical-request.html But some how response always says signature that I'm creating is not matching. Below is my code and response from Amazon. If I'm doing anything wrong please help me. Task 1 Canonical Request Task 2 creating String-to-sign Task 3 Calculating Signature Task 4 Add the Signing Information to the Request Executing Curl Request Response from Amazon I'm using codeiginter for this. Please help me . Thanks in advance
563e330661a8013065267baf	X	That works like a charm, thank you.
563e330661a8013065267bb0	X	I am attempting to create a REST API in PHP and I'd like to implement an authentication scheme similar to Amazon's S3 approach. This involves setting a custom 'Authorization' header in the request. I had thought I would be able to access the header with $_SERVER['HTTP_AUTHORIZATION'], but it's nowhere to be found in var_dump($_SERVER). The apache_request_headers() function would solve my problem, but my host implements PHP as CGI, so it's unavailable. Is there another way I can access the complete request headers in PHP?
563e330661a8013065267bb1	X	You'll need to do some mod_rewrite wizardry to get your headers past the CGI barrier, like so: Note that if you're using mod_rewrite for other purposes, it could end up being $_SERVER['REDIRECT_HTTP_AUTHORIZATION'].
563e330661a8013065267bb2	X	Try When using CGI interface instead of Apache Module interface, HTTP headers should be available as environment variables.
563e330761a8013065267bb3	X	There is a fantastic PECL extension that allows for all sorts of HTTP related access. PECL_HTTP and more specifically http://php.net/http and http://php.net/manual/en/function.http-get-request-headers.php.
563e330761a8013065267bb4	X	I have a node app and using the aws-sdk I'm able to successfully call the getSignedUrl() method and get a url to a specific file. However I'd like to be able to grant * access recursively inside a specific directory rather than just a single file. Is this even possible?
563e330761a8013065267bb5	X	A Pre-Signed URL permits access to private objects stored on Amazon S3. It is a means of keeping objects secure, yet grant temporary access to a specific object. It is created via a hash calculation based on the object path, expiry time and a shared Secret Access Key belonging to an account that has permission to access the Amazon S3 object. As such, each pre-signed URL is unique to each object and cannot act as a wildcard for an entire directory. Some alternatives: See also: AWS CLI copy command
563e330761a8013065267bb6	X	Without commenting on the actual question - you don't actually need to have three copies of it around at once - as you describe it, you can discard the BufferedImage before you call toByteArray() to build the third copy.
563e330861a8013065267bb7	X	this is not a discussion forum, and you are trying to turn this Question into a debate. Alternatively, if you are trying to get something done about this, you are talking to the wrong people. You might have more luck if you put together a concrete proposal >>WITH WORKING CODE<< and lots of motivating examples, and submitted it to the Apache Commons IO folks.
563e330861a8013065267bb8	X	That is another bad thing, as the class is locked now to that implementation/internals as it exposed it.
563e330861a8013065267bb9	X	This will fail if the internal array needs to be reallocated due to continued output into the ByteArrayOutoutStream after the internal array has been wrapped in a ByteArrayInputStream. The reallocation cannot be monitored because it takes place in a private method.
563e330861a8013065267bba	X	>> The current implementation uses a single byte array .... That doesn't prevent you to have InputStream that knows all the internals and does exactly what is needed. BTW, commons-io has different implementation and private method that actually provides InputStream
563e330861a8013065267bbb	X	>> is thread safe: i am sorry i was not clear on this. I didn't want to say the is not thread safe. It is, you can not corrupt the internal state. But how useful is to write to it from multiple threads. It is similar with StringBuffer that being thread safe i almost useless.
563e330861a8013065267bbc	X	>> The API would need to be more complicated... I guess you want to say the implementation would be more complicated, but that is not the concern.
563e330961a8013065267bbd	X	>> When you expose the byte array... I did not ask to expose the byte array. But my idea was to have something like getInputStream() that will actually have read-only access.
563e330961a8013065267bbe	X	@Op De Cirkel - I'm not going to debate this. SO is not a discussion forum.
563e330961a8013065267bbf	X	I know that pipes are way to approach the problem, and it has it's own pros and cons. But my questions is about ByteArrayOutputStream. >> It wasn't designed for what you have in mind. Whatever is the purpose, you always have to duplicate the array? It simply doesn't feel right.
563e330961a8013065267bc0	X	It's an easy way to capture byte-oriented output that would otherwise have to go to a file, socket, or some other hard-to-recover destination. The purpose of forcing a copy of the contents is to insulate the result from the effects of further writes (kind of the opposite of what you want, I gather).
563e330961a8013065267bc1	X	There are many java standard and 3rd party libraries that in their public API, there are methods for writing to or reading from Stream. One example is javax.imageio.ImageIO.write() that takes OutputStream to write the content of a processed image to it. Another example is iText pdf processing library that takes OutputStream to write the resulting pdf to it. Third example is AmazonS3 Java API, which takes InputStream so that will read it and create file in thir S3 storage. The problem araises when you want to to combine two of these. For example, I have an image as BufferedImage for which i have to use ImageIO.write to push the result in OutputStream. But there is no direct way to push it to Amazon S3, as S3 requires InputStream. There are few ways to work this out, but subject of this question is usage of ByteArrayOutputStream. The idea behind ByteArrayOutputStream is to use an intermidiate byte array wrapped in Input/Output Stream so that the guy that wants to write to output stream will write to the array and the guy that wants to read, will read the array. My wondering is why ByteArrayOutputStream does not allow any access to the byte array without copying it, for example, to provide an InputStream that has direct access to it. The only way to access it is to call toByteArray(), that will make a copy of the internal array (the standard one). Which means, in my image example, i will have three copies of the image in the memory: How this design is justified? Moreover, there is second flavor of ByteArrayOutputStream, provided by Apache's commons-io library (which has a different internal implementation). But both have exactly the same public interface that does not provide way to access the byte array without copying it.
563e330961a8013065267bc2	X	Luckily, the internal array is protected, so you can subclass it, and wrap a ByteArrayInputStream around it, without any copying.
563e330961a8013065267bc3	X	My wondering is why ByteArrayOutputStream does not allow any access to the byte array without coping it, for example, to provide an InputStream that has direct access to it. I can think of four reasons: The current implementation uses a single byte array, but it could also be implemented as a linked list of byte arrays, deferring the creation of the final array until the application asks for it. If the application could see the actual byte buffer, it would have to be a single array. Contrary to your understanding ByteArrayOutputStream is thread safe, and is suitable for use in multi-threaded applications. But if direct access was provided to the byte array, it is difficult to see how that could be synchronized without creating other problems. The API would need to be more complicated because the application also needs to know where the current buffer high water mark is, and whether the byte array is (still) the live byte array. (The ByteArrayOutputStream implementation occasionally needs to reallocate the byte array ... and that will leave the application holding a reference to an array that is no longer the array.) When you expose the byte array, you allow an application to modify the contents of the array, which could be problematic. How this design is justified? The design is tailored for simpler use-cases than yours. The Java SE class libraries don't aim to support all possible use-cases. But they don't prevent you (or a 3rd party library) from providing other stream classes for other use-cases. The bottom line is that the Sun designers decided NOT to expose the byte array for ByteArrayOutputStream, and (IMO) you are unlikely to change their minds. (And if you want to try, this is not the right place to do it. You might have more success convincing the Apache Commons IO developers of the rightness of your arguments, provided that you can come up with an API design that isn't too dangerous. Alternatively, there's nothing stopping you from just implementing your own special purpose version that exposes its internal data structures. The code is GPL'ed so you can copy it ... subject to the normal GPL rules about code distribution.
563e330961a8013065267bc4	X	I think that the behavior you are looking for is a Pipe. A ByteArrayOutputStream is just an OutputStream, not an input/output stream. It wasn't designed for what you have in mind.
563e330961a8013065267bc5	X	Brilliant! You've helped me understand things immensely. First of all, I now see "how" the files are actually getting saved to disk (this was previously unknown to me). It is happening in GetStream of MultipartFileStreamProvider (which is base of MultipartFormDataStreamProvider, from which my custom provider is deriving). Now, what I can do is derive directly from MultipartStreamProvider and override GetStream so that it does not save to disk but rather commits to S3, as you have suggested. Many thanks!
563e330a61a8013065267bc6	X	I would really like to see the part clarified where the stream gets written to the myAWSStream. I would like to write something to an Azure Blob storage instead of AWS. But I don't know how (in method GetStream) to get access to the stream. In this method I would like to do: BlobService.StoreImageToBlobFromStream(stream)
563e330a61a8013065267bc7	X	That the solution appears to be "let's create our own class by copying a load of code from this class and the rest from this class" almost makes me cry.
563e330a61a8013065267bc8	X	Yeah it is unexplainable that MS would ship an API that requires access to the filesystem to do processing of such a basic scenario. It'd like they couldn't think that people might not to want to have access to file writing and perform disk IO to handle a form POST. Grr.
563e330a61a8013065267bc9	X	@Kiran can you please elaborate on the full solution this is unusable to me as I don't understand where you got myAWSStream var.
563e330a61a8013065267bca	X	I have an ASP.Net Web API application that allows clients (html pages and iPhone apps) to upload images to. I am using an async upload task as described in this article. Everything works great when I want to save to the file system because that's what this code does automatically, behind the scenes it seems. But, I don't want to save the uploaded files to the file system. Instead, I want to take the uploaded stream and pass it through to an Amazon S3 bucket using the AWS SDK for .Net. I have the code set up to send the stream up to AWS. The problem I can't figure out is how to get the uploaded content stream from the Web API method instead of having it automatically save to disk. I was hoping there would be a virtual method I could override in MultipartFormDataStreamProvider which would allow me to do something else with the uploaded content other than save to disk, but there doesn't seem to be. Any suggestions?
563e330a61a8013065267bcb	X	You could override MultipartFormDataStreamProvider's GetStream method to return a stream which is not a file stream but your AWS stream, but there are some issues doing so(which I will not elaborate here). Instead you could create a provider deriving from the abstract base class MultipartStreamProvider. Following sample is heavily based on the actual source code of MultipartFormDataStreamProvider and MultipartFileStreamProvider. You can check here and here for more details. Sample below:
563e330a61a8013065267bcc	X	Up until recently, I've been hosting my dev app on a single heroku instance. The API and the angular app are hosted from the same Express.js server. Along with a front-facing sales page, separate from the angular app, sitting at the base domain. It's time for production, so I need to split the two into the api server and any other service to host the static pages. My Current Setup all on an express.js server I would love to use a service like Amazon S3, Cloudfront, or Divshot to host #1, #3, and #4 on a CDN service. And keep the API (#2) on Heroku or AWS ec2. Right now, my only thought is that I'd have to run a separate server myself and host the static files with Nginx, apache, or express.js to allow for the routing based on url because as far as I can find, the CDN services don't allow for .htaccess redirecting and such. Thank you for the time.
563e330a61a8013065267bcd	X	To simplify, you only have 2 categories here: You should consider exposing all your static assets under mydomain.com/* hosted from whatever CDN, and have the dynamic stuff being served from your (probably AWS) instance @ api.mydomain.com It is not the exact layout that you had in mind, but that one is trivial to setup. The only minor addition here, is that you might need to setup CORS due to the double domains.
563e330b61a8013065267bce	X	Can you rephrase the first sentence "I have the name of files in the list with folders." to something for understandable? Which list?
563e330b61a8013065267bcf	X	@J.C.Leitão , I am getting that list from amazon S3 and i want o index the list in my database
563e330b61a8013065267bd0	X	thanks for the info. I am using amazon S3 as my storage. Also i am thinking of building the index every week or when i add new files. I am thinking of first removing the old notes and repopulating every time. i run the cron job of updating the file index. Is that ok
563e330b61a8013065267bd1	X	i didn't undertsand , why i don't need to update the index. because suppose i delete 3 files from S3 but my database will still have those files
563e330b61a8013065267bd2	X	I mean, if you use django-admin to manage your files you don't need to update index. For example if you are going to delete or create a file you should delete or create it through django-admin. (for delete you have to create signal in which you delete a file physically stackoverflow.com/questions/5372934/…). Of course if you use external tools to manage your files that don't work through django you should maintain your index (tables in this case).
563e330b61a8013065267bd3	X	I have the name of files in the list with folders. The list contains 2000 file names like this and so on. I want to index those files in database so that i can have hierarchical directory structure. IN my Django app i want to display first the root level menus like countries --- US , Australia, canada Then i someone click on country then it get the second level of folders and so on and in end i want to see files if there are no more folders. rather than querying my storage evry time , i want to store all that info in my database so that my web pages are displayed from DB and when user click download then i get the file from my Storage i am not able to find how should i make the Model or database table for that
563e330b61a8013065267bd4	X	I suggest following way: Create models to store your tree structure and files for example: After that move your files in one or few directories (read this How many files in a directory is too many?) also you can rename them (for example by using hash from files). Update the model File to put there new paths to your files. Having done this you are able to easy show files and build path to files etc. For the model Node use [django-mptt][1] (there are other solutions for django, google it) to get an efficient API to manage a Tree-like model. You can also create your own Django Storage Backend (or find there are many solutions on the Internet). Updated You can add new files by using django admin. You should use amazon s3 django storage backend http://django-storages.readthedocs.org/en/latest/backends/amazon-S3.html. Change: In this case you have not to update index.
563e330b61a8013065267bd5	X	Thx for your answer Stefan. I checked out the tutorial you linked and indeed this fits what I plan to implement. Main difference is, that all of this steps should be done automatically, in my system. The user is just dropping the file into the browser. That's the point where data and communication flow between server and client starts being not that obvious to me. How can the client get informed about the transcoding has finished if server starts this process?
563e330b61a8013065267bd6	X	Im writing a single-page-web-app (angularJs) and a server back-end (node.js). The communication between them is done via REST. Currently im trying to implement the following scenario: Upload big files from browser to S3 public bucket. Copy uploaded file to private bucket on S3 Transcode uploaded file to HTML 5 compatible format (AWS Elastic Transcoder) Store Meta-Object about the file in DB to access later I'm racking my brains to get a well working design of the communication/ data-workflow between server and client, but always got stuck at the following questions? Store file meta-object at the end or at the beginning of the process. If it is at the beginning, i have to store and handle some state information? Who should start copying uploaded files to private bucket. Server or client? If it is the server, how can the client get informed about the job succeeded? Who starts the transcoding process? If it is the server, how can the client get informed about the job succeeded? How would you do this?
563e330c61a8013065267bd7	X	there is a pretty good tutorial which describes the use case you are planning to implement: http://www.bitcodin.com/blog/2015/02/create-mpeg-dash-hls-content-for-amazon-s3-and-cloudfront/ If your transcoding system has a RESTfull API (like bitcodin which is used in this tutorial, or any other service) you can do your application also client-side and use the API calls to get the state of your transcodings, etc. However, using the API you can do the same also server-side, whatever fits better for you. I personally would store the metadata infos at the beginning of the process, as this is the point of time where you generate the "asset" in your database/CMS/etc.
563e330c61a8013065267bd8	X	I I have been searching a lot for answer to this question. However after reading a lot of resources including Spring Security documentation as well I am still away from completely understanding as to how to authenticate a user for REST API using Spring Security . Here is what I want to do: From above I am not sure how to do this in Spring Security. What I have understood is as follows: 1) Use a <http> element along with custom filter.I have done this as follows <http use-expressions="true" create-session="stateless" authentication-manager-ref="restAuthenticationManager" entry-point-ref="jkwebRestAuthenticationEntryPoint"> <intercept-url pattern="/api/**" access="isAuthenticated()"> </intercept-url> <custom-filter ref="jkWebSecurityHmacAuthenticationFilter" position="FORM_LOGIN_FILTER"/> </http> I can post more code for clarity if needed.
563e330c61a8013065267bd9	X	Here is the best example I have found. https://code.google.com/p/spring-rest-hmac/ which is mirrored on GitHub here No warranty implied, proceed at your own risk, etc. I started copying over code but you should really review the whole package. It's probably a good starting point, but you'll need to work hard to make this secure I would think.
563e330c61a8013065267bda	X	for documents like submitted forms you may want to use a nosql server like couchdb or mongodb, for relational data use a rdbms like e.g. mysql or postgresql. sqlite is a bad idea for scaling, more then 10 users at a time make it unusable due to its performance.
563e330c61a8013065267bdb	X	What web-host are you using? Every web host offers some sort of database system hosted on their servers.
563e330c61a8013065267bdc	X	FYI: StackMob is ceasing operations. This is why I never trust tiny startups especially when there is a danger of lock-in.
563e330c61a8013065267bdd	X	I have a website that I've built (hosted on Amazon S3) and it works great. The only problem is that all of the data is static. I'd like to create a SQL database in the cloud that would allow me to store basic text data from users after they submit forms. I'm still a novice web-developer but I've used sqlite3 for several of my Java desktop apps and I'd like to use that SQL knowledge to create this online database. I guess what i'm asking (in my ignorance) is: how can I create a sqlite-type database that is stored in the cloud and that I can query against using javascript? Where do I get started? Is there a service like Amazon AWS or Azure or something where I can create this database and then use some sort of jQuery/Javascript API to query data from it? I don't need a ton of storage space and my queries would be very basic SQL type stuff. Any help would be greatly appreciated. Thanks in advance.
563e330d61a8013065267bde	X	StackMob or Parse if you want a (client-side) JavaScript API with user management, facebook/twitter integration, data store (with geospatial), and push notifications. StackMob also lets you host your website. For more flexibility, less service lock-in, and cheaper scalability: I would suggest CouchDB (though you would likely still use a hosting service like Cloudant). CouchDB can host your website, and provides a HTTP API for storing data, to which your client-side JavaScript can make REST calls.
563e330d61a8013065267bdf	X	StackMob has a free package that you can use. You can use the JS SDK to write your HTML5 app and save stuff to the StackMob DB. You can host your HTML5 on StackMob for free and point your own domain to it as well. There is also S3 integration. Some references: JS SDK JS SDK Tutorial Hosting your HTML5 Custom Domains
563e330d61a8013065267be0	X	SQLite isn't really a good choice for web facing applications due to its scaling issues. Both AWS and Azure support SQL databases. They also each support alternatives like MongoDB and Redis. For something as basic as you describe the only real difference is cost.
563e330d61a8013065267be1	X	As you mentioned your website is hosted on Amazon S3 I am sure it is a static website with lots of JavaScript embedded HTML files. Due to having a static website, I can understand your requirement to use a database which can be connected from your static site and to be very honest there are not a lot options you have. Static website are considered to have no dependency on database so honestly you have very limited choice because what you are looking for is "A Database which is accessible over HTTP which you can call from scripting language withing HTML" If you have ability to write back to S3 directly from your JavaScript code you can create a JavaScript based database within your static site which is complex but consider a choice. In my thinking you are better off to have an extra-small instance in Windows Azure (or your choice of cloud service) and connect with a cloud based database which will be comparative cheaper and fit for your requirement. Or unless Amazon can come up with a DB accessible from status content as S3, you really have no great choices here.
563e330d61a8013065267be2	X	Since you are already familiar some of AWS's offerings, you should check out: But to do what you are asking (access data via JavaScript), check out www.stackmob.com. You can host an HTML5 application with data access via backbone (javascript based framework) on StackMob.
563e330d61a8013065267be3	X	And this is my web config setting <httpRuntime useFullyQualifiedRedirectUrl="true" maxRequestLength="2147482624" executionTimeout="9999" requestLengthDiskThreshold="95360" />
563e330d61a8013065267be4	X	check with amazon for max file size that they allow
563e330d61a8013065267be5	X	ok .But When i try to upload file of size 6 gb it shows me error http 400 and it does not executes the controller code also
563e330d61a8013065267be6	X	Thanks for replying but do i need to break file in chunks and the upload.If yes how would i get file in stream if i upload file whose size is more than gb my controller does not get executed
563e330d61a8013065267be7	X	I am Doing this in browser
563e330d61a8013065267be8	X	Thank u for replying maxRequestLength="2147482624" that is the max can i do using jquery or html file uploader like BlueImp can you suggest any thing like that i am searching not getting any solution
563e330d61a8013065267be9	X	Unfortunately, "Questions asking us to recommend or find a book, tool, software library, tutorial or other off-site resource are off-topic for Stack Overflow" -- stackoverflow.com/help/on-topic
563e330d61a8013065267bea	X	I am trying to upload files of size more than 3 gb to amazon when i upload file whose size is small they get uploaded happily but When i try to upload big file it does it show me error http 400 Can Any one tell me where i am going wrong or do i need upload file in chunks. thanks in advance
563e330e61a8013065267beb	X	First, you're not allowed to upload files of more than 5gb (6gb MUST FAIL, but not 2gb file): But uploading large files can occur various issues, so, to avoid problems with Single Upload, is recommended Multipart Upload Upload objects in parts—Using the Multipart upload API you can upload large objects, up to 5 TB. The Multipart Upload API is designed to improve the upload experience for larger objects. You can upload objects in parts. These object parts can be uploaded independently, in any order, and in parallel. You can use a Multipart Upload for objects from 5 MB to 5 TB in size. For more information, see Uploading Objects Using Multipart Upload. For more information, see Uploading Objects Using Multipart Upload API. Check here The following Java code example uploads a file IN PARTSto an Amazon S3 bucket:
563e330e61a8013065267bec	X	@JordiCastilla is correct about S3 multipart and the 5GB threshold... but your first problem is a local one: Hmmm. That's about 2 GiB. So, when you say your controller isn't firing, that suggests your 400 error isn't even from S3 as the question implies. Your local configuration is causing the browser's request to be canceled in flight because it's larger than your local configuration allows. Your first step seems like it would be to increase this configuration value, bearing in mind that you will subsequently also need to transition to multipart uploads to S3 when you cross the 5GB threshold. Remember, also, that a request in progress is also eating up temporary disk space on your server, so having this value even as large as it already is (not to mention setting it even larger) could put you at risk for a denial of service attack, where spurious requests could disable your site by exhausting your temp space.
563e330e61a8013065267bed	X	Check out this lib . It uploads the file in chunks of configurable sizes. As your requirement is to get it done in a browser this should work.
563e330e61a8013065267bee	X	@Justice - Thank you for a complete and very well-reasoned answer. It is exactly the reasoning that I needed for using S3. The assets stored on S3 are not super-critical as far as privacy goes, and the URLs are only available to logged-in users. I suppose I have to do some kind of salted hash to generate the random number.
563e330e61a8013065267bef	X	+1 one for logical points. Generating a new randomized url for each update to the resource is also important.
563e330e61a8013065267bf0	X	If you really need the ACLs, this is definitely how to do it. However, on Heroku, and depending on the access patterns for these assets, this strategy will force you to "crank your dynos" much faster than otherwise.
563e330f61a8013065267bf1	X	Justice: I'm not sure that it's any worse than it would be to store the file locally and stream it out through your application, though. If you wanted to lock the files down in any non-trivial way, streaming through the application is basically the only solution. Of course, few applications have that kind of requirement. I'm also used to working in a dedicated server environment, so maybe my advice is not as applicable to heroku.
563e330f61a8013065267bf2	X	Thank you for a very complete answer.
563e330f61a8013065267bf3	X	I want to ask more about "That controller downloads the file using the API, then streams it out to the user with correct mime-type, cache headers, file size, etc." So your app will download the file into your app server and then serve that file to the app user from your app server? Or does your download/{s3-path} show a webpage that contains a time-expiry inclusive link? like this one docs.aws.amazon.com/AmazonS3/latest/dev/S3_QSAuth.html
563e330f61a8013065267bf4	X	They have a similar feature for signed URLs in the PHP SDK. docs.aws.amazon.com/aws-sdk-php/guide/latest/… I think this is the best current solution for this poster's kind of problem.
563e330f61a8013065267bf5	X	I fired up a sample application that uses Amazon S3 for image hosting. I managed to coax it into working. The application is hosted at github.com. The application lets you create users with a profile photo. When you upload the photo, the web application stores it on Amazon S3 instead of your local file system. (Very important if you host at heroku.com) However, when I did a "view source" in the browser of the page I noticed that the URL of the picture was an Amazon S3 URL in the S3 bucket that I assigned to the app. I cut & pasted the URL and was able to view the picture in the same browser, and in in another browser in which I had no open sessions to my web app or to Amazon S3. Is there any way that I could restrict access to that URL (and image) so that it is accessible only to browsers that are logged into my applications? Most of the information I found about Amazon ACLs only talk about access for only the owner or to groups of users authenticated with Amazon or AmazonS3, or to everybody anonymously. EDIT----UPDATE July 7, 2010 Amazon has just announced more ways to restrict access to S3 objects and buckets. Among other ways, you can now restrict access to an S3 object by qualifying the HTTP referrer. This looks interesting...I can't wait until they update their developer documents.
563e331061a8013065267bf6	X	S3 is a separate service and does not know about your sessions. The generic solution is to recognize the benefits and security properties that assigning each asset a separate, unique, and very long and random key, which forms part of the URL to that asset. If you so choose, you can even assign a key with 512 effective bits of randomness, and that URL will remain unguessable for a very long time. You have to determine if this is sufficient security. If it isn't, then maybe S3 isn't for you, and maybe you need to store your images as binary columns in your database and cache them in memcached, which you can do on Heroku.
563e331061a8013065267bf7	X	For files where privacy actually matters, we handle this as follows: Using this method, you end up using a lot more bandwidth than you need, but you still save on storage. For us this works out, because we tend to run out of storage much more quickly than bandwidth. For files where privacy only sort of matters, we generate a random hash that we use for the URL. This is basically security through obscurity, and you have to be careful that your hash is sufficiently difficult to guess. However, when I did a "view source" in the browser of the page I noticed that the URL of the picture was an Amazon S3 URL in the S3 bucket that I assigned to the app. I cut & pasted the URL and was able to view the picture in the same browser, and in in another browser in which I had no open sessions to my web app or to Amazon S3. Keep in mind that this is no different than any image stored elsewhere in your document root. You may or may not need the kind of security you're looking for.
563e331061a8013065267bf8	X	Amazon's Ruby SDK (https://github.com/aws/aws-sdk-ruby) has useful methods that make it a snap to get this done. "url_for" can generate a temporary readable URL for an otherwise private S3 object. Here's how to create a readable URL that expires after 5 minutes: object = AWS::S3.new.buckets['BUCKET'].objects['KEY'] object.url_for(:read, :expires => 300).to_s AWS documentation: http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html#url_for-instance_method
563e331061a8013065267bf9	X	I think the best you can do is what drop.io does. While the data is in principle accessible to anyone, you give it a large and random URL. Anyone who knows the URL can access it, but your application controls who gets to see the URL. Kind of security through obscurity. You can think of it as the password included in the URL. This means that if you are serious about security, you have to treat the URL as confidential information. You have to make sure that these links do not leak to search engines, too. It is also tricky to revoke access rights. The only thing you can do is invalidate a URL and assign a new one.
563e331061a8013065267bfa	X	http://twitpic.com/show/full/<imageid> this is what I was looking for. thanks.
563e331161a8013065267bfb	X	So in this URL: http://twitpic.com/2paihn The Twitpic ID is: 2paihn And the actual image URL is: http://s3.amazonaws.com/twitpic/photos/large/163413275.jpg?AWSAccessKeyId=0ZRYP5X5F6FSMBCCSE82&Expires=1284740401&Signature=6lgT6ruyyUDDjLOB7d42XABoCLU%3D I've tried getting the integer id through the api (i.e. 163413275) and replacing it in the s3.amazon.com url, but this seems to only work some of the time. Most of the time I get an 'access denied' message when I request the amazon-hosted image. Do you know of another hack to do this?
563e331361a8013065267bfc	X	Use http://twitpic.com/show/thumb/<imageid> to get a thumbnail version.
563e331361a8013065267bfd	X	This is the direct link to the image: http://twitpic.com/show/full/2paihn You can use it in an img tag or link it. It will forward to the amazon S3 url with a new expiration date and signature so whoever you're showing it to won't get the access denied message. Source: http://dev.twitpic.com/docs/thumbnails/
563e331361a8013065267bfe	X	It certainly does. Thanks so much. The only part I'm still not clear on is uploading whole folders - does software like Cyberduck enable you to upload multi-layered directories and it "converts" the files to flat files with slashes in their names for you?
563e331361a8013065267bff	X	Yep. Cyberduck and other tools handle all of that stuff for you. :)
563e331361a8013065267c00	X	Currently, I have a website which serves dynamic (PHP-MySQL) content from an Apache server, and serves static content (JavaScript, images) from a separate Lighthttpd server. For reasons of scale I would like to use Amazon Cloudfront and possibly S3. To be honest I'm not entirely sure how S3 or CloudFront work. I'm used to the normal server behaviour of "upload a file... it becomes available" and S3 "buckets" and CloudFront edge-mirroring are daunting. I need a better understanding of how this works and have some questions: 1) I don't want to store any images on my own servers. I want them to be entirely in the cloud. Am I correct that this means I will need to use S3 for storage as an "origin server"? Will CloudFront on its own not be enough? Is CloudFront just the edge-CDN service? 2) We currently upload images via a PHP script which FTPs them to our image server, or via manual FTP upload. How will that change if I use S3? I heard you can't FTP to it? :( 3) If using S3, can I still create hierarchical directories and store images inside these? The images are stored various folders deep and I can't afford to change the code, but I heard S3 was a flat "bucket"? 4) Finally, I heard that with CloudFront, if a file changes, you have to issue an invalidation request, which costs money. Is this because CloudFront is caching the image from the origin? I'm not used to this as in my current setup I just replace an image via FTP and it updates! Is there no way to imitate this classic behaviour? Sincerest thanks for help.
563e331361a8013065267c01	X	1) I don't want to store any images on my own servers. I want them to be entirely in the cloud. Am I correct that this means I will need to use S3 for storage as an "origin server"? Will CloudFront on its own not be enough? Is CloudFront just the edge-CDN service? S3 is designed for the long-term, reliable storage of data with eleven 9s of durability. Buckets (as they're called) are region-specific and live in one of Amazon's regional data centers. Conversely, CloudFront is designed as a series of edge servers. By default, when you request an object (i.e., file) from a CloudFront hostname, that object is pulled from the origin location and cached in the nearest CloudFront edge location for 24 hours (this can be adjusted programmatically). At the end of 24 hours the cache expires, and CloudFront will pull a fresh copy the next time that object is requested. A common setup is to configure CloudFront to use S3 as its origin location. CloudFront also has the ability to use any server, if that's what you prefer (it sounds like you don't). 2) We currently upload images via a PHP script which FTPs them to our image server, or via manual FTP upload. How will that change if I use S3? I heard you can't FTP to it? :( S3 isn't an FTP server, so it doesn't speak the (S)FTP protocol. However, nearly all FTP clients for Mac OS X include support for Amazon S3. Amazon S3 has a web service API, so you can automate the push using one of the AWS SDK's if you'd like. One tool, Cyberduck, does SSH, SFTP, FTP, Amazon S3, and a few other things. It's available for both Mac and Windows. There are also other tools out there that provide a GUI for uploading to S3 as simply as though you were uploading via FTP. 3) If using S3, can I still create hierarchical directories and store images inside these? The images are stored various folders deep and I can't afford to change the code, but I heard S3 was a flat "bucket"? Yes and no. Yes, S3 is a flat file system, but files can have slashes in their names. For example, "abc/def/ghi/jkl.txt" is not actually 3 folders and a file, but rather one file with slashes in its filename. Most GUI tools choose to visualize this as folders and subdirectories, and the S3 URL looks just like any other URL. Speaking personally, I've never needed to do anything different for S3 than I used to do for SFTP. 4) Finally, I heard that with CloudFront, if a file changes, you have to issue an invalidation request, which costs money. Is this because CloudFront is caching the image from the origin? I'm not used to this as in my current setup I just replace an image via FTP and it updates! Is there no way to imitate this classic behaviour? Right. Because CloudFront caches the source file to the nearest edge server. By default, the expiration is 24 hours, but you can set it as low as 1 hour, or even expire it sooner with an "invalidation request". I've seen this take anywhere from 3-15 minutes to complete, because CloudFront has to check all of the edge servers to make sure that they're all cleared. If you don't want the caching, you can just use S3 straight-up. This is the closest equivalent to replacing an image via FTP, but then you lose all of the benefits of using a CDN in the first place. According to the Amazon CloudFront pricing page: "No additional charge for the first 1,000 files that you request for invalidation each month. $0.005 per file listed in your invalidation requests thereafter." That's half-of-a-penny for each file you invalidate over 1,000 in a month. I use CloudFront regularly and have never crossed that limit, but if you're running a larger site with lots and lots of changes, then it's certainly a possibility. I hope this helps! :)
563e331361a8013065267c02	X	I tested a whole range of client applications and found CloudBerry S3 Explorer for windows and CrossFTP for Mac more powerful then Cyberduck, but I work a lot with private streaming video and audio, so my requirements are a bit higher then displaying images on a site. But to answer your last question, you can drag a folder with subfolders into a bucket, the hierarchy is respected. You can work folders in a bucket, just as you work with FTP. But you have to make sure to set your images to public or your images will not show. Standard, any file is uploaded as a private file, requiring a signed URL to access it. But with a client application, you can set the inheritance of a bucket, so that files become automatically public. You can find a lot of info about this here: http://www.miracletutorials.com/category/s3-amazon-cloudfront/ Cheers, Rudolf+++
563e331461a8013065267c03	X	I have to research a bit more, but wouldn't client_body_in_file_only cause more disk access and thus decreased performance? The Nginx docs say it should be used for debugging primarily.
563e331461a8013065267c04	X	@aergistal no, it works in production many years for us, all is perfect. I talked to Nginx core team developers, they confirmed it is pretty stable for production work load.
563e331461a8013065267c05	X	It does solve but only one particular problem with client side upload, what about everything else?
563e331461a8013065267c06	X	just added some info about image cropping and PDF preview
563e331461a8013065267c07	X	Upload files with multipart/form-data is straight forward and works well most of time until you started to be focused on big files upload. If we look closely what happens during a file upload: client sends POST request with the file content in BODY webserver accepts the request and initiates data transfer (or returns error 413 if the file size is exceed the limit) webserver starts to populate buffers (depends on file and buffers size), store it on disk and send it via socket/network to back-end back-end verifies the authentication (take a look, once file is uploaded) back-end reads the file and cuts few headers Content-Disposition, Content-Type, stores it on disk again back-end performs all you need to do with the file To avoid such overhead we dump file on disk (Nginx client_body_in_file_only) and manage the callback to be send further down the line. Then queue worker picks the file up and do what required. It works for inter-server communication pretty slick but we have to solve similar problem with client side upload. We also have client-side S3 upload solution. No back-end interaction happens. For video upload we manage the video to convert to the format h.264 Baseline / AAC with Zencoder. Currently we use modified Flash uploader based on s3-swf-upload-plugin with combination of Zencoder JS SDK which is really efficient but uses Flash. Question. How to reach the same goal with HTML5 file uploader? Does Filepicker.io and Zencoder solve the problem? What is the recommended way to manage HTML5 file upload with no back-end interaction? The requirements are the following: Does https://www.filepicker.com make a good job?
563e331461a8013065267c08	X	The requirements are the following: HTML5, not flash Filepicker now supports a full responsive widget that is pure html and css. to upload video with post-processing to make it compatible with HTML5 players and mobile Filepicker now offers the ability to transcode most video formats to h264 & webm for mobile playback. https://www.filepicker.com/documentation/file_processing/video_conversion/video to upload images wtih post-processing (resize, crop, rotate) Filepicker does offer Crop & rotate in the new widget as well as resize, sharpening and watermarking via API. to upload documents like PDF with a preview functionality We offer the ability to convert from 19 different file formats to numerous output formats. https://www.filepicker.com/documentation/file_processing/document_conversion/document
563e331561a8013065267c09	X	I'm using filepicker for 2 years now, and without doubt it's worth the price. don't try to manage file upload (from google drive, from ios, from my camera, from dropbox...) Filepicker handles that very well and provide you a ready to use url. Spend more time working on your core business, file upload is really easy to delegate
563e331561a8013065267c0a	X	To upload a big files to S3 there is a REST API for Multipart Upload, which works the following way the API is also available for calling from javascript and the uploaded file can be split to multiple requests using File/Blob slice API The only problem is that to be able to authenticate to S3 from javascript you need to pass your authentication details. This is usually solved by some interlayer like PHP so the authentication details are not stored in javascript files. Similar question on SO: HTML5 and Amazon S3 Multi-Part uploads EDIT
563e331561a8013065267c0b	X	Rafał Łużyński do you know if Sphinx works with nosql databases
563e331561a8013065267c0c	X	sphinxsearch.com/about as you can see, they have support for nosql in xml. Non-SQL storage indexing. Data can also be streamed to batch indexer in a simple XML format called XMLpipe, or inserted directly into an incremental RT index
563e331561a8013065267c0d	X	We are working for a client to redesign an existing system which basicaly deals with a lot of files. The files(more than 5 million) are currently stored on the servers filesystem.The client wants the new system to store the file in S3. The files also have metadata associated(name,authors name,price ,description etc.). The search functionality is also to be redesigned.The following are the basic requirements Also , based on the file description, the system should also be able to give recommendation for similar files. I do not have experience with creating such solution before,so asking for help and suggestion. I was thinking on the lines of following solutions: There was this project that I found,that is very similar to what I require http://www.thriftdb.com - On the home page it says its a datastore with search builtin. Please let me know if this question should be a community wiki. Thanks in advance.
563e331561a8013065267c0e	X	Amazon has a custom AMI for Lucene/Solr and we have been happily using it in our projects. Lucene has a powerful indexing capability and executes at exceptional speeds. I would strongly recommend using Apache Lucene/Solr for all your search needs.
563e331561a8013065267c0f	X	You're in luck, announced today: http://aws.amazon.com/about-aws/whats-new/2012/04/11/aws-announces-cloudsearch/
563e331561a8013065267c10	X	About searching files and filtering by attributes, the best would be Sphinx Search Engine which is used in filestube (google was using it also years ago). I dont know if it will work on amazon servers.
563e331561a8013065267c11	X	At the web app I'd developing there is an option for users to download large files via browser. The files could be stored at different remote storages, for example Amazon S3. If the file download took more than a few minutes it would be handy to track it's progress at the server side for various reasons. So the web app and node.js API are located at one place and the file could be located anywhere. What I was thinking is to proxy the file download through node.js. So it looks like --> user clicks the file download button at web app --> node asks S3 for a file and streams it to the user and updates the DB to track the progress as the file size and data sent are known. What I'd like to know: Thanks a lot for your answers !
563e331661a8013065267c12	X	I am working on a project in python that is starting to overwhelm my low-end windows lap-top and I wanted to ask for advice about how to find the additional computing power I think I need. Here are some details about my project: I am processing and analyzing a fairly large database of text from the web. Approximately 10,000 files each equivalent to on average approximately 500 words or so (though with a lot of variance around this mean). The first step is pulling certain key phrases and using GenSim to do a fairly simple similarity analysis. This takes my computer a while but it can handle it if I'm gentle. Second, once I have identified a short list of candidates I fingerprint each candidate document to more closely assess similarity. Each file requires fingerprinting and comparison over 2-10 other files - so its not really an n-to-n comparison of the sort that would require months of computer time I don't think. It is this second step where my computer starts to struggle. I was considering looking into running the script in an EC2 environment but when I started reading about that on here, I saw a a comment to the effect that effectively doing so requires a linux sys admin level of sophistication - I am about as far from that level of sophistication as any member of this site can be. So is there another option? Or is getting a fairly simply python script running on ES2 not so hard. The part of the script that seems the most resource-intensive is below. For each text file, it creates a list of fingerprints by selecting certain text files from amdt_word_bags trim according to criteria in PossDupes_1 (both of which are lists). It uses the fingerprintgenerator module which I found here: https://github.com/kailashbuki/fingerprint.
563e331661a8013065267c13	X	How about using Amazon's Elastic Map Reduce (EMR). This is Amazon's hadoop service which basically runs on top of EC2. You can copy you your data files to AmazonS3 and have your EMR cluster pick up the data from there. You can also send your results to files in Amazon S3. When you launch your cluster you can customize how many EC2 instances you want to use and what size for each instance. That way you can tailor how much CPU power you need. After you are done with your job you can tear down your cluster when you are not using it. (Avoiding paying for it) You can also do all of the above programmatically too. For example python I use the boto Amazon API which is quite popular. For getting started on how to write python map reduce jobs you can find several posts on the web explaining how to do it. Here's an example: http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/ Hope this helps.
563e331661a8013065267c14	X	Could you give more details on these methods, and what they require/ask? Bit hard to tell what you need to do otherwise.
563e331661a8013065267c15	X	To be honest the main question here is where do those images originate from? If they are from form uploads then the image is already stores in a temporary location so you would have no need to store the image yourself.
563e331661a8013065267c16	X	@Peter, they are not from form uploads. I'm pulling one image off of a CDN, cropping it, and sending the cropped version back to the CDN (Amazon S3 CDN + their classes & methods).
563e331661a8013065267c17	X	You can do it like now or You can send the resource data in base64_encodeed form - that will be string. Don't worry about the data amount, in PHP 5 the parameters are transferred just as a pointers to a memory...
563e331661a8013065267c18	X	Thanks! Very helpful answer.
563e331661a8013065267c19	X	I have an image resource that is manipulated with imagecopyresampled. I need to pass that image to a set of methods that expect a string input, not a resource. But I don't need to store the file locally. Is this the proper way: Is that right? Seems sloppy. Note: the image is not coming from a file upload and hence can't be accessed with $_FILES["Filedata"]["tmp_name"]
563e331661a8013065267c1a	X	I took a look at the Amazon S3 PHP API: http://docs.amazonwebservices.com/AWSSDKforPHP/latest/index.html#m=AmazonS3/upload_part I assume you are using something like the upload_part method that takes a string filename. In that case, unless you plan to modify their library, you will need to store the file to disk and pass them the filename so they can read the file and perform the upload. Besides the steps mentioned in your question you can take a look at imagedestroy to make sure you are freeing up the memory for your image resource after it is written to disk with imagepng. And then, as you stated, you can delete your temp file with unlink after your upload is complete. I agree, it does seem a bit wasteful, but in this case necessary since the API doesn't seem to provide an alternative.
563e331761a8013065267c1b	X	Hi codenoob. Any news on this? Did you finish your solution? Did you find the time try app engine?
563e331761a8013065267c1c	X	Hey Johe, I've implemented it and am working on the iOS client now. I used Ruby with Sinatra running on Heroku with static files on Amazon S3 and a MongoDB database running at MongoHQ. It was very simple to implement a RESTful protocol using Sinatra and everything is extremely scalable, and I only pay for the resources I use. I haven't tried the app engine because this worked perfectly for me.
563e331761a8013065267c1d	X	Link to Heroku: heroku.com
563e331761a8013065267c1e	X	This looks very interesting, I will give it a go. I think I can get away with the free Blossom service for now and in case the app's demands grow, the prices seem very reasonable going up. I might try the google app engine at some in the future though. Thanks for the info!
563e331761a8013065267c1f	X	I have decided to go with Sinatra and Heroku for now because it seems very simple to do what I want it to do. I have looked at GAE and already have some experience in Java so I will definitely try this in the future too. Thanks for the tip!
563e331761a8013065267c20	X	I am developing an iPhone app and would like to create some sort of RESTful API so different users of the app can share information/data. To create a community of sorts. Say my app is some sort of game, and I want the user to be able to post their highscore on a global leaderboard as well as maintain a list of friends and see their scores. My app is nothing like this but it shows the kind of collective information access I need to implement. The way I could implement this is to set up a PHP and MySQL server and have a php script that interacts with the database and mediates the requests between the DB and each user on the iPhone, by taking a GET request and returning a JSON string. Is this a good way to do it? Seems to me like using PHP is a slow way to implement this as opposed to say a compiled language. I could be very wrong though. I am trying to keep my hosting bills down because I plan to release the app for free. I do recognise that an implementation that performs better in terms of CPU cycles and RAM usage (e.g. something compiled written in say C#?) might require more expensive hosting solutions than say a LAMP server so might actually end up being more expensive in terms of $/request. I also want my implementation to be scalable in the rare case that a lot of people start using the app. Does the usage volume shift the performance/$ ratio towards a different implementation? I.e. if I have 1k request/day it might be cheaper to use PHP+MySQL, but 1M requests/day might make using something else cheaper? To summarise, how would you implement a (fairly simple) remote database that would be accessed remotely using HTTP(S) in order to minimise hosting bills? What kind of hosting solution and what kind of platform/language? UPDATE: per Karl's suggestion I tried: Ruby (language) + Sinatra (framework) + Heroku (app hosting) + Amazon S3 (static file hosting). To anyone reading this who might have the same dilemma I had, this setup is amazing: effortlessly scalable (to "infinity"), affordable, easy to use. Thanks Karl! Can't comment on DB specifics yet because I haven't implemented that yet although for my simple query requirements, CouchDB and MongoDB seem like good choices and they are integrated with Heroku.
563e331761a8013065267c21	X	Have you considered using Sinatra and hosting it on [Heroku]? This is exactly what Sinatra excels at (REST services). And hosting with Heroku may be free, depending on the amount of data you need to store. Just keep all your supporting files (images, javascript, css) on S3. You'll be in the cloud and flying in no time. This may not fit with your PHP desires, but honestly, it doesn't get any easier than Sinatra.
563e331761a8013065267c22	X	It comes down to a tradeoff between cost vs experience. if you have the expertise, I would definitely look into some form of cloud based infrastructure, something like Google App Engine. Which cloud platform you go with depends on what experience you have with different languages (AppEngine only works with Python/Java for e.g). Generally though, scalable cloud based platforms have more "gotchas" and need more know-how, because they are specifically tuned for high-end scalability (and thus require knowledge of enterprise level concepts in some cases). If you want to be up and running as quickly and simply as possible I would personally go for a CakePHP install. Setup the model data to represent the basic entities you are managing, then use CakePHP's wonderful convention-loving magic to expose CRUD updates on these models with ease!
563e331861a8013065267c23	X	The technology you use to implement the REST services will have a far less significant impact on performance and hosting costs than the way you use HTTP. Learning to take advantage of HTTP is far more than simply learning how to use GET, PUT, POST and DELETE. Use whatever server side technology you already know and spend some quality time reading RFC2616. You'll save yourself a ton of time and money.
563e331861a8013065267c24	X	In your case its database server that's accessed on each request. so even if you have compiled language (say C# or java) it wont matter much (unless you are doing some data transformation or processing). So DB server have to scale well. here your choice of language and DB should be well configured with host OS. In short PHP+MySQL is good if you are sending/receiving JSON strings and storing/retrieving in DB with minimum data processing. next app gets popular and if your app don't require frequent updates to existing data then you can move such data to very high scalable databases like MongoDB (JSON friendly).
563e331861a8013065267c25	X	Can you post your messages or logs?
563e331861a8013065267c26	X	that's a great question, how should I proceed? in my NuGet, the only thing left to install are the AWSSDK 2 and the ImageResizer packages for 3.4.2 --> which ones should I install first? or should I remove the old AWSSDK 1 & IR first?
563e331861a8013065267c27	X	When in doubt, remove AWSSDK and IR completely, then re-install IR and let it install AWSSDK and any required binding redirects.
563e331861a8013065267c28	X	I'm currently using 3.4.1 with and the original Image Resizer S3 I'm trying to do a couple of upgrades here like net451, mvc 5, web api 2 etc. and also move to the latest Amazon AWS SDK 2.0.8.2 & 3.4.2 of IR but the process is filled with errors as the new AWSSDK 2 says that it can't find the proper S3 package or a compatible one. STEPS? 1. Should I remove AWS SDK 1 & IR 3.4.1 completely 1a. Then re-install AWS SDK 2 & IR 3.4.2 afterwards Is there a proper way that has worked for people to get to the latest version of IR 3.4.2?
563e331861a8013065267c29	X	Did you try setting debug="true" on the root configuration element?
563e331861a8013065267c2a	X	re c3p0, you might try overriding korma's version choice and using c3p0-0.9.5-pre4. c3p0 0.9.5.x supports logging directly to the slf4j api without use of the log4j bridge, and configuration might be more straightforward that way. It'll probably happen automatically if you remove the log4j api, but to be sure you log to slf4j, add to c3p0.properties or as a System property 'com.mchange.v2.log.MLog= slf4j' See mchange.com/projects/c3p0-0.9.5-pre4/#configuring_logging
563e331961a8013065267c2b	X	@Alex: yep, I have tried that, and I don't recall it telling me anything useful. That said, I'll certainly try it again.
563e331961a8013065267c2c	X	@SteveWaldman Thank you for that excellent tip! I'll give a different version of c3p0 a shot.
563e331961a8013065267c2d	X	How can I have my application code log at the DEBUG level, but restrain certain library dependencies to the WARN level? Specifically, I'm having a hard time controlling c3p0 and the Amazon SDK. Can anyone tell me why my setup isn't working? Because I don't understand why I'm still seeing AWS's incredibly verbose DEBUG logging still in the log files. Below I've included my logback.xml and relevant excerpts of my project.clj.
563e331a61a8013065267c2e	X	It never fails that I spend 30 minutes looking for something, and then 30 seconds after I post I realize the combination of keywords that get me some results. Posting some potential answers now, but will be interested in comments from anyone who has used these tools. Literally it occurred to me to search for odbc for the cloud and found a blog on the exact same subject: janakiramm.net/blog/do-we-need-odbc-for-the-cloud
563e331a61a8013065267c2f	X	Oxtio bascially have developed code to talk to virtually ALL the cloud providers (they support LOADS) and built a web app on top of it to let users administrate credentials. They briefly mentioned an API some years ago but doesn't look like it's been launched. blog.otixo.com/tag/api-2
563e331a61a8013065267c30	X	out of curiosity, did you write all the authorization and background syncing for every cloud service, or were you able to find a library to do it for you?
563e331a61a8013065267c31	X	Do any APIs/Libraries/tools exist that act as adapters/provider interfaces for accessing different cloud storage services through a common interface? Something similar to ODBC or OLE-DB, except for cloud storage instead of databases. Such that, if I wrote a front end for taking notes, and I utilized such an API, and let the user provide configuration for which cloud storage provider they have an account with, the API library would handle translating my cloud.Save() call into the commands specific to whiever provider was being utilized. This would allow my front-end app to be cloud storage provider agnostic. So maybe I wrote some chrome extension or portable thumb drive app for storing notes, or encrypting and storing passwords, or some such, and you tell it which cloud storage provider you have an account with, and it uses it for syncing. This way your use of that tool doesn't tie you to a specific cloud provider. As long as you backup your data, you could migrate to another provider and just reconfigure the app should you become unhappy with that provider or they go bankrupt. WebDAV for example is one potential candidate since it seems some storage services offer it, but that is not quite what I have in mind, since it depends on the storage providers to offer that as an option. I also don't know enough about WebDAV to know if it really would serve in the capacity I'm imagining. But feel free to post that as an option with pros/cons for comment/discussion. I more imagine something that is a middle layer external to each cloud provider. Of course since each provider offers a different web service for interacting with files, the middle layer would have adapter for each backend. But on the front-end, it would expose a common API that is provider agnostic. Does anything of this type exist? Even just an open source GUI that allows you to store files in any provider, which would imply that in its source code exists the beginnings of such a middle layer. I would think someone has already made a tool that helps you unify all the free GB that you can get from various services. Sort of a JBOD layer for the cloud(although that is not the goal of this post, the point being such a tool accessing many different services would imply it has the beginnings of a middle layer for standardizing access to them). My main interest though is in abstractions for personal cloud storage services, that would be appropriate for applications used by individuals, to put the control of storage in the hands of the individual so that they can have the freedom to move between personal cloud storage services. It seems what I've found so far is more oriented for CDN, websites, or services. Please make seperate posts per suggestion so that votes and comments/discussion can take place specific to that suggestion.
563e331a61a8013065267c32	X	Kloudless provides a common API to several different cloud storage APIs (Dropbox, Box, GDrive, OneDrive, etc.). Kloudless also provides SDKs in popular languages and UI widgets to handle authentication and other user interactions. You can find more information and sign up here: https://developers.kloudless.com/ Full disclosure: I work at Kloudless.
563e331a61a8013065267c33	X	Apache Libcloud: "a unified interface to the cloud" http://libcloud.apache.org/
563e331a61a8013065267c34	X	jclouds: "jclouds presents cloud-agnostic abstractions, with stable implementations of ComputeService and BlobStore." http://jclouds.org/
563e331a61a8013065267c35	X	A couple of months ago I did a survey of personal cloud storage aggregator services and applications. And one seems relevant to your question. Oxtio is a service that connects multiple cloud storage services and includes a WebDAV service for accessing it's own service.
563e331b61a8013065267c36	X	Check out Boto, a highly regarded Python library which provides an abstraction layer atop Amazon's S3 and Google Cloud Storage. https://github.com/boto/boto
563e331b61a8013065267c37	X	Cloud storage providers each have different specifics which makes it hard to use exactly one interface for all (or even some) of them. CloudBlackbox package of our SecureBlackbox product offers a unified interface for major storage providers (S3, Azure, Google Drive, SkyDrive/OneDrive, Dropbox) with focus on security of the data, but due to mentioned specifics we have individual classes (descendants of one superclass) to serve each provider. SecureBlackbox is available for use from .NET, Java, C++ on Windows and Delphi.
563e331b61a8013065267c38	X	-StorageMadeEasy (SME) -Otixo (But they do not offer FREE tier anymore since Feb 2013) -Joukuu -Gladinet -Egistec CloudHub ... All of above allows you to connect several cloud storages, but they do not actually combine it. If you wan to combine several personal cloud storages, you need to make it yourself, which is what I am doing for the past few months. So far I have combined several clouds (Dropbox, Box, Google Drive, Skydrive) using their Android API/SDK, then I process the data splitting/merging/compression/encryption inside my Android application (not a good choice, just for the sake of prototype) In the future, maybe I will add more providers that has an API, such as Amazon S3, SugarSync, but right now there is lack of manpower. If you just want to connect multiple clouds on Android (not combining), then you can try ES File Explorer or ASTRO File Manager, and several other applications
563e331b61a8013065267c39	X	I think webdav is the ultimate protocol:
563e331b61a8013065267c3a	X	Probably easier and a better user experience to just convert them to the desired specs on the server.
563e331b61a8013065267c3b	X	Thanks, I will check out Aurora.js. Re: my use case, the reason for this setup is this: HTTP uploads are not practical in my situation, because a script would have to be on each and every destination server to reassemble the uploaded chunks. There's no guarantee that each destination server would support the necessary scripting language, or have the necessary computing power to stitch together the uploaded chunks while maintaining performance. As for storage, the reason I don't use Amazon S3 is cost, not only the total cost but also my client's need to distribute financial responsibility.
563e331b61a8013065267c3c	X	You need to script only on one server which writes audio metadata to DB and then pushes the raw data forward to the final server or persistent storage like S3. Because the operation is mostly IO bound if your scripts are async and properly written it won't tax the server CPU, only bandwidth.
563e331c61a8013065267c3d	X	The point is not to prevent malicious users from uploading prohibited content. Rather it is to prevent people who don't know what they're doing from uploading audio files that won't play properly, or will take too long for end users to download.
563e331c61a8013065267c3e	X	I am building an application that allows authenticated users to use a Web browser to upload MP3 audio files (of speeches) to a server, for distributing the audio on a network. The audio files need to use a specific bit rate (32kbps or less) to ensure efficient use of bandwidth, and an approved sampling rate (22.050 or 44.100) to maximize compatibility. Rather than validate these requirements following the upload using a server-side script, I was hoping to use HTML5 FileReader to determine this information prior to the upload. If the browser detects an invalid bit rate and/or sampling rate, the user can be advised of this, and the upload attempt can be blocked, until necessary revisions are made to the audio file. Is this possible using HTML5? Please note that the question is regarding HTML5, not about my application's approach. Can HTML5 detect the sampling rate and/or bit rate of an MP3 audio file? FYI note: I am using an FTP java applet to perform the upload. The applet is set up to automatically forward the user to a URL of my choosing following a successful upload. This puts the heavy lifting on the client, rather than on the server. It's also necessary because the final destination of each uploaded file is different; they can be on different servers and different domains, possibly supporting different scripting languages on the server. Any one server would quickly exceed its storage space otherwise, or if the server-side script did an FTP transfer, the server's performance would quickly degrade as a single point of failure. So for my application, which stores uploaded audio files on multiple servers and multiple domains, validation of the bit rate and sampling rate must take place on the client side.
563e331c61a8013065267c3f	X	You can use FileReader API and Javascript built audio codecs to extract this information from the audio files. One library providing base code for pure JS codecs is Aurora.js - then the actual codec code is built upon it https://github.com/ofmlabs/aurora.js/wiki/Known-Uses Naturally the browser must support FileReader API. I didn't understand from your use case why you need Java applet or FTP. HTTP uploads work fine for multiple big files if done properly using async badckend (like Node.js, Python Twisted) and scalable storage (Amazon S3). Similar use case is resizing incoming images which is far more demanding application than extracting audio metadata out from the file. The only benefit on the client side is to reduce the number of unnecessary uploads by not-so-technically-aware users.
563e331c61a8013065267c40	X	Given that any user can change your script/markup to bypass this or even re-purpose it, I wouldn't even consider it. If someone can change your validation script with a bit of knowledge of HTML/Javascript, don't use HTML/Javascript. It's easier to make sure that it is validated, and validated correctly by validating it on the server.
563e331d61a8013065267c41	X	agree totally about the "not about CRUD" part. Its a shame how confused people are about an idea that's so simple and perfect for many distributed situations.
563e331d61a8013065267c42	X	"REST is much more focused towards solving the distributed client/server interaction than it is about dealing with server to server interactions" Can you elaborate on this? why is REST useful or focussed only on client-server interactions and not server-to-server?
563e331d61a8013065267c43	X	This is the first sane explanation of REST I've seen on this site. Thanks for helping to clear this up for people, it's nasty how much misinformation is being propagated on REST.
563e331e61a8013065267c44	X	@Jessemon Providing domain models over the wire is considered an anti-pattern of REST.
563e331e61a8013065267c45	X	@Jessemon You will often hear what you are doing being referred to as a HTTP API also. I appreciate you taking my comment as constructive, that's how I intended it. :-)
563e331e61a8013065267c46	X	Flickr isn't REST. Here's a quote from Roy Fielding: "Flickr obviously don’t have a clue what REST means since they just use it as an alias for HTTP. Perhaps that is because the Wikipedia entry is also confused. I don’t know."
563e331e61a8013065267c47	X	REST can handle encryption (simple as using HTTPs) and transactions (see infoq.com/interviews/mark-little-qcon08 ). My experience with the WS* stack is that, though it my have the aforementioned, it gets complicated, fast, and the amount of overhead/meta data required for these things is too much compared to the actual data being passed.
563e331f61a8013065267c48	X	Https works on SOAP too, however the Transactions coordination between multiple parites and in the same time being interoprable with different technologies is a SOAP purpose
563e331f61a8013065267c49	X	Saying that "requiring enterprise features" means you can't use ReST is not accurate. That you may want long-running transactions hidden behind a ReST interface may mean that some of your system is based on SOAP or EDI. It doesn't matter as far as ReST is concerned, the client should be unaware that such a thing as a transaction exists.
563e331f61a8013065267c4a	X	I know sites like Facebook are now using REST services, but I am wondering of other applications that use REST and if there are specific situations when the use of REST is more warranted than other methodologies.
563e331f61a8013065267c4b	X	REST is not about CRUD data services. Yes you can use REST to do a CRUD like services but that's like saying Regular Expressions are for parsing email addresses. Here is the best presentation I have seen to-date on the REST versus SOAP/RPC debate. REST is much more focused towards solving the distributed client/server interaction than it is about dealing with server to server interactions. REST is about getting content in front of the user so they can choose what to do with it. REST is not about creating an Http based data access layer to decouple your application logic from its data store. Atom Pub is a good REST implementation. The Netflix API is one of the best commercial REST apis. The Twitter API fails most of the RESTful constraints. If you want accurate information about REST go to these places: Don't listen to the big vendors on the subject they are more interested in making their existing products buzzword compliant.   Follow-up: There are a few reasons that I believe REST interfaces are more suitable for client/server interactions than server to server interactions. This is just my opinion and I am not trying to claim that this perspective is held by anyone other than me! The benefits of caching and a stateless server become far more apparent when you are supporting many clients accessing a single server. A server-server communication is often 1-1 and rarely has a large number of servers communicating with a single server. REST is all about loose coupling. The idea is that you can continue to evolve the server without having to update clients. If you are considering implementing a REST service on server A that will be called by server B that is in the same room then the benefits of the loose coupling are diminished. It is not going to kill you to update a piece of software on both machines. The hypermedia constraint is about giving users choices based on the current application state. REST interfaces support ad-hoc exploration of a hyperlinked system. Server-server communication tends to focus on achieving a specific task. e.g. Process this batch of data. Trigger these events based on a schedule. Inherently there is no user sitting there making decisions as to which path to follow. The path has been predetermined based on parameters and conditions. In a server-server communication scenario it may be critical to achieve maximum throughput. A binary protocol may be more suitable than Http. Latency may be critical in a server to server type of communication. In a client-server environment where one end is driven by a human the performance requirements are quite different and I believe the REST constraints are more suited to that type of interaction. REST recommends the use of standard media-types as HTTP payloads. This encourages serendipitous re-use of the services provided. I think there are many more opportunities to re-use services that are intended for use by client applications than those aimed at other servers. When designing REST interfaces I like to think that the consumer of the service is a piece of software that is under the direct control of an end-user. It is no coincidence that a web browser is referred to as a User-Agent.
563e331f61a8013065267c4c	X	SOAP is the most popular alternative to REST, and I found a few good links describing their differences and when to use which: The gist of it is that REST is much more simple than its alternatives (especially SOAP), and should be used when all you need is basic functionality (create/read/update/delete), and your service is stateless. If you want an example application that uses REST, CouchDB does. (I can't think of any other ones off the top of my head.) On top of that, lots of websites use it, such as Flickr, del.icio.us, Bloglines, and Technorati.
563e331f61a8013065267c4d	X	There are LOTS of REST interfaces out there: flickr, and Google's data APIs come to mind as two big examples. REST is great for simple data interaction and stateless connections (similar to HTTP itself). SOAP is a common alternative, and is often used for more complex connections. REST is very popular these days and is a good place to start if you're just learning why you'd want to have a data interface. Designing REST interfaces is easy to learn and has low barriers to entry.
563e331f61a8013065267c4e	X	There are many examples out there. GData and the Atom Pub Protocol are probably the finest. Twitter seems to have a nice REST API also. Amazon's S3 service is also quite "RESTful". Unfortunately, many services that claim to be RESTful violate the very core priciples of REST as laid out by Roy Fielding in his dissertation that described the REST architectural style. REST is an architectural style, not a set in defined standard or implementation. This makes it more difficult to say what is and isn't a REST service, that's why you'll often hear "RESTful". REST can be a great (and simple) alternative to SOAP, XMLRPC, and in some cases things like DCOM and CORBA. It can be a very simple way to facilitate basic distributed computing and a simple way to expose an API... especially due to the face that it integrates so nicely into the ubiquitous HTTP.
563e331f61a8013065267c4f	X	You should consider what your clients want! Building a big SOAP service that nobody wants to consume will be a waste of your time. Similarly, if your potential users are steeped in SOAP then maybe that's what you should give them. If you don't know what your users want, consider the sentiment of the industry. Most companies that expose a public API these days expose a REST API. I really like how Foursquare has documented theirs: https://developer.foursquare.com/overview/
563e331f61a8013065267c50	X	REST is efficient when your ultimate goal of the data is the CRUD operations, usually within a web UI, usually with AJAX, Flash, Silverlight kind of experiences, when security, encryption, transactions are not the concern, however if your requirements includes any enterprise like features mentioned before (Transactions, Encryption, Interoperability ... etc) SOAP is the solution.
563e331f61a8013065267c51	X	I'm trying to show a preview of a few (partially randomly selected) Instagram photos. Till recently, I used to save the image url, show a preview and added a button to go to the actual content - as I had no intention of saving the content myself (I don't own it after all) and I wanted to redirect users to the actual Instagram post. I noticed this does not work anymore - in some cases because they moved the images to another server (they moved from Amazon s3) and in other cases due to caching policies in the Instagram CDN. Example: This is the post: post to Barack Obamas re-election celebration. I have saved the old Amazon url which does not work as there is a new (but temporary) url. I can't use Instagrams embedding option (it would break too many thing). Also, using the Instagram API seems to require an access-token, but I don't want to create them on the server and hand them to the user, nor do I want to force the user to sign-in to Instagram. Is there a way to pull the thumbnail without an access-token, or some other way to request the image url from Instagram?
563e332061a8013065267c52	X	ah thanks for your response and the comment about scaling. I didn't know, the reason I wanted to use base64 is because I would not have to deal with uploading a file and saving it to amazon and getting a link to the file back. I thought this would be easy since I can treat the image the same as any data and populate the page with it using img src. So I guess your recommendation of storing the string on amazon s3 or something would work. It would still be easier than transferring the actual image file. I wonder where I can store and retrieve string values, nearly free and fast?
563e332061a8013065267c53	X	This would make no sense to store dataUrl on Amazon S3, see my edit.
563e332061a8013065267c54	X	I have a Meteor app and I am interested in getting image upload to work in the simplest possible manner. The simplest manner I can come up with is to somehow convert the image to a base64 string on the client and the save it to the database as a string. How is it possible to convert an image on the users filesystem to a base64 string and then save it to the database?
563e332061a8013065267c55	X	You can use an HTML5 file input : HTML Then listen to the change event and use a FileReader to read the local file as a base64 data url that we're going to store in a reactive var : Then we can use the reactive var value to allow/disallow form submission and send the value to the server : You will need to define a server method that saves the dataUrl to some collection field value, what's cool about dataUrls is that you can use them directly as an image tag src. Note that this solution is highly unscalable as the image data won't be cachable and will pollute the app database regular communications (which should only contain text-like values). You could fetch the base64 data from the dataUrl and upload it to Google Cloud Storage or Amazon S3 and serve the files behind a CDN. You could also use services that do all of this stuff for you like uploadcare or filepicker. EDIT : This solution is easy to implement but comes with the main drawback that fetching large base64 strings from mongodb will slow your app from fetching other data, DDP communications are always live and not cachable at the moment so your app will always redownload image data from the server. You wouldn't save dataUrls to Amazon, you would save the image directly, and it would be fetched by your app using an Amazon URL with a cachable HTTP request. You have two choices when it comes to file upload : you can upload them directly from the client using specific javascript browser APIs or you can upload them within Node.js (NPM modules) APIs in the server. In the case you want to upload from the server (which is usually simpler because you don't need to require that the users of your apps authenticate against third party services, only your server will act as a trusted client to communicate with Amazon API), then you can send the data that a user want to upload through a method call with a dataUrl as argument. If you don't want to dive into all this stuff consider using uploadcare or filepicker, but keep in mind that these are paid services (as is Amazon S3 BTW).
563e332061a8013065267c56	X	Not sure if this is the best way, but you can easily do this with a file reader. In the Template event handler where you get the file contents, you can pass the file to the reader and get back a base64 string. For example, something like this:
563e332061a8013065267c57	X	Does your server serve the audio resource with an Access-Control-Allow-Origin: * response header? Generally, a script cannot read cross-origin resources unless it is allowed by a CORS response header from the server when the the resource is served. (If you are not serving CORS responses, it appears that Chrome is wrong here to allow you to read the resource.)
563e332061a8013065267c58	X	@apsillers we have followed the proper steps outlined here: docs.aws.amazon.com/AmazonS3/latest/dev/cors.html and are still unable to get that header. All we can get is a Error: Access Denied. I guess I am confused as to why I don't get this error in Chrome or Safari.
563e332061a8013065267c59	X	It appears that Firefox ignores CORS headers that should allow it to read cross-origin audio files, per this bug. It appears that Chrome is too permissive (plays even when CORS is missing) and Firefox is too strict (does not play even when CORS is present). Is it possible to host the media on the same origin as your player?
563e332061a8013065267c5a	X	Unfortunately, due to the nature of our business, that is not an option. If its a bug, then it's a bug, and we just have to wait until it is resolved or fixed. Thank you once again for your assistance.
563e332061a8013065267c5b	X	The Web Audio API has a ways to go before it will see widespread mature implementation (that bug is over a year old). A possible solution you could implement now would be to use a same-origin reverse proxy to fetch the media (e.g., http://myorigin.com/fetch?path=http://otherorigin.com/song.mp3). You'd need to set up a server to fetch the media and serve it on your origin. (Note it will not work for credential-protected content.)
563e332061a8013065267c5c	X	As of Version 42, Chrome correctly blocks cross-origin file access using the Web Audio API. You can create a simple Audio object to play cross-origin audio, but you cannot create a MediaElementAudioSourceNode from that to, say, analyze the raw audio data.
563e332061a8013065267c5d	X	A good follow up on what your options are: stackoverflow.com/questions/30603872/…
563e332161a8013065267c5e	X	I have been trying to get this to run correctly so days now with no luck. I have created a custom audio player, that accesses an MP3 on a S3 Amazon server. The audio player has custom controls enabled by Javascript, and a Audio Visualizer made possible by the Web Audio API. Now the problem I am running into is this: Work fine on Chrome. Safari out right says it can't run the Web Audio API, but the audio will still play. In Firefox, the entire thing shuts down. Click play... nothing. I thought it was a CORS issue, so we set the proper headers on the server and still nothing. BUT... if I deactivate the Web Audio API visualizer, then I can get the player to play just fine. http://jsfiddle.net/murphy1976/yqqf7uL1/1/ Here is my jFiddle. I have separated the Audio Player controls Script from the Visualizer Script with comments so you can see how it will work in Firefox, and how it will NOT work in Firefox. I read somewhere that this issue that I'm running into MAY be a bug with Firefox. I just want to make sure so that I can stop beating my skull over this. Could I put a call to CORS here?:
563e332161a8013065267c5f	X	The same-origin policy says that scripts run on some origin cannot read resources from another origin. (An origin is a domain, plus a scheme and port, like http://foo.example.com:80.) Note that the same-origin policy does not prevent cross-origin media from being displayed to the user. Rather, it prevents scripts from programmatically reading cross-origin resources. Consider the <img> tag: a page on example.com can show a cross-origin image from other.com, but a script on example.com's page cannot read the contents of that image. The user can see it; the page cannot. The Web Audio API can read the contents of audio files. If an audio file is from a different origin, this kind of reading is not allow by the same-origin policy. A user can listen to a cross-origin audio file, but a script on the page cannot read the contents of the file. When you attempt to feed a cross-origin audio file into an analyzer script (e.g., so that you can draw a visualization on a canvas), the same-origin policy should stop you. You are attempting to violate the same-origin policy, and the browser is correctly stopping you by refusing to play the audio in way that would allow you to read the file contents. Note that Chrome does not prevent such cross-origin file reading for audio files, and this is incorrect behavior. The correct solution is to have your media servers serve the audio files with a CORS Access-Control-Allow-Origin: * HTTP response header. However, this currently does not work in Firefox, which is incorrect behavior. If Firefox hopes to have a compliant implementation, this will be fixed eventually.
563e332161a8013065267c60	X	Confirmed that there is a bug in Firefox for using the createMediaElementSource method on a cross domain source: https://bugzilla.mozilla.org/show_bug.cgi?id=937718
563e332161a8013065267c61	X	alert($location.search().GID this alert will show id?
563e332161a8013065267c62	X	Yes its getid from the api and paticluar image will be opend
563e332161a8013065267c63	X	it just like a click function to the each image the it shows that particular image on next page with 100% width @PareshGami
563e332161a8013065267c64	X	this function is working on my pc brower but not working in mobile app
563e332161a8013065267c65	X	just display in html with {{gallery}} it will display image path?
563e332161a8013065267c66	X	Hi was was building a ionic moible app in that, i have gallery were images come dynamically through API call from s3 Amazon bucket, i want to view image in another page by clicking on it i was donw with it but it was not working in android mobile here is my code app.js gallery.html galleryView.html Please help me out Thanks in Adavance
563e332161a8013065267c67	X	seriously? -2? Is my question that problematic?
563e332161a8013065267c68	X	"a hosted zone that is deleted within 12 hours of creation is not charged; however, any queries on that zone will be charged at the rates below" (from the cited link)
563e332161a8013065267c69	X	I'm using the AWS API for Route53 & S3 and I would like to test out some things (like Hosted Zones) that are not free, within some sort of Sandbox, so that I won't need to actually pay for them. Lots of major services give out some sort of a Sandbox or Testing environment (like Stripe), so that you could test the things that should cost money, without actually paying for it. Does Amazon have something like that (specifically AWS) ?
563e332161a8013065267c6a	X	AWS doesn't provide a sandbox/testing environment. They do provide the free tier which should help you test things without spending too much money. S3 is covered under the free tier. Route53 doesn't have a free tier, but it shouldn't be very expensive for you to test.
563e332261a8013065267c6b	X	Right now in my rails app I'm using Carrierwave to upload files to Amazon S3. I'm using a file selector and a form to select and submit the file, this works well. However, I'm now trying to make posts from an iPhone app and am receiving the contents of the file. I'd like to create a file using this data and then upload it using Carrierwave so that I can get the correct path back. May file model consists of: where path is the Amazon S3 url. I'd like to do something like this to build the files: Would really love someone to point me in the right direction. Thanks!
563e332261a8013065267c6c	X	Here is what I wrote to perform an upload to s3 from an ios application through carrierwave : First the Photo model Second in the Api::V1::PhotosController Then the call from my iPhone application using AFNetworking In the JSON response I can get the new instance of Photo with the image.url attribute set to the url in the s3.
563e332261a8013065267c6d	X	Alright, I have a working solution. I'm going to best explain what I did so that others can learn from my experience. Here goes: Assuming you have an iPhone app that takes a picture: On the rails side I set up a method specifically for handling mobile images, this should help you post the image to your Amazon S3 account through Carrierwave: This works for me for posting and I feel should be pretty extendable. For the class methods: Not saying this code is perfect, not even by a longshot. However, it does work for me. I'm open to suggestions if anyone thinks it could be improved. Hope this helps!
563e332261a8013065267c6e	X	May I ask how is it possible that it is "not available"?
563e332261a8013065267c6f	X	I am developing Restful API layer my app. The app would be used in premises where HTTPS support is not available. We need to support both web apps and mobile apps. We are using Node/Expressjs at the server side. My two concerns are: Is there a way we could setup secure authentication without HTTPS? Is there a way we could reuse the same authentication layer on both web app (backbonejs) and native mobile app (iOS)?
563e332261a8013065267c70	X	I think you are confusing authenticity and confidentiality. It's totally possible to create an API that securely validates the caller is who they say they are using a MAC; most often an HMAC. The assumption, though, is that you've securely established a shared secret—which you could do in person, but that's pretty inconvenient. Amazon S3 is an example of an API that authenticates its requests without SSL/TLS. It does so by dictating a specific way in which the caller creates an HMAC based on the parts of the HTTP request. It then verifies that the requester is actually a person allowed to ask for that object. Amazon relies on SSL to initially establish your shared secret at registration time, but SSL is not needed to correctly perform an API call that can be securely authenticated as originating from an authorized individual—that can be plain old HTTP. Now the downside to that approach is that all data passing in both directions is visible to anyone. While the authorization data sent will not allow an attacker to impersonate a valid user, the attacker can see anything that you transmit—thus the need for confidentiality in many cases. One use case for publicly transmitted API responses with S3 includes websites whose code is hosted on one server, while its images and such are hosted in S3. Websites often use S3's Query String Authentication to allow browsers to request the images directly from S3 for a small window of time, while also ensuring that the website code is the only one that can authorize a browser to retrieve that image (and thus charge the owner for bandwidth). Another example of an API authentication mechanism that allows the use of non-SSL requests is OAuth. It's obsolete 1.0 family used it exclusively (even if you used SSL), and OAuth 2.0 specification defines several access token types, including the OAuth2 HTTP MAC type whose main purpose is to simplify and improve HTTP authentication for services that are unwilling or unable to employ TLS for every request (though it does require SSL for initially establishing the secret). While the OAuth2 Bearer type requires SSL, and keeps things simpler (no normalization; the bane of all developers using all request signing APIs without well established & tested libraries). To sum it up, if all you care about is securely establishing the authenticity of a request, that's possible. If you care about confidentiality during the transport of the response, you'll need some kind of transport security, and TLS is easier to get right in your app code (though other options may be feasible).
563e332261a8013065267c71	X	If you mean SSL, No. Whatever you send through your browser to the web server will be unencrypted, so third parties can listen. HTTPS is not authentication, its encyrption of the traffic between the client and server. Yes, as you say, it is layer, so it's interface will be independent from client, it will be HTTP and if the web-app is on same-origin with that layer, there will be no problem. (e.g. api.myapp.com accessed from myapp.com). Your native mobile can make HTTP requests, too.
563e332261a8013065267c72	X	In either case of SSL or not SSL, you can be secure if you use a private/public key scenario where you require the user to sign each request prior to sending. Once you receive the request, you then decrypt it with their private key (not sent over the wire) and match what was signed and what operation the user was requesting and make sure those two match. You base this on a timestamp of UTC and this also requires that all servers using this model be very accurate in their clock settings. Amazon Web Services in particular uses this security method and it is secure enough to use without SSL although they do not recommend it. I would seriously invest some small change to support SSL as it gives you more credibility in doing so. I personally would not think you to be a credible organization without one.
563e332361a8013065267c73	X	I'm totally interested in this topic as well, sad to see no answers yet
563e332361a8013065267c74	X	A client of mine needs to accept a bunch of different video files and convert them to FLV. My experience with FFMEG on a previous project has highlighted that there will be some troublesome files. Depending on the price my client will pay for a professional service. What are people using and how are you finding the service? Thanks.
563e332361a8013065267c75	X	<biased answer alert> I recommend Zencoder (http://zencoder.com), built by the same folks that built Flix Cloud (http://flixcloud.com). We've put a ton of work into handling troublesome files, and we can support a wider range of input files than anyone out there. We're also the fastest service on the market and have (we think) a very developer-friendly API.
563e332361a8013065267c76	X	I can't give any recommondations, as I'm searching for a service to implement for many clients myself. However, perhaps I can help anyone else who is also looking. It seems there are a couple type of services: In looking at these services, a few things to consider:
563e332361a8013065267c77	X	encoding.com is pretty good and cheap. I used them for conversion of uploaded user files to FLV. After that encoding.com can upload files to AWS S3 or to your FTP account with 'ping' request. Should be enough for automation.
563e332361a8013065267c78	X	In searching for a provider for this, I did extensive testing with the key companies in this arena -- Encoding.com, Zencoder, Ankoder, and Heywatch. I found Heywatch and Ankoder to be less than recommendable. Encoding.com was my first choice (as a hunch) to begin the search, and their service performed very well and their documentation was adequate. Zencoder was the clear winner for me and is what we decided to go with. They have exceptional documentation and a clean API. They deal with issues fast whenever their is an problem that occurs. I would recommend them for anyone who is looking for transcoding, and since they are now a part of Brightcove ($$), I only see the service getting stronger. Note: I am not affiliated with any of these companies and think my response is relatively unbiased.
563e332361a8013065267c79	X	I'm currently looking at services for this as well, just found encoding.com from an answer to this question I also have been looking at CDN's becuase I also need to ensure that the videos don't overwhelm my servers, not sure but I thought some of them said full service media including transcoding. if you need a CDN to deliver the video too it may come with transcoding. Now I'm getting into server stuff, maybe this should topic should move to server fault?
563e332361a8013065267c7a	X	flixcloud.com from the makers of the on2 vp6 codec which is very good quality. i prefer this over encoding.com as they dont charge a monthly fee.
563e332461a8013065267c7b	X	Don't forget BitsOnTheRun, they offer some great service and reasonable prices.
563e332461a8013065267c7c	X	As a cofounder I'm biased, but Transloadit also offers clientside integration.
563e332461a8013065267c7d	X	This has been asked before
563e332461a8013065267c7e	X	I didn't find it, where?
563e332461a8013065267c7f	X	Here stackoverflow.com/questions/3748/…
563e332461a8013065267c80	X	True, didn't find that. Thanks
563e332461a8013065267c81	X	Could you please explain me the last paragraph ( Regarding security ) in terms of the technical details or any pointers would be very helpful. Thank you.
563e332461a8013065267c82	X	(For all you googlers out there) If you have your site's root configured to a "public" folder (as in my_website/public/ instead of just my_website/), you can store the images in the my_website/my_images folder with the rest of your app. Then your img tags would reference "my_website/image.php?img_id=55" instead of "my_website/avatar.png", and your image.php script would, after verifying your credentials and parsing the id you hand it, return the actual image. That way, the image is only viewable by the proper logged in user.
563e332461a8013065267c83	X	Good warning about the number of files on the same directory. It can give errors too hard to find in a production environment.
563e332461a8013065267c84	X	I had hit this problem before. NTFS behaved unpredictably with some 10,000 files in a folder.
563e332561a8013065267c85	X	I'm writing an application that allows users to upload images onto the server. I expect about 20 images per day all jpeg and probably not edited/resized. (This is another question, how to resize the images on the server side before storing. Maybe someone can please drop a .NET resource for that in the comment or so). I wonder now what the best practice for storing uploaded images is. Is it a) I store the images as a file in the file system and create a record in a table with the exact path to that image. or b) I store the image itself in a table using an "image" or "binary data" data type of the database server. I see advantages and disadvantages in both. I like a) because I can easily relocate the files and just have to change the table entry. On the other hand I don't like storing business data on the web server and I don't really want to connect the web server to any other datasource that holds business data (for security reasons) I like b) because all the information is in one place and easily accessible by a query. On the other hand the database will get very big very soon. Outsourcing that data could be more difficult.
563e332561a8013065267c86	X	I generally store files on the file-system, since that's what its there for, though there are exceptions. For files, the file-system is the most flexible and performant solution (usually). There are a few problems with storing files on a database - files are generally much larger than your average row - result-sets containing many large files will consume a lot of memory. Also, if you use a storage engine that employs table-locks for writes (ISAM for example), your files table might be locked often depending on the size / rate of files you are storing there. Regarding security - I usually store the files in a directory that is outside of the document root (not accessible through an http request) and serve them through a script that checks for the proper authorization first.
563e332561a8013065267c87	X	Flickr use the filesystem -they discuss the reasons here
563e332561a8013065267c88	X	The only benefit for the option B is having all the data in one system, yet it's a false benefit! You may argue that your code is also a form of data, and therefore also can be stored in database - how would you like it? Unless you have some unique case:  It is not necessary to use filesystem to keep files. Instead you may use cloud storage (such as Amazon S3) or Infrastructure-as-a-service on top of it (such as Uploadcare): https://uploadcare.com/upload-api-cloud-storage-and-cdn/ But storing files in the database is a bad idea.
563e332561a8013065267c89	X	We have had clients insist on option B a few times on a few different backends, and we always ended up going back to option A eventually. Large BLOBs like that just have not been handled well enough even by SQL Server 2005, which is the latest one we tried it on. Specifically, we saw serious bloat and I think maybe locking problems. One other note: if you are using NTFS based storage (windows server, etc) you might consider finding a way around putting thousands and thousands of files in one directory. I am not sure why, but sometimes the file system does not cope well with that situation. If anyone knows more about this I would love to hear it. But I always try to use subdirectories to break things up a bit. Creation date often works well for this: Images/2008/12/17/.jpg ...This provides a decent level of seperation, and also helps a bit during debugging. Explorer and FTP clients alike can choke a bit when there are truly huge directories.
563e332561a8013065267c8a	X	I have recently created a PHP/MySQL app which stores PDFs/Word files in a MySQL table (as big as 40MB per file so far). Pros: Cons: I'd call my implementation a success, it takes care of backup requirements and simplifies the layout of the project. The performance is fine for the 20-30 people who use the app.
563e332561a8013065267c8b	X	I use uploaded images on my website and I would definitely say option a). One other thing I'd highly recommend is immediately changing the file name from what the user has named the photo, to something more manageable. For example something with the date and time to uniquely identify each picture. It also helps to strip the user's file name of any strange characters to avoid future complications.
563e332661a8013065267c8c	X	Definitely resize the image, and check it's format if you can. There have been cases of malicious files being uploaded and served by unwitting hosts- for instance, the GIFAR vulnerability allowed you to hide a malicious java applet in a GIF file, which would then be able to read cookies in the current context and send them to another site for a cross-site scripting attack. Resizing the images usually prevents this, as it munges the embedded code. While this attack has been fixed by JVM patches, naively serving up binary files without scrubbing them opens you up to a whole range of vulnerabilities. Remeber, most virus scanners can only run against the filesystem- if you store your binaries in the DB, you won't be able to run a scanner against them very easily.
563e332661a8013065267c8d	X	Most implementations are option A. With option B, you open a whole big can of whoop4ss when you marshall those bits from the database into something that can be displayed on a browser... Also, if the db is down, the images are not available. I don't think that space is too much of an issue... Terabyte drives are a couple hundred bucks now. We are implementing with option A because we don't have the time or resources to do option B.
563e332661a8013065267c8e	X	There's sort of a hybrid approach in SQL Server 2008 called the filestream datatype that was talked about on RunAs Radio #74, which is sort of like the best of both worlds. Most people don't have the 2008 otion, but if you do, this option looks pretty cool
563e332661a8013065267c8f	X	We use A. I would put it on a shared drive (unless you don't plan on running more than one server). If the time comes when this won't scale for you then you can investigate caching mechanisms.
563e332661a8013065267c90	X	Absolutely, positively option A. Others have mentioned that databases generally don't deal well with BLOBs, whether they're designed to do so or not. Filesystems, on the other hand, live for this stuff. You have the option of using RAID striping, spreading images across multiple drives, even spreading them across geographically disparate servers. Another advantage is your database backups/replication would be monstrous.
563e332661a8013065267c91	X	For auto resizing, try imagemagick... it is used for many major open source content/photo management systems... and I believe that there are some .net extensions for it.
563e332661a8013065267c92	X	Option A. Once the image is loaded you can verify the format and resize it before saving. There a number of .Net code samples to resize images on http://www.codeproject.com. For instance: http://www.codeproject.com/KB/cs/Photo_Resize.aspx
563e332661a8013065267c93	X	For security reasons, it is also best practise to avoid problems caused by IE's Content Sniffing which can allow attackers to upload JavaScript inside image files, which might get executed in the context of your site. So you might want to transform the images (crop/resize them) somehow before storing them to prevent this sort of attack. This answer has some other ideas.
563e332661a8013065267c94	X	If they are small files that will not need to be edited then option B is not a bad option. I prefer this to writing logic to store files and deal with crazy directory structure issues. Having a lot of files in one directory is bad. emkay? If the files are large or require constant editing, especially from programs like office, then option A is your best bet. For most cases, it's a matter of preference, but if you go option A, just make re the directories don't have too many files in them. If you choose option B, then make the table with the BLOBed data be in it's own database and/or file group. This will help with maintenance, especially backups/restores. Your regular data is probably fairly small, while your image data will be huge over time.
563e332661a8013065267c95	X	Well, I have a similar project where users upload files onto the server. Under my point of view, option a) is the best solution due to it's more flexible. What you must do is storing images in a protected folder classified by subdirectories. The main directory must be set up by the administrator as the content must no run scripts (very important) and (read, write) protected for not be accesible in http request. I hope this helps you.
563e332761a8013065267c96	X	Bugger might help. You use Chrome dev tools to access it.
563e332761a8013065267c97	X	Thanks for your reply, but bugger won't start on my server, throwing an ECONNREFUSED even though I'm root. Have looked at node-inspector but it doesn't give me any visibility over the network tab, which I think would be crucial to understand if it's any external call that's causing it.
563e332761a8013065267c98	X	Have you tried using something like New Relic to help check your app?
563e332761a8013065267c99	X	Thanks for the idea. I've installed it but New Relic doesn't seem to be returning much data of use. I'll keep it installed just in case.
563e332761a8013065267c9a	X	Many thanks for your response. I have been using Firebase 1.0.19 for development - but I have rebuilt the app using some methods that I'll detail in my answer, and it hasn't crashed yet (24h +)
563e332761a8013065267c9b	X	I'm developing an app using NGinx + Node.js + Express + Firebase that simply takes input from a mobile app and stores it to Firebase, optionally uploading files to S3. In its simplest terms, the "create" function does this There are a few other functions that I have implemented as an API. My trouble is coming from an intermittent spike in CPU usage, which is causing the nginx server to report a gateway timeout from the Node.js application. Sometimes the server will fall over when performing authentication against a MongoDB instance, other times it will fall over when I'm recieving the input from the Mobile app. There doesn't seem to be any consistency between when it falls over. Sometimes it works fine for 15+ various requests (upload/login/list, etc), but sometimes it will fall over after just one request. I have added error checking in the form of: Which will throw errors if I mistype a variable for example, but when the server crashes there are no exceptions thrown. Similarly checking my logs shows me nothing. I've tried profiling the application but the output doesn't make any sense at all to me. It doesn't point to a function or plugin in particular. I appreciate this is a long winded problem but I'd really appreciate it if you could point me in a direction for debugging this issue, it's causing me such a headache!
563e332761a8013065267c9c	X	This may be a bug in the Firebase library. What version are you using? I've been having a very similar issue that has had me frustrated for days. Node.js + Express + Firebase on Heroku. Process will run for a seemingly random time then I start getting timeout errors from Heroku without the process ever actually crashing or showing an error. Higher load doesn't seem to make it happen sooner. I just updated from Firebase 1.0.14 to latest 1.0.19 and I think it may have fixed the problem for me. Process has been up for 2 hours now where it would only last for 5-30 min previously. More testing to do, but thought I'd share my in-progress results in case they were helpful.
563e332761a8013065267c9d	X	It seems the answer was to do with the fact that my Express app was reusing one Firebase connection for every request, and for some reason this was causing the server to lock up. My solution was to create some basic middleware that provides a new reference to the Firebase on each API request, see below: I then simply call this middleware on my routes: This middleware will soon be extended to provide Firebase.auth() on the connection to ensure that any API call made with a valid authToken would be signed to the user on Firebase's side. However for development this is acceptable. Hopefully this helps someone.
563e332761a8013065267c9e	X	I want to use LZO compression on my Elastic Map Reduce job's output that is being stored on S3, but it is not clear if the files are automatically indexed so that future jobs run on this data will split the files into multiple tasks. For example, if my output is a bunch of lines of TSV data, in a 1GB LZO file, will a future map job only create 1 task, or something like (1GB/blockSize) tasks (i.e. the behavior of when files were not compressed, or if there was a LZO index file in the directory)? Edit: If this is not done automatically, what is recommended for getting my output to be LZO-indexed? Do the indexing before uploading the file to S3?
563e332761a8013065267c9f	X	Short answer to my first question: AWS does not do automatic indexing. I've confirmed this with my own job, and also read the same from Andrew@AWS on their forum. Here's how you can do the indexing: To index some LZO files, you'll need to use my own Jar built from the Twitter hadoop-lzo project. You'll need to build the Jar somewhere, then upload to Amazon S3, if you want to Index directly with EMR. On side note, Cloudera has good instructions on all the steps for setting this up on your own cluster. I did this on my local cluster, which allowed me to build the Jar and upload to S3. You can probably find a pre-built Jar on the net if you don't want to build it yourself. When outputting your data from your Hadoop job, make sure you use the LzopCodec and not the LzoCodec, otherwise the files are not indexable (at least based on my experience). Example Java code (same idea carries over to Streaming API): Once your hadoop-lzo Jar is on S3, and your Hadoop job has outputted .lzo files, run your indexer on the output directory (instructions below you got a EMR job/cluster running): Then when you're using the data in a future job, be sure to specify that the input is in LZO format, otherwise the splitting won't occur. Example Java code:
563e332761a8013065267ca0	X	Update: Azure now supports CORS.
563e332861a8013065267ca1	X	BTW, you may want to update your blog post... browsers will throw errors if you try to manually set the content-length header of a request.
563e332861a8013065267ca2	X	Is it possible to create an html form to allow web users to upload files directly to azure blob store without using another server as a intermediary? S3 and GAW blobstore both allow this but I cant find any support for azure blob storage.
563e332861a8013065267ca3	X	Do take a look at these blog posts for uploading files directly from browser to blob storage: http://coderead.wordpress.com/2012/11/21/uploading-files-directly-to-blob-storage-from-the-browser/ http://gauravmantri.com/2013/02/16/uploading-large-files-in-windows-azure-blob-storage-using-shared-access-signature-html-and-javascript The 2nd post (written by me) makes use of HTML 5 File API and thus would not work in all browsers. The basic idea is to create a Shared Access Signature (SAS) for a blob container. The SAS should have Write permission. Since Windows Azure Blob Storage does not support CORS yet (which is supported by both Amazon S3 and Google), you would need to host the HTML page in the blob storage where you want your users to upload the file. Then you can use jQuery's Ajax functionality.
563e332861a8013065267ca4	X	Now that Windows Azure storage services support CORS, you can do this. You can see the announcement here: Windows Azure Storage Release - Introducing CORS, JSON, Minute Metrics, and More. I have a simple example that illustrates this scenario here: http://www.contentmaster.com/azure/windows-azure-storage-cors/ The example shows how to upload and download directly from a private blob using jQuery.ajax. This example still requires a server component to generate the shared access signature: this avoids the need to expose the storage account key in the client code.
563e332861a8013065267ca5	X	You can use HTML5 File API, AJAX and MVC 3 to build a robust file upload control to upload huge files securely and reliably to Windows Azure blob storage with a provision of monitoring operation progress and operation cancellation. The solution works as below: Get the sample code here: Reliable Uploads to Windows Azure Blob Storage via an HTML5 Control
563e332861a8013065267ca6	X	I have written a blog post with an example on how to do this http://blog.dynabyte.se/2013/10/09/uploading-directly-to-windows-azure-blob-storage-from-javascript/ the code is at GitHub It is based on Gaurav Mantris post and works by hosting the JavaScript on the Blob Storage itself.
563e332861a8013065267ca7	X	Now I'm got some idea to choose putObjectRequest.setProgressListener(new ProgressListener() { @Override public void progressChanged(ProgressEvent progressEvent) { System.out.println(progressEvent.getBytesTransfered()+">> Number of byte transferd"); } }); Still I'm not getting true status
563e332861a8013065267ca8	X	where does request come from?
563e332861a8013065267ca9	X	Thank you Eli, But i figure out the problem , the problem is My server is sits on local host so from my browser to my server data is being transfer very fast , but when my server is tiring to send the file to S3 it is taking time ,(Because now only the file is going out from my computer). I have debug the cody found that AWS SDK making HTTPClient request to put the file in S3 and this one is taking time.Is there any way so that the file can directly transfer to S3.
563e332961a8013065267caa	X	Hi Krushna, now I think I get you: the progress on your browser shows data being uploaded very fast and then at the end it seems to get stuck, doesn't it? If this is the case you did not disable buffering of client requests in the server.
563e332961a8013065267cab	X	Continuing my previous comment... You need to disable buffering for the servlet(?) handling the upload. Then your code will be called immediately after web server parses the http header and you can start streaming the file to S3 at the same time the client streams the file to you. Sending the file directly from the browser is not recommended (even if you could somehow bypass the same server sandbox) because you'd have to share your AWS private key with the client.
563e332961a8013065267cac	X	My code is written above, I'm using s3Client.putObject(putObjectRequest); can you give me any idea , where I will write the code to disable buffering. The AWS SDK internally reading the file after completing it , it's making a HTTPClient request to upload the file to S3
563e332961a8013065267cad	X	Your problem is in mpf.getBytes().length getBytes() reads the whole file and returns it as a byte array, use getSize() instead. Sorry for misleading you earlier, I got my response and request buffering mixed up. Usually it is response buffering that can be turned off. Requests are not usually buffered unless you have a proxy somewhere.
563e332961a8013065267cae	X	I'm uploading multiple files to Amazon S3. By using the below code. I have create the custom inputstream to get number byte consumed by Amazon S3 , I got the idea from the question :- Upload file or InputStream to S3 with a progress callback My ProgressInputStream class is below } But this not working properly, it printing immediately up to the file size like below But actual uploading taking more time (more then 10 times after printing the lines) What i should do so that i can get a true upload status. Please help me
563e332961a8013065267caf	X	I got the answer of my questions the best way get the true progress status by using below code The problem with my previous code was , I was not setting the content length in meta data so i was not getting the true progress status. The below line is copy from PutObjectRequest class API Constructs a new PutObjectRequest object to upload a stream of data to the specified bucket and key. After constructing the request, users may optionally specify object metadata or a canned ACL as well. Content length for the data stream must be specified in the object metadata parameter; Amazon S3 requires it be passed in before the data is uploaded. Failure to specify a content length will cause the entire contents of the input stream to be buffered locally in memory so that the content length can be calculated, which can result in negative performance problems.
563e332961a8013065267cb0	X	I going to assume you are using the AWS SDK for Java. Your code is working as it should: It shows read is being called with 4K being read each time. Your idea (updated in the message) is also correct: The AWS SDK provides ProgressListener as a way to inform the application of progress in the upload. The "problem" is in the implementation of the AWS SDK it is buffering more than the ~30K size of your file (I'm going to assume it's 64K) so you're not getting any progress reports. Try to upload a bigger file (say 1M) and you'll see both methods give you better results, after all with today's network speeds reporting the progress on a 30K file is not even worth it. If you want better control you could implement the upload yourself using the S3 REST interface (which is what the AWS Java SDK ultimately uses) it is not very difficult, but it is a bit of work. If you want to go this route I recommend finding an example for computing the session authorization token instead of doing it yourself (sorry my search foo is not strong enough for a link to actual sample code right now.) However once you go to all that trouble you'll find that you actually want to have a 64K buffer on the socket stream to ensure maximum throughput in a fast network (which is probably why the AWS Java SDK behaves as it does.)
563e332961a8013065267cb1	X	I've recently switched to Android Studio from Eclipse, and its better for the most part. But now I'm at the point of wanting to create libraries to be reused later. I know about modules, but don't want to use them, as it seems to copy a duplicate in each project (I'd rather have a reference to a lib as in Eclipse). So I've turned my attention to a Maven/Gradle solution. Ideally I'd like to be able to export my lib to a local repo and/or maven repo, and reference it through that via gradle. I've been looking for quite a while now, and each answer is different, and none of them worked for me. This is the closest I've found to what I'm looking for, but there is an error in javadoc creation. These sites (link and link) require a Bintray or Sonatype account to publish the libraries globally. Publishing the library is an end goal for me so I'd like to keep that option open if possible, but for now I just want my libs to be private. It would be pretty great if there was a plugin that I could just specify a maven repo to export to, but I haven't found anything that looks promising yet. So my question is: Is there a recommended "simple" way to export a library in Android Studio, which I can reference then through Gradle? Bonus Marks: Would I be able to obfuscate libraries with proguard once published? Currently setting minifyEnabled=true on my lib results in the .aar file not being generated when building.
563e332961a8013065267cb2	X	You can use maven-publish plugin to publish your artifacts to any repository you have access to. I am using it combined with Amazon S3. The repository can be your local maven repo (the .m2 directory), local Artifactory, Archiva or any other maven repository server or a hosted solution. S3 works well and I am sure there are hosted Artifactories etc out there. If you are using an external repository, add it to the repositories section, but just using the plugin should give you the choice of publishing to local maven. The gradle task is called publishToMavenLocal. You need to define your publication. Publication is simply a set of artifacts created by your build, in your case it will probably be a jar, but it can be anything. Short example how do I use it with S3. The publishing -> repositories -> add is the place where you specify all repos where you want to upload your archives. If you want to use library from your local maven, simply add: And then refer to your lib as to any other dependency with group id, artifact id and version. As far as I know, obfuscated libraries are the same as the non-obfuscated, just with changed names of identifiers. If the requirement is to publish an obfuscated library, that should be fine. Just don't obfuscate the API interfaces and classes (the entry points). If you publish non-obfuscated library, I am not sure if it gets obfuscated in the final archive.
563e332961a8013065267cb3	X	I added the Bump API to my answer as it looks to be a very appealing way to implement data transfer for small payloads.
563e332961a8013065267cb4	X	Just on Bump API, I found that it does not use bluetooth! It's sending data with NFC technology which is just for a distance of a few centimeters. Or am I missing something here?
563e332961a8013065267cb5	X	According to the Bump website their API is discontinued as of Jan 31, 2014.
563e332a61a8013065267cb6	X	Thanks, I updated the answer accordingly.
563e332a61a8013065267cb7	X	@user1227928 BLE does not require MFi. But Android and iOS still cannot connect due to a bug in android: code.google.com/p/android/issues/detail?id=58725
563e332a61a8013065267cb8	X	This MFi for Bluetooth is ludicrous. Imagine being restricted to certain WIFI airports only. I don't see why Apple keeps on putting useless locks on industry standards. These political decisions are so annoying.
563e332a61a8013065267cb9	X	I've been reading up on how to transfer data between iOS devices over Bluetooth using GameKit. I'm not writing a game, per se, but do have a need to transfer a small amount of binary data between two devices. Between two iSO devices, this is easy enough. However, I was wondering if it is possible to transfer data between an iOS device and an Android device via the same mechanism. Has anyone come across documentation/tutorial that would explain how to do this? Is it even technically possible? Or has Apple put in some sort of restriction that would prevent this? The other option I discovered was Bonjour over Bluetooth. Would this be a more suitable option for this type of operation?
563e332a61a8013065267cba	X	This question has been asked many times on this site and the definitive answer is: NO, you can't connect an Android phone to an iPhone over Bluetooth, and YES Apple has restrictions that prevent this. Some possible alternatives: Coolest alternative: use the Bump API. It has iOS and Android support and really easy to integrate. For small payloads this can be the most convenient solution. Details on why you can't connect an arbitrary device to the iPhone. iOS allows only some bluetooth profiles to be used without the Made For iPhone (MFi) certification (HPF, A2DP, MAP...). The Serial Port Profile that you would require to implement the communication is bound to MFi membership. Membership to this program provides you to the MFi authentication module that has to be added to your hardware and takes care of authenticating the device towards the iPhone. Android phones don't have this module, so even though the physical connection may be possible to build up, the authentication step will fail. iPhone to iPhone communication is possible as both ends are able to authenticate themselves.
563e332a61a8013065267cbb	X	Could use Google Play game services?, it doesn't use bluetooth but has alot of features. Tutorial on it here
563e332b61a8013065267cbc	X	Why does CORS not work for IE? I got one using jquery fileupload plugin, but I have not tested it on IE. This post blog.appharbor.com/2013/01/10/… seems to indicate that it would work on IE
563e332b61a8013065267cbd	X	@d33pika - it uses the magical jQuery fileupload plugin. I've mentioned in the question that the task of extracting just that much logic out of the plugin is quite very very hard
563e332b61a8013065267cbe	X	@d33pika there is also the issue of NO CORS in IE
563e332b61a8013065267cbf	X	@JibiAbraham: Well, IE has had CORS since version 8, but keeping with tradition, it obviously decided to go its own ways. XDomainRequest - Restrictions, Limitations and Workarounds
563e332b61a8013065267cc0	X	The basic plugin is not that complicated: github.com/blueimp/jQuery-File-Upload/wiki/Basic-plugin . Check it out.
563e332b61a8013065267cc1	X	"Tries to save a reference to the iframe's document object" - that would throw an uncatchable error would it not?
563e332b61a8013065267cc2	X	The error is catchable.
563e332b61a8013065267cc3	X	I'll give this a shot, I've got everything but the error handling down, will let you know, tx
563e332b61a8013065267cc4	X	I really do appreciate you taking the time to respond even after such a long time. The post messages thing is a nifty trick, thank you for sharing. Hopefully there will be many who find its uses :)
563e332b61a8013065267cc5	X	S3 takes over the frame in the case of an error, unfortunately, I've come to the sad conclusion that IE users most definitely will just have to settle for "Something went wrong" instead of what exactly went wrong
563e332b61a8013065267cc6	X	Oh I see... that's a shame but that means you know if it has been taken over or not, just by sending a postMessage to said iFrame, if you get a reply all is good, if not it has been taken over because of an error...
563e332c61a8013065267cc7	X	@JibiAbraham I updated the script to trace if the iframe has been taken over by S3
563e332c61a8013065267cc8	X	Sigh, we're back to this. I can easily enough use CORS on any decent enough browser to directly upload files to my AWS S3 bucket. But (it was coming), with IE I have to fall back to Iframes. Easy, set up a hidden Iframe, create a form, set its target to Iframe name/id, submit form. If the upload is successful, the Iframe is redirected to a url I specify and I can access the whatever I need to. But if an error occurs, since the Iframe is now on an AWS domain, I won't have access to the XML content of the error. Infact, I won't even know that an error has occurred. I've seen brave people on the internet talking about hosting an html file, on the same bucket to which files are to be uploaded, and then using postMessages to route the Iframe content, or something of that sort. Could someone please explain to me how to achieve this mythical solution? The jQuery file uploader by Blueimp seems to solve this, but by God the code is so jQueryified that I haven't been able to get the gist of it.
563e332c61a8013065267cc9	X	Almost everything you need to know about how the jQuery File Upload plugin does iframe uploads is in its Iframe Transport plugin (along with supporting result.html page). As an introduction, you may want to read their user instructions on their Cross domain uploads wiki page, specifically the Cross-site iframe transport uploads section. (Note that according to their Browser support page, niceties like upload progress are not supported for IE <10, so I wouldn't consider these possible using the iframe transport, at least without significant effort.) (Also, I don't believe any S3 upload implementation using the File Upload plugin has access to the XML content of a file upload error) The Iframe Transport plugin adds a new Ajax "transport" method for jQuery and is not specific to the File Upload plugin. You may want to read the documentation for jQuery.ajaxTransport() to understand the API that jQuery provides for adding a new transport. I'll try to summarize what the Iframe Transport plugin is doing, and how it relates to uploading files to Amazon S3: When a file upload is triggered, the send() function is called. This function: Creates a hidden form element Creates an iframe element with src="javascript:false;", and binds a load event handler to the iframe Appends the iframe to the hidden form, and appends the hidden form to the document. When the iframe is created and its "page" loaded, its load event handler is called. The handler: Clears itself from the iframe, and binds another load event handler Configures the hidden form: The form's action will be the URL for the S3 bucket The form's target is set to the iframe, so that the server response is loaded in the iframe Other fields, e.g. AWSAccessKeyId, are added. Specifically, success_action_redirect is set to the URL of result.html on your server, e.g. http://example.org/result.html?%s. Normally, the %s token should be replaced with the upload results by server-side code, but with S3 this can be hard-coded with a success value by your code, since Amazon will redirect to this URL only if the upload succeeded. File input fields from the original form are moved into the hidden form, with cloned fields left in the original fields' place Submits the hidden form Moves the file input fields back into the original form, replacing the cloned fields The file(s) are uploaded to S3. If successful, Amazon redirects the iframe to the success_action_redirect URL. If not successful, Amazon returns an error, which is also loaded in the iframe. The iframe's load event handler is called. The handler: Tries to save a reference to the iframe's document object. If the file upload failed, the handler saves an undefined instead. Calls the complete callback with a success code and a reference to the iframe's document object (or undefined) Removes the hidden form (and iframe) Before control is returned to your code, the iframe's document object is passed to a converter (at the bottom of the Iframe Transport plugin), depending on what type of data you were expecting. The converter extracts that data from the document object and returns it (or undefined if the file upload failed) to your callback(s). Your callback(s) (success and/or complete as passed to jQuery.ajax()) is called. A success code is always returned by the plugin, and so any error callback will not be triggered. If the data passed to your callback(s) is the value you included in the success_action_redirect, then the file upload succeeded. If the data is undefined, then the file upload failed. Update: If the error XML page stays on the same origin as the S3 bucket, then another page from the S3 bucket, loaded into another iframe, can access the original iframe's content (because they are from the same origin). Your main page can communicate with this second iframe using postMessage() (or easyXDM's FlashTransport, if you need to support IE6/7).
563e332c61a8013065267cca	X	This problem, of providing accurate feedback to users using browsers with no FileReader or FormData support has troubled me a lot as wel. I spent a whole 3 days trying to come up with a solution and finally came up with something close to nothing. Lets get down to the facts: Ok, then there is no other way of uploading the file than using an iframe. Right? So, jQuery File Upload using jQuery Iframe Transport as @jeferry_to describes so well is the tool for the job. *Actually the tool/plugin doesn't change a thing.. What now? Well... we need to access the S3 response inside the transport iframe. But we can't because its on a different domain. So we decide to deal with it by using this trick involving a second iframe. The setup: The scenario: First of all we need to modify jQuery Iframe Transport so that it does not auto remove the auto-generated form and transport frame. We need to do this cause #postMessage which will use later is asynchronous by nature and we don't want the iframe gone by the time we try to access it. Ok, everything should work now cause everything is done by the book. Nahh, you should not even bother. You see... if you force a modern browser to use the iframe transport instead of the XHR2 the above solution will indeed work like a charm. However that's pointless. We want it to work in IE8 + 9. Well... in IE8/9 it sometimes work, it sometimes doesn't. Usually it doesn't. Why? Because of the IE's friendly HTTP error messages. Oh yes you read just fine. In case of an error, S3 responds with an HTTP error status depending on the error (400, 403 etc). Now, depending on the status and the length of the response as shown here, IE discards the S3 response and replaces it with a friendly error message. In order to overcome this, you must make sure the response is always > 512 bytes. In this case you cannot guarrantee anything like that cause you don't control the response. S3 does and the typical errors are less than 512 bytes. In short: The iframe trick works on those browsers that do not need it, and doesn't on those who do. Unfortunately, I can't think of anything else so that case is closed for me now.
563e332c61a8013065267ccb	X	Summarizing my answer in the comments: IE has CORS support with some restrictions: http://www.html5rocks.com/en/tutorials/cors/ and this implementation of direct upload to S3 looks much simpler than jquery fileupload and its not in jquery: http://codeartists.com/post/36892733572/how-to-directly-upload-files-to-amazon-s3-from-your Hope this helps!
563e332c61a8013065267ccc	X	AS for the "postMessage" scenario, maybe the iframe should contain a simple javascript [edit] for iframes taken over by an errormessage IFRAME script Now parent knows the iFrame perfectly well and can track it's status (depending on if it's answering a simple postMessage) PARENT script IF iframe is not responding with it's "code" iFrameTakenOver will be permanently set to false checking that will verify if an error has occured or not.
563e332c61a8013065267ccd	X	Thanks a lot for your thoughts. Even since it's possible to perform anonymous requests to S3 if the bucket is publicly available, your assumption seems true, that multipart uploads are just not made for my case. And that pre-signed URLs are not available for multipart-uploads is too bad. However, I decided to use the AWS IAM mechanism to set a writeonly-policy for the bucket, and to store the credentials for a new, accordingly configurated user in the applet. From security perspective, it should be fine.
563e332c61a8013065267cce	X	@schneck: Facilitating an IAM write-only policy is an excellent alternative indeed, I've focused too much on how you are trying to achieve your goal rather than the actual use case - you could take that even further by Making Requests Using IAM User Temporary Credentials to entirely avoid storing permanent credentials within your applet.
563e332c61a8013065267ccf	X	Isn't the danger with a write-only, public bucket is that someone could spam your bucket using nothing more than curl? Or am I missing something?
563e332c61a8013065267cd0	X	I'm trying to upload a file with the Amazon Java SDK, via multipart upload. The idea is to pass an upload-id to an applet, which puts the file parts into a readonly-bucket. Going this way, I avoid to store AWS credentials in the applet. In my tests, I generate an upload-id with boto (python) and store a file into the bucket. That works well. My Applet gets a "403 Access denied" from the S3, and I have no idea why. Here's my code (which is partially taken from http://docs.amazonwebservices.com/AmazonS3/latest/dev/llJavaUploadFile.html): In the applet debug log, I find this, then: Do you find any obvious failures in the code? Thanks, Stefan
563e332c61a8013065267cd1	X	While your use case is sound and this is an obvious attempt indeed, I don't think the Multipart Upload API has been designed to allow this and you are actually violating a security barrier: The upload ID is merely an identifier to assist the Multipart Upload API in assembling the parts together (i.e. more like a temporary object key) not a dedicated security mechanism (see below). Consequently you still require proper access credentials in place, but since you are calling AmazonS3Client(), which Constructs a new Amazon S3 client that will make anonymous requests to Amazon S3, your request yields a 403 Access denied accordingly. What you are trying to achieve is possible via Uploading Objects Using Pre-Signed URLs, albeit only without the multipart functionality, unfortunately: A pre-signed URL gives you access to the object identified in the URL, provided that the creator of the pre-signed URL has permissions to access that object. That is, if you receive a pre-signed URL to upload an object, you can upload the object only if the creator of the pre-signed URL has the necessary permissions to upload that object. [...] The pre-signed URLs are useful if you want your user/customer to be able upload a specific object [...], but you don't require them to have AWS security credentials or permissions. When you create a pre-signed URL, you must provide your security credentials, specify a bucket name an object key, an HTTP method (PUT of uploading objects) and an expiration date and time. [...] The lenghty quote illustrates, why a system like this likely needs a more complex security design than 'just' handing out an upload ID (as similar as both might appear at first sight). Obviously one would like to be able to use both features together, but this doesn't appear to be available yet.
563e332d61a8013065267cd2	X	Interesting info, but not sure about this, as the access to the files are more file based than credential based. Every download/upload has to be approved by our server permissions policies. I guess I could configure permissions on every resource too, is that the way to go? I will have a look at this approach. Also, the python clients can be several thousands, can this be an issue?
563e332d61a8013065267cd3	X	The IAM Limits documentation don't seem to have any limits to these types of authentication.
563e332d61a8013065267cd4	X	I am exploring this approach, so far so good, I think it might be a better approach than pre-signed URLs, but I still have to figure out a valid permissions approach per-resource and user policies.
563e332d61a8013065267cd5	X	I was aware about this pre-signed URLs, what I dont really fully understand is why I cannot use boto in the client side. With that URL I can use python-requests to handle it, but I have to implement file management/streaming, error checks, and some other things that are already implemented in boto.
563e332d61a8013065267cd6	X	I want to directly upload/download files to Amazon S3 from python clients, running in some users machines. I have a server, that hosts the access Id and Secret keys, as they cannot be in the users side, that can be used to generate a pre-signed url, and that the clients can connect via API to request these pre-signed urls. I have found many examples of JS, but not a single one with python also in the client side (not web based). I have tried to use boto on the client side, but there seems there is no simple way to take advantage of the boto API, but sign the requests with the remote signature. Is there a way I can use boto to handle the transfers from the client side? So far it seems the best way is to build my own client with python-requests, but I think it pretty much sounds to reinvent the wheel. So far I have been able to monkey-patch HmacKeys (from boto.auth, in boto2), so the provider.secret_key is no longer required (and doesn't raise NotReadyToAuthenticate()) and I can override the signing, injecting an API call for remote signing. But this seems very tricky, fragile and difficult to maintain. Is there any other way with boto to achieve this?
563e332d61a8013065267cd7	X	Rather than signing URLs (which is typically used when making calls via web browser), you should generate temporary credentials via the AWS Security Token Service (STS). From your server, issue the GetFederationToken API call to generate temporary credentials: Your Python app would then use these credentials when calling boto. The user will only be allowed to make APIs that you have permitted within your policy, for the time-frame specified.
563e332d61a8013065267cd8	X	Boto3 gives the ability to create a pre-signed URL for any method call: From the Boto3 documentation: generate_presigned_url(ClientMethod, Params=None, ExpiresIn=3600, HttpMethod=None) Generate a presigned url given a client, its method, and arguments Parameters: Returns: The presigned url I just used it to sign a list_buckets() call and it returned a big URL, eg: Pasting it into a browser returned the bucket list in XML.
563e332d61a8013065267cd9	X	we've been using SimpleSavant as ORM for SimpleDB and it works really well but so far I haven't seen a project of that quality for S3 or other services like SQS, will go through the list and check them out, thanks
563e332e61a8013065267cda	X	Please avoid adding signatures to your posts, as per the FAQ - stackoverflow.com/faq#signatures
563e332e61a8013065267cdb	X	Does anyone know of a high-level SDK for interacting with AWS? The SDK provided by Amazon is good and the REST/SOAP API well documented but I often find that I still end up having to write common, high level operations myself. Take for instance, the S3 client, it gives you the ability to put/get/list objects, etc. but it's sadly missing the ability to do high level operations such as create folder, move file to a different folder, etc. You could use tools like Cloud Berry or the Amazon web console to do this sort of things manually, but sometimes you will want to build some automation into your app like periodically backing up some data into a backup folder with time stamp. Cheers, UPDATE: sorry if I left the question a little too open, seeing as AWS covers so many different things, but in particular I'm looking for a high-level library for S3.
563e332e61a8013065267cdc	X	There are some open source projects on CodePlex. The entire list is at http://www.codeplex.com/site/search?query=AWS&ac=8
563e332e61a8013065267cdd	X	The AWS SDK for .NET provides some high-level interfaces for S3. The Amazon.S3.IO namespace contains FileInfo and DirectoryInfo abstractions and the Amazon.S3.Transfer utilities allow for simple upload and download, including for large files.
563e332e61a8013065267cde	X	Depending on what exactly you need you can find CloudBlackbox package of our SecureBlackbox product useful. CloudBlackbox provides an almost-uniform API for accessing different cloud storages, and offers built-in encryption mechanisms. CloudBlackbox offers high-level API for .NET.
563e332e61a8013065267cdf	X	A newer version of AWS sdk implements S3FileInfo and S3DirectoryInfo. You can use it like .net FileInfo and DirectoryInfo. The problem is that it does not support large files.
563e332e61a8013065267ce0	X	I went to the repo, looked around, didn't see anything pertaining to the REST API for Elastic-Beanstalk. Sorry Obi-Wan, your mind has been clouded...lol
563e332e61a8013065267ce1	X	Oh, now I understood (silly: why you just don't use the AWS SDK anyway?)
563e332e61a8013065267ce2	X	Last time I checked the docs, ElasticBeanstalk only had a REST API...
563e332f61a8013065267ce3	X	No, the AWS SDK supports that since 2011. I know because thats what we've already had when I decided to write my Maven Plugin for it (beanstalker.ingenieux.com.br) ;)
563e332f61a8013065267ce4	X	I've just started working with the AWS Java SDK and need to deploy/update an elastic beanstalk application from my application. Currently, I have only found documentation for a REST api that allows the creation of an application version. As with the rest of Amazon's REST api, authentication is needed with a menagerie of params. The docs on ELB don't have specifics on authentication, whereas the docs for S3/EC2 have plenty of explanation. I am specifically asking about the "Signature Version" parameter? Does anybody have an idea of what that would be for the ELB REST api? Could anybody that has successfully worked with the ELB API point in the right direction for authentication? Thanks in advance!
563e332f61a8013065267ce5	X	Use the source, luke. In particular, those versions are bound to a specific version (see this announcement for an idea) All the AWS Services keep a common core. In particular, besides the Beanstalk-API specific part, you need to know the common part of the AWS apis, which are described in this document)
563e332f61a8013065267ce6	X	Thanks for answering sir, Actually I am new to AWS SDK, I need to get book details from its ISBN, to be more simple, I want that when I give ISBN number to service, it should give me detail of book (if present on amazon server). I am not getting the point that which will I have to use either S3, EC2 or DynamoDB. Can you please help me....?
563e332f61a8013065267ce7	X	@azeem S3 is a storage service you can use to store files, EC2 a virtual server environment that could be used for any number of purposes, and DynamoDB is a NoSQL database. If you are interested in learning more about these services, I recommend you read up on their appropriate pages on aws.amazon.com. You can also have a look at the samples included with the AWS SDK for iOS to see examples of how a number of the services could be used in a mobile application.
563e332f61a8013065267ce8	X	@azeem: More specifically, Amazon Web Services is the name of Amazon's infrastructure services. The web service used to look up information about books is called the Product Advertising API and falls under Amazon Associates. As such, since the Advertising API is not part of AWS, the SDK does not have support for it.
563e332f61a8013065267ce9	X	@RyanParman :: Does your answer means that using AWS iOS SDK, I cann't get book information from Amazon server if I have ISBN number of that book?
563e332f61a8013065267cea	X	@BobKinney : thanks for answering and clearing my concepts about AWS. Hope you'll not mind if I ask something more. I am usedto with google api of book family. As it uses only a simple url address and its response is the complete book detail. Is Amazon not offering any url/web service likewise google?
563e332f61a8013065267ceb	X	I am new to AWS iOS SDK. I have installed AWS iOS SDK and also look the sample project given by AWS iOS SDK. But unfortunately I didn't get anything suitable for me. My simple task is to search books with their ISBN number on Amazon server. Can I have sample Xcode project that is requesting "itemlookup" from AWS? OR Can I have sample Xcode project that is making request to get item from AWS? I have spent more then 8 hours on this but didn't find anything. Now atlast I come to SO. I hope that I'll get suitable help from here. Thank you in anticipation.
563e332f61a8013065267cec	X	The AWS SDK for iOS is for accessing Amazon Web Services like S3, EC2, and DynamoDB. What you are looking for is the Advertising API. There are a number of questions on SO that should give you some starting ideas.
563e332f61a8013065267ced	X	After a great discussion and help of Bob Kinney and Ryan Parman. I went to right path and found this on SO. In this question the last answer has a link of Sorceforge sample code of 'amazon advertising api' Sample Code Here Although this code has some errors, which I solved by googling them, but at last I got the desired results. Hope this will help you greatly...
563e332f61a8013065267cee	X	Just trying to understand this because I am trying to do something like this.../user/update?email=new@example.com&userid=123&sig=some_generated_string now couldn't someone hijack some_generated_string and send that as if it were from themself? (I'm sure they can't but maybe you can tell me why?)
563e333061a8013065267cef	X	@jasondavis - yes, someone could sniff that particular request, it is true. But because the signature is going to be different for every single request, sniffing them does no good. At best, all they can do is send the exact same request! They can't generate a malicious request, because to do that, you need the secret API key. And if they try to muck with the data in the request they sniffed, say changing the email to evil@example.com, the signature will no longer match, and the REST server will reject the request when it attempts to validate it.
563e333061a8013065267cf0	X	Isn't this method insecure if the user is making the request from javascript and thus exposing their API key to the whole world? Someone could view source, get the API key and use the same methods to generate a valid request to make call to the server. Or am I missing something.
563e333061a8013065267cf1	X	@zombat (I don't think so, but anyway..) Is there a method to do that when user fills html form? when I say 'user' I mean a registered user, not a developer who sends his request via his locally PHP file where the secret api key is stored. If there isn't, what should I do?
563e333061a8013065267cf2	X	@zombat that is a very good question baris-usakli has asked. How to hide the API key in a javascript web app? For the phone app market that is using the same API, even there how to create a private API for each downloaded phone app and let the API know about it?
563e333061a8013065267cf3	X	This method is simple, but inherently insecure. Someone could easily sniff your API Key and start using it for themselves. If you want your API keys to be private, you don't want to be sending them as part of the request.
563e333061a8013065267cf4	X	Works for Campaign Monitor: campaignmonitor.com/api/getting-started/#authentication
563e333061a8013065267cf5	X	Campaign Monitor will have a lot of explaining to do the first time one of their customers gets their API key hijacked because they were working on an open wireless connection at Starbucks.
563e333061a8013065267cf6	X	Lots of services authenticate in that manner. Campaign Monitor, Freshbooks; even Facebook uses access tokens (although a token can be limited in what it can access). It's a pretty common way to authenticate an API.
563e333061a8013065267cf7	X	Access tokens are a common method, but API Keys are not. Access tokens expire, so it's not as big a security hole if one gets leaked. But if I have your API Key, I can generate as many access tokens as I want. There's a good reason that all these services give you a "secret" key... it's supposed to be a secret. Any service that is having you pass your secret key is doing it wrong.
563e333061a8013065267cf8	X	Does that not then go against REST conventions?
563e333061a8013065267cf9	X	@MartinBean what do suggest?, I dont want to use the REST API to login users, i only want them to have a api key to grant them access to the API when they are logged in
563e333061a8013065267cfa	X	I think Martin Beans comment is only word poking instead of adding anything. His suggested answer looks even worser than mine, although I have not outlined what a session is, zombat describes this way better, check zombat's answer, it has many valid points: stackoverflow.com/a/8567909/367456
563e333061a8013065267cfb	X	It has valid points because RESTful APIs assume the use of a secure server.
563e333161a8013065267cfc	X	Am working with phil sturgeon REST_Controller for codeigniter to create a REST api, so far i've been able to create a simple library for generating api keys for the users. My problem is now sending the api key to the API for each request, how i do this without having to manually send it for every request.
563e333161a8013065267cfd	X	You should look into request signing. A great example is Amazon's S3 REST API. The overview is actually pretty straightforward. The user has two important pieces of information to use your API, a public user id and a private API Key. They send the public id with the request, and use the private key to sign the request. The receiving server looks up the user's key and decides if the signed request is valid. The flow is something like this: This methodology ensures the API key is never sent as part of the communication. Take a look at PHP's hash_hmac() function, it's popular for sending signed requests. Generally you get the user to do something like put all the parameters into an array, sort alphabetically, concatenate into a string and then hash_hmac that string to get the sig. In this example you might do: Then add that $sig onto the REST url as mentioned above.
563e333161a8013065267cfe	X	The idea of REST is that it's stateless—so no sessions or anything. If you want to authenticate, then this is where keys come in, and keys must be passed for every request, and each request authenticates the user (as REST is stateless, did I mention that?). There are various ways you can pass a key. You could pass it as a parameter (i.e. http://example.com/api/resource/id?key=api_key) or you can pass it as part of the HTTP headers. I've seen APIs that specify you send your username, and an API key as the password portion of the HTTP basic access authorization header. An example request: Where martinbean would be my account username on your website, and 4eefab4111b2a would be my API key.
563e333161a8013065267cff	X	You need a session. First the user authenticates via REST. For the rest of the session, the user is then authenticated.
563e333161a8013065267d00	X	Have any of these answers been helpful?
563e333161a8013065267d01	X	@Kylar: Yes, several. Difficult to pick the right one to accept.
563e333161a8013065267d02	X	"An origin server MUST NOT send a validator header field (Section 7.2), such as an ETag or Last-Modified field, in a successful response to PUT unless the request's representation data was saved without any transformation applied to the body (i.e., the resource's new representation data is identical to the representation data received in the PUT request) and the validator field value reflects the new representation." In other words, return an ETag only if the PUT entity exactly matches that which would be returned from a subsequent GET?
563e333161a8013065267d03	X	@dkarp that's correct.
563e333161a8013065267d04	X	I have found neither suggestion nor prohibition to return the representation of a resource in the response to PUT request. Otherwise, the answer is fine.
563e333261a8013065267d05	X	Disagree about 201. Assuming every item has a default ACL, then the PUT correctly returns 200.
563e333261a8013065267d06	X	Both of those are quite wrong. 202 is for continuing processing. 205 is to tell the UI to clear a form.
563e333261a8013065267d07	X	Well, you're correct, maybe I interpret them wrong. However, I continue to see no point in introducing separate logic and differentiate this situation from the normal case when the user removes another user. This can be handled on UI level if it is so important with appropriate warning message.
563e333261a8013065267d08	X	My RESTful service includes a resource representing an item ACL. To update this ACL, a client does a PUT request with the new ACL as its entity. On success, the PUT response entity contains the sanitized, canonical version of the new ACL. In most cases, the HTTP response status code is fairly obvious. 200 on success, 403 if the user isn't permitted to edit the ACL, 400 if the new ACL is malformed, 404 if they try to set an ACL on a nonexistent item, 412 if the If-Match header doesn't match, and the like. There is one case, however, where the correct HTTP status code isn't obvious. What if the authenticated user uses PUT to remove themselves from the ACL? We need to indicate that the request has succeeded but that they no longer have access to the resource. I've considered returning 200 with the new ACL in the PUT entity, but this lacks any indication that they no longer have the ability to GET the resource. I've considered directly returning 403, but this doesn't indicate that the PUT was successful. I've considered returning 303 with the Location pointing back to the same resource (where a subsequent GET will give a 403), but this seems like a misuse of 303 given that the resource hasn't moved. So what's the right REST HTTP status code for "success, and thus you no longer have access"?
563e333261a8013065267d09	X	200 is the appropriate response, because it indicates success (as any 2xx code implies). You may distinguish the user's lack of permission in the response (or, if you don't wish to, 204 is fine). Status codes make no contract that future requests will return the same code: a 200 response to the PUT does not mean a subsequent GET can't return 403. In general, servers should never try to tell clients what will happen if they issue a particular request. HTTP clients should almost always leap before they look and be prepared to handle almost any response code. You should read the updated description of the PUT method in httpbis; it discusses not only the use of 200/204 but indicates on a careful reading that returning a transformed representation in immediate response to the PUT is not appropriate; instead, use an ETag or Last-Modified header to indicate whether the entity the client sent was transformed or not. If it was, the client should issue a subsequent GET rather than expecting the new representation to be sent in response to the PUT, if for no other reason than to update any caches along the way (because the response to a PUT is not cacheable). Section 6.3.1 agrees: the response to a PUT should represent the status of the action, not the resource itself. Note also that, for a new ACL, you MUST return 201, not 200.
563e333261a8013065267d0a	X	You're confusing two semantic ideas, and trying to combine them into a single response code. The first: That you successfully created an ACL at the location that you were attempting to. The correct semantic response (in either a RESTful or non-RESTful scenario) is a 201 Created. From the RFC: "The request has been fulfilled and resulted in a new resource being created." The second: That the user who executed the PUT does not have access to this resource any more. This is a transient idea - what if the ACL is updated, or something changes before the next request? The idea that a user does not have access to a resource of any kind (and this includes an ACL resource) only matters for the scope of that request. Before the next request is executed, something could change. On a single request where a user does not have access to something you should return a 403 Forbidden. Your PUT method should return a 201. If the client is worried about whether it has access any more, it should make a subsequent request to determine it's status.
563e333261a8013065267d0b	X	You might want to take a look at HTTP response code "204 No Content" (http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html), indicating that the "server has fulfilled the request [to be removed from the ACL] but does not need to return an entity-body, and might want to return updated metainformation" (here, as a result of the successful removal). Although you're not allowed to return a message body with 204, you can return entity headers indicating changes to the user's access to the resource. I got the idea from Amazon S3 - they return a 204 on a successful DELETE request (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html), which seems to resemble your situation since by removing yourself from an ACL, you've blocked access to that resource in the future.
563e333261a8013065267d0c	X	Very interesting question :-) This is why I love REST, sometimes it might get you crazy. Reading w3 http status code definitions I would choose (this of course is just my humble opinion) one of those: On the other hand (just popped-up in my mind), why should you introduce a separate logic and differentiate that case and not using 200 ? Is this rest going to be used from some client application that has an UI? And the user of the rest should show a pop-up to the end-user "Are you sure you want to remove yourself from the ACL?" well here the case can be handled if your rest returns 200 and just show a pop-up "Are you sure you want to remove user with name from the ACL?", no need to differentiate the two cases. If this rest will be used for some service-to-service communication(i.e. invoked only from another program) again why should you differentiate the cases here the program wouldn't care which user will be removed from the ACL. Hope that helps.
563e333261a8013065267d0d	X	Best recommendation here. I too second that you employ a module for your httpd. Much easier than coding a full blown abstraction in your app.
563e333261a8013065267d0e	X	I'm having problems when I perform a seek operation. Any clue on how I could implement a seek using this method?
563e333261a8013065267d0f	X	No, it's not. The X-Sendfile header is removed, the file appears to be located at the requested location. Furthermore, x-sendfile can read files that are not normally web-accesible
563e333261a8013065267d10	X	The second solution will work and is secure. It is however not a very good idea to pass large files (video) trough php.
563e333261a8013065267d11	X	What is the best way to password protect quicktime streaming videos using php/.htaccess. They are being streamed using rtsp, but I can use other formats if necessary. I know how to do authentication with php, but I'm not sure how to setup authentication so that will protect the streaming files urls so that a user can't just copy the url and share it. Or am I overthinking this and I can just use a normal authentication scheme and place the files in a protected directory?
563e333261a8013065267d12	X	Both nginx and lighttpd web servers have X-Send-File headers you can return from PHP. So you can do your checks in PHP and then conditionally server out the file. Lighttpd also has a neat module called mod_secure_download that allows you to programatically generate a URL that will only be valid for a short time period. Nginx, and possibly lighttpd, allow you to cap the download speed, so you're not sending out streaming data faster than it can be consumed. Either way, you want to use your web server for serving files. Serving them through PHP is possible, but slow.
563e333261a8013065267d13	X	Try to use Amazon S3 service, it got it's quirks but it makes sense once you get familiar with it. There are hooks in their API to achieve temporally URL's that are active for specified time, so you can freely show url to visitor because it won't work 10 minutes or so later. It's almost trivial thing to do with php (around 15 lines of code), there are a lot of examples on their forums so you dont need to go from scratch and read full documentation on how to achieve this. What kind of authorization you will do before generate and show links it's up to you. You can also have it look like it's served from your domain like video.yourdomain.com instead of standard s3 URL's. Last thing, it's cheap - we payed around 2 US$ for the month of testing and deployment when I uploaded 8 GB and downloaded it 3 times completely and initialized download for around 100 times. The person I was doing this for is so satisfied by price that he wants to move all of his downloadable media to s3. Now, re reading everything I wrote it looks like commercial/spam but I'm so satisfied with service because I coded everything for audio files earlier, and it took days until everything worked just fine and this took couple of hours to implement (mostly getting familiar with service).
563e333361a8013065267d14	X	You might want to take a look at: mod_xsendfile (for apache) It enables you to internally redirect to a file. So you could point your download link to checkCredentials.php This module is also available for other webservers. If I remember correctly, the idea originally comes from lighttpd, but - as Josh states- is also available for nginx.
563e333361a8013065267d15	X	In Jacco's solution is it possible to inspect the headers, and find the url for the file and download it without authentication? A possible solution I can think of would be to put the files somplace that is inaccessible to anyone but the browser, i.e. using a .htaccess deny all. Would that work?
563e333361a8013065267d16	X	Try to get and display the body of the 400 Bad Request response (sorry, not a C# person so I can't tell you specifically how). The 400 response has multiple possible meanings, 2 of which are related to the signature itself, and the others indicate other problems. There should be a body on that response, that gives more information.
563e333361a8013065267d17	X	Thanks for the reply. I have solved it finally by myself. (edited my post and provided the answer)
563e333361a8013065267d18	X	I was actually writing this for windows phone 7 when I got stuck above, which didn't have any SDK till now. But the GA release link seems encouraging as it says 'support for windows phone 8', let me see if this makes my life easier. Nevertheless, thanks for the quick reply and pointers. I am still keeping this question open to see if the above can be fixed somehow
563e333361a8013065267d19	X	I tried with looking at the SDK but it was little too much for me to tweak and/or adopt it. Got my problem solved myself though. Thanks for the help nevertheless.
563e333361a8013065267d1a	X	I am trying follow the example given at AWS documentation Signing AWS requests and make a ListUsers call in C#. I have arrived till the last stage of generating the signature (i.e ready to submit the signed request given at signature-4 request examples). But the code I pasted below is throwing 'bad request' exception when submitted. Output i get is: Can some one help me what wrong I am doing here? Edit: I solved this finally by myself. I had to transform it to below. (answer is for RDS though I believe it's visible what the differences are).
563e333361a8013065267d1b	X	Can some one help me what wrong I am doing here? Maybe, but I strongly suggest to skip this endeavor all together and just use the excellent AWS SDK for .NET for all your AWS API interactions instead, because it indeed helps take the complexity out of coding by providing .NET APIs for many AWS services including Amazon S3, Amazon EC2, DynamoDB and more. But if you really need or want to do it yourself, I suggest to simply take a look at the source code of these very SDKs, which are all available at GitHub, including the one for the AWS SDK for .NET - regarding the issue at hand, you might want to start looking into AWS4Signer.cs for example.
563e333361a8013065267d1c	X	Does ContentProvider pattern lack any required feature ?
563e333361a8013065267d1d	X	I want to develop a application where part of the data is dynamic like picture , show timing etc.Their are many content management system that use HTML5 and CSS but i want to also use the native iOS or Android Ui like the UISplitView for iPad.How is this possible ? whats the best way to manage and use dynamic data ?
563e333361a8013065267d1e	X	I have been digging into this very exact answer. The best answer I can come up with is called parse.com. Which may not be 100% of what you are looking for. However. What it does is serve as a central database that talks to multiple platforms(windows 8, iOS, Android) and offers up an api for use with every platform with lots of documentation to make programming super easy. http://deployd.com/ also This site is something Ill be looking into which uses a simplified node.js desktop for programming easy objective based functions with a database. Definitely am still looking. Either way the bast thing is to call your view...bring in a few objects...and have these databases feed your objects to specifically answer your question. As a developer Im used to Joomla and magento. These arent necessarily ios friendly. Anyway, best of luck.
563e333361a8013065267d1f	X	I'd suggest taking a look at Cloud CMS (http://www.cloudcms.com). Cloud CMS is a cloud content management system that is built around JSON schema. Unlike traditional web content systems, Cloud CMS works with JSON and binary files (either through MongoDB GridFS or Amazon S3). It provides full-text search, structured query and an entire suite of enterprise features for things like workflow, analytics, users and groups and more. From an iOS or Android viewpoint, you really only need to interact with the REST API. You can do that directly or use one of the client libraries. Disclaimer: I'm one of the founders of the company. Would love to find out what you think and learn what we can do to improve things. We're having a great time reinventing CMS for mobile.
563e333461a8013065267d20	X	Stackoverflow isn't a recommendation nor code-handout site, and you haven't given any requirements as to the actual needs and requirements for a storage system (aside from having to be free). If free, maybe try Dropbox.
563e333461a8013065267d21	X	is it possible to access dropbox from java code?????
563e333461a8013065267d22	X	Yes. They even have an official SDK for Java.
563e333461a8013065267d23	X	Am using CloudBees to deploy my Java EE application. In that I need to write and read files and I wont find any cloud file system from CloudBees. Please suggest me any free cloud file system storage and java code to access that file system.
563e333461a8013065267d24	X	Using jclouds you can store stuff in several different clouds while using a consistent API. http://www.jclouds.org/
563e333461a8013065267d25	X	You can store files - however they will be ephemeral and not shared in the cluster. To achieve that, you would need to store in a DB or s3 or similar (there is also an option of webdav).
563e333461a8013065267d26	X	file system on RUN@Cloud is indeed not persistent neither distributed. File stored there will "disappear" when application is redeployed/restarted and will not be replicated if application scale out on multiple nodes in cluster. Best option afaik is to use a storage service (amazon s3 to benefit from low network latency from your RUN instance) using jclouds neutral API (http://www.jclouds.org/documentation/quickstart/aws/), that can be configured to use filsystem storage (http://www.jclouds.org/documentation/quickstart/filesystem/) so that you can test on you own computer, and cache filestore content in temp directory - as defined by System.getProperty("java.io.temp") - to get best performances. This will require a "warm-up" phase for this cache to get populated from filestore, but you'll then get best of both words.
563e333461a8013065267d27	X	Does the IBM DataWorks Data Load API support CSV files as input source?
563e333461a8013065267d28	X	The answer is yes. To accomplish this, you have provide the structure of the file in the request payload. This is explained in the API documentation Creating a Data Load Activity. This an excerpt of the documentation: Within the columns array, specify the columns to provision data from. If Analytics for Hadoop, Amazon S3, or SoftLayer Object Storage is the source, you must specify the columns. If you specify columns, only the columns that you specify are provisioned to the target... The Data Load application included in DataWorks is provided just as an example and assumes the input file has 2 columns, the first being an INTEGER and the second one a VARCHAR. Note: This question was answered on dW Answers by user emalaga.
563e333661a8013065267d29	X	You should do this on the server - it requires user intervention to download it locally, and that just seems hacky and unfriendly.
563e333661a8013065267d2a	X	@Archer thanks for your thoughts. The real scenario is to use Dropbox Chooser so there would be explicit user intervention to "choose" the files from one's own Dropbox. Also, jQueryFileUpload already uses FileReader() to give a preview of files selected by <label for='myComputerFiles'>. Obviously, I don't want to be hacky and unfriendly but it kinda seems to me that handling this client-side is everybody's (Dropbox's/jQueryFileUpload/FileReader()) intention as well as the way of the future.
563e333661a8013065267d2b	X	I understand what you're asking, but I honestly believe this is best done on the server. Reason? You would have to first download the file (which is an indeterminate process), and then upload it to the server. If you pass the URL to the server then it can perform the whole process in 1 action - a download (which is effectively the same as you uploading it). Also, the ability to read local files, which is what FileReader is for, does not mean you should download files to just upload them again. That's bad logic and your users will not appreciate it.
563e333661a8013065267d2c	X	Also, Dropbox Chooser is not meant to be a way to download files. It's meant to be a replacement for downloading file, or uploading them to other servers... ...without having to worry about the complexities of implementing a file browser, authentication, or managing uploads and storage
563e333661a8013065267d2d	X	If there is an API call on S3 that allows you to specify a URL then that would be the most obvious thing to use. If you can't do that then you either need to download the file for the user (onto your server) and then upload the file to S3, or you're back to the original idea of downloading at the client and uploading from there. Either way, the introduction of S3 obviously adds another layer of complication, but I'd initially look at getting a URL from the client and getting that file on my server so I could do anything I wanted after that.
563e333661a8013065267d2e	X	I'd like to use jQueryFileUpload to upload a file that is not on my computer but rather is at an external website so all I have is its URL, e.g., https://dl.dropboxusercontent.com/s/wkfr8d04dhgbd86/onarborLogo64.png. I'm at a total loss on how to do this but I think it involves adding the file data programmatically rather than using the traditional <label for='myComputerFiles'>-based selection of files. If this is correct, what next FileReader()? Any thoughts would be appreciated.
563e333761a8013065267d2f	X	You should do this on the server - it requires user intervention to download it locally, and that just seems hacky and unfriendly. Reason? You would have to first download the file (which is an indeterminate process), and then upload it to the server. If you pass the URL to the server then it can perform the whole process in 1 action - a download (which is effectively the same as you uploading it). Also, the ability to read local files, which is what FileReader is for, does not mean you should download files to just upload them again. That's bad logic and your users will not appreciate it. Also, Dropbox Chooser is not meant to be a way to download files. It's meant to be a replacement for downloading file, or uploading them to other servers... ...without having to worry about the complexities of implementing a file browser, authentication, or managing uploads and storage. Since you're using S3, if there is an API call on S3 that allows you to specify a URL then that would be the most obvious thing to use. If you can't do that then you either need to download the file for the user (onto your server) and then upload the file to S3, or you're back to the original idea of downloading at the client and uploading from there. Either way, the introduction of S3 obviously adds another layer of complication, but I'd initially look at getting a URL from the client and getting that file on my server so I could do anything I wanted after that. This previous question may be of some help in this area... How to upload files directly to Amazon S3 from a remote server?
563e333761a8013065267d30	X	I think you're right about Parse, I have just had some doubts because I can't find any documentation or tutorials related specifically to what I'm doing. Could you possibly point me in the right direction?
563e333761a8013065267d31	X	Yeah, definitely. This is images: parse.com/docs/ios_guide#files/iOS This is how to save objects (such as text): parse.com/docs/ios_guide#objects/iOS This is how to retrieve them intelligently (parse.com/docs/ios_guide#queries/iOS) I really recommend just reading through the parse documentation. It's really well written and you will be able to adapt it to what you are doing.
563e333761a8013065267d32	X	Awesome I really appreciate it! I had looked through the parse documentation before but I couldn't find any of this for some reason. That's exactly what I needed.
563e333761a8013065267d33	X	I have a news application that i am in the process of building, and of course news updates a lot, so I have to constantly update my stories, so I need a backend of some sort that will let me update my stories over the air without updating the actual app. I found Parse.com and they have some awesome stuff, but with the way my app is built I don't think I can use them. I will have to update UIImageviews, UItextviews, and the names of Buttons. With Parse I can only seem to find help regarding the PFQueryTableViewController, which I could use this, but that requires completely recoding and some redesigning of my app to fit into that. So unless there's another way, I guess I will suck it up and get to work. So is there a simpler way to do this, or maybe a better service that works more towards what I'm describing?
563e333761a8013065267d34	X	What you are asking basically is how to do network communication. It sounds like to me you can do what you want with just Parse. You just store the images and text and then call the information from the parse backend when you are loading. From there you can update the UITextViews, button names, and UIImageViews however you want dynamically (using the .text, setTitle, and .image properties and methods respectively). You could also use Amazon S3 for image storage... but the API is less well documented for that. This is probably your best bet, unless you really want to delve in more deeply and learn how to use NSURLConnection or AFNetworking to communicate with a back-end that you build on a django, ruby, etc. server that you host yourself on a server.
563e333761a8013065267d35	X	Thanks for reply.This is what i came across.Can you please share the piece of code implemented in .net.As am new to this am unable to customize this in my solution.
563e333761a8013065267d36	X	The above steps doesn't work for me. I can able to retrieve metadata if the file is in my local system.I need to retrieve the metadata of the file which is in amazon without downloading the file.
563e333861a8013065267d37	X	@user1918612 See suggestion 1 of step 2 above. You do not need to download any files in this case.
563e333861a8013065267d38	X	I would like to read the metadata of a file from the amazon s3. Is there any work around to achieve it. For ex: My image is in amazon s3, I would like to read the 'date taken' metadata property of the image and return it to my application. Thanks in advance
563e333861a8013065267d39	X	If you need to download the 'metadata' of the s3 object itself, you will need to perform a HEAD operation on the file in question. That will return just the header information, which would include any metadata that had been included in the object. Amazon S3 is very specific as to how metadata can be put into the header of an object. Otherwise you will by default get things such as file size, server date, owner name, and few other pieces. If you are trying to dig out metadata information that is actually part of the object itself, like inside the image file, then you are out of luck. You will need to download the entire file first. Not unless you can get what you need from a pre-defined byte range in the object, because then you could perform a GET operation and specify a byte-range that you wish to download. AWS S3 HEAD documentation can be read here: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html
563e333861a8013065267d3a	X	Step 1: Create the code to read metadata from any image file and test it anywhere just to see that the code works. Here is one SO question on how to read metadata and here is another SO question covering the same. Step 2: I see two options: Suggestion 1: Write a small program that runs on your amazon s3 implementing the code from step 1. Call this small program from your application. (Normal client-server implementation) Suggestion 2: Somehow give access to your application to do whataver it needs on the amazon s3 server so that the code from step 1 can reside in your application. (I don't know how specifically to do this, but it should be possible)
563e333861a8013065267d3b	X	I am new to Amazon AWS and Glacier. I am trying to write a WPF Windows-based C# client that uploads my archived backup data to the glacier cloud. However, the API reference don't seem to offer a cancel command. Only upload, download, list. What I'm trying to do is run each upload operation (which can take 1 hour or more with large files) asynchronously using TPL. However I want the upload to be cancellable, which .NET 4.5 would support nicely, but the Amazon API does not. Is there a way to do that anyway? Thanks.
563e333861a8013065267d3c	X	The recommended way to handle your scenario in Amazon Glacier (and Amazon S3 as well btw.) is to Upload archives in parts via Multipart Upload, see Uploading an Archive in Amazon Glacier: Depending on the size of the data you are uploading, Amazon Glacier offers the following options: Uploading Large Archives in Parts (Multipart Upload) — In a single operation, you can upload archives from 1 byte to up to 4 GB in size. However, we encourage Amazon Glacier customers to use Multipart Upload to upload archives greater than 100 MB. [...] [emphasis mine] Upload archives in parts — Using the Multipart upload API you can upload large archives, up to about 40,000 GB (10,000 * 4 GB). Uploading Large Archives in Parts (Multipart Upload) provides the details on the latter, specifically regarding Complete (or Abort) Multipart Upload: After uploading all the archive parts, you use the complete operation. [...] If you abort a multipart upload, you cannot upload any more parts using that multipart upload ID. All storage consumed by any parts associated with the aborted multipart upload is freed. If any part uploads were in-progress, they can still succeed or fail even after you abort. [emphasis mine] So you still can't abort the uploads of parts that are in progress as such, thus the key for the desired user experience and/or network bandwidth reduction is choosing a small enough part size. Depending on your use case you might also want to check into List Multipart Uploads (GET multipart-uploads): This multipart upload operation lists in-progress multipart uploads for the specified vault. An in-progress multipart upload is a multipart upload that has been initiated by an Initiate Multipart Upload (POST multipart-uploads) request, but has not yet been completed or aborted. [...]
563e333861a8013065267d3d	X	Can you reproduce that with regular XHR request? (PDF.js is using that under the hood)
563e333861a8013065267d3e	X	I forced PDFJS.disableWorker to true and now it works
563e333861a8013065267d3f	X	Your solution is not really addresses the issue and forces PDF.js to work in sub-optimal mode. You can reproduce that with regular XHR request in the worker (if redirect is used with CORS). It is reported as a bug for Firefox at bugzilla.mozilla.org/show_bug.cgi?id=1206121
563e333861a8013065267d40	X	I got a website i need to improve. This site contains pdf files unique to each user, and PDF.js library is used to allow users viewing them. Those pdf files are generated automatically by first call by link like httр://website.com/api/client/255/product/90/livePdfPreviewFile which returns Content-Type: application/pdf, then generated file will be cached in Amazon S3 storage - after the next call by the same link it will be downloaded from AS3 by my server and given to client for preview instead of re-generating. So link is passed as GET-parameter to PDF.js script like this: and everything works fine, but I had to reduce server load and simply redirect my server's response directly to file in Amazon S3 storage instead of using my server as gateway for those files. I have configured CORS settings using AmazonS3 API (this is simple configuration without allowed and exposed headers mentioning, but it works with Google Chrome): Then I have changed code responsible for returning pdf file to client so it could redirect to cached file: But there is the problem - those redirections do not work in Firefox, but work fine in other browsers such as Google Chrome! When I pass file to PDF.js located directly in AS3 storage it works for Firefox too, also it works when file is not cached in AS3 and pdf-generating link returns pdf file instead of redirection. I tried to set other CORS parameters, but then I realized that Firefox does not even try to perform any queries to AS3 server. This is what Chrome does. First query: And receiving file which location we have obtained by query above: After that Pdf is shown perfectly in browser. But Firefox does not even try to perform redirection. All this browser does is receiving 302 from my server, after that it does nothing. It doesn't even try to perform any queries to AS3. So this response for a first query is all I've got: And that was the last query Firefox has performed, nothing more in Console and Network tabs of Firebug. Instead PDF.js does this: PDF.js v1.0.1040 (build: 997096f) Message: Unexpected server response (0) while retrieving PDF "http://127.0.0.1:8080/api/client/255/product/90/livePdfPreviewFile". So why does not Firefox even try to perform query to file which location was obtained by redirection? I have debugged javascript of PDF.js, but looks like Firefox throws exception on self.load and handles it as "Unexpected server response" lately:
563e333861a8013065267d41	X	I ran into the same issue. The 302 to another host using CORS only worked correctly on Chrome. The change that allowed it to work on Firefox and IE (Safari so far still doesn't work) was including all the source up front. Before I had After the change Good luck.
563e333961a8013065267d42	X	Thanks Inmyth for your answer. Setting the objects to public works great but I need to restrict access to different users, thats why I was wondering if I could assign users and access rights that would allow users without using Amazon api's to access the data.
563e333961a8013065267d43	X	I amended my answer and I apologized. At first I interpreted your question as providing public access without IAM. But from your comment and the original question, Cloudfront api remains the most feasible way in such scenario.
563e333961a8013065267d44	X	Thanks inmyth, I have a better idea now for my solution
563e333961a8013065267d45	X	I am looking into using AWS Cloudfront to develop an API that third parties can use to access data stored in an S3 bucket. I have looked around alot and can not find any good examples if this is possible in the way I want to achieve it. Basically what I need to be able to do is to provide access to JSON data in the S3 bucket using HTTP requests using signed URL's that are signed using access keys that are generated for each user in Amazon's Identity and Access Management Console. I then assign user specific policy's to each user to define what buckets, folders and object they can access. Is it possible for third parties to access the data in this way without using the cloudfront API's to create signed url's or be AWS account holders? Can I just provide them with the url to the object and their specific access key that I generated so the request will be something like this which they can access through a browser: I have tried this myself but only get an access denied error - signature required, but the only way I can see to get this signature is using the cloudfront api to create the signed url's. Anyone have any ideas if this approach is possible or a better approach to take? Thanks
563e333961a8013065267d46	X	The other alternative to Cloudfront private content would be to set up your own server. This would be fine for other AWS services like database. But since the resource is an S3 object it would be difficult to mask the url unless you are the one serving the request. So unfortunately Cloudfront is still the feasible way for access restriction. Please be noted you would still need to set a server to verify users who would get the signed url.
563e333961a8013065267d47	X	Note that this is for Linux VMs only (Windows doesn't have rsync).
563e333961a8013065267d48	X	I see that you can convert an instance stored instance to be EBS backed as this question shows. However, I want to do the opposite, take an EBS backed instance and convert it to be Instance Store backed. Is this possible? How do I do this?
563e333961a8013065267d49	X	Launch an instance-store instance from an AMI that uses the same kernel used by your EBS-backed AMI. Create an EBS volume from the snapshot underlying the EBS AMI. (Alternatively, launch an instance of the EBS AMI and Stop the instance when it begins booting. Detach the root volume from the instance - this is usually the volume attached to /dev/sda1.) Attach the EBS volume to the instance-store instance. rsync the contents of the EBS volume to the root volume. Create an instance-store AMI using the standard methods. Detach the EBS volume and delete it. If you launched an EBS instance in step 2, terminate it.
563e333961a8013065267d4a	X	You can try this: Good luck!
563e333961a8013065267d4b	X	In my android application i want to store images from my android application to google cloud storage. For that i am referring Mobil Backend starter example.In that they have backend database which is deployed on cloud storage. But i am not getting how i can deploy my own database on cloud. And what are the steps to query cloud database in android app.Please help me to implement this functionality. Can any one suggest me tutorial or link which provide proper guidelines for using and storing DB on google cloud. Thank you.
563e333a61a8013065267d4c	X	If you are writing your own App Engine application that needs to use storage, you have several options in the Google Cloud: The Mobile Backend Starter using the Datastore API, that provides a NoSQL like datastore in the Google Cloud that your App Engine application can interact with via the Datastore API. By default you get a total of 1GB of space in the free tier, after which you have to pay per use for your storage requirements. There is the Cloud Storage API, that allows you to save objects to Google Cloud Storage Service. This service is analogous to Amazon S3 service and you can save your data, classified into groups i.e. buckets. This is a paid service. Refer to https://cloud.google.com/products/cloud-storage/ If you prefer to deal with SQL, you can look at Google Cloud SQL, which gives you a MySQL Instance in the cloud. This is a paid service too. Refer to https://developers.google.com/cloud-sql/ Finally, if you application prefers that you use the Google Drive account of the User itself, then you can look at directly integrating with Google Drive API. Recently Google introduced good updates to their Android Drive API. https://developers.google.com/drive/android/ In all the above cases, when it comes to interacting with the App Engine application, it is advisable that you expose the Data Services via a REST like API in your App Engine application.
563e333a61a8013065267d4d	X	Using google app engine you can setup a local datastore for testing, when you deploy your appengine code it will create the same data store on appengine too [without the data]. Basically if you follow the steps in the link you have mentioned it will setup eclipse and app engine, the app engine service does not run inside your android ecosystem. It can be modelled as a REST based URL server, where you can define endpoints as mentioned here : Java GAE As for tutorials : Default Google Docs! I have a full app here, almost full application! Sample App
563e333a61a8013065267d4e	X	I have a drop-down in my application for which i populate the data using an Ajax call as shown below. This works fine and my Web API URL is "http://example.com/Service.svc/json/getairports" But my worry is security since anyone can view this URL using view source or developer tools. So i wanted to bring in a token and pass it to the service like "http://example.com/Service.svc/json/getairports?token=SECUREKEY" but i wonder how this can solve the problem since secure key also visible in the view source. So my question is how can i keep the secure key invisible in the client side or dynamically passed only when the ajax call is initiated? Just for information, i will be using HTTPS in production so that sniffing over the wire is taken care. Also not that, this service is going to be used only within the same application though Web API service might be hosted on a separate node. Kindly advise if there might be some alternative but simple solution for the above scenario. i am aware of other advanced mechanisms such as OAuth,HMAC, Amazon S3, etc. But i just want to have a simple solution.
563e333a61a8013065267d4f	X	What exactly are you trying to solve? Do you want to prevent a user from calling your API programmatically? If your browser can get something, so can a user with the power of view-source - so that's a fruitless effort. Really though, it sounds like you want to prevent CSRF. This answer should be helpful.
563e333a61a8013065267d50	X	I'm having a bit of trouble understanding your specific question here. Where is this markdown coming from, and what is its purpose? The last step in your workflow is a bit confusing. I'm not really sure what you're trying to do with Fine Uploader, other than upload files. If you're looking to account for a portion of your workflow that occurs after the files are uploaded, then Fine Uploader's job is already done. Please advise with a bit more detail in the context of Fine Uploader.
563e333a61a8013065267d51	X	Hi @RayNicholus - after Fine Uploader is done uploading, I'd like to make a button appear on the result page, that when clicked, grabs all the resulting URLs, formats them in Markdown syntax, and pastes to the clipboard. Then, I would switch to a system that accepts Markdown, and paste all the Markdown-formatted URLs I have on the clipboard.
563e333a61a8013065267d52	X	(Can't seem to make a newline in here, sorry). I'd like to ask what API structure, method or event could be used to accomplish this, if any exists. Does the resulting grid of files, for instance, exist in an array that I can use.
563e333a61a8013065267d53	X	The workflow I'm thinking of is making documentation. Fine Uploader seems like an excellent and easy way to get files uploaded to an S3 bucket, and I just want to try to take away the tedium of having to open every uploaded file to get the URL, for each in a group of say 20 screenshots, and then format it in Markdown.
563e333a61a8013065267d54	X	And by the way, @RayNicholus, thanks for responding.
563e333a61a8013065267d55	X	Much appreciated, @RayNicholus! That points me in the right direction within what you've developed. I'm sorry I cannot upvote you, but I have no "reputation" on here, so...
563e333a61a8013065267d56	X	I just found Fine Uploader today, after having searched for a javascript uploader that will also support posting the file to Amazon S3. I read the documents as much as I could and searched this site, but I don't think there's anything about this specifically. As a user of wikis and Markdown (it's ubiquitous, here, on github, in our internal ERP database and so on), I'd like to be able to easy copy-paste a "syntax complete" string, after a file is uploaded, because that would really make documentation creation easier. The workflow I envision - Then I can paste the result into whatever textarea I want. Something like: For bonus points, I'd like to add an icon to represent the non-image file type, to its left. Something like: I imagine there's a way to do this, with a cursory look at the Events and API methods. But would you be so kind as to point me at events or API methods of interest? Please advise. If this is the wrong place for this and if it needs to be posted at your github, I will do so. Let me know, please. Thank you for your assistance in advance. Kind regards Rick
563e333a61a8013065267d57	X	It sounds like you are simply looking for a way to easily retrieve the url in S3 of each uploaded file. This can be done by having your server return the URL of the file in the response to the upload success POST request sent by Fine Uploader. Fine Uploader will return the response (assumed to be JSON) to your onComplete event handler. For example, say your server returns the following response to an upload success POST: {"url": "http://mys3_url.tdl/path/to/this_is_image_1.png"}. You can access this response in your onComplete event handler like this: At this point, you can so whatever you please with these URLs.
563e333a61a8013065267d58	X	Thanks Phil. I 'll look for this option.
563e333a61a8013065267d59	X	But can i directly play the videos form Amazon S3(direct stream channel) as it supported in YouTube, as i don't want to first download the Video from Amazon and then play it on my server.
563e333a61a8013065267d5a	X	We have an urgent functionality in our project, where multiple users can ask their questions and queries in form of videos files(pre-recorded or record from application) and admin/sub admin can reply them in form of video file(recorded from application). Considering the nature of application, there would be lot of data streaming on server and lot of storage required to kept the video files on server. So we are planning to store and kept the videos files separately on some centralized third-party server like "YouTube". YouTube API allows to store and upload the videos files on their server but that is account specific. Is there any possibility that we can store all our application video files on the YouTube server in one centralized account? I tried with YouTube Data upload API 2.0, but that require an authentication login (Google account) form the end-user who is uploading the video files. But i don't want that my end-users would be know where the files are going on. So, requirement is that i have to upload the videos(without providing the Google account credentials) in one centralized account(that would be kept at some property file). Is there any possible solution for this scenario? If above solution fails, then please provide some alternatives considering the scenario. One solution I found that i store the videos on some cloud server like "Amazon S3". does that would be good alternative? Please help. Regards, Arun
563e333b61a8013065267d5b	X	I have developed an API which allow user to upload upto 20 photos from web browser to our server. User has option of choosing multiple files then user clicks upload. At this time, I am hitting the API one by one, uploading the photos in Amazon S3, and fetching the URL for the photos and displaying in the browser. I have tested this functionality in my local, and it is working absolutely fine. But,users are complaining that if they trying to upload 20 files of 4 to 5 MB each, their chrome browser is crashing with Aw Snap error.  What can be the reasons? I do not think, the production server is having any issue for larger files. I googled about this error, it says this can happen due to issue in server response too. What can be the issues? Do I need to clear browser cache with every upload? We use Angular JS for front end. I suggested the users https://www.wiknix.com/solved-aw-snap-error-in-google-chrome/ https://productforums.google.com/forum/#!topic/chrome/QhPKNnqk_b4 to use steps mentioned in above, but they are still complaining. Can it be server issue? Additional info: If user is trying to upload photo of 1-2MB size each, its working absolutely fine.
563e333b61a8013065267d5c	X	We ended up just switching to mp4 for now. It's compatible on most platforms, and the quality/performance is good.
563e333b61a8013065267d5d	X	It's WAY too late to switch players. We chose Flowplayer because it's open source and the license fees were much more reasonable.
563e333b61a8013065267d5e	X	We have a lot of unique integrations between the Flowplayer API, Google Maps and other aspects of the website. However, we may be able to pull something from Video for Everything and tailor it to our purposes. Thanks for the link!
563e333c61a8013065267d5f	X	While this link may answer the question, it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes.
563e333c61a8013065267d60	X	Explaining why it's sweet and will work well for the OP would improve the quality of your answer.
563e333c61a8013065267d61	X	We've used Flowplayer to create a virtual tour. A plugin was dropped in to deliver the proper player to iPad, iPhone and iPod (http://flowplayer.org/plugins/javascript/ipad.html). However, it's not working on iPhone or iPod. Additionally, the flash player unsuccessfully loads on Android phones. So… We have a tour that works on desktop, laptop and the iPad, but no mobile support. Does anyone know a universal solution to deliver an hmtl5 or js-driven Flowplayer for mobile? I've looked around on Stackoverflow and Flowplayer's forums without finding an answer. I'd like to save whatever time possible as I'd hate to have to troubleshoot for each unique mobile platform and OS version like we did for IE7/8/9. We are using: Unfortunately, I can't share the site as the client doesn't allow contractors to publicly take credit for work done for them. As I'm looking for a general solution, and a not a site-specific work around, don't think it will prevent anyone from answering. That said, let me know if you need more info. Thanks in advance!
563e333c61a8013065267d62	X	Here is what you're looking for: http://www.longtailvideo.com/players/ It detects the browser/device type and serves HTML5 video or flash player.
563e333c61a8013065267d63	X	You Can Go For This
563e333c61a8013065267d64	X	This one is also sweet, and it works with iOS out of the box: http://www.videojs.com/
563e333c61a8013065267d65	X	I am a Vimeo Pro user and they have an option of downloading my videos. My concern so far is once i publish the video download URL in a membership site and a person distributes it over the web, i have no control over it but to delete the video itself. I was told i can generate the download URL through the Vimeo API to come up with expiring download links. But when I tried to get a video download URL through the API playground, it seems it expires after a certain period of time. If I go this way, that means I have to replace my video download URL's in my wordpress site every time a URL expired. Is there a way for me to pull the non-expiring download URL of the video but the users in my Wordpress site see an expiring download link when they click on it - like Amazon S3 download URLs? If there is, what are the codes needed and where would I place the codes in Wordpress core files like functions.php etc? Thanks for your help and instructions from the basic steps.
563e333c61a8013065267d66	X	I can't give wordpress specific examples, but one way might be to create your own wordpress page that generates the download link, and redirects your user to the download link. Then you can control access via the wordpress link
563e333c61a8013065267d67	X	Module Installed : Video, Video.js, Zencoder API Versions Installed :- Zencoder library -2.1.2, Video.js--3.2.0, Video transcoder: Zencoder--1.2 I successfully get the Zencoder API key after creating account in Zencoder. Postback URL was shown as localhost/VideoSample/postback/jobs. I uploaded mp4 video in Video content type and got the following error: "Something went wrong with transcoding big_buck_bunny.mp4. Please check your recent log entries for further debugging." When I visit recent log entries I found the following error:- After a bit of study in Internet I found that Zencoder need some public hosted IP or public server for sending the transcode video. In my case the site is not public as I am working on localhost. Basically after a research I found two options for getting the ob done. Zencoder provides a tool called Zencoder Fetcher to transcode the video free of cost. It needs Ruby and Ruby Gems to installed on windows 7. I downloaded Ruby and RubyuGems and follow this resource for installing material. http://blog.zencoder.com/2011/08/25/fetcher-making-it-even-easier-to-integrate-with-zencoder/ When I give my API key then I got following message.Notification retrieved :0. I gave the url "zencoderfetcher" as mentioned inside "Postback URL for Zencoder" in admin/config/media/video/transcoders. But I get following message after saving the option. "The postback URL cannot be retrieved: php_network_getaddresses: getaddrinfo failed: No such host is known. (0)." Then I provide the url "localhost/zencoder/notifications_handler" in the same place and I get the message again as follows:- "The postback URL cannot be retrieved: missing schema (-1002)." I need transcoding badly in my project. Please let me know if it is possible to merge and work zencoderfetcher with video module. If yes then it would be very kind if you provide any reference or steps for this. Note:- All url has http as prefix.
563e333c61a8013065267d68	X	I already answered this in the Drupal forums, but I'll answer here as well in case anyone else finds this. Essentially all Fetcher does is query the notifications API to get the most recent notifications, and then POSTS those to localhost:3000 (or whatever you set as the local address). This only applies to the notification, and the video must still be uploaded somewhere, so I'm not quite sure what you mean when you say, "Zencoder provides a tool called Zencoder Fetcher to transcode the video free of cost." It sounds like what's happening here is the module is trying to validate the address, so zencoderfetcher as the notification url won't work. There are other projects that do similar things, such as localtunnel that might solve the issue. With localtunnel you get a valid URI to post your notifications to, but it also requires Ruby / RubyGems.
563e333d61a8013065267d69	X	Thanks so much John! I wasn't aware of the user-data option, but that seems like the best approach here. There'll be some hacking, but I think there's enough there for me to work with. Thanks so much!
563e333d61a8013065267d6a	X	Pretty simple question, but I can't find anyone addressing it, or really any mention of the problem. Basically I'm looking to add a small amount of information to a remote ec2 box by associating an environment variable with the box when I'm spinning it up. I've seen some mention of the concept of tags, but I'm looking for something that I can naively check and access from within the instance, and it's not clear if tags provide that functionality. Ideally the interface to add these environment variables would also not be accessible by any external party after the instance has been instantiated. I realize I could achieve a similar effect by setting up a secure database, but that seems overly involved for just trying to add a couple pieces of metadata to the instance. Not looking for a handout, but any link to some documentation on this would be much appreciated. I'm currently using boto (code below), so something that fits into the boto framework would be ideal, but if I have to drop down to amazon's REST api it wouldn't be the end of the world.
563e333d61a8013065267d6b	X	There are several ways to pass information to an Amazon EC2 instance, but not all of them would necessarily meet your requirement for it not being accessible after launch. User Data When launching an Amazon EC2 instance, User Data can be specified. The contents of the User Data is accessible from within the instance by accessing the URL: Your code on the instance could query this URL (which is intercepted by the hypervisor, and viewable only from the instance itself) to access the information. Another use for User Data is that it can execute as a script. The script could set an environment variable that your code can then access. However, the User Data can be viewed via the EC2 Management Console or via a DescribeInstanceAttribute call, so this might not meet your requirement for security. Tags Another option is to use Tags. These are Name-Value pairs associated with an EC2 instance (or other objects within AWS). Tags can be retrieved via a call to 'DescribeTags', but boto has some shortcuts to access them. Tags are a great way to associate information with an instance, and tags can also be used to identify specific instances (eg by environment, project, owner...). However, the values stored in Tags are viewable in the EC2 Management Console and via API calls. Other options
563e333d61a8013065267d6c	X	I'm trying to design my first public API, and I'm trying to learn how REST works with authentication, especially in the context of completely client-side apps using js-frameworks, e.g., angularJS. Say you have a client which is a browser application (i.e., HTML, JS, CSS only) served as static files from something like nginx using a javascript framework to consume a REST service from, e.g. something that requires a secret access key that's used to create a signature for each request to the service, something like Amazon S3. In terms of authentication in this scenario, where you don't have a server-side application, how would the secret access key be handled, i.e., how do you get it, where do you store it, etc.? It would seem like a horrible security situation to serve the key for each request (even if it only happens once to bootstrap the application). And even if you do have a light server-side application--how do you securely inform the client (which still calls the authenticated 3rd party API itself) what the signature should be for every request it could possibly make? I'm very confused by how this is supposed to be designed from either end.
563e333d61a8013065267d6d	X	I was going to write a long answer, but I think @Arjan covered everything in this Stack Overflow post. He and his team rolled their own solution, but he addresses the key concerns in REST authentication. Of course you can use something like OAuth, OpenID, or SAML depending on your situation. Here is a nice comparison of the three approaches. Note the difference between the secret and the key. Also note that tokens need to be sent with each request because REST is stateless.
563e333d61a8013065267d6e	X	I've done a few AngularJS apps and the way that I've found is to use an HttpModule like this one: The most important part is inside CheckPassword method, there is where you should validate the credentials. Another point is this line response.Headers.Add("WWW-Authenticate", string.Format("Basic realm=\"{0}\"", Realm)); if you don't comment this line, the classic login requested form will show up, and if you do comment this line you have to catch the 401 error in your requests. If you want to know about realm: What is the “realm” in basic authentication. Plus, you will need to register the module in your web.config file: Then I've added these two methods to deal with the authentication token: The btoa method: The btoa() method of window object is used to convert a given string to a encoded data (using base-64 encoding) string.. Taken from: http://www.w3resource.com/javascript/client-object-property-method/window-btoa.php. And last I've added the authtoken to the request header using the beforeSend: Please do note using jQuery outside an angular directive is not recommended, AngularJS best practices dictates jQuery code must be always placed inside a directive. Hope it helps.
563e333d61a8013065267d6f	X	Same issues found when sharing through iOS Device
563e333d61a8013065267d70	X	We had the same issue and your fix worked... thanks!
563e333e61a8013065267d71	X	thank you SOOOO much.
563e333e61a8013065267d72	X	So it would add the picture to the users album? That's not what I want. This is just for when a user comments on a picture. It's supposed to show the thumbnail, their comment, a description of the picture, and a link to the picture. Not actually add it to their album.
563e333e61a8013065267d73	X	You should be able to use the same method to upload to your Page or App. For a page upload you will need to generate an access_token for page, and same for app.
563e333e61a8013065267d74	X	REViSE - user must be owner of photo. You will need to upload the photo as the user to your wall photos, "if it is possible". I will try to get a working sample of this going and post back if i am successful.
563e333e61a8013065267d75	X	I don't want any picture uploaded. It worked for a picture on my server that wasn't uploaded, just by providing the image URL. It didn't add the picture to my album (I was the user) and I saw the thumbnail.
563e333e61a8013065267d76	X	I was trying to find this on facebook's site in their documentation but so far no luck. I'm sure others must have run into this before. I use Amazon S3 for storing images. I didn't know ahead of time that if I named my bucket as my domain name with subdomain I could link that way, so until I move all of the pictures I have to link to mybucket.s3.amazonaws.com domain. When I include a picture from there with a post to the wall the picture doesn't show up. If I change the picture to one on the server itself the picture does show up. It seems that the domain name of the picture must match my app? I looked at bugzilla and didn't see this mentioned. Facebook's forum says to post questions here. I'm using the C# Facebook SDK from CodePlex. My code looks like (with error handling and authentication check removed): I verified that imageUrl does indeed have a correct picture, the domain name just doesn't match. The picture on amazon s3 has public read access. I can view it from my browser so I don't think it's a permission problem. I've tried a few different pictures with the same problem. Only time it's worked so far is when the picture was on the server itself. So, my question is, is it a problem with me, or does facebook block images that don't match the domain name specified on the app?
563e333e61a8013065267d77	X	I'am facing the same issue as well. Based on my observations it seems that facebook does not like it when the picture url has more than one sub-domain. I tried the below 2 URL variations for the same image.. mybucket.s3.amazonaws.com - throws an error s3.amazonaws.com/mybucket - works fine Now i have to figure out how to change the URL structure for the image while passing it to the FB graph API.
563e333e61a8013065267d78	X	You can upload the picture from that url, then add its object id in the post. Refer to: http://developers.facebook.com/blog/post/526/?ref=nf Uploading Photos to the Graph API via a URL Earlier this year, we released support for uploading photos directly via the Graph API. This requires sending the photo as a MIME-encoded form field. We are now enhancing our photo upload capability by introducing the ability to upload photos simply by providing a URL to the image. This simplifies photo management for a number of use cases: To upload a photo via a URL, simply issue an HTTP POST to ALBUM_ID/photos with the url field set to the URL of the photo you wish to upload. You need the publish_stream permission to perform this operation. You can also include an optional message parameter to set a caption for the photo.
563e333e61a8013065267d79	X	I would log it as a bug. If this is really the case, which I kinda doubt, you could create a 301 redirect on your own domain for each image that redirects to the Amazon url.
563e333e61a8013065267d7a	X	I have experience with yii2 and angular.js both but separately. I have 2 Questions; is it possible to use angular.js in yii2's view? asking possible instead of feasible because i think problem may arrived at routing. Also is it fair enough(for performance) to use Yii2 and angular.js together?(both are MVC so for modular, manageable code) i searched for long but unable to find any proper resource. can any one please explain! thanks in advance.
563e333e61a8013065267d7b	X	YES you can use angularJs in Yii2 views after implementing a different rooting approach, here is a tutorial to start with. But NO, I don't recommend doing so while both Yii2 and angularJs are great frameworks with native support of REST. So the proper way is to use AngularJs to build your frontend and use Yii2 just to provide server API. Here is a good structure to do so : structure by @AlekseiAkireikin from this stackOverFlow post Yii RESTful API framework will provide a clean api which can communicate with your built-in angularJs app or maybe future mobile app or even providing resources and/or services to other websites or software. If you care about performance then go with both and use REST. A well structured Restful app is great to build easily a good caching system with a flexible strategy behind. You can even host your backend and DB in a server (like amazon EC2) providing only json (and/or xml) data for minimum bandwidth use, and having your frontend stored on an optimized CDN (like amazon S3 or other CDN provider) with lower cost and faster answers. Here is 2 examples implementing AngularJs and Yii2 within REST : this and this.
563e333f61a8013065267d7c	X	After thinking about this overnight I have decided that you are absolutely right. It just doesn't make sense not to use the database. I have also decided that Pyro is a bad fit here and that I should just do what normal people do and use a cron job with a lock file.
563e333f61a8013065267d7d	X	We don't use cron. We have our batch system as a little WSGI server and we make an HTTP request with urllib2 to wake it up. It gets the Request ID from the WSGI request; gets the details with ordinary Django ORM.
563e333f61a8013065267d7e	X	This is sort of what I planned to do with Pyro, but the problem I foresee is that a sudden server outage could leave documents half-processed and there would be no new request message to re-initiate processing. If I use a cron job I know that I can just pick the old 10 unfinished jobs from the Request table and I will pickup any that got cutoff during the outage.
563e333f61a8013065267d7f	X	I suppose I should have phrased that last comment as a question as clearly you have a way of dealing with this problem: what is your strategy?
563e333f61a8013065267d80	X	We don't like frequent crontab polling requests. Too much overhead in the database doing a SELECT every few minutes. The requests are relatively rare, so we use RESTful notification of a WSGI server.
563e333f61a8013065267d81	X	I am working on a Django application which allows a user to upload files. I need to perform some server-side processing on these files before sending them on to Amazon S3. After reading the responses to this question and this blog post I decided that the best manner in which to handle this is to have my view handler invoke a method on Pyro remote object to perform the processing asynchronously and then immediately return an Http 200 to the client. I have this prototyped and it seems to work well, however, I would also like to store the state of the processing so that the client can poll the application to see if the file has been processed and uploaded to S3. I can handle the polling easily enough, but I am not sure where the appropriate location is to store the process state. It needs to be writable by the Pyro process and readable by my polling view. Of course, there are also some data integrity concerns with decoupling state from the database (what happens if the server goes down and all this data is in-memory?). I am to hear how more seasoned web application developers would handle this sort of stateful processing.
563e333f61a8013065267d82	X	We do this by having a "Request" table in the database. When the upload arrives, we create the uploaded File object, and create a Request. We start the background batch processor. We return a 200 "we're working on it" page -- it shows the Requests and their status. Our batch processor uses the Django ORM. When it finishes, it updates the Request object. We can (but don't) send an email notification. Mostly, we just update the status so that the user can log in again and see that processing has completed.   Batch Server Architecture notes. It's a WSGI server that waits on a port for a batch processing request. The request is a REST POST with an ID number; the batch processor looks this up in the database and processes it. The server is started automagically by our REST interface. If it isn't running, we spawn it. This makes a user transaction appear slow, but, oh well. It's not supposed to crash. Also, we have a simple crontab to check that it's running. At most, it will be down for 30 minutes between "are you alive?" checks. We don't have a formal startup script (we run under Apache with mod_wsgi), but we may create a "restart" script that touches the WSGI file and then does a POST to a URL that does a health-check (and starts the batch processor). When the batch server starts, there may be unprocessed requests for which it has never gotten a POST. So, the default startup is to pull ALL work out of the Request queue -- assuming it may have missed something.
563e333f61a8013065267d83	X	I know this is an old question but someone may find my answer useful even after all this time, so here goes. You can of course use database as queue but there are solutions developed exactly for that purpose. AMQP is made just for that. Together with Celery or Carrot and a broker server like RabbitMQ or ZeroMQ. That's what we are using in our latest project and it is working great. For your problem Celery and RabbitMQ seems like a best fit. RabbitMQ provides persistency of your messages, and Celery exposes easy views for polling to check the status of processes run in parallel. You may also be interested in octopy.
563e333f61a8013065267d84	X	So, it's a job queue that you need. For your case, I would absolutely go with the DB to save state, even if those states are short lived. It sounds like that will meet all of your requirements, and isn't terribly difficult to implement since you already have all of the moving parts there, available to you. Keep it simple unless you need something more complex. If you need something more powerful or more sophisticated, I'd look at something like Gearman.
563e333f61a8013065267d85	X	Would you dont mind to review complete file? Its the code you are looking for in it. function isFileViewableImage($filename) { $ext = strtolower(pathinfo($filename, PATHINFO_EXTENSION)); $viewableExtensions = array("jpeg", "jpg", "gif", "png"); return in_array($ext, $viewableExtensions); }
563e334061a8013065267d86	X	My preview works fine but I'am generating a thumbnail on Amazon and get problems if the file extension are in upper case. Therefore I want to upload the image in lowercase. That code is not about the uploaded file (if the files uploads with upper or lower case extension).
563e334061a8013065267d87	X	You'll either need to change the name in S3, or client-side before the file is uploaded to S3.
563e334061a8013065267d88	X	I am using a lambda function (updated my question with Lambda function) to create a thumbnail in another bucket. My lambda function will not work if I upload images with extension in uppercase. My lambda function works if I upload image.jpg but not Image.JPG. Therefore, I want to change the file extension to lowercase before/when uploading.
563e334061a8013065267d89	X	docs.fineuploader.com/branch/master/api/…
563e334061a8013065267d8a	X	I'am using this script https://github.com/FineUploader/server-examples/blob/master/php/s3/s3demo.php. The problem is that if I upload a picture named image.JPG (extension in upper case) I get problems to display the image. I want to change the file extension to lowercase before uploading but can not find where in the code I should add/change it. Where in the code should I add $ext = strtolower(pathinfo(xxx, PATHINFO_EXTENSION)); to get all uploaded extensions saved in lower case? Code from the link. AWS Lambda function http://docs.aws.amazon.com/lambda/latest/dg/walkthrough-s3-events-adminuser-create-test-function-create-function.html Update Tested with But the uploaded file is still in uppercase in my bucket and I get the file extension in upper case in I want to make the file extension to lowe case before I upload. Update 2 I tried this code but the problem persists. If I choose to upload image.JPG is the image saved with the name random432.JPG on Amazon S3, not random432.jpg (with extension in lower case, as I want it to be saved). No errors in Chrome console.
563e334061a8013065267d8b	X	The proper way to change a file name in Fine Uploader would be to use the setName API method inside of a submit event handler. For example, to ensure all file extensions are lower-case:
563e334161a8013065267d8c	X	And is it the same just using return OP.upload(Req, name) instead of the async one? If I still use the 3rd lib of amazons3.
563e334161a8013065267d8d	X	No, it's better to wrap it with future+async as documented by the page (ThreadPools) you mentioned. If you don't use future+async you'd have to reconfigure the default thread pool.
563e334161a8013065267d8e	X	I am using amazon s3 to upload photos as my service. According to http://www.playframework.com/documentation/2.1.1/ThreadPools , the code must be blocking code. "when your code may block include: Using REST/WebService APIs through a 3rd party client library (ie, not using Play’s asynchronous WS API)". "Note that you may be tempted to therefore wrap your blocking code in Futures. This does not make it non blocking, it just means the blocking will happen in a different thread. You still need to make sure that the thread pool that you are using there has enough threads to handle the blocking." But now my code is : So is it equal to the code below? (cause i am using a WebService APIs through a 3rd party client library) Will there be any problems if I still use async methods? I ask that because my server have crushed some times. the dump info is: We can see that resource <0x0000000715dd6038> is locked. On the same time, all the other thread are waiting for this resource. Then the system stucked. Is the problem caused by forcing blocking code running in ascy way?
563e334161a8013065267d8f	X	The way you're wrapping the blocking call is correct, this is not causing the issue (but maybe the 3rd party client library is causing issues). Regarding the S3 communication, I recommend to use a non-blocking/async api, e.g. jclouds has async operations (you then need to convert the java Future to a play Promise), or try to just use play's WS.
563e334161a8013065267d90	X	Thanks. But render stream seams useful but how do I stream data into a blob column in the first place, efficiently. I have seen the To Blob or Not to Blob article. But the question really is how to do it in the first place. I can see several advantages of blobs in DB. One way to backup, one way to vertically scale, one disaster recovery, etc. With S3 or other file stores, you now have two mechanisms for your data.
563e334161a8013065267d91	X	Can you point me to an example of how multipart upload are saved to a blob field by a Rails Controller. I did not find any information on this. For file uploads, it seems that rack middleware can handle multipart but when it gets to a rails controller every thing is in a params hash so either it is uploaded to a file or it is all in memory. In general the problem I have is that there is no Rails ActiveRecord way of moving large data into a blob without sucking every thing into memory first.
563e334161a8013065267d92	X	I have a question about how to efficiently store and retrieve large amounts of data to and from a blob column (data_type :binary). Most examples and code out there show simple assignments but that cannot be efficient for large amounts of data. For instance storing data from a file may be something like this: Clearly this would read the entire file content into memory before saving it to the database. This cannot be the only way you can save blobs. For instance, in Java and in .Net there are ways to stream to and from a blob column so you are not pulling every thing into memory (see Similar Questions to the right). Is there something similar in rails? Or are we limited to only small chunks of data being stored in blobs when it comes to Rails applications.
563e334161a8013065267d93	X	If this is Rails 4 you can use render stream. Here's an example Rails 4 Streaming I would ask though what database you're using, and if it might be better to store the files in a filesystem (Amazon s3, Google Cloud Storage, etc..) as this can greatly affect your ability to manage blobs. Microsoft, for example, has this recommendation: To Blob or Not to Blob Uploading is generally done through forms, all at once or multi-part. Multi-part chunks the data so you can upload larger files with more confidence. The chunks are reassembled and stored in whatever database field (and type) you have defined in your model. Downloads can be streamed. There is a large tendency to hand off upload and streaming to third party cloud storage systems like amazon s3. This drastically reduces the burden on rails. You can also hand off upload duties to your web server. All modern web servers have a way to stream files from a user. Doing this avoids memory issues as only the currently uploading chunk is in memory at any give time. The web server should also be able to notify your app once the upload is completed. For general streaming of output: To add a stream to a template you need to pass the :stream option from within your controller like this: render stream: true. You also need to explicitly close the stream with response.stream.close. Since the method of rendering templates and layouts changes with streaming, it is important to pay attention to loading attributes like title, etc. This needs to be done with content_for not yield. You can explicitly open and close streams using the Live API. For this you need the puma gem. Also be aware that you need a web server that supports streaming. You can configure Unicorn to support streaming.
563e334261a8013065267d94	X	show us each GET command you used, and paste the result here.
563e334261a8013065267d95	X	I'm using s3curl.pl that Amazon supplies. And here are the results of the root directory.
563e334261a8013065267d96	X	Got it, s3-curl.zip
563e334261a8013065267d97	X	What do you mean?
563e334261a8013065267d98	X	For reference: GET Bucket (List Objects) When I do a get request on the root bucket it comes back with test/ and test/subdir/ both 0 bytes. Which is correct, there should be 2 folders up there. When I upload a file to test/subdir/file. The root bucket has an item with the key=test/subdir/file. test/ and test/subdir/ are still 0 bytes. When I do a get request on test/subdir/ it returns nothing. What's going on here? Note: I do not have access to the console.
563e334261a8013065267d99	X	Greg, this might sound confusing at first, but the truth is that there's no such thing as "a folder" in Amazon S3. I'll explain. The data structure of S3 is like a flat list of objects -- not like a tree. When you think you have a "file" called puppy.jpg inside a "folder" called pics, what you actually have is an object which key is pics/puppy.jpg. Note that the / character is not any more special than the . character, or the p characters. You might be thinking, Bruno is nuts, I see folders in the AWS Management Console. True, you see the folders. But they are actually emulated by the GUI. When you create a folder through the AWS Management Console, what it will actually do is create an object which name is the full path of the "folder", with a trailing slash, and 0 bytes. Just like the test/ object (not "folder") and the test/subdir/ object (not "folder") you mention in your question. To actually identify and draw "folders", the AWS Management Console (as well as many other S3 browsing tools) is doing is some API magic with the parameters delimiter and prefix. Now, knowing the fact that there's no such thing as a folder, and that they are emulated through the use of those 0-byte, trailing-/ objects, it should be easy to understand why you see the test/ object as a 0-byte object... The same reasoning would explain why you see nothing when you do a GET on a "folder" -- you are actually downloading a 0-byte object! Finally, as a conclusion, there's no easy way to obtain from S3 the size of "a folder" (they don't exist...). The only way would be for you to list all the objects with that prefix and add their sizes. Or keep an index of your object ("files" and "folders") in some kind of database with more advanced querying capabilities.
563e334261a8013065267d9a	X	I need off help regarding amazon s3 with folders, The problem i get with the Amazon s3 class by undesigned is it doesnt support folders, it will only show you the full file name it gives you these three options out off the array. so as you can see it gives you the options name time size and hash no folders options so i am trying to find a work around. from above as you can see Cocaine VIP_Shufunk_192.mp3 is in the Music folder and their is also a folder Music/dnb/ which contains lots off files. What i am looking to do is find a what just to show files that are within a certain folder. so far ive tried. ok so if i have a folder called Music i can have the following. ok so this will show all my files within music but the problem with this is it shows everything including folders within the music folder. I dont want it to show files that are within a folder within the music folder say Music/Dnb i dont want it to show these files only files within the Music folder not the Music/dnb folder??? i have tried the following. can anyone think off a solution to this??? Thanks
563e334261a8013065267d9b	X	Amazon S3 is a flat file system. There is no such thing as folders. There are simply really long file names with slashes in them. Most S3 tools will visually display these as folders, but that's not how S3 works under the hood. I've never used the Undesigned Amazon S3 class before, so I'm not sure about the specifics of that library. I used CloudFusion's S3 class for a long time until Amazon forked it to create the official PHP SDK (which is what I use now). In the PHP SDK, you can do: That will list all objects in your S3 bucket that have a file name (no real folders, remember?) that begins with Music/dnb/.
563e334261a8013065267d9c	X	
563e334361a8013065267d9d	X	There is no way around that in the Amazon API. You need to filter your amazon request using the prefix as you are and then filter out the 'subfolders' on the client. The best solution actually, is not to try to navigate your S3 storage. Instead you should maintain an 'index' of your files in a proper database (MySql, SimpleDb etc) that you search and query against, and then just retrieve the file from S3 when needed.
563e334361a8013065267d9e	X	So, Here's the work around. To list your directories you do this. This will list all the folders in MYBUCKET/Music. This is a away you get aroung with having a flat file store. (This is written in scala)
563e334361a8013065267d9f	X	I've been asked to create a music streaming website and mobile app for a non-profit organization. The organization wants to upload all their music somewhere. Then users can stream and listen to the music from a website (desktop and mobile), iphone app and android app. Although I've worked with video cloud platforms like brightcove to store and serve videos, I've never worked with audio cloud platforms before to store and serve audio. So my questions are: Do I really need an audio cloud platform? Can I get buy with just a web server and amazon S3? (Assume "light" traffic). What are some popular audio cloud platforms that I should research? Platforms must offer APIs that will let me build client side apps in IOS, Android and Web. Additional I tried uploading MP3s to soundcloud (an audio sharing platform). Then I used their javascript API to play a track This played ok on desktop browsers, but didn't work in android browsers or ios safari browsers. Are there comparable services to Soundcloud that have these issues resolved? Or would I save more time serving music via web server?
563e334761a8013065267da0	X	You could try using brightcove, vimeo, youtube etc... Then on the webpage, you can use css to make your player invisible to humans. Then write your own player controls. that could be a suitable replacement for soundcloud. I posted an answer to your other question, which i'll repeat here because it's relevant Reasons to use audio cloud platforms: you need to transcode your audio files to other formats you want a more robust content delivery network (eg. serve content to a user from a server that's closest to them) pre-built CMS system Reasons to use your own web server
563e334761a8013065267da1	X	I've got a amazon S3 server that is connected to Simple DB. in this server I've got different buckets, now, since i'm limited in space i need to delete some content from this buckets from time to time. This deletion needs to be done for specific buckets and based to date (nothing older then a week), of course the deletion needs to be done in both of the server and the DB, and to run as a scheduled task in the server (the server is Windows server 2008 + SQL 2008 R2) Can anyone suggest a script (Any language will be ok) for doing this task ?
563e334761a8013065267da2	X	For S3 objects, you can use the S3 lifecycle feature to "expire" (delete) the objects when it crosses a specific age: Object Expiration - Amazon Simple Storage Service I am not aware of such a convenient way to do this on Simple DB. You might have to write a periodic script using Simple DB API do delete stuff on interval.
563e334761a8013065267da3	X	Be aware that this configuration WILL NOT work if you specify port 465! (Because it uses different authentication mechanism). 587 Seems to work just fine! .... Just lost 2 hours figuring out that...
563e334861a8013065267da4	X	Thanks for the mention to Mailjet. Since 2014, Mailjet has released a new major version of its platform. The V1 is still working but it's better to start with the V3. The SMTP address is now in-v3.mailjet.com
563e334861a8013065267da5	X	I have moved an application to amazon builded in symfony2 and using swiftmailer for sending emails, I am not able to send emails from the application. So searching around the solution for sending emails. Please let me know if any solutions for sending email SES or configuring SMTP for symfony2.
563e334861a8013065267da6	X	I got mine to work with the following details: And lastly, make sure the send FROM email address is verified.
563e334861a8013065267da7	X	I never played with AWS SES but you can use mailjet to send email. You just need to configure Swiftmailer Transport to use their SMTP and you're done. They also ensure that your email are well sending (ie: not in spam) by providing several technique. You will have to setup some of them. They do not provide example for Swiftmailer, but here a good one for Zend (you will see how easy it is):
563e334861a8013065267da8	X	Amazon SES has an excellent Php API from the official AWS Php SDK: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/service-ses.html As you can see, SES is not used as a SMTP gateway (like Mailjet does) but as a HTTP API (via Guzzle, a Php library for HTTP queries). It's always an important decision to rely or not on Amazon services such as S3/SeS/SNS etc... maybe you don't want to depend on no Amazon technology?
563e334861a8013065267da9	X	Thanks for the reply but I still want to hear about EC2.
563e334861a8013065267daa	X	AppHarbor is hosted on AWS if you're asking for latency or other reasons.
563e334861a8013065267dab	X	I thought AppHarbor was hosted on Azure?
563e334861a8013065267dac	X	I want to move my projects to EC2 and need suggestion if EC2 support my project. Basic Requirement: 1 - MVC3 2 - EntityFramework 3 - SQLServer R2 4 - FullTrust 5 - .NET 3.5/4 and which service should I go for?
563e334961a8013065267dad	X	I would use AppHarbor.com. It has all of the features you mentioned (it's also cloud based) and it's free. https://appharbor.com/ You can also integrate this with Amazon via the Amazon SDK API. (You can acquire this .dll via Nuget Package Manager) - This way you can manage all your files in S3 and deliver javascript via a Content Delivery Network if you wanted.
563e334961a8013065267dae	X	You could also use a BaaS provider that offers API extensions out of the box so you can host your custom logic on their infrastructure (kind of makes them like a PaaS provider as well). CloudMine, for one, offers this. You can write your custom app in JavaScript or any JVM language (such as JRuby or Java). Might save you even more money and provide you what you need. See cloudmine.me/docs/custom-code
563e334961a8013065267daf	X	@MarcW Yes, that's an excellent way to go about it too.
563e334961a8013065267db0	X	Thanks for the answer! Here is another relevant question: stackoverflow.com/questions/12047815/…
563e334961a8013065267db1	X	We are developing an application which requires the client (mobile device) to send files of size 5MB or more to the server component for processing and we would like some advice on the following: Is there any way to combine a Backend-as-a-Service (BaaS) platform with our own data-storage (hosted in our particular case in AWS)? We essentially would prefer if the files from the client are sent directly to our own database in the cloud rather than be stored in the BaaS servers. In other words, we need a BaaS platform or a solution that allows unbundling/bypassing its data-storage feature so that we can use the BaaS only for the rest of its facilities (such as the client authentication, the REST API etc). We have our own servers in EC2 which are needed for the main processing part of the files and only need the BaaS platform for conveniences that will kick-start our application in a short amount of time. Pulling the files from the BaaS platform's own data-storage to the EC2 servers would induce overall latency overhead as well as extra bandwidth cost in most cases.
563e334961a8013065267db2	X	I'd faced a similar dilemma while building my app. In my case, I had to upload and store photos uploaded by users somewhere AND I didn't want to build a backend myself. So, I decided to use Amazon S3 to store the photos uploaded by the user and used SimpleDB as it offered me greater flexibility and ease of use than using a MySQL backend. Now, obviously, SimpleDB is not a Backend-as-a-Service platform but I was looking for the same convenience as you are. So what I'm suggesting is that you use a Backend-as-a-Service platform like Parse (which has an excellent freemium model), CloudMine (another great service but with tight limitations on the freemium model i.e only 500 free users/month) or Kinvey (which markets itself as the first BaaS platform, I don't have much information about it but it's definitely worth a look). And use S3 for your data storage. This way you can use the BaaS for client authentication, the REST API etc as you mentioned and you can continue using S3. All you need to do is create an appropriate naming scheme for your S3 buckets and objects such that you can easily identify which object belongs to which user, this can be done easily using a prefix-based naming scheme (seeing as S3 doesn't offer the ability to create sub-folders in buckets). Now whenever you need to pull some client information you can make a call to your BaaS with the client authenticated details and whenever you need access to your data-storage you can make a call to S3 using the Android SDK provided by AWS to retrieve the objects that belong to that particular user. Seeing as you plan on using EC2 to process those files transferring those files from S3 to EC2 should not cost you any extra bandwidth (I might be wrong here because I haven't looked into it but as far as I can remember transferring data within AWS is free). Do let me know if you have additional questions.
563e334961a8013065267db3	X	I have a Rails app that catalogues recorded music products with metadata & wav files. Previously, my users had the option to send me files via ftp, which i'd monitor with a cron task for new .complete files and then pick it's associated .xml file and a perform metadata import and audio file transfer to S3. I regularly hit capacity limits on the prior FTP so decided to move the user 'dropbox' to S3, with an FTP gateway to allow users to send me their files. Now it's on S3 and due to S3 not storing the object in folders i'm struggling to get my head around how to navigate the bucket, find the .complete files and then perform my imports as usual. Can anyway recommend how to 'scan' a bucket for new .complete files.....read the filename and then pass back to my app so that I can then pick up it's xml, wav and jpg files? The structure of the files in my bucket is like this. As you can see there are two products here. I would need to find both and import their associated xml data and wavs/jpg
563e334961a8013065267db4	X	Though Amazon S3 does not formally have the concept of folders, you can actually simulate folders through the GET Bucket API, using the delimiter and prefix parameters. You'd get a result similar to what you see in the AWS Management Console interface. Using this, you could list the top-level directories, and scan through them. After finding the names of the top-level directories, you could change the parameters and issue a new GET Bucket request, to list the "files" inside the "directory", and check for the existence of the .complete file as well as your .xml and other relevant files. However, there might be a different approach to your problem: did you consider using SQS? You could make the process that receives the uploads post a message to a queue in SQS, say, completed-uploads, with the name of the folder of the upload that just completed. Another process would then consume the queue and process the finished uploads. No need to scan through the directories in S3. Just note that, if you try the SQS approach, you might need to be prepared for the possibility of being notified more than once of a finished upload: SQS guarantees that it will eventually deliver posted messages at least once; you might receive duplicated messages! (you can identify a duplicated message by saving the id of the received message on, say, a consistent database, and checking newly received messages against the same database). Also, remember that, if you use the US Standard Region for S3, then you don't have read-after-write consistency, you have only eventual-consistency, which means that the process receiving messages from SQS might try to GET the object from S3 and get nothing back -- just try again until it sees the object.
563e334961a8013065267db5	X	Thanks jspcal. I'd vote you up, but I don't have the rep yet. I was wondering about Cloudfront. This is for integration with an ecommerce store, so perhaps Cloudfront will provide the programmatic ability to control number of downloads and not just time limit like S3. I had looked at Cloudfront but thought it was just a CDN for caching. Does it provide the api to control number of downloads?
563e334a61a8013065267db6	X	yeah cloudfront gives you access to the # of dl's. it's pretty advanced, fetchapp is probably easier to get started with
563e334a61a8013065267db7	X	Thanks for the info, Cloudfront needs a better marketing page that lists that as that is a clear shortcoming of S3. Glad you answered. I had written them off and I didn't know about fetchapp at all.
563e334a61a8013065267db8	X	Is there a way to control number of downloads of digital content on Amazon S3 or via some middle man software that talks to S3? I already use their timed links, but I would like to control number of downloads also. Any ideas of how to accomplish this using S3 or suggestions about alternative services that could? Thanks!
563e335b61a8013065267db9	X	couple solutions: Amazon CloudFront is a content delivery system that has an api and integrates with Amazon's other web services. that's probably what you want. fetchapp is another service that is very nice... they actually use S3 on the back-end... you could roll your own digital download protector pretty easily with a script as well...
563e335b61a8013065267dba	X	Can you clarify what you mean by secure. Is this paid for content that you don't want anyone to get for free, personal/private info or are there other reasons?
563e335b61a8013065267dbb	X	It could be paid or free content, but definitely don't want anyone who doesn't have the app to have access to the content. Just would like to know how is done on production apps, because I couldn't find any detailed info about it. Mostly the reason is paid for content.
563e335b61a8013065267dbc	X	In order to download and save a file on the external SD card (could be a zip file with some media files or a database file), what is the best practice of doing it? How do I secure this zip file from public access on the web server? How do I access the folder from the app in order to download the file? Is it better to use Amazon S3 cloud storage or Google Cloud Storage and their APIs? How usually the apps on the market secure and access their downloads? Could anybody share sources and knowledge how can I do that, where I can find full documentation or which services I should use? Thank you!
563e335c61a8013065267dbd	X	So I now see that you must compute the signature hash by encrypting with your key. So that answers my first question (I was wrong in my initial assessment). Azure lets you create unlimited number of containers, so I will have a container per customer. The customer will call my web service that will then generate the signature that is unique to the customer. This way any encryption keys, container names, etc. can be protected on my server and not on the client system.
563e335c61a8013065267dbe	X	I will need to allow reads also, for a Restore feature that I have yet to implement. I think I will use a web service to generate the SAS any time the client needs to do an operation on the cloud.
563e335c61a8013065267dbf	X	I am writing a backup service to backup a SQL database and save it in cloud storage. I have actually implemented this already with Amazon S3 but as the application will be distributed to customers, I cannot have the API keys stored with the application. I considered using a web service to provide the keys, but it's not my best option at this time (Because it still leaves the possibility of keys being stolen.) So I was looking into Windows Azure, the Blob Service and saw they have these Shared Access Signatures that can be used to provide access to resources in the cloud. You can also use the signature to put blobs in the cloud. But reading through the MSDN docs, I can't help but think this is an insecure system. Anyone who knows 1. the exact names of containers for my account and 2. how to form the signature will be able to access the objects. You do not need a secret key when using this signature. At least that is my impression reading the docs. So finally to my question. Am I correct in my assessment of the shared access signatures with Azure, if not, why? And can anyone suggest an alternative way of doing what I am trying to accomplish.
563e335c61a8013065267dc0	X	Shared Access Signatures can be scoped at either a specific container or a specific blob. They can then specify what permissions they give (read, write, list blobs), and they can specify how long they're valid. The only way to create a SAS is to have the storage key, but anyone who has the SAS can use it to do what it allows them to. It sounds like you want to allow all your customers to write blobs but not read them? If so, a SAS that only specifies write permissions should do the trick. But I assume you also want to limit (or meter) usage by individually customers? If so, you'll probably need something active on the server (a web service?) that authorizes each use and generates a specific, short-expiry SAS to allow that operation. Then you can track and bill for each use.
563e335c61a8013065267dc1	X	I'm been experimenting with Fine Uploader. I am really interested in the chunking and resume features, but I'm experiencing difficulties putting the files back together server side; What I've found is that I have to allow for a blank file extension on the server side to allow the upload of the chunks, otherwise the upload will fail with unknown file type. It uploads the chunks fine with file names such as "blob" and "blob63" (no file extension) however is does not merge them back at completion of upload. Any help or pointers would be appreciated. And this is the server side script (PHP):
563e335c61a8013065267dc2	X	In order to handle chunked requests, you MUST store each chunk separately in your filesystem. How you name these chunks or where you store them is up to you, but I suggest you name them using the UUID provided by Fine Uploader and append the part number parameter included with each chunked request. After the last chunk has been sent, combine all chunks into one file, with the proper name, and return a standard success response as described in the Fine Uploader documentation. The original name of the file is, by default, passed in a qqfilename parameter with each request. This is also discussed in the docs and the blog. It doesn't look like you've made any attempt to handle chunks server-side. There is a PHP example in the Widen/fine-uploader-server repo that you can use. Also, the documentation has a "server-side" section that explains how to handle chunking in detail. I'm guessing you did not read this. Have a look.) in the Widen/fine-uploader-server repo that you can use. Also, the documentation has a "server-side" section that explains how to handle chunking in detail. I'm guessing you did not read this. Have a look. Note that, starting with Fine Uploader 3.8 (set to release VERY soon) you will be able to delegate all server-side upload handling to Amazon S3, as Fine Uploader will provide tight integration with S3 that sends all of your files directly to your bucket from the browser without you having to worry about constructing a policy document, making REST API calls, handling responses from S3, etc. I mention this as using S3 means that you never have to worry about handling things like chunked requests on your server again.
563e335c61a8013065267dc3	X	Did you ever solve this ?
563e335c61a8013065267dc4	X	I'm implementing a REST service using WCF which will be used to upload very large files. The HTTP headers in this request will communicate information which will be validated prior to allowing the upload to proceed (things like permissions, available disk space, etc). It's possible this validation will fail resulting in an error response. I'd like to do this validation prior to the client sending the body of the request, so it has a chance to detect failure before uploading potentially gigabytes of data. RESTful web services use the HTTP 1.1 Expect: 100-continue in the request to implement this. For example Amazon S3's REST API can validate your key and ACLs in response to an object PUT operation, returning 100 Continue if all is well, indicating you may proceed to send your data. I've rummaged around the WCF documentation and I just can't see a way to accomplish this without doing some pretty low-level hooking into the HTTP request processing pipeline. How would you suggest I solve this problem?
563e335d61a8013065267dc5	X	is that all they have reported? have they given wordpress version, plugin version, browser vendor and version etc?? somethign must have changed, has your plugin been updated? hae they updated wordpress? the video file? have they moved servers/domain?? has anyone else reported this?
563e335d61a8013065267dc6	X	I am having issues with some of our users saying they have upgraded or changed theme and then they can no longer access our api. For instance this user. http://mindfulnessexercises.com/nature-sounds-woodland-bridalway/ See the spinning audio player this is their question. We recently changed our theme to “Headline News,” and found out that the audio shortcode seemed to be not working any more. We are sure that the file from Amazon S3 is working and is in public just like how it’s working using your plugin before. I cannot understand what has changed and why this may no longer be working for them has Wordpress changed anything with the admin-ajax.php file with the new update. With my plugin i am doing the following. PHP Has anything changed i am so stuck on how to bug test this because i can set it up on another host provider and i will work fine no issue at all. Can anyone suggest how i can bug test this, would really really appreciate some help. Thanks
563e335d61a8013065267dc7	X	Thanks for the tip! Just FYI, I think you have a type here. {"Content-Disposition": "attachment"} threw an error but {"Content-Disposition"=> "attachment"} worked properly.
563e335d61a8013065267dc8	X	{"Content-Disposition": "attachment"} is Ruby 1.9 only. Use {"Content-Disposition"=> "attachment"} if you're still on 1.8.
563e335d61a8013065267dc9	X	My application is using Rails 2 backend, Heroku for hosting, Paperclip for file uploads, and Amazon S3 for file storage. Right now users can upload files with paperclip + s3 - this works flawlessly. After upload, an icon appears on their dashboard, linked to the file location (in s3 bucket). When the icon is clicked, the browser opens the file in a new window (for most file types - PDF, MP3, img, etc). Instead of opening, I want the file to be automatically downloaded when the user clicks the file's icon (like Gmail attachments). The solution should be able to work for any file type and cross-browser. Is there a helper to do this in rails, or is javascript needed? I'm really stuck on this one so anything to point me in the right direction would be greatly appreciated. Thanks!
563e335d61a8013065267dca	X	Please try the following: This should tell the Paperclip Gem to set the "Content-Disposition" header to the value "attachment" for newly uploaded files. Note that you have to manually edit the already uploaded file, e.g. with Cyberduck or another FTP Client.
563e335e61a8013065267dcb	X	When you transfer the file, you need to set a Content-Disposition header with a value of attachment; filename=yourfilename.pdf. If it's transfered directly from S3, you'll need to tell S3 to set the Content-Disposition headers as well. Possibly also Content-Type. Note that if you tell S3 to associate a Content-Disposition header, it will always transmit this header. FWIW, here's Amazon's documentation on doing a PUT for an Object: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectPUT.html
563e335e61a8013065267dcc	X	I am planing to develop an VoIP iOS app and use Twilios SDK. I am making the choice to either use LiveCode, Appery.io, PhoneGap or build a native Objective C app. I am going to build the app for iOS, Android and HTML5 so the ideal would be to develope in JavaScript for all platforms, but as I understand the support for WebRTC is laking on the iPhone so the alternativ for iOS is the native twilio SDK. My requirements is: I have seen several Twilio projects that use PhoneGap but none that are using LiveCode. I have already built an iOS VoIP app in Objective C, but I want to be able to release it on several platforms also such as for Android and build a HTML5 app, without redoing everything.
563e335e61a8013065267dcd	X	This isn't really a programming question and should perhaps not be asked here. You can create an external for LiveCode and quickly create an interface using the LiveCode IDE. This is probably a quick and easy way to make a working app. If you're starting with LiveCode but are experienced in Objective-C, creating an external won't be a problem for you. LiveCode doesn't contain native iOS controls, which means that you have to emulate the GUI. If you use PhoneGap, you also will need to compile a plugin for PhoneGap using Objective-C, but you can use a framework, such as JQuery, to get the right GUI. Either way, you will have to compile the SDK and you'll need to be quite profound in Objective-C. LiveCode will meet all your requirements. However, Apple will deny your app if you use PayPal for in-app purchases. You'll have to use Apple's in-app purchasing feature. I believe this is possible in LiveCode now. I'm not sure how easy it is. I'm not sure about file listings either. On iOS, you won't have complete access to all files on the phone. This isn't a LiveCode limation but a limitation of the OS.
563e335e61a8013065267dce	X	I’ve implemented the graph API POST /me/photos in my iOS app and its working fine. Same way I implemented the /me/videos with host graph-video.facebook.com suggested in Facebook documents and this link : [cURL - is there a way for upload a video to facebook without form? I get success response for this too like below but the video is not showing up on my Facebook account. Here is the code I have written : if I use the URL as "/me/videos" like I use /me/photos instead of https://graph-video.facebook.com/me/videos, I get the below error: I tried with both .mp4 and .mov which are in supported videos. I'm sure there is no issue with video because the same video I upload to amazon S3 before posting to FB and I can play the uploaded video. Here is one sample: https://tagg-social-staging.s3.amazonaws.com/uploads/posts/videos/36/post-video.mp4 Note: I'm not posting the video using the above URL, but as multipart /form-data
563e335e61a8013065267dcf	X	I have fixed the posting video issue using the sample code provided here: https://developers.facebook.com/blog/post/2011/08/04/how-to--use-the-graph-api-to-upload-a-video--ios/ I don’t know how long it will work as it’s a deprecated code but I used the method ([FBRequestConnection startWithGraphPath:@"/me/videos") of latest v2.2 by changing API parameters : “source” to “video.mov” for video “message” to “description” for caption Now I have one more issue: I need to tag friends from my app. I use /{photo-id}/tags for tagging a photo which is working fine and I tried the same API to tag a video as I’m not able to get any other API from FB docs. I get the below error for while tagging friends for a video: Is there any API to tag a video from mobile app?
563e335e61a8013065267dd0	X	Have you tried the suggestion in the answer of the first question you link ? stackoverflow.com/a/3871531/428236
563e335e61a8013065267dd1	X	I want to use the Amazon.S3.IO API because it's a more simplier implementation and matches perfecty with existing interfaces.
563e335e61a8013065267dd2	X	Thanks for the answer. May i suggest to think about changing/providing the S3FileStream class, which could use a configurable range to download a file. I think Azure did implement it with the class BlobStream.
563e335e61a8013065267dd3	X	Possible Duplicate: How to upload files to Amazon S3 (official SDK) that are larger than 5 MB (approx)? I try to use the Amazon.S3.IO API. If i write 10mb there is no problem. If i write 21mb i get an exception: The request was aborted: The request was canceled. StackTRace: at System.Net.ConnectStream.CloseInternal(Boolean internalCall, Boolean aborting) at System.Net.ConnectStream.System.Net.ICloseEx.CloseEx(CloseExState closeState) at System.Net.ConnectStream.Dispose(Boolean disposing) at System.IO.Stream.Close() at System.IO.Stream.Dispose() at Amazon.S3.AmazonS3Client.getRequestStreamCallback[T](IAsyncResult result) at Amazon.S3.AmazonS3Client.endOperation[T](IAsyncResult result) at Amazon.S3.AmazonS3Client.EndPutObject(IAsyncResult asyncResult) at Amazon.S3.AmazonS3Client.PutObject(PutObjectRequest request) at Amazon.S3.IO.S3FileStream.Flush(Boolean flushToS3) at Amazon.S3.IO.S3FileStream.Dispose(Boolean disposing) at System.IO.Stream.Close() at System.IO.StreamWriter.Dispose(Boolean disposing) at System.IO.TextWriter.Dispose() at S3FileSystem_Sample.Program.createFile(S3DirectoryInfo rootDirectory, String filename) in c:\Program Files (x86)\AWS SDK for .NET\Samples\S3FileSystem_Sample \S3FileSystem_Sample\Program.cs:line 106 at S3FileSystem_Sample.Program.Main(String[] args) in c:\Program Files (x86)\AWS SDK for .NET\Samples\S3FileSystem_Sample\S3FileSystem_Sample\Program.cs:line 59 at System.AppDomain._nExecuteAssembly(RuntimeAssembly assembly, String[] args) at System.AppDomain.ExecuteAssembly(String assemblyFile, Evidence assemblySecurity, String[] args) at Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly() at System.Threading.ThreadHelper.ThreadStart_Context(Object state) at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx) at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx) at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state) at System.Threading.ThreadHelper.ThreadStart() Any idea i could set the timeout? Click here and here for more related questions.
563e335e61a8013065267dd4	X	Unfortunately the Amazon.S3.IO API does not support setting a timeout but as a Developer on the SDK I will add that feature request to our backlog. Keep in mind the Amazon.S3.IO API is a really easy interface to work with S3 but it is not well suited for large files because it buffers them into memory until you have passed the entire contents of the file to the API and then it writes it to S3. For large files you should use the TransferUtility found in the the Amazon.S3.Transfer namespace.
563e335f61a8013065267dd5	X	what size are the images currently?
563e335f61a8013065267dd6	X	360x480 for youtube, I need to reduce it down to 140x220. So about 50% that I need to reduce in CSS.
563e335f61a8013065267dd7	X	Here's my situation, I'm doing some basic Sinatra application hosted on heroku. The app is requesting the 25 top most popular videos on youtube by youtube RESTful api, but it only shows thumbnail images on the app not the videos (user has to click it to see the video). Also, the app is requesting my 10 most recent photos from my flickr account by flickraw gem, also show that on the app. When I run google chrome pagespeed to test the performance, I get 56/100. It suggests me that I should serve scaled images rather than reduce it by css or html (I got the smallest images I could get but I somehow need to reduce them down a bit to fit my page), which obviously I cannot do that, those files are on youtube and flickr. Should I scale them down on the fly and cache that on Amazon S3? or what would be the best strategies to boost my pagespeed? Right now it takes about 8 seconds to load the entire page.
563e335f61a8013065267dd8	X	The most important factor is how often do you request YouTube and Flickr. You should make the request at a given time interval and cache the result / store it in memory.
563e335f61a8013065267dd9	X	You want to read about share nothing architecture. Your user data needs to move to s3 or other storage mechanism.
563e335f61a8013065267dda	X	@talai : Thanks Talai . what is fast to access by PHP(codeignitor) S3 or EBS? and is there any link to guide how to store images on S3 via Codeignitor.
563e335f61a8013065267ddb	X	I don't know about Codeignitor, but there is a PHP SDK for Amazon AWS that you can use. And in my opinion, EBS is faster than S3. The difference is that you can make the files stored on S3 accessible to anybody and from anywhere (if you want to, you can configure the security settings), but for EBS, you must pass through an EC2 instance.
563e335f61a8013065267ddc	X	Recently My Website shifted on Amazon. Codeignitor folder I have folder name 'UPLOAD'.this folder is used for uploaded images and files. I make AMI image from EC2 Instance. I have setup Auto scaling of Ec2 instances. When my old ec2 instance is failed then automatically new instance is created. But My all data from "UPLOAD" of folder on old ec2 instance has lost. I want to separate "UPLOAD" folder in codeignitor from ec2 instance. So whenever new instance is create it will get UPLOAD folder and its contents without loss. I want to separate this upload folder. so when new instance is create then it will get this data. how to do this. Thanks for Advance. Note . I have used MYSQL on Amazon RDS.
563e335f61a8013065267ddd	X	You can use a shared Elastic Block Storage mounted directory. If you manually configure your stack using the AWS Console, go to the EC2 Service in the console, then go to Elastic Block Storage -> Volumes -> Create Volume. And in your launch configuration you can bind to this storage device. If you are using the command line tool as-create-launch-config to create your launch config, you need the argument --block-device-mapping "key1=value1,key2=value2..." If you are using Cloudformation to provision your stack, refer to this template for guidance. This assumes Codeignitor can be configured to state where its UPLOAD directory is.
563e336061a8013065267dde	X	As said by Mike, you can use EBS, but you can also use Amazon Simple Storage Service (S3) to store your images. This way, whenever an instance starts, it can access all the previously uploaded images from S3. Of course, this means that you must change your code for the upload, to use the AWS API and not the filesystem to store your images to S3.
563e336061a8013065267ddf	X	I could not figure this out. I ended up setting permisions manually using S3Hub, a s3 manager for the mac
563e336061a8013065267de0	X	Is there a way to change the permission of every single file in a S3 bucket using either aws-s3 gem or right_aws gem? I can't find it in the documentation. Do I have to do each file individually? I would like to grant "everyone" view permission.
563e336061a8013065267de1	X	I do not believe these gems are supposed to set permissions, that might be the reason you do not find this feature in the docs. Set your permissions in AWS console or through their API, amazon also has command line tools for setting S3 permissions. The gem aws-s3 (and probably right_aws as well) is for reading and storing files in S3 from ruby. Setting permissions is a bit different discipline.
563e336061a8013065267de2	X	RightAws::S3::Grantee.new(key, "http://acs.amazonaws.com/groups/global/AllUsers", ["READ"], :apply, "AllUsers") works for me.
563e336061a8013065267de3	X	According to the docs, the gem provides an acl= method for S3Object. You can pretty easily iterate through each object in the bucket and programmatically set the acl on each object. http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html#acl%3D-instance_method
563e336061a8013065267de4	X	I have Amazon S3 where all of my files are stored. Currently my users can go to a link where they can stream, but not download, audio and video files. How can I set up a link through either Amazon S3 or perhaps Amazon CloudFront that will allow someone to download an MP3 file or something of that nature? Thanks for any advice!
563e336061a8013065267de5	X	You must set the file's content header to something other than the media type the browser understands. For example: This used to be a big issue if you wanted to have both features (ability to display/view and ability to download) and you used to have to proxy the file download through your EC2 or other annoying ways. Now S3 has it built in: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html You can override values for a set of response headers using the query parameters listed in the following table. These response header values are only sent on a successful request, that is, when status code 200 OK is returned. The set of headers you can override using these parameters is a subset of the headers that Amazon S3 accepts when you create an object. The response headers that you can override for the GET response are Content-Type, Content-Language, Expires, Cache-Control, Content-Disposition, and Content-Encoding. To override these header values in the GET response, you use the request parameters described in the following table. (linke above)
563e336061a8013065267de6	X	check these out http://css-tricks.com/snippets/php/generate-expiring-amazon-s3-link/ http://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html hope it helps :)
563e336061a8013065267de7	X	So value_as_string contains a string representation of the encrypted file? Why can't you just decrypt the string after retrieving it from S3?
563e336161a8013065267de8	X	You're correct I'm using boto, not boto3, but isn't boto3 the recommended library? It looks like that's the Amazon recommended API aws.amazon.com/sdk-for-python
563e336161a8013065267de9	X	Seems like your example relies on getObject being automatically decrypted on the fly... but that's in Java. Is this true for Python's boto3 as well?
563e336161a8013065267dea	X	Have edited the answer to state that no version of Boto supports client-side encryption. There does exist this utility (I have not used it) that wraps Boto, providing the ability to specifcy client-side keys. pypi.python.org/pypi/s3-encryption/0.1.0
563e336161a8013065267deb	X	It's not. Client side encrypted, and I know the key.
563e336161a8013065267dec	X	Did you write the code to encrypt it? If so, it should be easy to write the decryption code.
563e336161a8013065267ded	X	With an unencrypted file, I can do the following: But if the file's encrypted, I need to change something about that. I can't figure out what from reading the docs. What do I change? I know the master symmetric key, which is a string like 30 chars or so long.
563e336161a8013065267dee	X	At this time, no version of Boto supports client supplied keys in it's API. Instead, you could use the AWS SDK. The general process is this: When downloading an object – The client first downloads the encrypted object from Amazon S3 along with the metadata. Using the material description in the metadata, the client first determines which master key to use to decrypt the encrypted data key. Using that master key, the client decrypts the data key and uses it to decrypt the object. http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html Here is an example, in Java, that shows creation of a key, upload of a file that gets encrypted with the client key, and retrieval of the file and decrypted with the client key: Source used/adapted from here: http://aws.amazon.com/articles/2850096021478074 In Python, no AWS "supported" .e.g. Boto option exists. However, there does exist a wrapper library for Boto that provides the ability to encrypt/decrypt using a client side-key is this library: https://pypi.python.org/pypi/s3-encryption/0.1.0
563e336161a8013065267def	X	If this is a file that AWS encrypted for you (by your setting the flag in boto to turn encryption on) then you don't have to do anything extra. S3 transparently encrypts and decrypts the contents. They're encrypted inside of S3, but in the clear in your program. So the same code works either way.
563e336161a8013065267df0	X	If there any specific problem with using urllib and a bunch of threads/greenlets? Or is this a general `what's the best possible solution' question?
563e336161a8013065267df1	X	@Carpetsmoker It seems I asked the wrong question. I have clarified description: "What solution allows me to read chunks from upload content and starts streaming this chunks to internal storages befor user will have uploaded whole file. All solutions that I know wait for a whole content before give management to the wsgi applications/python web server"
563e336161a8013065267df2	X	Thanks! I take note of this solution with twisted. As you wish you may see my own solution with gevents below
563e336161a8013065267df3	X	Thanks for sharing your solution!
563e336161a8013065267df4	X	I am trying to create python intellectual proxy-server that should be able for streaming large request body content from client to the some internal storages (that may be amazon s3, swift, ftp or something like this). Before streaming server should requests some internal API server that determines parameters for uploading to internal storages. The main restriction is that it should be done in one HTTP operation with method PUT. Also it should work asynchronously because there will be a lot of file uploads. What solution allows me to read chunks from upload content and starts streaming this chunks to internal storages befor user will have uploaded whole file? All python web applications that I know wait for a whole content will be received before give management to the wsgi applications/python web server. One of the solutions that I found is tornado fork https://github.com/nephics/tornado . But it is unofficial and tornado developers don't hurry to include it into the main branch. So may be you know some existing solutions for my problem? Tornado? Twisted? gevents?
563e336161a8013065267df5	X	Here's an example of a server that does streaming upload handling written with Twisted: This is a tac file (put it in streamingserver.tac and run twistd -ny streamingserver.tac). Because of the need to use self.channel._path this isn't a completely supported approach. The API overall is pretty clunky as well so this is more an example that it's possible than that it's good. There has long been an intent to make this sort of thing easier (http://tm.tl/288) but it will probably be a long while yet before this is accomplished.
563e336261a8013065267df6	X	It seems I have a solution using gevent library and monkey patching: It works well when I try to upload huge file:
563e336261a8013065267df7	X	Is this mean that Filepicker will copy my files from S3 again or it just creates a Filepicker URL for them? I need the second option :)
563e336261a8013065267df8	X	Yes, second option creates new file and store it under your s3. The response will be similar to store method response. Using GET request filepicker.io/api/file/EmlSqNgR0CcgiKJQ70aV/… return converted file from Filepicker conversion servers. Once you request file with specific parameters it is cached for one month. And the file is hosted on our S3 buckets.
563e336261a8013065267df9	X	I have old files prior filepicker what I now copied to the S3 bucket. Can I access them with the filepicker API to get them cropped? I didn't find any relevant info in the documentation. According to the Stackoverflow threads, it seems I should store them again. Is it right?
563e336261a8013065267dfa	X	To use filepicker.io conversion feature file has to be available via filepicker API. So first store amazon url: https://s3.amazonaws.com/your_own_bucket/ZynOv436QOirPYbJIr3Y_5qYoopVTsixCJJiqSWSE.png Using Filepicker REST API: Sample response: Now you can convert filepicker url https://www.filepicker.io/api/file/EmlSqNgR0CcgiKJQ70aV Using GET request https://www.filepicker.io/api/file/EmlSqNgR0CcgiKJQ70aV/convert?w=200&h=250 Or using POST request to store converted file
563e336261a8013065267dfb	X	will the time to upload files go back down to "5 or 6 seconds" eventually even if you don't restart the app?
563e336261a8013065267dfc	X	No, it doesn't go back. I need to restart the app server
563e336261a8013065267dfd	X	If you're using SSL for S3, you may look into AES-NI if you've already taken other steps to optimizing S3 performance.
563e336261a8013065267dfe	X	@CleversonSchmidt It sounds like you have a resource-leak of some sort
563e336261a8013065267dff	X	@HyperAnthony If AES-NI is the problem, shouldn't happen every time?
563e336261a8013065267e00	X	Thanks a lot! I'm passing an Inputstream to AmazonS3.putObject and setting the content lenght. I will check the TransferManager API and see if it helps.
563e336261a8013065267e01	X	I'm currently working on a server app (JEE) and getting some problems to upload files to AWS S3. I'm using the Java SDK (S3client.putObject) to upload these files. When the server starts, everything happens as expected. Files are generated in the server (EC2 instance) and uploaded to S3 in a few seconds. But after some days, the performance degrades a lot. Files that usually took 5 or 6 seconds to be uploaded need now 10 to 30 minutes (yes, minutes). I profiled the app and the culprit here is the section that does the upload using the AWS Java SDK. Strangely the CPU utilization goes near 100% and stays there for minutes. As this is basically an IO operation, I don't understand why it may need so many CPU cyles to run. Has anyone eve experienced this behavior? Any tips on where to look? PS: file size goes from 1 to 50 MB. Thanks a lot! Updates: The EC2 instance that creates the files and uploads them to S3 is m1.large. I'm using the 1.6.4 AWS SDK version .
563e336261a8013065267e02	X	I can't think of any reason why the SDK code would cause your CPU to go so high. My first guess would be some sort of garbage collection issue. When you upload your data, are you passing in a File object to AmazonS3.putObject, or some sort of stream (including FileInputStream)? Streams can be a little tricky to deal with, since they aren't guaranteed to be repeatable and you have to explicitly provide the Content-Length in the ObjectMetadata as part of your upload, otherwise the SDK has to buffer your upload in memory to calculate the total length. That'd be the very first thing I'd recommend checking out. On a side note.. you should check out the TransferManager API in the SDK. It gives you a nice simple interface to uploading and downloading files to/from Amazon S3, and have several optimizations built in. If that still doesn't turn up a clue, then I'd recommend making a dead simple repro case for this. Write a single class file that simply uploads a random File to the same S3 key, and leave that running for the same duration as your application code. If you're able to reproduce the problem in that simple setup, then we can take a look at the code and help get it debugged, but with all the other variables involved in your full application code, we can't do much more than guess at what could be happening.
563e336361a8013065267e03	X	Why do you have a .json at the end of the URLs?
563e336361a8013065267e04	X	That is the response format. .json tells the server to respond with json, .xml tells the server to respond with xml format. Rather that making it an optional parameter behind the ?. blog.apigee.com/detail/…
563e336361a8013065267e05	X	Never seen content negotiation done on the URL, only in headers. On the URL it means you lose benefits of caching and more.
563e336361a8013065267e06	X	@ScottRoepnack then you should consider the Accept HTTP header.
563e336361a8013065267e07	X	@Oded If you used an Accept header, you'd also have a Vary: Accept, so caching wouldn't be affected. Conneg in extension has been discussed before; I'd agree with Shonzilla's answer there though.
563e336361a8013065267e08	X	A MAC is meant to prove message authencity and protect against tampering with - it has nothing to do with user authentication
563e336361a8013065267e09	X	Added one of examples, how to handle user/client authentication without knowing of "login URL" beforehand
563e336361a8013065267e0a	X	Here is another two nice articles with stateless auth examples for REST services: blog.jdriven.com/2014/10/… technicalrex.com/2015/02/20/…
563e336361a8013065267e0b	X	"since each request can include credentials without impacting a human user" 3-way authentication and OAuth were invented because the thing in the quotes is bad. If you supply credentials with each request without a mechanism on the server to revoke them, that would be unsecure if used w/o SSL.
563e336461a8013065267e0c	X	Whenever there is a concept of users, something has to get passed from client to server to identify which user. An OAuth token can certainly serve as the "credentials" here, instead of an actual user/password combination. Securing the channel with TLS is certainly always a good thing, but that's almost beside the point. Even if you use a cookie, some sort of token still gets sent to the server with every request, just with a cookie header instead of an authentication header.
563e336461a8013065267e0d	X	And if you're not using TLS or OAuth for whatever reason, is sending a user/password every time really worse than sending it only once? If the attacker can obtain the user/password, the attacker can likely also obtain the session cookie.
563e336461a8013065267e0e	X	The difference between a cookie and an authentication header being credentials is that cookies are always associated to a particular domain. This means that when the API receives a cookie, it knows where it came from (was written by the same domain earlier). With a header, you never know and you have to implement specific checks for this. In general I agree, they are both credentials, but I think that passing credentials is not login. Login is the active action of opening the door. In the case of 3-way auth, only the first approval of the client would be login.
563e336461a8013065267e0f	X	I am creating a REST api, closely following apigee suggestions, using nouns not verbs, api version baked into the url, two api paths per collection, GET POST PUT DELETE usage, etc. I am working on the login system, but unsure of the proper REST way to login users. I am not working on security at this point, just the login pattern or flow. (Later we will be adding 2 step oAuth, with an HMAC, etc) Possible Options What is the proper REST style for logging in users?
563e336461a8013065267e10	X	Principled Design of the Modern Web Architecture by Roy T. Fielding and Richard N. Taylor, i.e. sequence of works from all REST terminology came from, contains definition of client-server interaction: All REST interactions are stateless. That is, each request contains all of the information necessary for a connector to understand the request, independent of any requests that may have preceded it. This restriction accomplishes four functions, 1st and 3rd is important in this particular case: And now lets go back to your security case. Every single request should contains all required information, and authorization/authentication is not an exception. How to achieve this? Literally send all required information over wires with every request. One of examples how to archeive this is hash-based message authentication code or HMAC. In practice this means adding a hash code of current message to every request. Hash code calculated by cryptographic hash function in combination with a secret cryptographic key. Cryptographic hash function is either predefined or part of code-on-demand REST conception (for example JavaScript). Secret cryptographic key should be provided by server to client as resource, and client uses it to calculate hash code for every request. There are a lot of examples of HMAC implementations, but I'd like you to pay attention to the following three: If client knows the secret key, then it's ready to operate with resources. Otherwise he will be temporarily redirected (status code 307 Temporary Redirect) to authorize and to get secret key, and then redirected back to the original resource. In this case there is no need to know beforehand (i.e. hardcode somewhere) what the URL to authorize the client is, and it possible to adjust this schema with time. Hope this will helps you to find the proper solution!
563e336461a8013065267e11	X	TL;DR Login for each request is not a required component to implement API security, authentication is. It is hard to answer your question about login without talking about security in general. With some authentication schemes, there's no traditional login. REST does not dictate any security rules, but the most common implementation in practice is OAuth with 3-way authentication (as you've mentioned in your question). There is no log-in per se, at least not with each API request. With 3-way auth, you just use tokens. This scheme gives the user the option to revoke access at any time. Practially all publicly available RESTful APIs I've seen use OAuth to implement this. I just don't think you should frame your problem (and question) in terms of login, but rather think about securing the API in general. For further info on authentication of REST APIs in general, you can look at the following resources:
563e336461a8013065267e12	X	A big part of the REST philosophy is to exploit as many standard features of the HTTP protocol as possible when designing your API. Applying that philosophy to authentication, client and server would utilize standard HTTP authentication features in the API. Login screens are great for human user use cases: visit a login screen, provide user/password, set a cookie, client provides that cookie in all future requests. Humans using web browsers can't be expected to provide a user id and password with each individual HTTP request. But for a REST API, a login screen and session cookies are not strictly necessary, since each request can include credentials without impacting a human user; and if the client does not cooperate at any time, a 401 "unauthorized" response can be given. RFC 2617 describes authentication support in HTTP. TLS (HTTPS) would also be an option, and would allow authentication of the client to the server (and vice versa) in every request by verifying the public key of the other party. Additionally this secures the channel for a bonus. Of course, a keypair exchange prior to communication is necessary to do this. (Note, this is specifically about identifying/authenticating the user with TLS. Securing the channel by using TLS / Diffie-Hellman is always a good idea, even if you don't identify the user by its public key.) An example: suppose that an OAuth token is your complete login credentials. Once the client has the OAuth token, it could be provided as the user id in standard HTTP authentication with each request. The server could verify the token on first use and cache the result of the check with a time-to-live that gets renewed with each request. Any request requiring authentication returns 401 if not provided.
563e336461a8013065267e13	X	I don't get why someone would close this as not constructive?
563e336461a8013065267e14	X	Not sure either. Seems like a useful question, should be improved if anything.
563e336461a8013065267e15	X	But should files be stored in databases?
563e336461a8013065267e16	X	most DBs have very decent blob storage capabilities that can scale up to multiple TBs of data. unless youre doing something really drastic i dont see any reason why not
563e336561a8013065267e17	X	Right, maybe I should reconsider database storage then. So you won't get problems with some thousands of jpg/png images of size 40 KB to a couple of MB? And it is not a bad alternative to file system storage?
563e336561a8013065267e18	X	Storing millions of images is actually one of the scenarios that MS Sql Server was designed against. I'd say most engines will handle it just fine. Every CMS I've met saves images in a database.
563e336561a8013065267e19	X	absolutely no issues for several thousands of files. the DB might be bigger than the size of the files due to BLOB storage overhead (see stackoverflow.com/questions/4659441/…) but nothing significant in absolute terms. also, the large the files, the less the overhead is felt
563e336561a8013065267e1a	X	lucene stores its index separately from the actual files. you could have a lucene index for files stored in a DB as well.
563e336561a8013065267e1b	X	As the title says, what is the prefered way of saving an uploaded file in a Java EE web application? I read some answers on other questions that saving the file to the filesystem of the servlet container is not recommended without further explanation. Some say you should save it to a database (but I doubt that from what I have read earlier) and some say that you should use JCR where the only implementation I can find is Apache JackRabbit, which doesn't seem to be very active? What would be the best option? Are there other than those mentioned? Reasons why you would choose one over the other is appriciated.
563e336561a8013065267e1c	X	Depending on your environment you'll probably want to do one of a few things: Your server is in the cloud. You'll want to use a shared cloud store service such as Amazon S3 (which has a nice API btw) You are hosted on a traditional server. In this case the best practice would be to use a shared NAS, but cloud storage is also an option unless your client has regulatory concerns You are primarily dealing with many small(er) files and you want them to be searchable. For this scenario you'd choose a BLOB database column. If you're handling large files (like video) you'll probably want to look into NAS/cloud storage instead and use the database as just a reference to the NAS/S3 location The reason for these options is because you don't want to sandbox your data to a running instance. This architecture allows for either additional instances of your application to be brought online or for a simple server migration and still have access to the shared data.
563e336561a8013065267e1d	X	JCR, as you've already seen, isnt all that popular. using the filesystem is not a very good idea both from a platform perspective (windows, for example has limits on max file path length, constraints on legal file names, and issues with >~100K files in a directory before it slows down to a crawl) and an architecture perspective - think about clustering your application: if you use any form of local storage you wont be able to cluster easily (as not all files are easily accessible tfrom all nodes), so you need to choose something accessible from all cluster nodes. DB is a good fit for that. some sort of cluster cache (or hadoop) might also be a good fit, depending on the specifics of your problem.
563e336561a8013065267e1e	X	In my opinion this message this question depends on what you want to save. Big files like HD video is are much faster accessed via filesystem. Using a database on the other hand makes it easier because you don't have to know the file are actually saved. Small amount and small files > database Otherwise filesystem Another pro for using filesystem as storage is the ability to implement a full text search framework like apache luscene.
563e336561a8013065267e1f	X	I'm writing a Perl script to transfer files from Amazon S3 to Google Cloud Storage. The files I want to transfer have custom metadata on them, so I'm using the Multipart Upload API (https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload), where I specify the metadata as json in the first part, and the file data in the second part. I'm constructing the upload request with the following code: This is all working absolutely correctly for files with a 'binaryish' Content-type (image/png, for example). But for files with a 'textish' Content-type (text/vtt, application/json etc), the upload fails with the error "You must specify the content type of the destination object". If I hack the Content-type to be 'application/octet-stream', the upload works, although the file is then stored with the wrong Content-type in Google Storage. The upload also works correctly if I change to doing a simple (non-multipart) upload, but of course I'm then not able to upload the metadata. Given that my current process works just fine for most files, I don't really want to have a separate process for text data of uploading it then adding the metadata separately. So, any ideas what I might be doing wrong?
563e336561a8013065267e20	X	Do you have to use AMF for the upload? You certainly can use it to do the upload but neither RemoteObject nor NetConnection dispatches any kind of progress event so the best you could do is have an indeterminate progress bar.
563e336561a8013065267e21	X	Hy! . Thanks ;)
563e336561a8013065267e22	X	I had to tackle a similar problem (uploading single photo from Flex to Django) while working on captionmash.com, maybe it can help you. I was using PyAMF for normal messaging but FileReference class had a built in upload method, so I chose the easy way. Basically system allows you to upload a single file from Flex to Google App Engine, then it uses App Engine's Image API to create thumbnail and also convert image to JPEG, then upload it to S3 bucket. boto library is used for Amazon S3 connection, you can view the whole code of the project here on github. This code is for single file upload only, but you should be able to do multi-file uploads by creating an array of FileReference objects and calling upload method on all of them. The code I'm posting here is a bit cleaned up, if you still have problems you should check the repo out. Client Side (Flex): Server side (Django on App Engine): Urls: Views: UploadService class
563e336561a8013065267e23	X	Is it possible to add a key to s3 with an utf-8 encoded name like "åøæ.jpg"? I'm getting the following error when uploading with boto:
563e336661a8013065267e24	X	From AWS FAQ: A key is a sequence of Unicode characters whose UTF-8 encoding is at most 1024 bytes long. From my experience, use ASCII.
563e336661a8013065267e25	X	@2083: This is a bit of an old question, but if you haven't found the solution, and for everyone else that comes here like me looking for an answer: From the official documentation (http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html): Although you can use any UTF-8 characters in an object key name, the following key naming best practices help ensure maximum compatibility with other applications. Each application may parse special characters differently. The following guidelines help you maximize compliance with DNS, web safe characters, XML parsers, and other APIs. Safe Characters The following character sets are generally safe for use in key names: Alphanumeric characters [0-9a-zA-Z] Special characters !, -, _, ., *, ', (, and ) The following are examples of valid object key names: 4my-organization my.great_photos-2014/jan/myvacation.jpg videos/2014/birthday/video1.wmv However, if what you really want, like me, is a filename that allows UTF-8 characters (note that this can be different from the key name). You have a way to do it! From http://www.bennadel.com/blog/2591-embedding-foreign-characters-in-your-content-disposition-filename-header.htm and http://www.bennadel.com/blog/2696-overriding-content-type-and-content-disposition-headers-in-amazon-s3-pre-signed-urls.htm (Kudos to Ben Nadal) you can do that by making sure that when downloading the file, S3 will override the Content-Disposition header. As I have done it in java, I include here the code, I'm sure you'll be able to easily translate it to Python :) : It should help :)
563e336661a8013065267e26	X	Just to be clear, the first policy you included does not match the second. Are you sure they are the same? Could you include the code you are using in your app? Are you using the transfer manager?
563e336661a8013065267e27	X	@BobKinney I've updated my question to show a simple node.js example to illustrate the problem.
563e336661a8013065267e28	X	@ChrisH, Thanks for your detailed question. I just have a comment and a question. My comment: this line var s3 = new AWS.S3(); should be after setting credentials (in third line), otherwise, errors will happen. And, the question is: What you wrote is for unauthenticated users, how would you do the same for Authenticated ones? Thanks!
563e336661a8013065267e29	X	Thanks but the trailing '/' is not making any difference. The SO question you linked to confirms what I already thought...and suggests that my IAM policy should be working.
563e336661a8013065267e2a	X	I have created an IAM policy to allow Cognito users to write to my S3 bucket, but I would like to restrict them to folders based on their Cognito ID. I've followed Amazon's instructions here and created a policy that looks like this: But when I try to upload using the v2 of the AWS iOS SDK I get an access denied error. If I modify the last path component of the resource to replace ${cognito-identity.amazonaws.com:sub} with the explicit identityId value I am getting from the SDK's AWSCognitoCredentialsProvider it works. My understanding was that these should equate to the same thing. Am I missing something in my policy, or should I be using a different path in my upload request? ** Update ** I originally had this problem in iOS, so tonight I tried doing the same thing in node.js and the result is identical. Here is the simple code I am using in node: And I get the same results that I get with iOS: unless I supply an explicit cognito ID in the IAM policy the API responds with 403. I've stripped my IAM policy down to the very bare minimum. This doesn't work: This does: I don't see what I'm missing here...the only documentation I've been able to find always shows the same example Resource value that I've been using.
563e336661a8013065267e2b	X	Unfortunately there is currently an issue with the roles generated via the Cognito console in combination with policy variables. Please update your roles' access policy to include the following to ensure policy variables are evaluated correctly: 2014-09-16 Update: We have updated the Amazon Cognito console to correct this issue for new roles created via the Identity Pool creation wizard. Existing roles will still need to make the modification noted above.
563e336661a8013065267e2c	X	You are missing last slash. Also try to consider this article.
563e336661a8013065267e2d	X	thank you for the reply , but this gives me a runtime exception since file might not have saved in s3 bucket ,
563e336661a8013065267e2e	X	exception AmazonServiceException { RequestId:E128351E5CB99880, ErrorCode:NoSuchKey, Message:The specified key does not exist. }
563e336661a8013065267e2f	X	so handle this exception; this indicates that the S3 file doesn't exits.
563e336661a8013065267e30	X	i thought handling exception anyway thanks
563e336761a8013065267e31	X	Thanks for the links @Naveen. Even thought 1.7.1 is depreciated but sometime old apps need it. That links helped.
563e336761a8013065267e32	X	i have integrated aws v1 sdk to in my ios application to upload videos in to S3 bucket in background mode using NSURLSession But now i want to check file availability in bucket before start uploading for that , i managed to get link to V2 sdk How can I check the existence of a key/file on an Amazon S3 Bucket using AWS iOS SDK v2? what is the link used in V1 ??
563e336761a8013065267e33	X	AWS SDK for iOS is depreciated now; so I believe the documentation link also must have been taken out. Version 1 of the AWS Mobile SDK is deprecated as of September 29, 2014 and will continue to be available until December 31, 2014. If you are building new apps, we recommend you use Version 2. If you are working on existing apps that use Version 1 (1.7.x or lower) of the AWS Mobile SDK, you can download v1 for Android here and iOS here. The API reference guides are included in the respective downloads. Apps built using Version 1 will continue to function after December 31, 2014. However, we highly recommend that you update your apps to the latest version so you can take advantage of the latest features and bug fixes. Source : http://aws.amazon.com/mobile/sdk/ I managed to find a sample code from AWS Mobile Blog [http://mobile.awsblog.com/post/Tx15F6J3B8B4YKK/Creating-Mobile-Apps-with-Dynamic-Content-Stored-in-Amazon-S3] to get the S3 object, you can extrapolate from there. Download Link for v1 iOS SDK : http://sdk-for-ios.amazonwebservices.com/aws-ios-sdk-1.7.1.zip
563e336761a8013065267e34	X	I'm trying to make a HTTP get request to https://elasticbeanstalk.us-east-1.amazonaws.com/?ApplicationName=MyApplicationName&Operation=DescribeEnvironments and getting I've tried setting my key and secret as username and password for basic HTTP auth, but clearly this doesn't work. So how do I add my key and secret to my remote request?
563e336761a8013065267e35	X	For most AWS usage scenarios it is highly recommended to use one of the many AWS SDKs to ease working with the APIs via higher level abstractions - these SDKs also take care of the required and slightly complex request signing, an explanation for the usually several options how to provide your AWS credentials can be found in the resp. SDK documentation: The AWS SDKs provide functions that wrap an API and take care of many of the connection details, such as calculating signatures, handling request retries, and error handling. The SDKs also contain sample code, tutorials, and other resources to help you get started writing applications that call AWS. Calling the wrapper functions in an SDK can greatly simplify the process of writing an AWS application. If you really have a need to use the AWS APIs via REST directly, Signing AWS API Requests will guide you through the required steps, see e.g. section Components of an AWS Signature 4 Request within Signature Version 4 Signing Process for the one that applies to AWS Elastic Beanstalk.
563e336761a8013065267e36	X	alestic.com/2012/01/ec2-ebs-boot-recommended
563e336761a8013065267e37	X	IMHO, this should be reopened, and then moved over to Server Fault.
563e336761a8013065267e38	X	A question closed as "not constructive" has 144 upvotes (as of Mar 30 2013)? Shouldn't it be reopened?
563e336761a8013065267e39	X	IMO this is a very constructive question and one of the first questions commonly asked when first learning about AWS -- look at the number of views.
563e336761a8013065267e3a	X	Where's the button to call out the mods as "non-constructive" ?
563e336761a8013065267e3b	X	Yes, the above were my thoughts as well... Hopefully somehow here writes about their preferences for instance-store as a comparison...
563e336861a8013065267e3c	X	@HelloWorldy: The comparison is really "An instance store can't do..." and list the things an EBS store can. There's no real benefit other than possibly a small cost savings (that can be offset by the convenience of stopping/starting EBS backed instances).
563e336861a8013065267e3d	X	Instance store backed EC2 can also be set to not accidentally terminate.
563e336861a8013065267e3e	X	I'm actually switching most of my EBS backed EC2 instances to using instance stores. It really depends on what you want to achieve. I'm switching because of better IO and because I view each EC2 instance as disposable at all moments, or: it will break down any minute and I will lose everything that's on such an instance. Architecting that way helps to get a real HA system. See also stu.mp/2011/04/the-cloud-is-not-a-silver-bullet.html
563e336861a8013065267e3f	X	@Jim: At least when I wrote the answer a year ago, you got much better IO by striping a number of EBS instances into a software RAID configuration than using instance storage. It's also much faster to launch a replacement instance from EBS backing than from S3 backing (instance storage is loaded from S3, which can be slow). I have not done much on AWS the last 6 months or so; things may have changed.
563e336861a8013065267e40	X	Is there any significant improvement of IO performance with EBS IOPS-kind of volumes compared to standard? Supposing, the above said holds for EBS IOPS volumes, as well.
563e336861a8013065267e41	X	Both technologies evolve. I'm wirting this comment in 2014, when I have "Provisioned IOPS" EBS, but - the "instance store" is now SSD, which is even faster than before!! Ephemeral storage will always win in terms of speed. So I use both - keep the "persistent" stuff on EBS, having all the temp files, logs, "TempDB" database, swap-file and other stuff on Instance-store. BENEFIT FROM BOTH!
563e336861a8013065267e42	X	What if you needed a distributed database which needs to store its data in a distributed and persistent manner. Wouldn't you need EBS because instance storage is not persistent?
563e336861a8013065267e43	X	@CMCDragonkai Of course you do. There are a lot of options these days, e.g. AWS started offering SSD-based storage. I would look into those and re-do the analysis (single vs. RAID, etc.). I would also look into getting the biggest instances possible because of network throughput. EBS is still an issue on instances like t1.micro.
563e336861a8013065267e44	X	Netflix makes the same recommendations as well.
563e336861a8013065267e45	X	So where do you store your block based persistent files?
563e336861a8013065267e46	X	S3 has in-built redundancy. EBS has none, so you'll need to deploy redundancy software on top of it.
563e336861a8013065267e47	X	I'm unclear as to what benefits I get from EBS vs. instance-store for my instances on Amazon EC2. If anything, it seems that EBS is way more useful (stop, start, persist + better speed) at relatively little difference in cost...? Also, is there any metric as to whether more people are using EBS now that it's available, considering it is still relatively new?
563e336961a8013065267e48	X	The bottom line is you should almost always use EBS backed instances. Here's why I'm a heavy user of Amazon and switched all of my instances to EBS backed storage as soon as the technology came out of beta. I've been very happy with the result. Keep in mind that any piece of cloud-based infrastructure can fail at any time. Plan your infrastructure accordingly. While EBS-backed instances provide certain level of durability compared to ephemeral storage instances, they can and do fail. Have an AMI from which you can launch new instances as needed in any availability zone, back up your important data (e.g. databases), and if your budget allows it, run multiple instances of servers for load balancing and redundancy (ideally in multiple availability zones).
563e336961a8013065267e49	X	99% of our AWS setup is recyclable. So for me it doesn't really matter if I terminate an instance -- nothing is lost ever. E.g. my application is automatically deployed on an instance from SVN, our logs are written to a central syslog server. The only benefit of instance storage that I see are cost-savings. Otherwise EBS-backed instances win. Eric mentioned all the advantages. [2012-07-16] I would phrase this answer a lot different today. I haven't had any good experience with EBS-backed instances in the past year or so. The last downtimes on AWS pretty much wrecked EBS as well. I am guessing that a service like RDS uses some kind of EBS as well and that seems to work for the most part. On the instances we manage ourselves, we have got rid off EBS where possible. Getting rid to an extend where we moved a database cluster back to iron (= real hardware). The only remaining piece in our infrastructure is a DB server where we stripe multiple EBS volumes into a software RAID and backup twice a day. Whatever would be lost in between backups, we can live with. EBS is a somewhat flakey technology since it's essentially a network volume: a volume attached to your server from remote. I am not negating the work done with it – it is an amazing product since essentially unlimited persistent storage is just an API call away. But it's hardly fit for scenarios where I/O performance is key. And in addition to how network storage behaves, all network is shared on EC2 instances. The smaller an instance (e.g. t1.micro, m1.small) the worse it gets because your network interfaces on the actual host system are shared among multiple VMs (= your EC2 instance) which run on top of it. The larger instance you get, the better it gets of course. Better here means within reason. When persistence is required, I would always advice people to use something like S3 to centralize between instances. S3 is a very stable service. Then automate your instance setup to a point where you can boot a new server and it gets ready by itself. Then there is no need to have network storage which lives longer than the instance. So all in all, I see no benefit to EBS-backed instances what so ever. I rather add a minute to bootstrap, then run with a potential SPOF.
563e336961a8013065267e4a	X	We like instance-store. It forces us to make our instances completely recyclable, and we can easily automate the process of building a server from scratch on a given AMI. This also means we can easily swap out AMIs. Also, EBS still has performance problems from time to time.
563e336961a8013065267e4b	X	Eric pretty much nailed it. We (Bitnami) are a popular provider of free AMIs for popular applications and development frameworks (PHP, Joomla, Drupal, you get the idea). I can tell you that EBS-backed AMIs are significantly more popular than S3-backed. In general I think s3-backed instances are used for distributed, time-limited jobs (for example, large scale processing of data) where if one machine fails, another one is simply spinned up. EBS-backed AMIS tend to be used for 'traditional' server tasks, such as web or database servers that keep state locally and thus require the data to be available in the case of crashing. One aspect I did not see mentioned is the fact that you can take snapshots of an EBS-backed instance while running, effectively allowing you to have very cost-effective backups of your infrastructure (the snapshots are block-based and incremental)
563e336961a8013065267e4c	X	I've had the exact same experience as Eric at my last position. Now in my new job, I'm going through the same process I performed at my last job... rebuilding all their AMIs for EBS backed instances - and possibly as 32bit machines (cheaper - but can't use same AMI on 32 and 64 machines). EBS backed instances launch quickly enough that you can begin to make use of the Amazon AutoScaling API which lets you use CloudWatch metrics to trigger the launch of additional instances and register them to the ELB (Elastic Load Balancer), and also to shut them down when no longer required. This kind of dynamic autoscaling is what AWS is all about - where the real savings in IT infrastructure can come into play. It's pretty much impossible to do autoscaling right with the old s3 "InstanceStore"-backed instances.
563e336961a8013065267e4d	X	I'm just starting to use EC2 myself so not an expert, but Amazon's own documentation says: we recommend that you use the local instance store for temporary data and, for data requiring a higher level of durability, we recommend using Amazon EBS volumes or backing up the data to Amazon S3. Emphasis mine. I do more data analysis than web hosting, so persistence doesn't matter as much to me as it might for a web site. Given the distinction made by Amazon itself, I wouldn't assume that EBS is right for everyone. I'll try to remember to weigh in again after I've used both.
563e336a61a8013065267e4e	X	I am writing a REST API and would like to implement an authentication system similar to AWS. http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html Basicly, on AWS the client encrypts the Authorization header with some request data using a secret key that is shared between client and server. (Authorization: AWS user: ) The server uses the key to decrypt the header using the shared key and compare to the request data. If successful, this means the client is legit (or at least is in possession of a legitimate key). The next step can be to execute the request or, preferrably, send the client a unique, time-based token (ex.: 30 minutes) that will be used on the actual request (added to a Token header, for example). This token cannot be decrypted by the client (uses a server-only key). On next requests, the server checks the token (not Authorization anymore) and authorizes the request to be executed. However, is it possible to have a man-in-the-middle, even on SSL-encrypted connections, that replays these token-authenticated requests? Even if the MITM does not know what's inside the message, he/she could cause damage for example by ordering a product many times. If the server receives a replayed message and the token is still within the valid timestamp, the server will assume this is a valid request and execute it. AWS tries to solve this with a timestamp requirement: A valid time stamp (using either the HTTP Date header or an x-amz-date alternative) is mandatory for authenticated requests. Furthermore, the client timestamp included with an authenticated request must be within 15 minutes of the Amazon S3 system time when the request is received. If not, the request will fail with the RequestTimeTooSkewed error code. The intention of these restrictions is to limit the possibility that intercepted requests could be replayed by an adversary. For stronger protection against eavesdropping, use the HTTPS transport for authenticated requests. However, 15 minutes is still enough for requests to be replayed, isn't it? What can be done to prevent replay attacks in this scenario? Or am I overthinking and a certain degree of uncertainty is acceptable if you provide enough mechanisms? I am thinking about requiring the client to add a unique string on each request body. This string will be transport-encrypted and unavailable to MITM for modification. On first receipt, the server will record this string and reject any new requests that contain the same string in the same context (example: two POSTS are rejected, but a POST and a DELETE are OK). EDIT Thanks for the info. It seems the cnonce is what I need. On the wikipedia diagram it seems the cnonce is only sent once, and then a token is generated, leaving it open to reuse. I guess it is necessary to send a new cnonce on every call with the same token. The cnonce should be included on the body (transport-protected) or shared-key-protected and included on a header. Body-protection seems the best (with obvious SSL) since it avoids some extra processing on both sides, but it could be shared-key-encrypted and included on a header (most likely prepended to the temp token). The server would be able to read it directly on the body or decrypt it from the header (extra processing).
563e336a61a8013065267e4f	X	A Cryptographic nonce, the unique string you mention, is indeed a good security practice. It will prevent requests to be reused. It should be unique for each petition, independently of their nature. Including a timestamp and discarding all petitions made past a certain expiration date is also a good practice. Keeps the used nonce registry short and helps preventing collisions. The nonce registry should be associated to a user, to also prevent collisions. And consumers should use cryptographically secure pseudorandom number generators. If a predictable seed for the pseudorandom number generator is used, such as microtime, two nasty things can happen.
563e336a61a8013065267e50	X	I'm developing a javascript application that is fully powered by a REST API. I have different stages of development (dev, stage, live) and I need to point to the correct REST API host depending on which environment it is hosted at. Currently the app gets built on each commit for each environment. By "built", I mean it I run a job (browserify) that starts from an entry point JS file and builds it into a single js file. That means we make a dev, stage, and live build for each commit. We are trying to move to a traditional Continuous Integration solution of extracting the config and using one build where we can change the config. This is where the issue comes to play. The build system (gulp) accepts REST API host configurations to be passed in as arguments to the build command. The first step we accomplished to achieve Continuous Integration is removing the configurations from the being baked into the build. (That was the easy part.) We end up with 2 files, config.js and app.js. We now need a way to manage a configuration that will be shipped with the static application. We are hosting on Amazon S3 and this is purely client side JS application so there is no way to set server environment variables that will contain the configurations. We need to be able to deploy the software with the correct configuration file that is part of the source. How do we manage a single build with a configuration file for each environment per commit?
563e336a61a8013065267e51	X	This is helpful, but I didn't want the overhead of running my own server (I'd rather let Amazon do this). But it's a good suggestion.
563e336b61a8013065267e52	X	So run your server on EC2 then...
563e336b61a8013065267e53	X	How do you hide the secret key?
563e336b61a8013065267e54	X	It's included in the signature , u can make a signature say by having parameters followed by an equal sign and it's values in abc order on a string the append the secret key to the end, take the sha hash or md5 hash of that and send it as the signature to the Service call, on the server side the same steps take place , only if the signatures match do you allow for the call to proceed
563e336b61a8013065267e55	X	If the attacker can read the secret key by running your binary through strings. Reverse engineering the generation of your signature is trivial. The trick is keeping the secret, secret.
563e336b61a8013065267e56	X	well its supose to be hard to reverse engineer hashes...this is what apis like facebook do...if it wasnt so safe they prolly wouldnt be doing it
563e336b61a8013065267e57	X	also u can use ssl as an added layer of security
563e336b61a8013065267e58	X	SSL would help, but if the service credentials were stored in the app binary, running "strings" will spill all the secrets, then the hacker can just login to S3 as me and do what he likes.
563e336b61a8013065267e59	X	If I have an app that connects to Amazon's S3 service, is it worth my time to hide/obfuscate the connection strings and API keys? I'm guessing that most hackers won't care all that much, but it would be financially painful if someone found this information and was able to upload data to my account! For instance, if I store a username/password (or Twitter/Facebook API key and secret), these may be easily found using "strings". A hacker could see the functionality, grab the secrets and use them for nefarious purposes. I've seen people suggest using a simple Rot13, or storing the strings backwards or something like that in the app binary. Are these useful? Has anyone done this or have any ideas/patterns/code to share? -dan
563e336c61a8013065267e5a	X	You can hide your secrets in a webserver you have full control over, and then having this server relay the query to Amazon. You can then use whatever encryption/validation method you like, since you are not relying on what is supported by Amazon. Once you have validated that the request is from your own application, you then rewrite the query including your secrets and then forward this to Amazon. The result from Amazon could then be relayed directly back to the application. In php this could for instance be done using something similar to this snippet (not showing your url rewrite):
563e336c61a8013065267e5b	X	You dont really need to hide them...what you should do is have an extra key such as a secret, that one IS hidden and is only present in the signature of the call (which can be an MD5 hash or sha (or whatever)) without that secret key people wont be able to just make calls since the signatures created by the server and the offender wont match since they dont know the secret key used...
563e336c61a8013065267e5c	X	I'm guessing that most hackers won't care all that much It just takes one who's bored enough. Has anyone done this or have any ideas/patterns/code to share? This is what SSL is for. You can encrypt all your transmissions or just the login process (which would return a session id that can be used for subsequent requests during the session).
563e336c61a8013065267e5d	X	I create an AWS IAM role called "my-role" specifying EC2 as trusted entity, i.e. using the trust relationship policy document: The role has the following policy: I launch an EC2 instance (Amazon Linux 2014.09.1) from the command line using AWS CLI, specifying "my-role" as instance profile and everything works out fine. I verify that the instance effectively assumes "my-role", by running: An example of such credentials retrieval response is something like: I run/install a Tomcat7 server and container on such instance, on which I deploy a J2EE 1.7 servlet with no issues. Such servlet should download on the local file system a file from an S3 bucket, in particular from s3://my-bucket/custom-path/file.tar.gz using Hadoop Java APIs. (Please, note that I tried hadoop-common artifact 2.4.x, 2.5.x, 2.6.x with no positive results. I'm gonna post below the exception I get when using 2.5.x) Within the servlet, I retrieve fresh credentials from the instance metadata URL above mentioned and use them to configure my Hadoop Java API instance: Obviously, myAwsAccessKeyId, myAwsSecretAccessKey, and mySessionToken are Java variables that I previously set with the actual values. Then, I effectively get a FileSystem instance, using: I am able to retrieve all the configuration related to the FileSystem (fs.getconf().get(key-name)) and verify everything is configured as assumed. I cannot download s3://my-bucket/custom-path/file.tar.gz using: If I use hadoop-common 2.5.x I get the IOException: org.apache.hadoop.security.AccessControlException: Permission denied: s3n://my-bucket/custom-path/file.tar.gz at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.processException(Jets3tNativeFileSystemStore.java:449) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.processException(Jets3tNativeFileSystemStore.java:427) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.handleException(Jets3tNativeFileSystemStore.java:411) at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:181) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) at org.apache.hadoop.fs.s3native.$Proxy12.retrieveMetadata(Unknown Source) at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:467) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1968) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1937) ... If I use hadoop-common 2.4.x, I get a NullPointerException: java.lang.NullPointerException at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:433) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1968) at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:1937) ... Just for the records, if DON'T set any aws credential, I get: AWS Access Key ID and Secret Access Key must be specified as the username or password (respectively) of a s3n URL, or by setting the fs.s3n.awsAccessKeyId or fs.s3n.awsSecretAccessKey properties (respectively). Fatal internal error java.lang.NullPointerException at org.apache.hadoop.fs.s3native.NativeS3FileSystem.listStatus(NativeS3FileSystem.java:479) at org.apache.hadoop.fs.shell.PathData.getDirectoryContents(PathData.java:268) at org.apache.hadoop.fs.shell.Command.recursePath(Command.java:347) at org.apache.hadoop.fs.shell.Ls.processPathArgument(Ls.java:96) at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:260) at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:244) at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190) at org.apache.hadoop.fs.shell.Command.run(Command.java:154) at org.apache.hadoop.fs.FsShell.run(FsShell.java:255) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84) at org.apache.hadoop.fs.FsShell.main(FsShell.java:308) Sorry for the long post, I just tried to be as much detailed as I could. Thanks for any eventual help out here.
563e336c61a8013065267e5e	X	You are using STS/temporary AWS credentials; these do not appear to be currently supported by the s3 or s3n FileSystem implementations in hadoop. AWS STS/temporary credentials include not only an (access key, secret key), but additionally a session token. The hadoop s3 and s3n FileSystem(s) do not yet support inclusion of the session token (i.e. your configuration of fs.s3n.awsSessionToken is unsupported and ignored by the s3n FileSystem. From AmazonS3 - Hadoop Wiki... (Note there is no mention of fs.s3.awsSessionToken): Configuring to use s3/ s3n filesystems Edit your core-site.xml file to include your S3 keys If you take a look at S3Credentials.java from apache/hadoop on github.com, you'll notice that the notion of a session token is completely missing from the representation of S3 credentials. There was a patch submitted to address this limitation (detailed here); however, it hasn't been integrated.  A Hadoop JIRA ticket describes how to configure the s3a FileSystem: From https://issues.apache.org/jira/browse/HADOOP-10400 : fs.s3a.access.key - Your AWS access key ID (omit for role authentication) fs.s3a.secret.key - Your AWS secret key (omit for role authentication)
563e336c61a8013065267e5f	X	"Do you prefer SOAP type services or REST/RPC style ones" should be "Do you prefer SOAP/RPC type services or REST style ones". SOAP is an example of the RPC concept applied to XML-over-HTTP. REST is altogether more subtle a concept.
563e336d61a8013065267e60	X	Really don't see what is not constructive in this post. As matter of facts, it's very useful to me today.This should not have been closed
563e336d61a8013065267e61	X	HTTP has "standards" for authentication (Authenticate header) and errors (5XX responses). Packaging authentication in the payload is usually a bad idea, it breaks intermediaries that may cache them or process them, and it prevents the authentication scheme from being resued across sites. When you breach serendipity, you also breach REST.
563e336d61a8013065267e62	X	Could you make that an actual link, please?
563e336d61a8013065267e63	X	Another thing that ties into this is ease: Its 1) easy for people to understand an RPC approach instead of a rest approach and 2) Easier to implement ("here's a function and its arguments and return value")
563e336d61a8013065267e64	X	I would like to assume amazon is profitable not because of its API but because of the rest of their services. Just because a big corperation has used a paticular method of coding does not make it the best or even mean that they did their homework. For all we know it was a random choice.
563e336d61a8013065267e65	X	I'm looking at it from the opposite direction. They picked the best tool/methodology for the job based on the fact that they have to sell a product/service with it. They've gone through iterations, they've supported customers. They see flaws that an idea out of a book hasnt gotten to yet.
563e336d61a8013065267e66	X	Amazon's API is not actually REST, it's just RPC.
563e336d61a8013065267e67	X	REST needn't be HTTP, it's protocol-independent.
563e336d61a8013065267e68	X	At my company we're starting to branch into web APIs to access and update our data; initially for partners but then likely to the public in future. At the moment the way the API will look (e.g. SOAP, REST, RPC) is completely open and we haven't made any decisions yet, so I'm interested in both examples of web APIs people think are good, and why you think that. What I'm interested in is opinions from people using different languages (we're likely to be offering the API to people using a number of platforms, particularly including .NET, Java, ActionScript and JavaScript) about web APIs that you think are good examples, and that you've had good experiences with. Some points I'd like to cover: Do you prefer SOAP type services or REST/RPC style ones? I suspect that people with platform support (e.g. .NET, Java) will prefer SOAP ones and people using languages without platform support will prefer the others, but I'd like to validate that assumption. Do you care whether an API is actually RESTful or whether it is a plain old RPC style HTTP GET/POST? If so, why do you care? Is it more important that an API describes itself correctly (i.e. don't claim to be RESTful if it's RPC style) than whether it actually is one of the two? We need to verify who is using the service. I've been looking at the Amazon S3 authentication which uses a public identifier and a private token that's used to hash the parameters of the request into a verification token (this is also similar to flickr). Have you used this type of authentication before, and how did you get on with it? Are there any hash algorithms you find problematic (i.e. not supported by your platform)? Would you prefer to send the hash in an HTTP header or in the URI? How should versioning be handled? Is it a good idea to have a /v1/ type subdirectory so that future versions can be added alongside, or would you do something differently like have the version in the request payload or query? How long would you expect a version of an API that you'd built against to be supported for (i.e. if v2 was introducted, what would be your expectancy around the lifetime of v1). Also, any other opinions and points to cover would be useful. I'm deliberately staying vague on the actual type of API we're implementing, as I'm looking for general guidance in terms of what people think are good APIs and implementation mechanisms, so this post and its answers will be useful to more people in the future.   Note: I have searched and can't find a generic question about this - they all seem specific to a certain type of API - but if it is a duplicate then please let me know. Also if it should be community wiki (I think people ought to get credit for answers so I haven't made it one) then please let me know and I'll change it to be.
563e336d61a8013065267e69	X	Here's my take. Although coming from a Java standpoint, I actually prefer REST. SOAP envelope with multiple namespaces and its complex structure is abomination. It tries to solve mostly imaginary problems, and doesn't solve anything efficiently. Only thing about SOAP I've found useful is that it has standards for authorization and errors. On the other hand, both could be solved much easier by including four standard attributes in root XML element - username, password, errorCode, errorDescription. Good API description and documentation is indeed all that matters. Difference between REST and SOAP in mature framework is mostly in a few lines of configuration. For SOAP, send hash as part of SOAP security; for REST, I like to package everything in payload and avoid HTTP headers for authentication. I have only subjective reasons though, since I had to battle with frameworks which don't easily expose HTTP headers. My personal preference is having different URIs for different protocol versions. In my experience, this gives you more flexibility in newer versions, and old clients which connect to unsupported versions of a protocol stop working immediately and for obvious reasons. Also, sometimes you can map old version of application to old URI, to avoid having legacy support code in new server version. As for how long you support old version of protocol... ideally, as long as you have clients which use it. This is more business than technical decision. You should support at least one previous protocol version. It's usually in your interest to push clients towards new version to lower legacy support costs; from the clients side, new version should mean new features, better protocol, and some sort of additional business incentive (if new features alone are not enough).
563e336e61a8013065267e6a	X	You might be interested in Joshua Bloch's presentation "How to Design a Good API and Why it Matters". Joshua Bloch is the author of "Effective Java" and a Principal Software Engineer and Chief Java Architect at Google. Abstract: http://portal.acm.org/citation.cfm?id=1176622 Slides: http://lcsd05.cs.tamu.edu/slides/keynote.pdf Video: http://www.youtube.com/watch?v=aAb7hSCtvGw
563e336e61a8013065267e6b	X	Versioning for REST using Content-Type headers is covered well here: http://barelyenough.org/blog/2008/05/versioning-rest-web-services/
563e336e61a8013065267e6c	X	The RPC approach is also a good option. It reduces the overhead, and projects like Ice,Google Protocol Buffers and Apache Thrift are making it easier to develop RPC based services easier. If you do not have to provide a web based API, then RPC can also be a choice you want to explore.
563e336e61a8013065267e6d	X	I'd see what Amazon is doing - http://aws.amazon.com/ - the guys making money off this stuff obviouslly will have learned more lessons than anyone else. Other API's I'd look at - salesforce.com and Microsofts CRM api was rather interesting. Twitter has a battle hardened REST api too.
563e336e61a8013065267e6e	X	IMHO, it all depends on what kind of apps you're offering. If you are doing important, big time transactions, then definitely go with SOAP (WS "death star" as they call it). But if you're offering social apps, then go with REST, since it's simpler and a better fit for public hacking.
563e336e61a8013065267e6f	X	REST, if done correctly, is easy to understand (models HTTP), straightforward (resource oriented) and can be parsed by pretty much every programming language (XML).
563e336e61a8013065267e70	X	If you can not make up your mind eventually you could implement them all. In these cases it is useful to look how others have done it. I recommend you Open Source XML Native Database eXist that offers the three types of interfaces you are intestigating.
563e336e61a8013065267e71	X	You would need to buffer the video outside of gridfs as well. Not sure how you would be able to read fro one end of the binary file to the other in a sequential manner for a video player (since the bytes could be scattered across the files disk space and only when you have the entire file do you understand the video) but it could work if you reformatted files to put the needed data at the front for a player to understand how to use the file.
563e336e61a8013065267e72	X	And also by the sounds of it the binary method on the filesystem might be better for you, I am unsure of the context (web or desktop app) but file system, cdn for web; local for desktop normally works better for videos.
563e336e61a8013065267e73	X	Thanks, that helps a lot. It seems clear to me now that I can use MongoDB only for the metadata. However, since I can't use a cloud solution such as Amazon for the binary data - I need to keep the files on my local machines - I'm wondering if there's another DB which is both easy-to-use and document oriented as MongoDB, but also supports binary data as well. Otherwise, my only recourse would be to use the local filesystem manually, which is less preferable.
563e336e61a8013065267e74	X	Mobix - I'm not aware of any other document db's that handle this any better. You could look into something like a dedicated Media Server such as Adobe's Flash Server but outside of that I'm not aware of any other options other than the filesystem. If this answer helped then please mark it as answered. Thanks.
563e336f61a8013065267e75	X	I'm working on a video server, and I want to use a database to keep video files. Since I only need to store simple video files with metadata I tried to use MongoDB in Java, via its GridFS mechanism to store the video files and their metadata. However, there are two major features I need, and that I couldn't manage using MongoDB: I tried writing the straightforward code to do that, but it failed. It seems MongoDB doesn't allow multi-threaded access to the binary (even if one thread is doing all the writing), nor could I find a way to add to a binary file - the Java GridFS API only gives an InputStream from an already existing GridFSDBFile, I cannot get an OutputStream to write to it. Thanks, Al
563e336f61a8013065267e76	X	I've used mongo gridfs for storing media files for a messaging system we built using Mongo so I can share what we ran into. So before I get into this for your use case scenario I would recommend not using GridFS and actually using something like Amazon S3 (with excellent rest apis for multipart uploads) and store the metadata in Mongo. This is the approach we settled on in our project after first implementing with GridFS. It's not that GridFS isn't great it's just not that well suited for chunking/appending and rewriting small portions of files. For more info here's a quick rundown on what GridFS is good for and not good for: http://www.mongodb.org/display/DOCS/When+to+use+GridFS Now if you are bent on using GridFS you need to understand how the driver and read/write concurrency works. In mongo (2.2) you have one writer thread per schema/db. So this means when you are writing you are essentially locked from having another thread perform an operation. In real life usage this is super fast because the lock yields when a chunk is written (256k) so your reader thread can get some info back. Please look at this concurrency video/presentation for more details: http://www.10gen.com/presentations/concurrency-internals-mongodb-2-2 So if you look at my two links essentially we can say quetion 2 is answered. You should also understand a little bit about how Mongo writes large data sets and how page faults provide a way for reader threads to get information. Now let's tackle your first question. The Mongo driver does not provide a way to append data to GridFS. It is meant to be a fire/forget atomic type operation. However if you understand how the data is stored in chunks and how the checksum is calculated then you can do it manually by using the fs.files and fs.chunks methods as this poster talks about here: Append data to existing gridfs file So going through those you can see that it is possible to do what you want but my general recommendation is to use a service (such as Amazon S3) that is designed for this type of interaction instead of trying to do extra work to make Mongo fit your needs. Of course you can go to the filesystem directly as well which would be the poor man's choice but you lose redundancy, sharding, replication etc etc that you get with GridFS or S3. Hope that helps. -Prasith
563e336f61a8013065267e77	X	I am currently building an application in C# that makes use of the AWS SDK for uploading files to S3. However, I have some users who are getting the "Request time too skewed" error when the application tries to upload a file. I understand the problem is that the user's clock is out of sync, however, it is difficult to expect a user to change this, so I was wondering, is there any way to get this error not to occur (any .NET functionality to get accurate time with NTP or the alike?) Below the current code I am using to upload files.
563e336f61a8013065267e78	X	If you're asking this question you have probably taken AWS as far as you can go with the provided code sample. I have found most of the async upload functionality provided by AWS to be more theoretical, or better suited for limited use cases, instead of being production ready for the mainstream- especially end users with all those browsers and operating systems:) I would recommend rethinking the design of your program: create your own C# upload turnstile and keep the AWS SDK upload functions running as a background process (or sysadmin function) so that AWS servers are handling only your server's time.
563e336f61a8013065267e79	X	Getting the time from a timeserver is actually the easier part of your challenge. There is no built-in C# functionality that I'm aware of to get an accurate time from a time server, but a quick search yields plenty of sample code for NTP clients. I found a good comprehensive sample at dotnet-snippets.com (probably overkill for your case), and a very streamlined version on Stack Overflow in a page titled "How to Query an NTP Server using C#?". The latter looks like it might be effective in your case, since all you need is a reasonably accurate idea of the current time. Now on to the real challenge: using that time with Amazon S3. First, some background, as it's important to understand why this is happening. The time skew restriction is intended to protect against replay attacks, as noted here: http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html Because of this, Amazon built the current timestamp into the authentication signature used in the AWS SDK when constructing the HTTP(S) request. However, the SDK always uses the current time (there's no way to override it in the SDK methods): https://github.com/aws/aws-sdk-net/blob/master/AWSSDK_DotNet35/Amazon.Runtime/Internal/Auth/AWS3Signer.cs#L119 Note that in all cases, the SDK uses AWSSDKUtils.FormattedCurrentTimestampRFC822 as the timestamp, and there's no way for the caller to pass a different value into the method. So this leaves you with two options that I can see: Bypass the Amazon SDK and construct your own HTTP requests using the time you retrieve from an NTP server. This is doable but not easy. Amazon discourages this approach because the SDK provides a lot of helpful wrappers to ensure that you're using the API as a whole correctly, handling a lot of the tedious message processing that you have to do yourself if you go straight HTTP. It also helps with a lot of the error handling and ensuring that things get cleaned up properly if a transfer is interrupted. Clone the Amazon SDK git repository and create your own fork with a modification to allow you to pass in the current time. This would also be difficult, as you'd have to work out a way to pass the time down through several layers of API objects. And you'd lose the benefit of being able to easily update to a new SDK when one is available. Sorry there's no easy answer for you, but I hope this helps.
563e336f61a8013065267e7a	X	I am developing social networking app on android.Rough idea of my app is that when user launches an app it will get all users of this app in near by location.So for storage purpose of users data I want to use web server. I don't have an idea what is best way to start with.Should i use Amazon web services ? (S3,Ec2) I was surfing internet got these buzz words. Please guide me what is best approach for database storage ? Should i write my own server api ? or What ?
563e336f61a8013065267e7b	X	These are some general things you will have to do:
563e336f61a8013065267e7c	X	I personally favor using a MySQL database with PHP to interface between the app and the backend! Your app can send requests to PHP and then your PHP webservice would write/read to the database and then return JSON to your app. I would say this is a very subjective question though as there as so many ways that you can write a web service.
563e336f61a8013065267e7d	X	Post your old code. Anyway, you should be able to use Uploadfy, as Angular is meant to be generic and adaptable. You can, for example, create a directive to wrap Uploadify.
563e336f61a8013065267e7e	X	And @danial is the creator of this library.
563e337061a8013065267e7f	X	hi, question about the policy and signature generation. where can i find this? thanks
563e337061a8013065267e80	X	I have an application in Angular.js. In the truth, I made an update in my old site, changed that for Angular. In old version, I was using Uploadfy jquery component for my uploads in Amazon S3. But now, with Angular, I can't use this. I want to use the directive "ngUpload". But I don't know how to do this. Anybody can help-me?
563e337061a8013065267e81	X	You can use angular-file-upload a lightweight angular directive which has support for file progress and file drop. You can follow the Amazon S3 issues here which has a sample code as how to send all those data along with the file upload: https://github.com/danialfarid/angular-file-upload/issues/23 S3 upload should work with version 1.1.1 above. another related issue: https://github.com/danialfarid/angular-file-upload/issues/23
563e337061a8013065267e82	X	You can upload directly to an S3 bucket from an HTML form if you include valid S3 parameters in your POST data. These are called pre-authorized HTML POST forms. The valid parameter values are generated on your own hosting server through an API call to the AWS API. The values are then added to your upload form as hidden input fields. Here's what they look like in your form: Amazon provides sample code for Java, Python and Ruby. http://aws.amazon.com/articles/1434
563e337061a8013065267e83	X	You can do this using a "public" IAM account and API key with the AWS JS SDK if you want to process the files after upload and move them elsewhere. For example, to process and store an uploaded file. Using Angular, AWS S3 JS SDK, CORS and IAM accounts. There's a bit of setup involved to lock it down so it doesn't get abused though. I'v documented the process of setting this up here: http://www.cheynewallace.com/uploading-to-s3-with-angularjs/
563e337061a8013065267e84	X	that's not the way to post on stack overflow, you have to correctly indent you're code. This is to make easier to help you for people.
563e337061a8013065267e85	X	ya i got it thanks @mautrok
563e337061a8013065267e86	X	Read this before asking
563e337061a8013065267e87	X	i am new to stack overflow but i kept the code which is required to my problem
563e337061a8013065267e88	X	i can't get what i was doing wrong in posting a question @evc
563e337061a8013065267e89	X	1.I was building a ionic Mobile App in that i have a gallery and i was trying to view a image as pop-up from gallery, pop-up was working fine and it fetch the image path too but unable to see image on pop-up. here my app.js I was fetching image from s3 Amazon by using api , this is html code in my gallery.html where i get the data from my controller i can see the image in gallery and not able to see on pop-up don't know what was the problem. please help me out of this Thanks in advance.
563e337161a8013065267e8a	X	Thank you! That was precisely the issue... everything works as expected now.
563e337161a8013065267e8b	X	that saved me as well, any reason this is not in the doc?
563e337161a8013065267e8c	X	We're working on updates for the docs as well as an illustrative end-to-end sample.
563e337161a8013065267e8d	X	@BobKinney is there any way you can give more context? Are you literally setting the value to @"temp" is the key supposed to be my "Developer Provider Name" Does assigning this have a setter? Can i just return it on a getter of my AWSAbstractIdentityProvider sublass?
563e337161a8013065267e8e	X	@RyanRomanchuk I updated my answer with additional context and a link to the end-to-end sample. Hope fhis helps.
563e337161a8013065267e8f	X	I have followed the blog post linked above, made sure that the unauthenticated role arn is different from my authenticated role arn still without any success. However, when I set the unauthRoleArn to be the same as the authRoleArn I am able to successfully post to AWS, but I don't understand why this works / don't feel comfortable deploying this.
563e337161a8013065267e90	X	Can you confirm that your trust policy amr points to authenticated?
563e337161a8013065267e91	X	I am trying to use amazon cognito with developer authenticated identities. My API is successfully returning an id and token. However, when I use these tokens to upload content to S3 I receive the following error: Below is my code for setting up the credentials provider. And I am using the template provided at http://docs.aws.amazon.com/mobile/sdkforios/developerguide/cognito-auth.html#create-an-identity-pool-that-supports-developer-authenticated-identities to create the identity provider. It appears to be an issue with Role Trust. I created the identity pool using the amazon web interface and have double checked that the identity pool id is correct. I have been able to successfully upload w unauthenticated identities, so I believe is not a role permissions issue.
563e337161a8013065267e92	X	Sorry for all the trouble. There is a small issue with how the identity provider and credentials provider interact that is not properly documented or handled well. The credentials provider pivots using the unauth or auth role arn based on whether or not there are logins attached on the provider. If you aren’t storing any additional logins on the provider, it will treat it as unauthenticated and use the unauth role and result in the STS error you are seeing. You can work around this by doing something like the following in your identity provider’s refresh: Update 2015-03-10: You may want to consider looking at our end-to-end example for a better method for handling this. This the sample, we include the the actual values for the user identifier, then pass the entire contents of the logins property to the backend.
563e337161a8013065267e93	X	If you're successfully able to do this while unauthenticated, there are a few possibilities here. First of all, make sure your unauthenticated role arn is different from your authenticated role arn. Additionally, ensure that, in the trust policy (accessible via the appropriate role from this link), the amr points to "authenticated". If you have any other questions, this blog post goes over the process at a high level.
563e337161a8013065267e94	X	When using Rackspace Cloudfiles, and having Akamai CDN distribution enabled, a typical download http (publicly accessible) URL for a file is something like this: But according to the API (and tools like Cyberduck), files have an Origin URL as well. Usually something sort of like this: When trying to access the Origin URL in the browser, I get an Unauthorized message. Is it possible to make a file publicly/anonymously accessible via it's Origin URL? With Amazon AWS S3 + CloudFront, you can set it up so you can access a file either through it's cached CDN URL or via it's uncached S3 Origin Bucket URL. Can you do the same thing with Rackspace Cloudfiles? If so, how?
563e337161a8013065267e95	X	1) Sounds like you need a source control system(checout svn or github, there are 3rd party providers) 2) You can restrict access in the gdocs interface, not sure if you can do it programmatically through the api though
563e337161a8013065267e96	X	Thanks shiplu. I got the point of 3rd party server. I am new to server providers. Will you suggest any cloud storage server personally? Also if i am using cloud how will i show preview of file in browser?
563e337261a8013065267e97	X	@VibhaJadwani using their API. Every provider have APIs
563e337261a8013065267e98	X	@VibhaJadwani I have included some links for you. I think you should give a try on Amazon S3
563e337261a8013065267e99	X	Thanks.. I will check both. Do they provide API? I am using PHP and once user upload file on my site i will need to upload that file to SkyDrive or dropbox using PHP.
563e337261a8013065267e9a	X	Yes, see msdn.microsoft.com/en-us/library/live/hh826521.aspx developers.google.com/drive dropbox.com/developers Al if the cloud storage I mentioned provides roll back to old files, and API. Google Drive and Microsoft SkyDrive provides online document viewer.
563e337261a8013065267e9b	X	I want to develop a website like file manager. Where user register and will get fix disk space lets say 20MB. Now user can upload their pdf, doc, txt, jpeg etc files upto their disk limit. I can develop upto this using PHP. Now below is my issue: 1) If user's files are corrupted they can rollback their folders before 2-3 days. Files must be secure and safe from viruses as users are uploading their important documents. Is there any 3rd party storage server who provides such facility? 2) Also all files should be previewed from browser. I am using Google doc viewer. Is is good and safe way to preview file in browser? But google links are accessible from all, I need to add some restrictions as file can be viewed only by their owner. I know it's a major task, but i just need some sort of logic. Please share your thoughts. Thanks.
563e337261a8013065267e9c	X	Any cloud storage service can be used for this. You'll get HDD space. There is not storage server who provides revision control system for this. You can use git, svn for this though. But as the files are binary you can not get full facility of these tools. How file will be previewed depends on you. If you use PHP you make the site and at the backend you use the API to interact with the storage service. Google doc is not an option for this if you use PHP. Also note Google links can be made private. I suggest you this, Some cloud storage service
563e337261a8013065267e9d	X	Try Microsoft SkyDrive or Google Drive or Dropbox
563e337261a8013065267e9e	X	I need to set up a server so that files can be uploaded from an iOS app. I don't know how best to proceed. I thought about FTP but not sure if there is a better option. any ideas appreciated GC Also I must add that I will be building the iOS app so can use server APIs in my code.
563e337261a8013065267e9f	X	It's not ideal to set up a blind File/FTP server and hardcode the details into your app because all it takes is one person to intercept the login details and they have access to your server where they can upload (and potentially execute) bad things. A possible idea could be to set up an API frontend on your server in a language of your choice (PHP, Ruby, Python or similar) where you can 'POST' images to the server. With the API frontend, you can also do validation to ensure that only valid images are being uploaded and all nefarious stuff is thrown away. Then, in your iOS app, you can set up code to interact with your API frontend and send the actual images which will then be stored on your server. This is a fairly conceptual idea rather than an absolute implementation idea. It does require some thinking/reading and more setup/coding on the server side. Edit: Just to add, if you only want a central location to store/get your images without controlling it on a per user basis then you may want to look into Amazon S3 as a File Server.
563e337261a8013065267ea0	X	I am currently trying to back up my EC2 instance using the Amazon ec2-api-tools and ec2-ami-tools tools utilities. I am using a standard Ubuntu 14.04 AMI from Amazon. To download and install the required utilities, I've updated /etc/apt/sources with the following: I can now bundle the image as expected, however when I try to upload the AMI to a bucket I receive the following error: I am using the Frankfurt data center. From research online it seems the newer centers as per January 2014 do not support older authentication schemes. I've tried following the guide at http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html to enabled V4, however when I attempt to update my CLI config I receive the following error: Does anyone know how to fix there errors so I can back up my bundle to S3?
563e337261a8013065267ea1	X	Nope, didn't realize there was a Cognito Sync service I'll check it out but I've already invested quite some time in Parse so it might be a bother to switch ha, but thanks for your help
563e337261a8013065267ea2	X	I'm using Parse as my backend but I'm also using the AWS s3 service so I need to also use Amazon Cognito. So far Amazon Cognito integrates with 3rd party identity providers such as Facebook, Google, and Twitter but not Parse. So I'm guessing I would use the Basic flow which https://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSCognitoCredentialsProvider.html#//api/name/getIdentityId since I'm using Parse as my backend I won't be able to use the Developer authenticated identities method. But I'm not entirely sure. But my only issue is how I could get session tokens for temporary AWS Service access for users? Most of the tutorials/blogs I find online explain how to access the tokens on the server side but not on ios unless I'm missing a crucial concept here
563e337361a8013065267ea3	X	If you're authenticating users with Parse, you should be able to use Developer Authenticated Identities, having the user send login credentials to your backend which would then validate those with Parse. The mobile SDKs can get credentials to access AWS resources. These are vended for users based on their state, which is where the roles you mentioned come in. If you log in (with Parse, for example), your credentials would allow you to access what the auth role dictates, but if you don't, they'd be scoped to what the unauth role dictates. The developer guide has more information on getting credentials. As far as your backend, have you looked into Cognito Sync? If you have and opted to go with Parse instead, was there a particular feature it was lacking that caused you to not use it?
563e337361a8013065267ea4	X	Geoff, are you saying here that EC2 can do the conversion for me for free?? You confused me by answering my first question with a comment about EC2 acting as an intermediate server, and then went right into a whole thing about FFmpeg and encoding.com.
563e337361a8013065267ea5	X	No - not for free. You would be paying for the server and would be responsible to creating and managing the entire encoding process. (possibly using ffmpeg to do the encoding). With encoding.com or similar services, I think you can point them to your files on S3 and they download, encode and then send them back to S3. Easier than doing it yourself if your encoding requirements are straightforward but probably more expensive too.
563e337361a8013065267ea6	X	Thanks Geoff. Btw hope you had a nice Thanksgiving (if you celebrate). Have you heard of Kaltura.com (the non-free service as opposed to Kaltura.org)? They are, from what I've read briefly, a big player in the video streaming space, and their service takes care of storage AND encoding in one package. It may be something I want to consider as an easier solution, but wondering if you have any experience with it.
563e337361a8013065267ea7	X	Sorry, I've not heard of them at all and I'm in the UK so no Thanksgiving for me... Here are some links for you: brightcove.com/en , encoding.com/aws , heywatch.com/page/home , viddler.com , bitgravity.com , ooyala.com
563e337361a8013065267ea8	X	@pr0tocol: I'd suggest you to look at this answer: stackoverflow.com/a/3931145 .
563e337361a8013065267ea9	X	I'm looking to do the following. I was hoping you expert could review the steps and let me know if it is possible, or at which step does my plan fall apart. I apologize if this format is not effective, but I think seeing my thought process in steps will be best. Thank you so much. Hopefully these steps convey my goal. My questions: Thanks so much!
563e337361a8013065267eaa	X	You would upload to an Intermediate server where you would encode the videos in the desired formats before storing them on S3. EC2 would probably be a good fit here as transfers between EC2 and S3 are free and very fast. FFmpeg is a common tool used to convert videos or you might consider a 3rd party solution like encoding.com Most of the big players in this space support pulling and pushing to S3 natively too. Amazon doesn't really have any service to index and track your S3 videos. Whilst you can query S3 using the APIs or add metadata to S3 objects, you're definitely better off tracking everything in your own database. Use whatever database you are comfortable with - you only need to store the S3 bucket and key (filename) of each file along with any other user data you wish to link.
563e337361a8013065267eab	X	Have you looked at this SO Post it outlines the chunking concept pretty concisely.
563e337361a8013065267eac	X	@engineersmnky Yes it does, however it's not clear how that would be adapted in this case, if it can be - can you show some sample code?
563e337361a8013065267ead	X	what version are you using because I am not as familiar with v2 but in v1 S3Object#write automatically chunks the upload. Your issue is that you are trying to use put_object where as i would create a S3Object and then write to it in chunks but version is very important here.
563e337361a8013065267eae	X	@engineersmnky we are using SDK v2
563e337361a8013065267eaf	X	have you tried just :body => File.open(filepath,'rb') seems body accepts just an IO which should chunk stream but like I said less familiar with v2 and they made the source difficult to decompose
563e337361a8013065267eb0	X	Trevor, great answer. That's exactly what we were after in part 1, and excellent recommendation on part 2. Question though, with the resource there does not appear to be a field for the MD5 hash like there is in the put_object method. Does it do this automatically when using the resource method?
563e337361a8013065267eb1	X	When using the v2 aws-sdk gem, a MD5 hash is automatically computed on all requests that accept them. This is true for both the client #put_object method and the resource #upload_file method. I'll add that to my answer above.
563e337461a8013065267eb2	X	Interesting that the put_object still accepts the MD5 option and the resource upload_file does not - good to know.
563e337461a8013065267eb3	X	The put_object method accepts an md5 in case the user already knows it, they can save on computation time. The upload_file method switches between put and multipart and has to compute md5s for parts when performing a multipart upload.
563e337461a8013065267eb4	X	Hey @trevor-rowe we have tested with the second option here and still get an out of memory error ("File too big for single read") - is there another way the Resource method can be used to mitigate this?
563e337461a8013065267eb5	X	thanks, it's actually not to do with the 5GB limit, it's hitting a memory limit before that, eg with a 1GB file. Will edit the question to say that. Your links are for the v1 SDK and the code uses the v2 SDK. The question also mentions we know about the multi part upload, however that does not address the question of how to read the file in chunks and pass to the S3 client. If that's the only way then we need that to be the answer, and would need to see some justification for that answer.
563e337461a8013065267eb6	X	We are uploading various files to S3 via the Ruby AWS SDK (v2) from a Windows machine. We have tested with Ruby 1.9. Our code works fine except when large files are encountered, when an out of memory error is thrown. At first we were reading the whole file into memory with this code: Then after Googling we found that there were ways to read the file in chunks with Ruby: This code did not resolve the issue though, and we can't find a specific S3 (or related) example which shows how the file can be read and passed to S3 in chunks. The whole file is still loaded into memory and throws an out of memory error with large files. We know we can split the file into chunks and upload to S3 using the AWS multi part upload, however the preference would be to avoid this if possible (although it's fine if it's the only way). Our code sample is below. What is the best way to read the file in chunks, avoiding the out of memory errors, and upload to S3? Note that we are not hitting the S3 5GB limit, this is happening for files for example of 1.5GB.
563e337461a8013065267eb7	X	The v2 AWS SDK for Ruby, aws-sdk gem, supports streaming objects directly over over the network without loading them into memory. Your example requires only a small correction to do this: This works because it allows the SDK to call #read on the file object passing in a small number of bytes each time. Calling #read on a Ruby IO object, such as a file, without a first argument will read the entire object into memory, returning it as a string. This is what has caused your out-of-memory errors. That said, the aws-sdk gem provides another, more useful interface for uploading files to Amazon S3. This alternative interface automatically: A simple example: This is part of the aws-sdk resource interfaces. There are quite a few helpful utilities in here. The Client class only provides basic API functionality.
563e337461a8013065267eb8	X	The size limit for a bucket in .put is 5GB. However there is "multipart" upload in s3 where you can upload the files with large size. These links might help you: http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/MultipartUpload.html
563e337461a8013065267eb9	X	Thanks for the quick answer! Ok so I understand now that SSL can only take care of the security part, but not the authentication part, But what do you mean by a client certificate? Do you refer to the certificate in the OAuth protocol (Including the time-stamp and/or nonce and token)? And when you say that the certificate uses SSL, in what way?.. These things aren't that clear to me.
563e337461a8013065267eba	X	I have updated the answer - please look it once again. The client certificate is not related to OAuth token.
563e337461a8013065267ebb	X	thanks! that was helpful, I accepted the answer but can't promote because i don't have enough reputation yet. one more thing, when you're talking about client authentication, you're talking about authenticating the web application or the user himself? This term often gets confused. when we're talking about authentication, are we talking about username+password? or something more complex like recognizing private keys from the client side? or both?
563e337461a8013065267ebc	X	Authentication is identifying a principal access your application. For most protocols (Digest, Basic, Form authentication) it is username plus password. For the client authentication – it is validation of the client side certificate and the extracting of the information from the client certificate.
563e337461a8013065267ebd	X	I was researching the topic of authentication protocols, specifically protocols that work well with JAVA and REST API, and had a question regarding the subject. The architecture of the required system is a simple client - server. I found a few helpful protocols like 2 legged OAuth, digest authentication, Amazons S3 protocol and of course SSL. I'm a beginner at this authentication business, and I don't quite understand why should we use all the other protocols there are out there instead of using just SSL? It has been said that SSL is allegedly slower, but I understood that was the case a long time ago, and nowadays this protocol doesn't have this problem. It is also confusing to me that all the other protocols ride on SSL anyway. I know that SSL blocks replay attacks and man in the middle attacks. How is SSL different, or not sufficient, in comparison to the protocols above? And what each protocol contributes that is different?
563e337461a8013065267ebe	X	You describe 2 different requirements. You can apply to your application one of them or both of them: 1) A data protection 2) An authentication The data protection means the confidentiality, the content integrity, replay attacks and man in the middle attacks and more. All these features can be achieved by SSL. SSL can be applied almost to all protocols. The authentication means that you want to control who access your application. There are plenty protocols, and you should select the authentication protocol depend on your requirements and time you want to invest in the authentication. Please consider OAuth2. In addition, client authentication is optional in SSL. The client authentication in SSL is performed using a client certificate (Generally it is X.509 certificates http://en.wikipedia.org/wiki/X.509) Look Client-Certificate Authentication: http://docs.oracle.com/javaee/1.4/tutorial/doc/Security5.html The client certificate is most secured authentication but most expensive from the infrastructure investment. At least it requires PKI infrastructure http://en.wikipedia.org/wiki/Public-key_infrastructure in your organization.
563e337461a8013065267ebf	X	Amazon S3 keeps your data on some of their disks, but isn't intended as a high volume, low latency content delivery network (CDN). Cloudfront is Amazon's CDN that keeps multiple copies of your data on disks that are "near" customers around the world. Cloudfront accepts data from S3 and has various streaming solutions. See aws.amazon.com/cloudfront/streaming
563e337561a8013065267ec0	X	I'm considering building a website that helps musicians collaborate remotely. To do this, they would need to share large (uncompressed) audio files. For the solution I'm considering, I'd like to be able to perform the following functions: My concern is the large bandwidth demand. Should I perform these actions on my own (hosted) server space, or is there a service with APIs I can use? I've checked out Amazon's S3 which allows me to host files, however I can't find anything that suggests I can stream from their services. I'm not sure that S3 is right for what I'm trying to achieve. Can someone provide some high-level architectural advice? Thanks in advance.
563e337561a8013065267ec1	X	What Paul mentioned in his comments is true... S3 is not designed to be a CDN. However, if your audio files aren't intended to be used by over a thousand people at a time, you don't actually need a CDN. You can put them on S3 and stream directly from there (over HTTP) without difficulty. It sounds like you're going to have a bunch of tracks that will only be accessed by a handful of people. S3 is fine for this. When it comes to publishing finished work that might be used by many, that would be a good time to use Cloudfront.
563e337561a8013065267ec2	X	CloudFront does support SSL: aws.amazon.com/cloudfront/pricing
563e337561a8013065267ec3	X	Thanks for the info.
563e337561a8013065267ec4	X	Thanks, I hadn't considered the cookies. Damn those cookies!
563e337561a8013065267ec5	X	If any of the HTTP pages reference HTTPS resources, any "snooping" that is done on the HTTP traffic could simply be used to locate the HTTPS resource directly. e.g. if (NON-SSL) example.com/index.htm references (SSL) foo.com/secret.js, even though secret.js will be encrypted during transmission, a "man in the middle" would know the location of "secret.js" and could just load it directly. My example assumes there's no authentication made against the HTTPS domain by the user - is that the case?
563e337561a8013065267ec6	X	This question has come up at my job a few times, and I was hoping to get some community backing. We are working on a Single Page WebApp, but require that the data coming from our services API be secure to prevent snooping of the data. We are also trying to iron out the prod environment details for where our SPA will be hosted. One of the options is using something like Amazon's S3, which doesn't support SSL (to my knowledge). There's a group that believes the whole site needs to be hosted over SSL. However, it's my understanding that SSL will only protect the transmission of the data. So the point I'm trying to make is that hosting the services from an HTTPS site and the client code from non-SSL based URLs will be just as secure as hosting everything from an SSL site. Could anyone clarify this for me? Thanks in advance.
563e337561a8013065267ec7	X	Yes, SSL just encrypts the transmission of the data, and does not offer any type of protection of the runtime environment on any client-side code. Now, it is generally considered a best practice to host everything over SSL, for these reasons:
563e337561a8013065267ec8	X	and also dealing with large multiple files using WebDav is not advantage, very slow on syncing. like for example if you had 2 thousand files like ics or vcf, getting the value from server ics and vcf file and because it was blob, you are open the file and streamining then close, and because there are many files to open transfering records to your device, this will take long, compare using REST api, it will take you seconds not minutes or hrs.
563e337661a8013065267ec9	X	I don't believe you understand WebDAV. What's the problem with big files, and how is it different in a "REST" API?
563e337661a8013065267eca	X	Sorry to say Big File my mistake, what i mean is large number of files, For example you had 5 thousand files and this are ics and vcf extension, getting there value to transfer into your device take a lot of time, unlike REST API JSON Response for those all ics and vcf content in one format to send take only seconds. In Summary it depend on your project if WebDav or REST API.
563e337661a8013065267ecb	X	WebDAV is an extension to HTTP. Nobody stops you from having custom handlers that extend it.
563e337661a8013065267ecc	X	Microsoft's new Windows Live Application Based Storage API is a RESTful API. More info is here. Why did they choose not to support WebDAV?
563e337661a8013065267ecd	X	WebDAV is for managing files on a remote server, but perhaps is not generic enough for what Microsoft was trying to accomplish. There's also a lot of hype around REST lately and they are probably in competition with S3 from Amazon which offers a REST API. It would have been good of them to provide their RESTful API as well as WebDAV.
563e337661a8013065267ece	X	Microsoft has ADO.NET data service which provides RESTful API for data. Perhaps they have used the framework for the storage.
563e337661a8013065267ecf	X	Im been using WebDav on some other project like android and see all kind of scenario compare from REST Api. Using WebDav, when you had multiple files like 5 thousand files or more and getting there value from the webdav server, open the file take time because of streaming or reading each of them, using REST Api in one content for all records in good format of json is faster than webdav.
563e337661a8013065267ed0	X	Also. The slowest part of a web transaction is the desktop. Do you want the (slow) Django instance pushing down endless streams of static content? Or do you want limit the (slow) Django instance to pushing the important dynamic HTML page?
563e337661a8013065267ed1	X	Admittedly, the django static module from django.views.static is good enough for most serving, and chances are if you get more traffic than they're able to deal with, you've either got a rather inefficient app, or you'll have the resources to deal with moving to serve static files separately.
563e337661a8013065267ed2	X	@sleepynate: "good enough for most serving". We didn't find that to be the case. We found that having Django do very, very little and having Apache do as much as possible made things more scalable. Python is slow compared to Apache.
563e337661a8013065267ed3	X	The asker here doesn't understand why in production, one should move away from d.v.static. My point was that by the time he has enough people using his app to tell the difference, he'll likely have the resources (separate server) to deal with it.
563e337661a8013065267ed4	X	As per this answer to Can I gzip JavaScript and CSS files in Django?: Your CSS and JS should not be going through Django on your production system. You need to configure Apache (or Nginx, or whatever) to serve these, and when you do so you'll be able to set up gzip compression there, rather than in Django. The answers to that question don't explain the reason for this requirement/advice. Is it just a good practice for speed to have static content (images/CSS/JS) served from a different server? Or is there more to it?
563e337761a8013065267ed5	X	Apache and Nginx are faster than Django (because they do much less and much simpler things). So serving CSS and JS with Django is a waste of resources. Although "should" is too strong here, IMO. "Should, if you have high traffic", rather.
563e337761a8013065267ed6	X	The content served by a Web server can be broadly categorized in to two categories. Static files (CSS/JS/Img...) generally won't change (i.e, they can be read from disk and send to client. (No pre processing required before pushing) Dynamic files (your dynamic html pages) usually needs to be processed in various ways (db data + form process + messages ...) and send to client. When a thing is not changing and remain same for any user, don't assign that job to web framework (its additional burden), let the web server handle it.
563e337761a8013065267ed7	X	In production environments you already have real http server which connects to django (either via mod_python, fcgi or wsgi) so its just common sense to serve files directly from it. It will send responses faster: It will consume much less resources: It will scale better: It can be outsourced: All such things added together will simply make your site load faster.
563e337761a8013065267ed8	X	@Yassir, seriously man. Search. I found 6 duplicates in seconds. I know there are more.
563e337761a8013065267ed9	X	oh sry they didn't appear when i was typing the question my bad it won't happen again
563e337761a8013065267eda	X	"Closed as spam"?!? I can understand closed as duplicate, but why spam?
563e337861a8013065267edb	X	Yeah, doesn't seem like spam to me, either.
563e337861a8013065267edc	X	Well, Yassir himself voted to close. So maybe he mis-clicked.
563e337861a8013065267edd	X	but wouldn't i have a problem when i back-up data ?
563e337861a8013065267ede	X	No - just as there are backup solutions for databases, there are backup solutions for system files. Of course, depending on the scale of your appication, a third-party service like James Avery proposes could be what you're looking for. But unless you're going huge-scale with images, I would definitely recommend system files.
563e337861a8013065267edf	X	I agree with you
563e337861a8013065267ee0	X	That was an informative link. Thanks anton. :)
563e337861a8013065267ee1	X	This was the answer for me. I was looking for an already-implemented file repository that stores on the filesystem but is managed through the DB.
563e337861a8013065267ee2	X	Exact Duplicate: User Images: Database or filesystem storage? Exact Duplicate: Storing images in database: Yea or nay? Exact Duplicate: Should I store my images in the database or folders? Exact Duplicate: Would you store binary data in database or folders? Exact Duplicate: Store pictures as files or or the database for a web app? Exact Duplicate: Storing a small number of images: blob or fs? I have to store user's profile image (100px * 100px) what is the best way to store it ? database or system file ? which one is better , faster ,safer ... ?
563e337861a8013065267ee3	X	Always store images, music files etc in system files on disk, and then store the url:s to them in the database. That will make it 1) faster 2) easier to configure security settings 3) better in any ways I can imagine
563e337861a8013065267ee4	X	SQL Server 2008 offers the best of both worlds.
563e337861a8013065267ee5	X	I am going to propose a third option, a third-party party place like Amazon S3 or Mosso Cloud Files. Both provide APIs that you can use to upload the file and both provide CDN capabilities so the files will load quicker then they would off your servers or pulling from your database. This is a nice option because it is the best of both worlds. The downside of storing images in the database is that it is additional stress on your application and database servers (locating the file and pulling it) and it also causes your database to grow in size which can mean you will need more hardware earlier. The downside of storing them in the file system is that you now have an issue with scaling as if you want to add additional web servers they would each need a copy of the image or you would need create a dedicated server for these images. Also, file system access could be a future bottleneck to worry about.
563e337861a8013065267ee6	X	database makes it easier to backup and restore and it works better than file system storage for small images. Microsoft had published a research paper on this topic: To BLOB or Not To BLOB.
563e337961a8013065267ee7	X	I'm creating an app for Windows Phone and Android. So right now Im building a webapi they both can use, but I want to secure it som non other then my applications can use it. How do I go about it? No one else then my apps is going to access these APIs. I don't want to implement OAuth. I've got two scenarios that I'm thinking of: First (I store username and hashed password on the client): Second (I store accesstoken on the client): The problem as I see the second approach is that the server sends accesstoken to the client, if anyone where to get this they would have the access of the user. How is it done in the real world?
563e337961a8013065267ee8	X	You could use a slight modification of First: Storing a password hash on the client, then sending the hash and comparing it with the hash in the database is equivalent to storing a plain text password in the database because the hash becomes the password. So, your apps should authenticate with a username and password like any human user would do. But your concerns for the second approach apply too. If somebody intercepts the message, he has your credentials. A more secure solution is HMAC authentication (now we're talking "real world"). An example is the Amazon S3 REST API - the linked documentation is also a good example how to implement it for your own API.
563e337961a8013065267ee9	X	I know this is not the first time the topic is treated in StackOverflow, however, I have some questions I couldn't find an answer to or other questions have opposed answers. I am doing a rather simple REST API (Silex-PHP) to be consumed initially by just one SPA (backbone app). I don't want to comment all the several authentication methods in this question as that topic is already fully covered on SO. I'll basically create a token for each user, and this token will be attached in every request that requires authentication by the SPA. All the SPA-Server transactions will run under HTTPS. For now, my decision is that the token doesn't expire. Tokens that expire/tokens per session are not complying with the statelessness of REST, right? I understand there's a lot of room for security improvement but that's my scope for now. I have a model for Tokens, and thus a table in the database for tokens with a FK to user_id. By this I mean the token is not part of my user model. I have a POST /users (requires no authentication) that creates a user in the database and returns the new user. This complies with the one request one resource rule. However, this brings me certain doubts: My idea is that at the time to create a new user, create a new token for the user, to immediately return it with the Response, and thus, improving the UX. The user will immediately be able to start using the web app. However, returning the token for such response would break the rule of returning just the resource. Should I instead make two requests together? One to create the user and one to retrieve the token without the user needing to reenter credentials? If I decided to return the token together with the user, then I believe POST /users would be confusing for the API consumer, and then something like POST /auth/register appears. Once more, I dislike this idea because involves a verb. I really like the simplicity offered in this answer. But then again, I'd need to do two requests together, a POST /users and a POST /tokens. How wrong is it to do two requests together and also, how would I exactly send the relevant information for the token to be attached to a certain user if both requests are sent together? For now my flow works like follows: The token never expires, preserving REST statelessness. Most of the current webapps require email validation without breaking the UX for the users, i.e the users can immediately use the webapp after registering. On the other side, if I return the token with the register request as suggested above, users will immediately have access to every resource without validating emails. Normally I'd go for the following workflow: This looks overcomplicated and totally ruins the UX. How'd you go about it? This is quite simple, for now I am doing it this way so please correct me if wrong: login is a verb and thus breaks a REST rule, everyone seems to agree on doing it this way though. Why does everyone seem to need a /auth/logout endpoint? From my point of view clicking on "logout" in the web app should basically remove the token from the application and not send it in further requests. The server plays no role in this. As it is possible that the token is kept in localStorage to prevent losing the token on a possible page refresh, logout would also imply removing the token from the localStorage. But still, this doesn't affect the server. I understand people who need to have a POST /logout are basically working with session tokens, which again break the statelessness of REST. I understand the remember me basically refers to saving the returned token to the localStorage or not in my case. Is this right? If you'd recommend any further reading on this topic I'd very much appreciate it. Thanks!
563e337961a8013065267eea	X	You are right, SESSION is not allowed in REST, hence there is no need to login or logout in REST service and /login, /logout are not nouns. For authentication you could use I prefer to use PUBLIC KEY and PRIVATE KEY [HMAC] Private key will never be transmitted over web and I don't care about public key. The public key will be used to make the user specific actions [Who is holding the api key] Private key will be know by client app and the server. The private key will be used to create signature. You generate a signature token using private key and add the key into the header. The server will also generate the signature and validate the request for handshake. Now how you will get private key? you have to do it manually like you put facebook, twitter or google api key on you app. However, in some case you can also return [not recommended] the key only for once like Amazon S3 does. They provide "AWS secret access key" at the registration response.
563e337961a8013065267eeb	X	what is the problem with the current answers?
563e337961a8013065267eec	X	Can someone merge answers?
563e337961a8013065267eed	X	This is spot on! Option 1 is what I was looking for. And you wrote a perfect answer, thank you! I wish I could give you the bounty, but the question's closed. how do I give you the bounty? If you re-open this, I can give you the bounty
563e337961a8013065267eee	X	@TIMEX, if this question will be reopened or it's answers merged with other question you can do it manually if you wish ;)
563e337a61a8013065267eef	X	What are the POST parameters? Because I get "Requires upload file" when I send the POSt request
563e337a61a8013065267ef0	X	By the way, the tutorial shows a <form> with ACTION to the facebook url. That's not what I want. I want to upload to Facebook using my backend
563e337a61a8013065267ef1	X	updated answer to be more generic and linked to documentation
563e337a61a8013065267ef2	X	Actually posting image to /me/photos will add em to Application Album (creating it if album not yet exists)
563e337a61a8013065267ef3	X	@juicy Can you give me a link stating this because I really think you're wrong. /me/photos is the path for the currently loged in user. I can give a reference: link read this: "Additionally, there is a special identifier me which refers to the current user. So the URL graph.facebook.com/me returns the active user's profile."
563e337a61a8013065267ef4	X	voila, photo object documentation
563e337a61a8013065267ef5	X	Same statement you can find in How-To: Use the Graph API to Upload Photos to a user’s profile developers blog post which is referenced here in answers. Added that to my answer for clarification. BTW, Sure you need to replace me with id of user you want for posting photo to profile other than current user (you should use application access_token for that).
563e337a61a8013065267ef6	X	I'd advice you to read again the question : "How can I upload this imgur image to my app's album?"
563e337a61a8013065267ef7	X	Possible Duplicate: Upload photo to users profile from photo URL, not input file field I have the access_token. It's with the publish_stream permission. Alright, now I want to upload an image to my default app's album on Facebook. The image is hosted on my S3.amazon.com. (but for example purposes, let's take this link: http://i.imgur.com/Ptyzk.jpg) How can I upload this imgur image to my app's album? (documentation on Facebook is weird, and I don't understand PHP).
563e337a61a8013065267ef8	X	To upload photo to "Application Album" you have two ways (actually first may interest you more). Option 1: Uploading Photos to the Graph API via a URL: App developers who host their images on Amazon S3 or a similar service can pass the S3 URL directly to Facebook without having to download the file to their application servers only to upload it again to Facebook. This improves performance and reduces costs for developers. To achieve this you need to issue POST request (or GET with argument method=post) to next URL: http://i.imgur.com/Ptyzk.jpg in url encoded form is http%3A%2F%2Fi.imgur.com%2FPtyzk.jpg (you only need to encode url if you passing it withing URL arguments, and must not encode if you passing it with post-data). As stated in photo object documentation, by posting photo to https://graph.facebook.com/USER_ID/photos: The photo will be published to an album created for your app. We automatically create an album for your app if it does not already exist. All photos uploaded this way will then be added to this same album. Option 2: Uploading image data You just need to issue POST request to http://graph.facebook.com/me/photos (see Create Photos section of user object documentation). The only required parameter is source which is multipart/form-data encoded file, you can either download it to temporary file and upload it later, or stream it to the Graph API (depending on technology you use to implement this functionality) There is couple of samples how to upload photo (in languages other than PHP): Update: It's not really clear where do you want photo to be published User's album or Application album (title and body of question are opposite on this). If you want to publish it to User's album you need to replace me with id of album you want photo be uploaded to. Otherwise use me (for current user) or other user id (this will require usage of application access_token).
563e337a61a8013065267ef9	X	This tutorial does exactly that: https://developers.facebook.com/blog/post/498/ You can choose which album you want to upload the picture to. If you want to do it on a more generic way, Facebook API tells you to issue a POST request to ALBUM_ID/photos with source and message parameters. Note that source must be of multipart/form-data type. More details at https://developers.facebook.com/docs/reference/api/album/
563e337a61a8013065267efa	X	I think the majority of this answers are correct if you would like to upload that photo to a user of your app. But as I understand you want to upload it to your app's connected album. To do that you should get you app album's id than you'll be able to post an image to it like: $facebook->api('album_id', 'POST', array(... To get the id of the album go to the Graph API explorer and do a GET to: /your_app_id/albums (replace your_app_id with your actual app id). That should give you all the details about your app albums. Than do a Graph API POST request using your preferred SDK (JS, PHP etc.) with the array containing the picture source and a title (this are mandatory) to the above address. NOTICE: The album application connection is deprecated and will be removed on March 1st, 2012. Read more at the end of this doc http://developers.facebook.com/docs/reference/api/application/
563e337a61a8013065267efb	X	Awesome! Thanks man! That is exactly what I am looking for!
563e337a61a8013065267efc	X	I am looking for recommendations on document templating modules for an application that I am creating. The application is an online service for a specific type of stores. I won't go into details about the service, mainly because I think it is irrelevant in regards to the problem. My problem is basically this. I want my customers to be able to create their own document templates for certain sales documents. By document templates I mean layout of manuals and invoices etc. I basically provide a JSON document with the information to be merged into the template. The JSON document can both include per document information such as logo, addresses but also arrays of information that should be presented as lists with totals and sub totals. The templates should also be able to handle paging and differentiate between first, last, even and odd pages. If it can handle different formats such as A4 and Letter that would be awesome as well. If localisation is supported, you should be able to hear my scream of joy. If I can get out of having to implement such a templating service myself, I would gladly pay for it. I am open to different approaches of creating the templates. If the templates need to be created by a professional or someone with knowledge of specific tools such as PhotoShop or other design tools, that isn't a showstopper. If I can create a set of perhaps 10 standard templates and then let customers pay for custom templates, possibly by going to a third party design bureau that is fine. Web browser editing is also an option. I would prefer to be able to externalise this completely from my application so I have call this as a web service or a REST Api, either hosted by me or by a cloud partner. That also means that I am rather indifferent when it comes to the language this is implemented in. The output format needs to be at least PDF. I just don't know of any options out there that does this, and was hoping for suggestions from you guys. Thanks, JP
563e337b61a8013065267efd	X	Your requirements look like a pretty amazing fit with Docmosis Cloud Services. It is a commercial cloud service to which you upload documents (normal doc or odt files with "Docmosis" plain text mark-up) to act as templates. Your applications then call the REST API to perform mail-merge, conversion (doc, pdf, odt, html, rtf etc) and delivery of documents (stream back, email, store to Amazon S3 or any combination). As to other requirements you mentioned, Docmosis supports: A look at the resources page of the website will show you the documentation for the REST API and template guides, code examples and SDKs. Please note I work for the company that created Docmosis. Hope that helps.
563e337b61a8013065267efe	X	Abandon in favor of what exactly? You should clarify that. Either way though, this probably isn't a constructive question fit for SO.
563e337b61a8013065267eff	X	For my new web project I am considering to abadon server-side processing of web pages in favor of just using static HTML5 pages. All dynamic content of the page will be loaded using ajax from a REST service. No need for php, jsp, jsf. I came across this post and it seems I am not the only one. What are the advantages and disadvantages using this approach? I can imagine there are more client-server requests since many REST calls have to be made in order to gather all the information needed to display the web page.
563e337b61a8013065267f00	X	I believe that we have much more PROS than CONS. One of the good ideias to give HTML pages trought a WEB app server, like apache, nginx, or ISS, is that you can apply more security and control to the container that is delivered. BUT, HOWEVER Use static content, like JS, CSS, and HTML5 to consume only services, is the next goal in software development. When you start to divide things like, API and UX, you can test then separatly, you can develop at same time, service and interface, and you have much more speed and quality to development. When we look at a web page, and the weight that the DOM have, and how much cost for the app server to give all that container to the user, and sometimes less then 10% of this is JSON from a service, we need to start to rethink the architecture of our web app. I have been developing apps like this since one year and a half now, all the projects, and be sure, we re not to come back to the past. Its very important to work oriented to services, and how to consume this services. If you use for example, Amazon S3 to host your HTML,JS, CSS, IMAGES, files, you dont need a app server, only the REST api to consume and give the content to the user. Costless and very,very faster.
563e337b61a8013065267f01	X	@RGBK - This is unrelated to your question--but what audio player is that? I've been looking for a good non-flash audio player...
563e337b61a8013065267f02	X	It's a highly customised Sound Cloud player. I used this: github.com/soundcloud/Widget-JS-API/wiki
563e337b61a8013065267f03	X	Aside from the slow loading, your site is incredibly impressive. (:
563e337b61a8013065267f04	X	@tylermwashburn Thanks!
563e337b61a8013065267f05	X	@RGBK, I suggest that you work on your approval rate :) People will be more willing to help you.
563e337b61a8013065267f06	X	Also, you can join JS files to reduce http requests, and use jquery hosted on the CDN
563e337b61a8013065267f07	X	This is true, I was trying to give him the basics.
563e337c61a8013065267f08	X	Hi Nick... I'm not a pro (designer who does some jquery hacking and WP)... would love to know how to do AJAX afterloads?! Could you give me any pointers? Also with regards to having 70 http requests... what are those? How can I streamline this? Thanks man
563e337c61a8013065267f09	X	Ok, those are great pointers. I gonna minimise the pants off this thing :-)
563e337c61a8013065267f0a	X	@RGBK, there is a lot of answers floating around on the net concerning jQuery ajax, just search. I'll edit my post to explain HTTP request.
563e337c61a8013065267f0b	X	Getting rid of the double load made it about twice as fast already.
563e337c61a8013065267f0c	X	Cant believe it, i just searched for "Vanilla DOM scripting" and your comment shows up first in the results... Google is... FAST! google.co.uk/…
563e337c61a8013065267f0d	X	Both are excellent, thanks for the tip :)
563e337c61a8013065267f0e	X	I've done that, and all javascript is placed after the content loads. Thing is if it does it AFTER, then the composition sticks to the left margin and clicks into place after the load (which is admittedly pretty massive) just wondering if there is a way i can make it fix first, then load after?
563e337c61a8013065267f0f	X	$(document)* document.ready isn't a function.
563e337c61a8013065267f10	X	Hi John, I'm not that tech, so forgive e if I'm wrong, but isnt this just jquery, not ajax? But ok, maybe I should put some inline javascript to do the layout adjustment first? I tried that by putting the repositioning script just below the body. Also, the elements are all given width and heights (most of them) even before the images load, so it should have what it needs to do the repos. Hmmm. :-/
563e337c61a8013065267f11	X	I'm making finishing touches on my site and am struggling to make the load of the page look less jumpy. best way to show you what i mean is to show the site: http://marckremers.com/2011 (Still not complete, in alpha phase) As you can see the contents sticks to the left, loads a ton of jquery and images into the page, and only then click into place (centre). I'm wondering if there is a way i can make it click into place first and then load the elements? I tried putting the reposition script just after , not even that works. Any ideas? Thanks
563e337c61a8013065267f12	X	With all of the images you have, your page is 1.5mb, coupled with 70 http requests. No wonder your site behaves the way it does. You should be using sprites on the smaller images to reduce http requests and as far as the large images go, you are loading all of the pictures at once. Even the ones that aren't displayed right away. The images that aren't displayed right away should be pulled in via AJAX after the page loads. If you want to go further into optimization I would also: I could reinvent the web site best practices wheel here, or I could send you to Yahoo best practices for web site optimization There is a ton of very important information there, read it and reference it.
563e337c61a8013065267f13	X	You loaded jQuery twice, once from your own site and another time from Google's CDN. For starters, you should probably move all the JavaScript to the bottom of your HTML. Then you need to optimize your if ... else that handles how many columns to display and your Google Maps iframe. To speed the visual up, instead of using jQuery, you should probably have some vanilla DOM scripting that dynamically creates some CSS styles for the projects and tb_tweets classes, so it doesn't have to wait for all your JavaScript to load before handling resizing of your projects and tb_tweets.
563e337c61a8013065267f14	X	use http://mir.aculo.us/dom-monster/ and do everything it tells you to do. If you want tools to figure out what is going on during page load, the chrome developer tools are hands down the best out there for client side optimization.
563e337c61a8013065267f15	X	A think you could do is put your javascript functions in the document.ready(function()), this way the functions will be loaded AFTER the page is loaded. I guess you don't need the functions for loading the site, just to interact with it?
563e337c61a8013065267f16	X	Generally you only want to trigger your events after the page has rendered, i.e., using Then, in your HTML you have placeholders so the page doesn't "expand" or "jump" as you put, with some kind of indication that the element is still loading. Then, when your ajax requests complete, simply populate the placeholders with the ajax response.
563e337c61a8013065267f17	X	+1 to this answer. Last two years I also prefer this way of web app setup. It also prepare your app for any kind of API and for mobile development
563e337d61a8013065267f18	X	but this is exactly my question. what is a better idea. your explanation is right but this is exactly what i've described. when I'm placing all my assets in public folder it becomes a static app. anyway, for your question, i want to achieve a fast development process but with an easy way to scale it up when needed.
563e337d61a8013065267f19	X	i also edited my question after some of your notes, so thank you.
563e337d61a8013065267f1a	X	I want to build a new single page app using Rails 4 and Angular and Capistrano for deployment process. I want all the front end to be a static app on Amazon S3, but I'm openminded for other suggestions. What's important to me is a fast developing process with the ability to scale up easily. I was wondering what is the best structure I should use: keep all assets in app/assets and set Bower path to vender directory. that way i can use rails precompile methods and enjoying Rails html tags for index.html, but i'm sure it will be easy to upload it to S3 and keep it separated. keep all assets including Bower components in public/app directory, which will keep it as a complete separate application, but then i need to use Grunt or any other service for precompiling assets. any other idea?
563e337d61a8013065267f1b	X	From my experience, I found this approach to work really well: There's no real reason to have both views and apis in the same app or built with the same technology (as in Rails). Now there are issues: So as a solution I introduced another web server app. Can be based on anything - pure rack, node etc. I chose rack. Solutions to the problems: As a bonus - Most apps today have a landing page/marketing site and the actual app. Sometimes it's better to maintain these separately. The web sever knows according to a cookie which app to present on www.yourapp.com - actual app or marketing site. On sign in - set a cookie on client side and voila.
563e337d61a8013065267f1c	X	First, I think there's a bit of a confusion here, let me try to clear it up. There are a couple of ways for achieving this When you have a static application, there's no need to go through the Rails asset pipeline, there are far better ways to manage assets when you are using the tooling for client side applications. For example, your client application will be an Angular application and you will manage assets with a combination of bower (dependencies) and grunt (build and distribution). There's no point of deploying to S3 with Capistrano, if it's a pure static application, you can use aws CLI in order to just upload your content. I'd go through a CDN as well. Something like Fastly works really well over Amazon S3. I have a Rake task that uploads to S3 and then clears the cache on Fastly (if I need to). As for your Rails application, it would act as an API, it should not have any assets If you have a combined application, some of the actions are served by the server (Rails) and just invokes some client side code (Angular). If this is the case, I would go through Rails asset pipeline and just keep everything as Rails best practice with compilation pre-deploy etc... It's one of those questions where "it depends" is the answer really, it all depends on what you want to achieve. When I have a client application, I try to have a pure client and have the server only as an API, with no assets at all, this way, I separate the concerns. EDIT 9/9/15 I'd have to say that as long as you can, I'd keep the apps separate. It's not always possible, especially with more complex apps. Most apps I have seen in the recent months have kept the client side and the server side code separate, I have seen less use of rails and more use of rails-api because of that (some even ditched rails completely for thinner solutions).
563e337d61a8013065267f1d	X	After some research, duplicity may provide some options to handle syncing of files to various cloud services.
563e337d61a8013065267f1e	X	Have you considered uploading all your original photos to Flickr? There's a Windows application called PhotoSync that helps you synchronise your photos locally and on the Flickr cloud. Your lo-fi requirement is not met, unfortunately. The next closest thing that offers lots of offline functionality is PixSteward available for the Mac only. Lets you edit tags and sync them back to Flickr etc.
563e337d61a8013065267f1f	X	The scenario is rather simple. I have too many photos on my laptop and manual backups are too much work. File systems like google drive and dropbox are good solutions but would still keep the file on my hard drive (as well as online). My idea is to put all originals online and keep lo-fi (~400K) versions of my photos offline for easy, convenient access. Additionally, I want to be able to still sort, delete, tag and edit photos offline using software like iPhoto. Changes should then be synced with the originals should I want a print quality version in the future. iPhoto offers non-destructive changes so theoretically, this is possible. In summary: ^ Ref: https://en.wikipedia.org/wiki/Comparison_of_file_hosting_services Would need API support, free space and possibly version control (preferably for free). Not exactly sure what the 'remote upload' restriction entails. I'm currently leaning towards: ^^ option to keep originals locally should be on a per file basis (user decides) Any help on the issue is appreciated. Resolving the first challenge of lo-fi local and original version backed up online is a major goal.
563e337d61a8013065267f20	X	You already pointed out that you're aware of several services that do what you need. You should probably just take a look at the offerings and see what works best for your application.
563e337d61a8013065267f21	X	Yes, thank you corey, it had occured to me to investigate the options. A specific recommendation is always a helpful timesaver though. That's why i asked.
563e337d61a8013065267f22	X	These types of questions are not allowed. See the FAQ under “What kind of questions should I not ask here?” — stackoverflow.com/faq
563e337e61a8013065267f23	X	He is just looking for advice. Or more specifically, recommendations from other devs who have had experience in this field. I don't really see anything wrong here.
563e337e61a8013065267f24	X	Thanks Christian. Yes, i'm uploading to S3 with paperclip. It just occurred to me that all of our video content is going to be uploaded from iphone, so the iphone should be able to send me the thumbnail along with the video. But i'll look at zencoder. Cheers
563e337e61a8013065267f25	X	I've just added video uploading with paperclip to a Rails 3 app I have on Heroku. It works fine but I don't know how to generate thumbnails for the videos. I guess I should use one of the video processing add-ons in heroku? Any recommendations for which are easiest to connect to Paperclip?
563e337e61a8013065267f26	X	We currently use the Zencoder encoding service to encode video uploads. Which is great, because their service provides thumbnail generation on video files. I would advise doing any resource intensive processes, such as encoding, outside the web app. As this can take an entire HTTP Request handler (or in your case a web dyno). Sure you can request for more web dynos. But these are expensive. An alternative option is to do the processing in the background. But such tasks would be better off outside the web server. This is where an API service like Zencoder comes well. Here's a set of instructions to get you started. Another alternative is Panda Stream. Which Heroku does offer as an addon. Both Panda Stream and Zencoder will do what you need. Both also go on different price models. Unfortunately, I cannot give you any more info on Panda, since I haven't used them. As far as which uploader to use. Paperclip will do the job fine. I am guessing, since you are using Heroku, and you are uploading to Amazon S3. On our environment, we are using Carrierwave.
563e337e61a8013065267f27	X	I am a newbie to cloud computing technology and want to move an image file from amazon s3 to amazon ec2 and run it. I will be using JAVA. Please help me. Thanks in advance. best regards, Nil
563e337e61a8013065267f28	X	You don't move an image from S3 to an existing EC2 instance and run it. Instead you can use the Amazon EC2 API to start and stop your instances. See RunInstances or StartInstances in the docs. You can also perform these functions using the AWS SDK for Java There are a lot of Amazon services available but they are well documented online. You need to give these a good read to understand how they all play together.
563e337e61a8013065267f29	X	Okay, in my head this is somewhat complicated and I hope I can explain it. If anything is unclear please comment, so I can refine the question. I want to handle user file uploads to a 3rd server. So we have The flow should be like: The Website requests an upload url from the storage clouds gateway, that points directly to the final storage server (something like http://serverXY.mystorage.com/upload.php). Along with the request a "target path" (website specific and globally unique) and a redirect url is sent. the Website generates an upload form with the storage servers upload url as target, the user selects a file and clicks the submit button. The storage server handles the post request, saves the file to a temporary location (which is '/tmp-directory/'.sha1(target-path-fromabove)) and redirects the back to the redirect url that has been specified by the website. The "target path" is also passed. I do not want any "ghosted files" to remain if the user cancels the process or the connection gets interrupted or something! Also entries in the websites database that have not been correctly processed int he storage cloud and then gets broken must be avoided. thats the reason for this and the next step these are the critical steps All files in tmp directory on the storage server that are older than 24 hours automatically get deleted. If the user closes the browser window or the connection gets interrupted, the program flow on the server gets aborted too, right? Only destructors and registered shutdown functions are executed, correct? Can I somehow make this code part "critical" so that the server, if it once enters this code part, executes it to teh end regardless of whether the user aborts the page loading or not? (Of course I am aware that a server crash or an error may interrupt at any time, but my concerns are about the regular flow now) One of me was to have a flag and a timestamp in the websites database that marks the file as "completed" and check in a cronjob for old incompleted files and delete them from the storage cloud and then from the websites database, but I would really like to avoid this extra field and procedure. I want the storage api to be very generic and use it in many other future projects. I had a look at Google storage for developers and Amazon s3. They have the same problem and even worse. In amazon S3 you can "sign" your post request. So the file gets uploaded by the user under your authority and is directly saved and stored and you have to pay it. If the connection gets interrupted and the user never gets back to your website you dont even know. So you have to store all upload urls you sign and check them in a cronjob and delete everything that hasnt "reached its destination". Any ideas or best practices for that problem?
563e337e61a8013065267f2a	X	If I'm reading this correctly, you're performing the critical operations in the script that is called when the storage service redirects the user back to your website. I see two options for ensuring that the critical steps are performed in their entirety: The problems like this that I've faced have all had pretty time-consuming processes associated with their operation, so I've always gone with Option 2 to provide prompt responses to the browser, and to free up the web server. Option 1 is easy to implement, but Option 2 is ultimately more fault-tolerant, as updates would stay in the queue until they could be successfully communicated to the storage server. The connection handling page in the PHP manual provides a lot of good insights into what happens during the HTTP connection.
563e337e61a8013065267f2b	X	I'm not certain I'd call this a "best practice" but a few ideas on a general approach for this kind of problem. One of course is to allow the transaction of REST request to the storage server to take place asynchronously, either by a daemonized process that listens for incoming requests (either by watching a file for changes, or a socket, shared memory, database, whatever you think is best for IPC in your environment) or a very frequently running cron job that would pick up and deliver the files. The benefits of this are that you can deliver a quick message to the User who uploaded the file, while the background process can try, try again if there's a connectivity issue with the REST service. You could even go as far as to have some AJAX polling taking place so the user could get a nice JS message displayed when you complete the REST process.
563e337e61a8013065267f2c	X	Have you researched AJAX 'long-polling' solutions for django? It seems like basically the same thing.
563e337e61a8013065267f2d	X	Yes, you're right, it's probably the same thing. AJAX means javascript on the client side, it's more about web browsers, in my case the client is a softphone application. But the server side is probably almost the same, excellent idea.
563e337f61a8013065267f2e	X	Any feedback or updates after testing? About to follow in your footsteps
563e337f61a8013065267f2f	X	Hi dsldsl. I'm pretty happy with gunicorn and gevent so far. I did a few simple tests, which were satisfactory, gunicorn behaves as it says it does: I tried running 1 single worker then opening (and not closing) a few connections, and that did not prevent the next full request to go through. But so far I haven't had the time to do real benchmarking. Enjoy!
563e337f61a8013065267f30	X	I am working on a web service implemented on top of nginx+gunicorn+django. The clients are smartphone applications. The application needs to make some long running calls to external APIs (Facebook, Amazon S3...), so the server simply queues the job to a job server (using Celery over Redis). Whenever possible, once the server has queued the job, it returns right away, and the HTTP connection is closed. This works fine and allows the server to sustain very high load. But in some cases, the client needs to get the result as soon as the job is finished. Unfortunately, there's no way the server can contact the client once the HTTP connection is closed. One solution would be to rely on the client application polling the server every few seconds until the job is completed. I would like to avoid this solution, if possible, mostly because it would hinder the reactiveness of the service, and also because it would load the server with many unnecessary poll requests. In short, I would like to keep the HTTP connection up and running, doing nothing (except perhaps sending a whitespace every once in a while to keep the TCP connection alive, just like Amazon S3 does), until the job is done, and the server returns the result. How can I implement long running HTTP connections in an efficient way, assuming the server is under very high load (it is not the case yet, but the goal to be able to sustain the highest possible load, with hundreds or thousands of requests per second)? Offloading the actual jobs to other servers should ensure a low CPU usage on the server, but how can I avoid processes piling up and using all the server's RAM, or incoming requests being dropped because of too many open connections? This is probably mostly a matter of configuring nginx and gunicorn properly. I have read a bit about async workers based on greenlets in gunicorn: the documentation says that async workers are used by "Applications making long blocking calls (Ie, external web services)", this sounds perfect. It also says "In general, an application should be able to make use of these worker classes with no changes". This sounds great. Any feedback on this? Thanks for your advices.
563e337f61a8013065267f31	X	I'm answering my own question, perhaps someone has a better solution. Reading gunicorn's documentation a bit further, and reading a bit more about eventlet and gevent, I think that gunicorn answers my question perfectly. Gunicorn has a master process that manages a pool of workers. Each worker can be either synchronous (single threaded, handling one request at a time) or asynchronous (each worker actually handles multiple requests almost simultaneously). Synchronous workers are very simple to understand and to debug, and if a worker fails, only one request is lost. But if a worker is stuck in a long running external API call, it is basically sleeping. So in case of high load, all workers might end up sleeping while waiting for results, and requests will end up being dropped. So the solution is to change the default worker type from synchronous to asynchronous (choosing eventlet or gevent, here's a comparison). Now each worker runs multiple green threads, each of which is extremely lightweight. Whenever one thread has to wait for some I/O, another green thread resumes execution. This is called cooperative multitasking. It's very fast, and very lightweight (a single worker could handle thousands of concurrent requests, if they are waiting for I/O). Exactly what I need. I was wondering how I should change my existing code, but apparently the standard python modules are monkey-patched by gunicorn upon startup (actually by eventlet or gevent) so all existing code can run without change and still behave nicely with other threads. There are a bunch of parameters which can be tweaked in gunicorn, for example the maximum number of simultaneous clients using gunicorn's worker_connections parameter, the maximum number of pending connections using the backlog parameter, etc. This is just great, I'll start testing right away!
563e337f61a8013065267f32	X	The maximum size of a string or bytea type in postgresql is 2Gigs, but the OP is having up to 2M. So he's got lots of head room there.
563e337f61a8013065267f33	X	any idea how I could then save the binary as an image e.g. through paperclip ?
563e337f61a8013065267f34	X	I edited my answer.
563e337f61a8013065267f35	X	Sorry what I mean is that I am receiving base64 format and need to convert to image but I have found solution now: see stackoverflow.com/questions/4641728/…
563e337f61a8013065267f36	X	I want to save an image as a base64 string as part of a model in Rails. Does anyone have advice for the migration file? I assume that simply setting a type of String would not be suitable, given that the size of the string is often large e.g. > 2MB.
563e337f61a8013065267f37	X	You could use either text or binary instead of string in your migration if you want to overcome the size limitation. http://api.rubyonrails.org/classes/ActiveRecord/ConnectionAdapters/TableDefinition.html#method-i-column The API documentation gives this example: The maximum size of TEXT or BLOB (binary) columns depends on your RDBMS (e.g. MySQL, PostgreSQL), available memory, and certain configuration settings. For instance, in MySQL, you should have a look at the max_allowed_packet option, which you can set to anything up to 1 GB. Regarding storage with Paperclip: Paperclip doesn't allow database storage out of the box, so you have to write some custom code for that. Google gives me this: http://patshaughnessy.net/2009/5/29/paperclip-sample-app-part-3-saving-file-attachments-in-a-database-blob-column It's outdated though, so I'm not sure if it's helpful. More importantly: Note that storing files in database is generally not recommended which is why Paperclip doesn't support it. Some reasons it's a bad idea: When images are stored in DB, every image request requires a call to your Rails app and the database, which has a massive negative effect on performance. If you store images as files or on Amazon S3, your app will scale much better. Your database becomes large very quickly, which makes it harder to backup your data. Since different RDBMS have different rules for column size, column types, etc., migrating large columns to a different database (e.g. from MySQL to PostgreSQL) may involve difficulties. So I hope you have a good reason to do it anyway.
563e337f61a8013065267f38	X	This was exactly what I was looking for. It would be nice to pick and choose between attributes (for each post in redis I'll want the username, the user's image and some other specific data while I think I would want to only have the user id in my mongo db) but this should be good enough.
563e337f61a8013065267f39	X	To specify what I meant by "I would only want the user id in my mongo db" I meant I would only want the user's id stored on their posts.
563e337f61a8013065267f3a	X	Is this still experimental or could I use it in a production app?
563e337f61a8013065267f3b	X	I want to use mongo to store a global feed's data but I want to cache the most recent posts in memory with redis. Is there a way to set up one model to use two adapters? If there isn't I think I'm going to try to have one controller use two models.
563e338061a8013065267f3c	X	It's an experimental feature, but you can have a single model use multiple connections, and the connections can use different adapters: In a Sails v0.9.x model, with mongo and redis defined in /config/adapters.js: In a Sails v0.10.x model, with mongo and redis defined in /config/connections.js: However, you can't specify that some attributes use one connection, and other attributes use another. If more than one adapter is involved, and the adapters have identical methods, then they will be merged together. This would be the case for using sails-mongo and sails-redis in the same model; both adapters have find, create, update and destroy, so your model would end up using only one of the adapters' methods (sails-redis, in the example above). What's the use of multiple adapters then? It's a way to add external API methods to a model. For example, if you had a File model representing a file living on Amazon S3, you could use a sails-s3 adapter with upload and download methods, alongside sails-mongo to store metadata about the file. Caching is also a feature which is in active development, but in the meantime your best bet would be to use a separate model for the cache, as you suggested.
563e338061a8013065267f3d	X	The closest thing is Requester Pays Buckets on S3. docs.aws.amazon.com/AmazonS3/latest/dev/… Also, this question doesn't really seem programming related.
563e338061a8013065267f3e	X	Well, to be honest, I couldn't decide on a better place to ask it. Also, I'd love it if something like this already existed, so as a Game Developer, I could make an interface to download new levels, and use a trivially simple API, and boom - I'm making money as long as people are downloading my level! It's like an instant Asset Store.
563e338061a8013065267f3f	X	And my problem with "Requester Pays" is that I'm stuck paying for storage costs, and there's no way to add more charges on top of that. I end up having to build a huge amount of infrastructure (an "Asset Store"), just to protect and profit from my assets.
563e338061a8013065267f40	X	Well you are looking at a low level file storage system and wanting it to do things like credit card payments for in-game level downloads. I think you need to be looking at something specific to your use case (games) that is purpose built to handle that sort of thing. Something like Google Play services or Steam or whatever works with the platform you are developing your game for.
563e338061a8013065267f41	X	While interesting and well-written, this is off-topic (for one of several reasons). I simply chose "Too Broad".
563e338061a8013065267f42	X	I envision something like S3 (Amazon Simple Storage Solution), but where the content creator gets to set a price per download, and assign which Amazon accounts receive which percentage of the revenue. It'd be great if the content creator could set their cut at zero, and then whoever is downloading the asset is paying exactly and only for the S3 download bandwidth. Or if the content creator could estimate how many downloads they would get in a month, schedule cost amortization for the S3 storage cost as well, and build that right in to the download cost. If the actual download rate is much different, perhaps the amortized cost could be configured to adjust itself rapidly, slowly, etc. Or if the content creator could set up an asset to pay for its own storage and bandwidth, with some initial allowance. As soon as the asset has a negative balance, the asset could be immediately deleted, so as not to incur any more costs to the creator. Or the creator could pre-empt this death, and withdraw revenue at any time. And if you put that all together, the content creator could say, "Amortize the storage cost (1k downloads per month expected, but adjust at a medium pace), die when it runs out of money, but I want to make $0.10 off every download, $0.01 of that to stay with the asset to pay for its own storage during lean months, $0.01 of that should go to my editor's account, and the other $0.08 go to me." And yes, Amazon might demand their cut for doing all of this processing. If I'm playing a video game, and click to download a new map, some Amazon API could be used to ask me to confirm my purchase, and then download the asset. All completely transparently to the person who created the map. If I'm browsing the web, an "are you sure, this costs $0.30" interstitial page could open, before allowing me to download. All hidden behind a simple URL. My point being that if this were built right in to S3, rather than requiring an entire storefront and Credit Card processing, etc, this would make it trivially easy for content creators to share and profit from their work. Does such a thing already exist?
563e338061a8013065267f43	X	Given that you are already using S3 - you can leverage Amazon DevPay to charge back your customer. Your customers doesn't required to be on AWS or have AWS account - Amazon.com account is sufficient. Currently DevPay is supported for EC2 and S3. Info from Amazon docs Q: What is Amazon DevPay? Amazon DevPay is a simple-to-use online billing and account management service that makes it easy for businesses to sell applications that are built in, or run on top of, Amazon Web Services. It is designed to make running applications in the cloud and on demand easier for developers. Amazon DevPay removes the pain of having to create or manage your own order pipeline or billing system, which is traditionally a challenge for online subscription services or applications running on demand. It allows you to quickly sign up customers, automatically meter their usage of AWS services, have Amazon bill them based on the prices you set, and collect payments. Amazon DevPay provides a simple web interface for pricing your application based on any combination of up-front, recurring and usage-based fees. It uses Amazon Payments to process payments from your customers, and lets you leverage Amazon’s trusted billing infrastructure, making it easy for tens of millions of Amazon customers to pay for your application using their existing Amazon accounts
563e338061a8013065267f44	X	Thanks, this seems the most practical solution.
563e338061a8013065267f45	X	Just be aware that you can't create more than 100 buckets per Amazon account.
563e338061a8013065267f46	X	@Geoff Appleford Thanks Geoff, I didn't know that.
563e338061a8013065267f47	X	I've just deployed a Codeigniter application to Amazon EC2 (using S3 for media and RDS for MySQL). I need to restrict user account access based on the amount of bandwidth used - accounts will be based on bandwidth e.g. Basic account £x for up to 20GB per month etc. However I've no idea what the best way to do this is. My EC2 instances are Ubuntu with Apache2 if that helps. Any ideas appreciated! Thanks!
563e338061a8013065267f48	X	You can use the amazon's API to check the bandwidth usage , instead of using the logs . as you have said that you are using s3 for managing the assets , it will be good if you create a new bucket for every user and check the bandwidth usage and then limit the account if he/she exceeds that limit.
563e338161a8013065267f49	X	Limiting bandwidth per each connection for linux pc You have to install "Lighttpd" tool, its an web server. it is useful to limiting the bandwidth per connection and I also install Axel for increase and decrease the number of connections. sudo apt-get install axel http://www.cyberciti.biz/tips/installing-and-configuring-lighttpd-webserver-howto.html per connection i add this command in this path /etc/lighttpd /lighttpd.conf. connection.k bytes-per-second=70 I limit the bandwidth for 70 kb/sec and I tested each connections for 5 times and i observed minimum time, maximum time and I calculated average time. I repeated this up to 10 connections.
563e338161a8013065267f4a	X	You can create custom log with Apache to track the number of bytes transferred. The difficult part will be associating that to a given user. Probably a combination of cookie and maybe remote IP. You would then take that log and process it in the background. S3/Cloudfront can provide similar logs, but you probably wont be able to associate it to a user.
563e338161a8013065267f4b	X	If you use output buffering in PHP, you can call ob_get_length() just before calling ob_end_flush() to get the size of the output buffer that you are sending to the client. If you also implement a PHP handler for all files (rather than allowing for direct links to the files via Apache), you'd have a pretty decent idea of exactly how much content you were sending each client.
563e338161a8013065267f4c	X	So you're saying the problem isn't with my use of an OrderedDict, correct?
563e338161a8013065267f4d	X	@IanMorris: Correct, sorry, I was called away mid-post.
563e338161a8013065267f4e	X	@IanMorris: Is your file field meant to contain '@log.txt'? If you need to stream a local file named log.txt you need to replace that with an open file object.
563e338161a8013065267f4f	X	Ah you could be exactly right there. I'm translating the API's curl example into Python, and that's the syntax used there. I'll take a look.
563e338161a8013065267f50	X	@IanMorris: ah, requests does not support the Curl @filename syntax.
563e338161a8013065267f51	X	I'm trying to POST a request to an Amazon S3 endpoint using Python's Requests library. The request is of the multipart/form-data variety, because it includes the POSTing of an actual file. One requirement specified by the API I'm working against is that the file parameter must be posted last. Since Requests uses dictionaries to POST multipart/form-data, and since dictionaries don't follow a dictated order, I've converted it into an OrderedDict called payload. It looks something like this before POSTing it: And this is how I POST it: The response is a 500 error, so I'm really not sure what the issue is here. I'm just guessing that it has to do with my use of OrderedDict in Requests—I couldn't find any documentation suggesting Requests does or doesn't support OrderedDicts. It could be something completely different. Does anything else stick out to you that would cause the request to fail? I could provide more detail if need be. Okay, update, based on Martijn Pieters' earlier comments: I changed the way I'm referencing the log.txt file by adding it to the already created upload_data dictionary like this: pprinting the resulting dictionary I get this: Does that value for the file key look correct? When I post it to a RequestBin I get this, which looks pretty similar to Martin's example: However, I still get a 500 returned when I try to POST it to https://instructure-uploads.s3.amazonaws.com/. I've tried just adding the open file object to files and then submitting all the other values in a separate dict through data, but that didn't work either.
563e338161a8013065267f52	X	You can pass in either a dict, or a sequence of two-value tuples. And OrderedDict is trivially converted to such a sequence: However, because the collections.OrderedDict() type is a subclass of dict, calling items() is exactly what requests does under the hood, so passing in an OrderedDict instance directly Just Works too. As such, something else is wrong. You can verify what is being posted by posting to http://httpbin/post instead: Unfortunately, httpbin.org does not preserve ordering. Alternatively, you can create a dedicated HTTP post bin at http://requestb.in/ as well; it'll tell you in more detail what goes on. Using requestb.in, and by replacing '@log.txt' with an open file object, the POST from requests is logged as: showing that ordering is preserved correctly. Note that requests does not support the Curl-specific @filename syntax; instead, pass in an open file object: You may also want to set the content-type field to use title case: 'Content-Type': ... If you still get a 500 response, check the r.text response text to see what Amazon thinks is wrong.
563e338261a8013065267f53	X	You need to split what you're sending into an OrderedDict passed to data and one sent to files. Right now AWS is (correctly) interpretting your data parameters as FILES, not as form paramaters. It should look like this:
563e338261a8013065267f54	X	Regarding the javascript -- I'd love to, but this is an API, so the iphones are sending along their lat/lng and can't do the reverse coding. Since we're on Heroku, I don't think I can do the file-system log; so maybe MongoDB might be the way to go.
563e338261a8013065267f55	X	So it's an iPhone app and not a browser running there and I'm guessing that you don't control how the lat/lng stuff is gathered or sent? And you're right about the filesystem on Heroku. But you could use S3 and only pay for the storage (not bandwidth) or use their recently launch MongoHQ add-on
563e338261a8013065267f56	X	Right on all accounts! Leaning towards a MongoHQ implementation... Any thoughts on grouping by time period? (getting a requests per second)? would you just Request.all.group_by the second/hour/whatever?
563e338261a8013065267f57	X	We are using reverse-geocoding in a rails webservice, and have run into quota problems when using the Google reverse geocoder through geokit. We are also implementing the simple-geo service, and I want to be able to track how many requests per minute/hour we are making. Any suggestions for tracking our reverse-geocoding calls? Our code will look something like the following. Would you do any of these? Note: I don't need the data in real-time, just want to be able to know in an hourly period, what's our usual and max requests per hour. (and total monthly requests) Thanks!
563e338261a8013065267f58	X	The first part here is not answering the question you are asking but my be helpful if haven't considered it before. Have you looked at not doing your reverse geocoding using your server (i.e. through Geokit) but instead having this done by the client? In other words some Javascript loaded into the user's browser making Google geocoder API calls on behalf of your service. If your application could support this approach than this has a number of advantages: If you still would like to log your geocoder queries and you are concerned about the performance impact to your primary application database then you might consider one of the following options:
563e338261a8013065267f59	X	I'm optimizing my ASP.net MVC 4 website. I'm currently using run time bundling to combine and minify my JavaScript and CSS content. I would like automate publishing of the minified content to a CDN (specifically Amazon Cloudfront) as they are created. I am trying to determine the best strategy for integrating bundled files with a CDN. My specific questions are:
563e338261a8013065267f5a	X	I don't personally buy into the "USE CDN FOR EVERYTHING STATIC!!" mentality, so I refuse to worry about copying local scripts to a CDN as you described. Sure, the big libraries can be referenced from existing major CDNs (Yahoo, Microsoft, Google), but for local scripts, it's really not worth the hassle, IMO. Following that line of thinking, I've grown very fond of SquishIt. There's no extra XML config or preinitialization necessary to use. Just include it in the master or layout file like so: Having said that, and more to your point: 1) I'm not aware of any bundling libraries that support automatic CDN deployment. The usual train of thought here is to have the CDN pull from your website directory and cache it. In this way, deployment is established via a pull mechanism, rather than a push. This article describes how to set up origin pull using CloudFront with a word press site. I'm sure the configuration is similar for ASP.NET. 2) Bundle from local copies. You probably reference the local copies in development already, so why add the CDN into the mix prior to go-live? 3) Most cloud storage systems (Amazon S3, Azure Storage, Rackspace Cloud Files) offer a way to publish files to the cloud that remain read-only to the public. This is API-dependent, so the method varies depending on your cloud storage provider.
563e338261a8013065267f5b	X	After more research I came across the Squishit.S3 library which does exactly what I needed. Essentially it piggy-backs on squishit, enabling bundled files to be copied to an S3/Cloudfront bucket at runtime. Configuration is a breeze and because it uses Amazon's APIs, credentials are used to write to the CDN. If you already use Squishit it's just a matter of adding a couple of default configuration lines to your global.asax and the rest is taken care of for you
563e338261a8013065267f5c	X	You may wish to revise; S3 is a storage service. You're thinking of Amazon EC2.
563e338361a8013065267f5d	X	ah yes, EC2 is what I meant. thanks.
563e338361a8013065267f5e	X	I don't believe that. any sources?
563e338361a8013065267f5f	X	Eucalyptus works with Xen or KVM just fine. I'm using it with Xen right now :)
563e338361a8013065267f60	X	@Chris: Thanks for the clarification, I wasn't aware.
563e338361a8013065267f61	X	Are there any open source applications that provide a set of features similar to that of Amazon EC2 or Rackspace Cloud? Basically, I want a tool that I can install on one or more servers that works with a virtual machine monitor like Xen and lets me create, destroy, and clone virtual machines on the fly using some sort of API.
563e338361a8013065267f62	X	Similar to what other posters have mentioned, I would recommend Eucalyptus. It can use either Xen or KVM to manage virtual machines through the same Query API that Amazon EC2 provides. It also provides an S3-compatible service for storing files in buckets named Walrus. In case you need proof that you can run it over Xen, here's from the config file for Eucalyptus:
563e338361a8013065267f63	X	In addition to Eucalyptus, mentioned in other answers, you might also wish to take a look at OpenStack. It's the other major open Amazon AWS-like cloud stack. It's backed by NASA, RackSpace, Citrix, Intel, AMD and Dell, amongst many others. (Declaration of interests: I am presently providing technical consultancy to Citrix; however, I am answering this question in a private capacity.)
563e338361a8013065267f64	X	Canonical is the company making Ubuntu. They also Make UEC (Ubuntu Enterprise Cloud) witch is build around Eucalyptus. The architecture of Eucalyptus was design for maximum compatibility with EC2 (Elastic Cloud Computing). UEC is open source and very easy to use. I personaly put my first cloud together using UEC (9.4).
563e338361a8013065267f65	X	It might be worth looking into using Eucalyptus with ubuntu, I believe that it might even be what amazon are using for their ubuntu based cloud services.
563e338361a8013065267f66	X	You want to get the difference between A and B, without knowing A?
563e338361a8013065267f67	X	@Kay: Point well taken, I'd rather say I'd like to get the difference between A and B, with only the "signature" of A in hand (series of hashes identifying A may be?)...there's really no shame in asking, you know
563e338461a8013065267f68	X	@Ken: please read my comment here, indeed there's a way to do that, the only thing left to know is which tool is the best (I could only find one software that does this: rdiff)
563e338461a8013065267f69	X	In the MSDN link mentioned in my question, I found this: "Compressing a Target Without a Source File Describes how the API functions can be used to create a delta without referencing a source file."
563e338461a8013065267f6a	X	I don't have much faith in MS, but unless I misread this, it seems somehow possible(?)
563e338461a8013065267f6b	X	Regardless of faith in MS, I think you misread the link you mentioned in your edited question. The link indicates If a CreatePatch function is called and the source file is specified as NULL or INVALID_HANDLE_VALUE, the output will be a compressed representation of the target file. which means that the delta will only describe the target B, not the difference between the source A and the target B.
563e338461a8013065267f6c	X	OK, after some research, I finally decided to un-accept the answer. First there's dropbox which obviously does implement something like this and second, patch without having both files at the same machine (ie. using a file signature) has already been implemented in rsync (well, kind of) and in rdiff
563e338461a8013065267f6d	X	Thank you, yes! This is kind of what I had in mind (hash for each slice/bloc). However, I was rather skeptical but your idea of super hashes is very interesting and may very well do the trick...I'm going to take a look at it, thanks!
563e338461a8013065267f6e	X	@Gdhami rsync works by calculating hashes of blocks (so it only has to send differences of blocks that actually differ in stead of sending the complete diff between A and B over the line). But it still needs the complete A and complete B.
563e338461a8013065267f6f	X	@JeroenWiertPluimers: I was kind of hoping to find some "magical" way to achieve this. Thank you guys for your help, I really appreciate it!
563e338461a8013065267f70	X	@Gdhami you are most welcome, I'm glad you found our help of use.
563e338461a8013065267f71	X	(OK, don't yell at me, it's very late here :)) I'm researching delta diff tools (commandline tools or components, it doesn't matter as long as I can call them from Delphi 2010) I have this project where I upload files to a server, I need to optimize upload and so it would really great if I can upload the delta file only instead of sending the new file and then comparing both old & new file versions on the server. I read about Duplicity here Duplicity is a variation on rdiff-backup that allows for backups without cooperation from the storage server, as with simple storage services like Amazon S3. It works by generating the hashes for each block in advance, encrypting them, and storing them on the server, then retrieving them when doing an incremental backup. The rest of the data is also stored encrypted for security purposes. This got me thinking, is there a tool (or a way) to generate a patch or delta file (I'm not sure what the proper term is) based on the new file, without having access to the original file? I mean let's say I have this file that I modified once: Is there a way to construct [ delta-file-1.diff ] based on the new file without having access to the old file? (may be by storing some kind of a signature for the original file?) I researched a lot this topic (rdiff, PatchAPI, ZDelta, XDelta, MSDelta, etc...) but I can't find any real-world working example on this. These references talk about this but I wanted to hear if anyone can guide me and/or suggest better tools that answer the question that I asked above. Compressing a Target Without a Source File Windows Patch API: Compressing a Target Without a Basis (Source) File Thanks in advance!
563e338461a8013065267f72	X	No you can not get the difference from A and B without a way to get A and B. You could reconstruct A from older versions of A and applying the differences. The signature of A won't cut it.
563e338461a8013065267f73	X	When you only append to a file or edit in blocks of known size (most likely not possible for text files), I guess hashing would be feasible. See eMule's AICH (eMule wiki/aMule wiki). Essentially you split a file into blocks of size N, and calculate the hash code of each block. Then you calculate a "super hash" out of M blocks. With that approach you could track down changed blocks without having to transfer much metadata. Otherwise: You cannot create the whole file out of a diff without knowing the base the diff was taken of. Neither can you create a diff without knowing the base.
563e338461a8013065267f74	X	For those interested: there's rdiff which does have a windows port and can be launched from Delphi, and librsync which is, if I understood correctly, the engine behind rdiff. Both require signature of the old file (which is much smaller than the file itself) and the complete new file. Reverse delta can be done to allow getting the new file from the old one.
563e338461a8013065267f75	X	Awesome. This is what I needed. Thank you!
563e338461a8013065267f76	X	Using fine-uploader 4.1.1 Hello, I need some clarification regarding the upload cancel feature. Is the default behavior to allow a cancelled multi-part upload to S3 to be resumed? If not resumable, is there a way to allow the upload to be cancelled and still retain the capability to resume? Also, if not resumable, what happens to the multi-part data after the upload is cancelled? Is it cleared out of S3 immediately, or does the data linger for the default 7 days? Thank you!
563e338461a8013065267f77	X	Cancelling the upload will remove all traces of it from the handler. So, the default behavior for a cancelled file is for it not to be resumed. Sounds like you'd benefit from the pause in-progress uploads feature. This would allow you to explicitly pause a file and to later resume right where the upload left off. Also, note that the upload will resume if you were to leave the page in the middle of an upload and tried to upload the same file again. As far as the MPE data: After you initiate multipart upload and upload one or more parts, you must either complete or abort multipart upload in order to stop getting charged for storage of the uploaded parts. Only after you either complete or abort multipart upload, Amazon S3 frees up the parts storage and stops charging you for the parts storage. ~ http://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html The abort MPU request is sent when the upload is cancelled, and the complete MPU request is sent when the MPE upload has successfully and completely finished.
563e338561a8013065267f78	X	None of them is ok. Parse is for analytics... I need a secure cloud storage with client-side encryption. I edit my question.
563e338561a8013065267f79	X	You can use any major service (Google Drive, S3, Azure, Dropbox) and perform client-side encryption yourself or use CloudBlackbox package of our SecureBlackbox product to do this automagically for you.
563e338561a8013065267f7a	X	Thanks for your answer.
563e338561a8013065267f7b	X	I want to create a new service and I need to find a secure cloud storage with client-side encryption. The service needs to be accessible with an API or a SDK or something like that. I made some researches and I found Mega. Do you know another? Is there an opensource solution to construct this kind of cloud? Thanks in advance for your answers.
563e338561a8013065267f7c	X	All cloud have data storage like. all these storage can be access by API. you can implement data encryption like Hashing, Symmetric, Asymmetric at client side before storing the data into cloud storage.
563e338561a8013065267f7d	X	I work for Adrenaline Mobility. We're a new entry into this area, but that's exactly what we do. (Sorry for the promotion, but it's quite relevant). We provide secure storage (client side encryption), credential management (login / signup / password reset), messaging (end-to-end encrypted messages between users, devices, etc). Our architecture is designed so that you don't need to trust us, or our storage backend. Feel free to email me at murph@adrenalinemobility.com if you want to discuss what we can do for you. We're pretty flexible and willing to work with you. --Murph
563e338561a8013065267f7e	X	I'm actually looking for any API that gives me usage with pricing. Is there any aws api that will provide me usage for EC2 instances, EBS volumes and S3 storage ?
563e338561a8013065267f7f	X	I'm trying to get the billing information from aws for ec2 instances, s3 buckets and ebs volumes using java api. I want to create our own api that gives specific entity wise hourly billing reports. Is there any java api for getting the same. I could find the same in aws java sdk api documentation.
563e338561a8013065267f80	X	There are no APIs to get AWS billing information. Instead what you can do is: For more information: See here
563e338561a8013065267f81	X	@bagui As per AWS official documentation, there is no as such API feature available to get actual billing usages data. Instead you can get the expected billing data as: To get started, all you need to do is to provide an Amazon S3 bucket for your billing data, give the AWS Billing system permission to write to it, and visit the Billing Preferences page to enable programmatic access: Once you have done this, we will generate an Estimated bill several times per day and store it in the bucket, where you can download and process it as desired. We will also generate a Final bill at the conclusion of each billing period. Billing Reports are generated as CSV files and include plenty of details: Here is a list of the fields (read the documentation for more information): Referred AWS documentation: Programmatic Access to AWS Billing Data Thanks
563e338561a8013065267f82	X	I think you're going to need to make your question a little more explicit. What's your requirement for "secure"? DRM can always be broken, etc. Also, what does "calling the videos into a browser window" mean? That's not a term I'm familiar with. I'm tempted to vote to close as not a real question, but I'll give you a chance to flesh it out :)
563e338561a8013065267f83	X	By 'deliver' do you mean stream ONCE and ONLY ONCE or do you mean you want allow the purchaser to get the actual video file?
563e338661a8013065267f84	X	Stream - possibly set an expiration on the stream
563e338661a8013065267f85	X	Revision on initial post: I'm looking to deliver video content after a purchase of content is complete. Here are my requirements So that is about it, as I said before I would most likely use PHP to call the file from the S3 service and stream it on the page, as long as the purchase can be confirmed. Is this feasible, do you see any hurdles or issues?
563e338661a8013065267f86	X	You can generate a signed URL with an expiration date. Check out query string authentication in amazon API. You can play with it using S3fm, an online file manager for S3. http://www.s3fm.com/ Just select expiration date in the Web URL dialog and copy the url.
563e338661a8013065267f87	X	According to Fielding your first statement is incorrect: "The REST style doesn't suggest that limiting the set of methods is a desirable goal. [...] In particular, REST encourages the creation of new methods for obscure operations, specifically because we don't want to burden common methods with all of the logic of trying to figure out whether or not a particular operation fits the 99.9% case or one of the others that make up 0.1%." Source: xent.com/pipermail/fork/2001-August/003191.html That being said, I like your idea of validity as a resource in itself.
563e338661a8013065267f88	X	Also, status code 400 is more for the server itself, such as Apache, when it receives a malformed request -- not when the media type is malformed in the mind of the application. That would be status code 422 Unprocessable Entity.
563e338661a8013065267f89	X	I have a Java component which scans through a set of folders (input/processing/output) and returns the list of files in JSON format. The REST URL for the same is: Now, I need to perform certain actions on each of the files, like validate, process, delete, etc. I'm not sure of the best way to design the REST URLs for these actions. Since its a direct file manipulation, I don't have any unique identifier for the files, except their paths. So I'm not sure if the following is a good URL: Edit: I would have ideally liked to use something like /file/fileId/validate. But the only unique id for files is its path, and I don't think I can use that as part of the URL itself. And finally, I'm not sure which HTTP verb to use for such custom actions like validate. Thanks in advance! Regards, Anand
563e338661a8013065267f8a	X	When you implement a route like http:///file/validate?path you encode the action in your resource that's not a desired effect when modelling a resource service. You could do the following for read operations GET http://api.example.com/files will return all files as URL reference such as GET http://api.example.com/files/path/to/first will return validation results for the file (I'm using JSON for readability) That was the simple read only part. Now to the write operations: DELETE http://api.example.com/files/path/to/first will of course delete the file Modelling the file processing is the hard part. But you could model that as top level resource. So that: POST http://api.example.com/FileOperation?operation=somethingweird will create a virtual file processing resource and execute the operation given by the URL parameter 'operation'. Modelling these file operations as resources gives you the possibility to perform the operations asynchronous and return a result that gives additional information about the process of the operation and so on. You can take a look at Amazon S3 REST API for additional examples and inspiration on how to model resources. I can highly recommend to read RESTful Web Services
563e338661a8013065267f8b	X	Now, I need to perform certain actions on each of the files, like validate, process, delete, etc. I'm not sure of the best way to design the REST URLs for these actions. Since its a direct file manipulation, I don't have any unique identified for the files, except their paths. So I'm not sure if the following is a good URL: POST http:///file/validate?path= It's not. /file/validate doesn't describe a resource, it describes an action. That means it is functional, not RESTful. Edit: I would have ideally liked to use something like /file/fileId/validate. But the only unique id for files is its path, and I don't think I can use that as part of the URL itself. Oh yes you can! And you should do exactly that. Except for that final validate part; that is not a resource in any way, and so should not be part of the path. Instead, clients should POST a message to the file resource asking it to validate itself. Luckily, POST allows you to send a message to the file as well as receive one back; it's ideal for this sort of thing (unless there's an existing verb to use instead, whether in standard HTTP or one of the extensions such as WebDAV). And finally, I'm not sure which HTTP verb to use for such custom actions like validate. POST, with the action to perform determined by the content of the message that was POSTed to the resource. Custom “do something non-standard” actions are always mapped to POST when they can't be mapped to GET, PUT or DELETE. (Alas, a clever POST is not hugely discoverable and so causes problems for the HATEOAS principle, but that's still better than violating basic REST principles.)
563e338761a8013065267f8c	X	REST requires a uniform interface, which in HTTP means limiting yourself to GET, PUT, POST, DELETE, HEAD, etc. One way you can check on each file's validity in a RESTful way is to think of the validity check not as an action to perform on the file, but as a resource in its own right: This could return a simple True/False, or perhaps a list of the specific constraint violations. The file-id could be a file name, an integer file number, a URL-encoded path, or perhaps an unencoded path like: Another approach would be to ask for a list of the invalid files: And still another would be to prevent invalid files from being added to your service in the first place, ie, when your service processes a PUT request with bad data: it rejects it with an HTTP 400 (Bad Request). The body of the 400 response could contain information on the specific error. Update: To delete a file you would of course use the standard HTTP REST verb: To 'process' a file, does this create a new file (resource) from one that was uploaded? For example Flickr creates several different image files from each one you upload, each with a different size. In this case you could PUT an input file and then trigger the processing by GET-ing the corresponding output file: If the processing isn't near-instantaneous, you could generate the output files asynchronously: every time a new input file is PUT into the web service, the web service starts up an asynchronous activity that eventually results in the output file being created.
563e338761a8013065267f8d	X	Even a simple pre-generated shell script with thousands of lines of wgets would work better then using a real browser. Then just start as many terminal sessions as you need.
563e338761a8013065267f8e	X	Agreed. There's many FAR better ways to do it, but because of reasons beyond the scope of this forum we're stuck with doing it in a web browser. Have even tried using elinks on the web server (it's requesting from localhost, nearly eliminates network lag in the process) and still no joy
563e338761a8013065267f8f	X	@duskwuff No, I wish there was. We're stuck with the parameters defined above. That said, I installed the firefox addon, it did not help, but to test the theory, I opened up a copy of firefox, IE, chrome, and safari, and the damn thing scaled properly. Instead of 1 requests per seconds, I was getting 4. If I open 4 tabs in firefox, even with the addon (configured a variety of ways), it sticks at 1req/sec, the page loads just take longer. Any idea whats going on here? Is the OS (Win7) some how limiting connections per program or something? I've set max connections in FF to 96.
563e338761a8013065267f90	X	Bit of an odd problem here. We're migrating a website to a new software platform. As part of this migration, we must copy files from one Amazon S3 bucket to another. There are hundreds of thousands of files. We also must use the software we have (phpFox) to do this. Basically a php framework. The job is broken in to segments that we call using an offset in the URL. Basically: Copy 10 files and update the database as necessary Increase offset by 10 Rinse, repeat. The API traffic is light, the load on the server is sub 1%, however, if we open more than two tabs on any one machine to the server, the script begins slowing down proportionally, as if the web server (Apache) is queuing the commands instead of running them in parallel. We've found that if we open two tabs on many machines, it scales up as expected. In order to either saturate our uplink or put any noticeable load on the server, we need to fill the room with laptops. While comical, this is also highly impractical and generally a pain in the ass. There has gotta be a better way here. I've tried increasing the max spare processes, and requests per child, etc etc 10 fold and there was no noticeable increase in speed. What are we missing? How do I nicely tell Apache to temporarily let anyone connect as many times as they like and go nuts? PHP 5.3.8 PHP Sapi: apache2handler PHP loaded extensions: Core date ereg libxml openssl pcre sqlite3 zlib bcmath calendar ctype curl dom fileinfo filter ftp gd hash iconv SPL json mbstring mysql session standard posix Reflection Phar SimpleXML sockets SQLite imap tokenizer xml xmlreader xmlwriter zip apache2handler MYSQL: MySQL 5.0.92-community Thanks for reading!
563e338861a8013065267f91	X	Most web browsers have a limit on the number of concurrent connections they'll make to a given web server. I'm not sure about all browsers, but one browser I know that it can be configured for is Mozilla Firefox, using the Fasterfox extension: https://addons.mozilla.org/en-US/firefox/addon/fasterfox-9148/ That being said -- is there really no way you can write a script to do the migration on the server, rather than running it through a web interface??
563e338861a8013065267f92	X	I am working on application in which users will upload huge number of images and i have to show those image webpage What is the best way to store and retrieve images. 1) Database 2) FileSystem 3) CDN 4) JCR or something else What i know is Database: saving and retrieving image from database will lead to lot of queries to database and will convert blob to file everytime. I think it will degrade the website performance FileSystem: If i keep image information in database and image file in filesystem there will be sync issues. Like if i took a backup of the database we do have take the backup of images folder. ANd if there are millions of files it will consume lot of server resources i read it here http://akashkava.com/blog/127/huge-file-storage-in-database-instead-of-file-system/ Another options are CDNs and JCR Please suggest the best option Regards
563e338861a8013065267f93	X	Using the File System is only really an option if you only plan to deploy to one server (i.e. not several behind a load balancer), OR if all of your servers will have access to a shared File System. It may also be inefficient, unless you cache frequently-accessed files in the application server. You're right that storing the actual binary data in a Database is perhaps overkill, and not what databases do best. I'd suggest a combination: This combination of Database and shared publicly-accessible storage is probably a good mix, giving you the best of both worlds. The Database also means that if you need to move / change or bulk delete images in future (perhaps deleting all images uploaded by an author who is deleting their account), you can perform an efficient Database query to gather metadata, followed by updating / changing the stored images at the S3 locations the Database says they exist. You say you want to display the images on a web page. This combination means that the application server can query the database efficiently for the image selection you want to show (including restricting by author, pagination, etc), then generate a view containing images referring to the correct CDN path. It means viewing the images is also quite efficient as you combine dynamic content (the page upon which the images are shown) with static content (the image themselves via the CDN).
563e338861a8013065267f94	X	CDNs may be a good option for you. You can store the link to the images along with the image information in your database.
563e338961a8013065267f95	X	I can't really help because I don't know much about S3's "protocol", but have had a look to the java library JetS3t? You could try to look how they did it there. jets3t.s3.amazonaws.com/downloads.html
563e338961a8013065267f96	X	@thibaultd I have looked at various other libraries and from what I can see, my class is pretty much the same. Just can't seem to find the issue
563e338961a8013065267f97	X	I made some unit tests based on docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html and by the time my code passed them, everything was working.
563e338961a8013065267f98	X	This looks like the way to go, at least using AmazonAuthUtils for the signing functions
563e338961a8013065267f99	X	I'm pretty sure the ordering is correct? Line 80 - 96 of S3Lite.m
563e338961a8013065267f9a	X	Sorry, my bad, you are using the correct ordering. I'll update my answer. Check out the second part anyway as its possible that might be the cause why you get a different signature than the one S3 generates.
563e338961a8013065267f9b	X	Ok, this is driving me nuts. I'm trying to create a simple AWS S3 client that would allow for basic interaction with S3, but it seems I'm doing something wrong and can't figure out what it is. It might be blatently obvious, but I'm not seeing it. My keys are correct and have been tested - no trailing whitespace etc. The issue seems to be with the signature, it keeps getting the 'the request signature we calculated does not match the signature you provided. Check your key and signing method' error from Amazon's REST API. I've create various categories that add the base64, HMAC SHA1 generation functionality and I've also looked through various online examples, but no success so far. The reason for not using the library provided by Amazon is because it's aimed at Cocoa Touch and I don't want to hack around to make it work on Cocoa. You can grab a copy of all the files/code here: https://www.dropbox.com/s/8ts9q71dz3uopxp/S3Lite.zip I am however following Amazon's documentation around authentication and to my simple mind, everything is being done correctly. Here's how I'm generating the signature: Here's how you go about working with it to attempt to perform a simple PUT request. Any help in the right direction would be greatly appreciated. S
563e338a61a8013065267f9c	X	Even if you aren't able to use the AWS SDK for iOS directly, it is open source, and you might get some ideas from examining the request signing code here: https://github.com/aws/aws-sdk-ios/blob/master/src/Amazon.S3/AmazonS3Client.m#L581
563e338a61a8013065267f9d	X	You need to make sure you include empty values in that string when the corresponding header is missing from the request (e.g. Content-MD5 is optional for PUT requests and meaningless for GET requests - you should only include its value in the string you're signing if your request includes that header in the API call to S3).
563e338a61a8013065267f9e	X	At the moment I am developing a S3 Client Framework based on AFNetworking 1.0 (due to compatibility with older operating systems). The framework itself is still in development but all the request signing methods for AWS4-HMAC-SHA256 are already implemented and working. You can find the framework on github: https://github.com/StudioIstanbul/SIAFAWSClient Feel free to fork it and implement your functions so we can work on it together. So far listing buckets and upload are implemented. Currently I am working on implementing BucketLifeCycle operations. Of course you can also just copy my -(NSString*)AuthorizationHeaderStringForRequest:(NSMutableURLRequest*)request function to your own code. The AWS documentation on this is not very good at the moment as it lacks some steps in creating the signing keys.
563e338a61a8013065267f9f	X	My goal is not a large scalable system, just a simple technology stack. I'm not growing a DB, Search, crawler, etc. Just a simple request, query, respond, and store. Any recommendations for technology stack for my purpose?
563e338b61a8013065267fa0	X	From what I've seen, you can build scalable system with any technology stacks. "thousands to ten-thousands of queries per second" is really high, so for me it's a "large scalable system". Any technology stacks have their success story. If you want to support this charge, you need to read this website. (and maybe consider using a key/value store as CouchDB instead of a relational database)
563e338b61a8013065267fa1	X	I'm building a webservice that is going to be under ridiculous load (thousands to ten-thousands of queries per second). My normal stack of apache, PHP, memcache and some DB will be able to handle it with a nice load balancer infront and lots of machines, but I'm wondering if there are better solutions. The endpoint will be hit by a beacon (via javascript on the client), I'll read the user's cookies, pull some small info on them from the DB, cache it, do some small calculation, send the response and if needed write to the DB and invalidate the cache. And good technology choices and/or hardware recommendations?
563e338b61a8013065267fa2	X	This isn't the kind of question that can be answered here in anything other than a broad overview. Some general pointers: As for your normal stack, I'd stick with it unless you've got a particular requirement you haven't mentioned that prohibits it. After all PHP is a proven technology that runs 4 or so of the top 20 sites on the Internet (Facebook, Wikipedia, Flickr and I think Yahoo). If it's good enough for them, it's good enough for you. More importantly, you know it. Technology stacks you know trump technology stacks you don't in almost every case. Beware the "greener pasture" trap of the latest hyped-up technology stack. Memcache is good. The other thing you might want to consider adding to the mix is beanstalkd as a distributed work queue processor. One important question to answer is: how well can you partition your application? Applications that easily lend themselves to partitioning are far easier to scale. Those that aren't tend to be modified in some way to make them easier to partition. A good example of this is a simple sharetrading application. You can could partition market information based on stock code (A-C on one server, D-F on another and so on). For many such application that will work well.
563e338b61a8013065267fa3	X	http://highscalability.com/ there is a lot to learn here, you'll probabily find your response.
563e338b61a8013065267fa4	X	You can also consider using BigPipe to boost your performance. Facebook is also using it massively and here is what they say about it : "To exploit the parallelism between web server and browser, BigPipe first breaks web pages into multiple chunks called pagelets. Just as a pipelining microprocessor divides an instruction’s life cycle into multiple stages (such as “instruction fetch”, “instruction decode”, “execution”, “register write back” etc.), BigPipe breaks the page generation process into several stages: Request parsing: web server parses and sanity checks the HTTP request. Data fetching: web server fetches data from storage tier. Markup generation: web server generates HTML markup for the response. Network transport: the response is transferred from web server to browser. CSS downloading: browser downloads CSS required by the page. DOM tree construction and CSS styling: browser constructs DOM tree of the document, and then applies CSS rules on it. JavaScript downloading: browser downloads JavaScript resources referenced by the page. JavaScript execution: browser executes JavaScript code of the page. The first three stages are executed by the web server, and the last four stages are executed by the browser. Each pagelet must go through all these stages sequentially, but BigPipe enables several pagelets to be executed simultaneously in different stages."
563e338b61a8013065267fa5	X	tornado looks like something I would try on this kind of problems http://bret.appspot.com/entry/tornado-web-server at least you know it's a tried and tested solution.
563e338b61a8013065267fa6	X	I can contribute a good component for your stack: MemCache.
563e338b61a8013065267fa7	X	hey James, i using a zencoder and and i need to ask some questions?
563e338b61a8013065267fa8	X	We've currently developed an ExpressionEngine site (php), and are using a paid JWPlayer to display video uploaded by the client. At present we're running into a number of issues, as the client is: And the player is chugging along terribly with multiple pauses throughout the video - sometimes buffering the entire clip before it is played. I know FFMPEG can be installed serverside, but I'm not sure of the way in which to go about this, and how it might interact between ExpressionEngine and JWPlayer. I'm also not sure about the formatting - the ability for this automated encoding process to also crop/resize the video to suit the player dimensions on the site. We would really like to have the videos playable on all browsers & iOS devices. A HQ option would also be great where applicable, but it's just a nice to have - as we're struggling with the formatting / encoding issues first and foremost. Any help figuring out the best process, and what tools I might need would be greatly appreciated.
563e338b61a8013065267fa9	X	I'd reccomend using a service like zencoder I've used them in the past and no matter what video format I've thrown at them it works great. (PS. I'm not affiliated with them at all) There is a PHP API with a whole lot of resizing, quality and format options. After you've uploaded your video you can send it to zencoder and they'll send you a response some time later with success or fail. They can put the processed video on Amazon S3 or FTP it to a server. You'll need a HTML5 player for iOS devices though, unless JWPlayer has come a long way since I used it last. You could get zencoder to output in mp4. and then you still only need mp4 for JWPlayer/flash and the HTML5 version for iOS, as long as your happy to use flash for all desktop browsers there's no problem. As far as the buffering issues you are having - I have found that using a CDN version of the swf for JWPlayer (or whatever player you are using) has caused it to load the entire video file before playing. Easily fixed by hosting it yourself.
563e338b61a8013065267faa	X	I have found many times the video conversion capabilities of different CMS to be limited, and often restricting video formats to what the developers thought was appropriate, such as FLV, which nowadays is turning obsolete for video delivery. One of the ways you can approach it is by creating a custom script to process the videos uploaded by your client using FFmpeg, which in fact can accept almost any video format, and generate the correct output formats and dimensions, ensuring that the resulting videos will be suitable for web playback using your player. The problem with the video buffering you are facing is because the video file is not prepared for progressive download or pseudo-streaming, so your browser needs to download the whole video before starting to play. This can be solved with programs like qt-faststart for MP4 and MOV video files, and flvtool2 for FLV files. So your script would need to also optimize the encoded videos using these tools. Also note that if you use an HTML5 video player (browser native or recent JWPlayer), then you can enjoy from random seeking the video files without buffering them. If starting from scratch is not an option, you can look into a commercial solution like tremendum transcoder which also uses FFmpeg and is quite simple to use, yet it does all you need in regards to dealing with different input formats and aspect ratios automatically. I have done a few setups this way, separating the CMS part from the video processing part, and it saved me some headaches.
563e338c61a8013065267fab	X	ah - I just realised this question is old :) yeah there's no "forum programming" any more, it's a couple of lines of code in parse
563e338c61a8013065267fac	X	Thanks - what was the basic forum system you used? Was it off the shelf, or custom? Would love to know more about possible Wordpress solutions.
563e338c61a8013065267fad	X	Well, I made my own simple forum by making MySQL tables and writing PHP scripts to return/add/edit information in them- I didn't bother to build a web interface, as I knew it would be used for an iPhone app. However, if you'd prefer a simpler way, Wordpress might be the way to go. There definitely are Wordpress forum plugins out there... but I'm currently searching for one now that would be able to communicate with $_POST or $_GET requests from an iPhone app.
563e338c61a8013065267fae	X	Of course, this is all assuming that you would be more comfortable with PHP and/or other web languages... if you'd rather skip the server-side scripting and communicate directly with Objective-C from the app to your database, you could. However, I wouldn't recommend that method as such things can get pretty complicated and confusing.
563e338c61a8013065267faf	X	I'm not too comfortable with PHP/MySQL, but I guess I need to start somewhere. Perhaps I could adapt something like this: phpeasystep.com/workshopview.php?id=12
563e338c61a8013065267fb0	X	Yeah- in fact, it would be easier than that, because you would only need to do the part of creating the databases, as you would skip all the PHP/HTML to displaying it to a page. Then, you could make a PHP scripts to return/edit/remove forum information.
563e338c61a8013065267fb1	X	I'm not familiar at all with the Google App Engine - are there any existing setups which approximate my needs? XML - yes, I've heard plists are quicker to parse on the iPhone. However, there's a possibility that there may be Android versions, so XML looks like the best option. I'm reasonable familiar with NSXMLParser too.
563e338c61a8013065267fb2	X	I don't know if there is anything that you can use out of the box for your needs. However, it is very easy to get started and develop a reasonable application will moderate effort.
563e338c61a8013065267fb3	X	I'm not really a server-side person – I generally do iPhone apps, though I've hacked together a few Wordpress sites. I'm curious as to what web technologies people would use for the back-end of an iPhone app whose front end presents as a basic forum. In other words, people can create new threads, and respond to them - with plain text only. The forum would not exist as a website.. the only way to access it would be on the phone. What technology would people recommend I use? Ruby-on-Rails with Amazon S3 storage? Could I even use existing forum software and pass and receive data to and from it? Perhaps even a forum Wordpress plug-in? Or is there a better way?
563e338c61a8013065267fb4	X	If you wanted to, you could use existing forum software and/or Wordpress to facilitate what you want, which would be easier than building your own forum from scratch. You could, with that existing framework, set up your own little API to communicate from the iPhone app to the server- for example, send a $_GET request to a PHP script on your server, which would return a list of forum topics. You could have similar PHP scripts that could do similiar functions, like adding a post or deleting topics. That's pretty much how I've got it set up on an iPhone app I recently made- my server has a basic forum system, and I just wrote a couple of PHP scripts to return information from a MySQL server. However, if you'd particularly prefer to use Wordpress/Amazon S3/whatever else, then I could give more specific instructions relating to those services. *EDIT* Here's an example PHP script you could use (after you've created databases): forumcategories.php <?php // insert database connecting logic here $query = mysql_query("SELECT * from categories"); echo "<categories">; while($row=mysql_fetch_array($query)){ echo "<category><id>" . $row['id'] . "</id><title>" . $row['title']; . "</title></category>;" } echo "</categories>"; ?> This is a really simple script- of course, you would need to add in code to connect to the database (which can be found easily online) and probably some error checking, but other than that, it will do the trick. What you would do in the iPhone app is send a request to http://yourserver/forumcategories.php and it would return XML listing all of the categories, which can easily be parsed with NSXMLParser and placed into a UITableView, for example.
563e338c61a8013065267fb5	X	Google App Engine is very good for what you describe. There are a lot of advantages to this approach: choice between Java and Python, access to the Google Accounts API, persistence/datastore APIs,... and you don't have to setup much to start working. I also recommend that your server app returns responses formatted according to Apple's XML Property List format, instead of any other XML or JSON format. You can avoid NSXMLParser (or any other parser) altogether and save time to use on other important stuff.
563e338c61a8013065267fb6	X	it's almost inconceivable you would do this by hand these days .. just use Parse.com or any bAAs competitor It just -- it would be unbelievable, incredible, to reprogram it from scratch. (regarding "forums" or the like, it is so simple to do in parse, they have as demo projects a number of full, completely working, example apps(both platforms) of that type of thing)
563e338c61a8013065267fb7	X	Not relevant, but what you are having are called questions, not doubts.
563e338c61a8013065267fb8	X	Quick question, are you sure you want to store all of your files to the Database? It might be easier to store them in a regular File System, and just save the file information in your EJB. That way, you don't have to deal with Database BLOBS or anything like that, and your EJB gets much less complicated.
563e338d61a8013065267fb9	X	That is a recommendation somebody told me before, and probably i would prefer to do it that way, in the case is easier.I think also the app will retrieve them faster to the UI when needed. If i do that i will just need to find a way to create unique identifiers as file names right?
563e338d61a8013065267fba	X	This is not going to work nicely with JSF without much hassle. Why don't you just use Tomahawk's <t:inputFileUpload> as suggested in one of your many previous questions about this subject: stackoverflow.com/questions/6537989/…
563e338d61a8013065267fbb	X	Rec'd for better answers.
563e338d61a8013065267fbc	X	I want to create a servlet that will allow me to upload image files from the client to the server. I am helping myself with the tutorial i found at apache site: http://commons.apache.org/fileupload/using.html On my way i am finding some complications and doubts: Question 1 I would like my servlet to prepare an object with all the values from the request(included images as byte[]) and pass it to an @EJB that will insert all in the database. Is that possible? Could you give some pseudo code tips on how to improve my current servlet to do that? Question 2 I thought about passing the request to the servlet through the managaged bean, instead from the JSF page, but i don't really know how to do it. Could you give me some tips? I also don't know how to do it in the normal way, from the page,what do you think would be the best way? This is what i did so far regarding to the managed bean: //What else do i need here to pass the request to the server? } This would be at the page inside a multipart form: Question 3 In my JSF page i have more or less 10 values almost all are Strings. I take them from the JSF and temporary store them in the JSF page. If the servlet could take all the values from the request, there would be no need for this attributes in the backing bean. Do you think is this approach a good thing to do? Will this be process transaction secure, or is there any risk?
563e338d61a8013065267fbd	X	Question 1- Absolutely you will need Unique Identifiers for your files, but that becomes less complicated if you do things like storing files in folders by date/username, etc... Here is a basic workflow for your program that you could use, based on what you have shown so far: Client computer -> FileUploadServlet (utilizing Apache Commons File Upload) Once inside the FileUploadServlet: a) Save the information from the request to a Database by way of your EJB including the file name, Mime Type, information, etc... b) While still inside the servlet, upload the file to your server, or if you need to, use a commercial solution such as Amazon S3 or Google Storage (by way of a Java API such as JetS3t) Afterwards, return any necessary information to the client. Question 2 - What is your reasoning for requesting throught the Bean, why not just make the Servlet the action instead, and collect the information from the request? I would not make the Bean's save method available on the JSF, as that sounds insecure and un-authenticated. Question 3 - Same as above, why store information, even if temporarily, when it is available elsewhere?
563e338d61a8013065267fbe	X	Doubt #1 : It looks like you use the EJB here as a service layer containing a DAO annotated with EJB annotations to make it a session bean. I do not like this approach and you'll run into issues caused by the difference of the EJB world and the HTTP request world. It is important to note that one of the biggest reasons to use EJB's is to manage transactions and transaction have to remain short, in the order of ms. This is for a number of reasons, like for example locks on the database. However when dealing with http requests with uploads, this is no longer valid. From another perspective is that a service layer should represent an abstraction of the database model and should show what you can do with the model from a user perspective. The user does not want to save an image to the database, the user wants to add a portrait to his profile. instead of I prefer functions like This explicitely states what it does, and also satisfies the requirement of short transactions. This means the profile entity is available for other requests which may come concurrently (like some status image, or inbox status, ...) Doubt #2 : When in Rome, do as the Romans do... and start by learning some basics of the language. In this case learn JSF from some tutorials. Doubt #3 : Intercepting the request parameter in flight between the browser and the JSF component breaks the component architecture and the data hiding principle. It will also bypass any security measures and validation implemented in the server side parts of the JSF components. If you use the JSF component framework, it makes sense to only ask the values from the components, not from the request itself. From your 3 doubts I feel you have a bigger doubt : Should I be using JSF? If it is mandated by your employer : suck it up, and start hitting the books... Learn which problems JSF and EJB's solve and frame your work in terms of those problems. If you have the freedom to choose : choose a lighter framework, e.g. Spring + Spring MVC. You'll gain experience and encounter those problems at your own pace.
563e338d61a8013065267fbf	X	Ive recently been experiementing with using django I want to be able to run an uploaded file through imagemagick on the model save function, I realised this wasn't possible due to the file not being available till the instance had been saved I've got it working were the save is called at the start which then allows access to the uploaded file then re saves once its populated the other fields Is there a better way to achieve this? it just kinda feels wrong calling the save function twice
563e338d61a8013065267fc0	X	It's better if you use the imaging library functions to collect image metadata. Then you just have to read the image data from the FileStorage instance of the image field. Just check for the documentation of file uploads. Also, when using Django's ImageField, it checks the uploaded file to see if is a valid image file, check the code for ImageField as a baseline to obtain more info about an image. I would define an clean method to the model, this will get called just before calling the model's save method. In clean, then check the data stream of the image field with the PIL functions to obtain the metadata (or whatever library you want to use for this, as long as it accepts a data stream, not a physical file) and then fill the description field with the metadata. Using the Django's Storage API is better, so you can extract metadata from files stored in the cloud (like Amazon S3), rather than just those stored in a local filesystem, making the code much more portable across deployments.
563e338d61a8013065267fc1	X	Use Django's pre_save signal (docs, see also Signals in Django's documentation): Django will now call do_something before saving MyModel objects.
563e338e61a8013065267fc2	X	I ended up using Cyberduck where there's a Windows and a Mac version :)
563e338e61a8013065267fc3	X	Using the AWS ruby SDK you can do it in 1 command (bucket.delete!) docs.aws.amazon.com/AWSRubySDK/latest/frames.html#!AWS.html
563e338e61a8013065267fc4	X	just to follow up on this, it worked perfectly, all the files were deleted within 24 hours and we were not even billed for any deletion requests as far as I can see. This is the definitive solution to the problem!
563e338e61a8013065267fc5	X	This I like. Much better than installing tools, finding out they can't cope with some aspect of my bucket, and repeating...
563e338e61a8013065267fc6	X	Just wanted to say thank you for this solution. A simple upvote cannot express how grateful I am. Works brilliantly.
563e338e61a8013065267fc7	X	I've been installing all sorts of command line tools to help delete millions of files from a bucket. This simple solution is the best. Thank you!
563e338e61a8013065267fc8	X	Awesome! Much thanks. One vote up! One small change. When you go in the rules window now, they have a checkbox to explicitly apply the rule to the entire bucket. It will not let you leave the Prefix field blank now.
563e338e61a8013065267fc9	X	I had problems with the aws/s3 gem as I have a european region. Solved by using the s3 gem - github.com/qoobaa/s3
563e338e61a8013065267fca	X	The solution from chris14679 (below) that uses lifecycle expiration rules is now the preferred method.
563e338e61a8013065267fcb	X	This answer has the most upvotes, but it cannot possibly compete with the simplicity of @chris14679 's comment below. I just deleted several million files in under 10 clicks and maybe 10 key strokes. Beautiful simplicity.
563e338e61a8013065267fcc	X	this worked, thank you.
563e338f61a8013065267fcd	X	Beautius, too easy.
563e338f61a8013065267fce	X	Wow. Thanks for the reference to SB. Worked great, and I didn't have to install FireFox to acomplish deleting S3 buckets.
563e338f61a8013065267fcf	X	Didn't work for me for some reason. But great tool for browsing buckets. Maybe add an option "delete everything" as well.
563e338f61a8013065267fd0	X	This is well-implemented code that is working very well for me.
563e338f61a8013065267fd1	X	@curthipster: Thanks. Note that amazon have recently added "object expiration", which renders the code less relevant. See here docs.amazonwebservices.com/AmazonS3/latest/dev/…
563e338f61a8013065267fd2	X	Great tool. I love command line stuff. +1 from me
563e338f61a8013065267fd3	X	@MaximVeksler FYI, the export command on the CLASSPATH is not right. It references "target/dependency/commons-logging-1.1.1.jar" but the actual jar in the dependency is version 1.1.3.
563e338f61a8013065267fd4	X	Now available at: s3fm.com
563e338f61a8013065267fd5	X	Currently doesn't support buckets in the EU though :(
563e339061a8013065267fd6	X	S3Fox and the AWS console don't support deleting all. I was sat there selecting 160 records (I've got about 20,000) for an hour until I got bored and found this question.
563e339061a8013065267fd7	X	Not a great idea if you have thousands of files.
563e339061a8013065267fd8	X	I've been interacting with Amazon S3 through S3Fox and I can't seem to delete my buckets. I select a bucket, hit delete, confirm the delete in a popup, and... nothing happens. Is there another tool that I should use?
563e339061a8013065267fd9	X	It is finally possible to delete all the files in one go using the new Lifecycle (expiration) rules feature. You can even do it from the AWS console. Simply right click on the bucket name in AWS console, select "Properties" and then in the row of tabs at the bottom of the page select "lifecycle" and "add rule". Create a lifecycle rule with the "Prefix" field set blank (blank means all files in the bucket, or you could set it to "a" to delete all files whose names begin with "a"). Set the "Days" field to "1". That's it. Done. Assuming the files are more than one day old they should all get deleted, then you can delete the bucket. I only just tried this for the first time so I'm still waiting to see how quickly the files get deleted (it wasn't instant but presumably should happen within 24 hours) and whether I get billed for one delete command or 50 million delete commands... fingers crossed!
563e339061a8013065267fda	X	Remeber that S3 Buckets need to be empty before they can be deleted. The good news is that most 3rd party tools automate this process. If you are running into problems with S3Fox, I recommend trying S3FM for GUI or S3Sync for command line. Amazon has a great article describing how to use S3Sync. After setting up your variables, the key command is Deleting buckets with lots of individual files tends to crash a lot of S3 tools because they try to display a list of all files in the directory. You need to find a way to delete in batches. The best GUI tool I've found for this purpose is Bucket Explorer. It deletes files in a S3 bucket in 1000 file chunks and does not crash when trying to open large buckets like s3Fox and S3FM. I've also found a few scripts that you can use for this purpose. I haven't tried these scripts yet but they look pretty straightforward. RUBY PERL SOURCE: Tarkblog Hope this helps!
563e339061a8013065267fdb	X	recent versions of s3cmd have --recursive e.g., http://s3tools.org/kb/item5.htm
563e339061a8013065267fdc	X	With s3cmd: Create a new empty directory s3cmd sync --delete-removed empty_directory s3://yourbucket
563e339061a8013065267fdd	X	This may be a bug in S3Fox, because it is generally able to delete items recursively. However, I'm not sure if I've ever tried to delete a whole bucket and its contents at once. The JetS3t project, as mentioned by Stu, includes a Java GUI applet you can easily run in a browser to manage your S3 buckets: Cockpit. It has both strengths and weaknesses compared to S3Fox, but there's a good chance it will help you deal with your troublesome bucket. Though it will require you to delete the objects first, then the bucket. Disclaimer: I'm the author of JetS3t and Cockpit
563e339061a8013065267fde	X	SpaceBlock also makes it simple to delete s3 buckets - right click bucket, delete, wait for job to complete in transfers view, done. This is the free and open source windows s3 front-end that I maintain, so shameless plug alert etc.
563e339061a8013065267fdf	X	I've implemented bucket-destroy, a multi threaded utility that does everything it takes to delete a bucket. I handle non empty buckets, as well as version enabled bucket keys. You can read the blog post here http://bytecoded.blogspot.com/2011/01/recursive-delete-utility-for-version.html and the instructions here http://code.google.com/p/bucket-destroy/ I've successfully deleted with it a bucket that contains double '//' in the key name, versioned key and DeleteMarker keys. Currently I'm running it on a bucket that contains ~40,000,000 so far I've been able to delete 1,200,000 in several hours on m1.large. Note that the utility is multi threaded but does not (yet) implemented shuffling (which will horizontal scaling, launching the utility on several machines).
563e339061a8013065267fe0	X	If you have ruby (and rubygems) installed, install aws-s3 gem with or create a file delete_bucket.rb: and run it: Since Bucket#delete returned timeout exceptions a lot for me, I have expanded the script:
563e339061a8013065267fe1	X	I guess the easiest way would be to use S3fm, a free online file manager for Amazon S3. No applications to install, no 3rd party web sites registrations. Runs directly from Amazon S3, secure and convenient. Just select your bucket and hit delete.
563e339061a8013065267fe2	X	One technique that can be used to avoid this problem is putting all objects in a "folder" in the bucket, allowing you to just delete the folder then go along and delete the bucket. Additionally, the s3cmd tool available from http://s3tools.org can be used to delete a bucket with files in it:
563e339161a8013065267fe3	X	If you use amazon's console and on a one-time basis need to clear out a bucket: You can browse to your bucket then select the top key then scroll to the bottom and then press shift on your keyboard then click on the bottom one. It will select all in between then you can right click and delete.
563e339161a8013065267fe4	X	I hacked together a script for doing it from Python, it successfully removed my 9000 objects. See this page: https://efod.se/blog/archive/2009/08/09/delete-s3-bucket
563e339161a8013065267fe5	X	One more shameless plug: I got tired of waiting for individual HTTP delete requests when I had to delete 250,000 items, so I wrote a Ruby script that does it multithreaded and completes in a fraction of the time: http://github.com/sfeley/s3nuke/ This is one that works much faster in Ruby 1.9 because of the way threads are handled.
563e339161a8013065267fe6	X	This is a hard problem. My solution is at http://stuff.mit.edu/~jik/software/delete-s3-bucket.pl.txt. It describes all of the things I've determined can go wrong in a comment at the top. Here's the current version of the script (if I change it, I'll put a new version at the URL but probably not here).
563e339161a8013065267fe7	X	I am one of the Developer Team member of Bucket Explorer Team, We will provide different option to delete Bucket as per the users choice... 1) Quick Delete -This option will delete you data from bucket in chunks of 1000. 2) Permanent Delete-This option will Delete objects in queue. How to delete Amazon S3 files and bucket?
563e339161a8013065267fe8	X	Amazon recently added a new feature, "Multi-Object Delete", which allows up to 1,000 objects to be deleted at a time with a single API request. This should allow simplification of the process of deleting huge numbers of files from a bucket. The documentation for the new feature is available here: http://docs.amazonwebservices.com/AmazonS3/latest/dev/DeletingMultipleObjects.html
563e339161a8013065267fe9	X	I've always ended up using their C# API and little scripts to do this. I'm not sure why S3Fox can't do it, but that functionality appears to be broken within it at the moment. I'm sure that many of the other S3 tools can do it as well, though.
563e339161a8013065267fea	X	Delete all of the objects in the bucket first. Then you can delete the bucket itself. Apparently, one cannot delete a bucket with objects in it and S3Fox does not do this for you. I've had other little issues with S3Fox myself, like this, and now use a Java based tool, jets3t which is more forthcoming about error conditions. There must be others, too.
563e339161a8013065267feb	X	You must make sure you have correct write permission set for the bucket, and the bucket contains no objects. Some useful tools that can assist your deletion: CrossFTP, view and delete the buckets like the FTP client. jets3t Tool as mentioned above.
563e339161a8013065267fec	X	I'll have to have a look at some of these alternative file managers. I've used (and like) BucketExplorer, which you can get from - surprisingly - http://www.bucketexplorer.com/. It's a 30 day free trial, then (currently) costing US$49.99 per licence (US$49.95 on the purchase cover page).
563e339161a8013065267fed	X	Try https://s3explorer.appspot.com/ to manage your S3 account.
563e339261a8013065267fee	X	I'm trying to set up a mechanism for easily pushing out changes from our (local) development/testing server to multiple endpoints in the cloud. The application is an ASP.NET web app that is version controlled using Mercurial. There are some challenges which prompted this: The system that I'm envisioning would be capable of the following: So, basically I was envisioning a client/server combination that would sit on every server and wait for update commands. The speed problem can be remedied using Amazon's S3, it seems. For some reason, uploading to S3, then downloading from within the cloud is much faster than uploading directly to the cloud. Ideally, we'd use their API in the app. The question is, are there tools that already do what I'm attempting to accomplish? And if not, what would be the best way to communicate the "update" command to the other servers?
563e339261a8013065267fef	X	In the end, we built a small client/server app that listens for remote commands and executes push/pull/update/patch, and anything else that might be necessary. To get around the throttling issue, we ended up using Amazon S3, as we had suspected we would. For local communication (we have a few local servers still), we built a simple TCP-based transfer method, but resorted to S3 when communicating with the cloud. It was built using the AWS SDK.
563e339261a8013065267ff0	X	I want to create a new user in IAM, and allow him to be able to create new EC2 instances, but be able to view/administer only those instances that he creates. Is this possible with IAM? This is the group policy I tried:
563e339261a8013065267ff1	X	Unfortunately this isn't yet available, at least not in the automatic fashion you are probably looking for - your use case has two aspects: The recent introduction of Resource-Level Permissions for EC2 and RDS Resources finally allows to constrain Amazon EC2 API actions to specific instances indeed, thus enabling your use case from that angle, e.g.: The Example IAM Policies feature one showing how to Allow a user all actions on an Amazon DynamoDB table whose name matches the user name, demonstrating the use of the policy variable ${aws:username} (see IAM Policy Variables Overview for details): This resource-level permissions aren't available for all API actions yet: This is a complex and far-reaching feature and we'll be rolling it out in stages. In the first stage, the following actions on the indicated resources now support resource-level permissions: EC2 actions not listed above will not be governed by resource-level permissions at this time. We plan to add support for additional APIs throughout the rest of 2013. Notably it lacks the ec2:Describe*actions you seem to be looking for as well. However, AWS has yet to publicly release any kind of auditing feature (which must be available internally due to the way Amazon IAM works), which means that there is no option to find out which particular IAM user has created a specific resource. As expected, AWS has meanwhile released AWS CloudTrail, which is a web service that records AWS API calls for your account and delivers log files to you: The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the AWS service. See my related answer to Logs for actions on amazon s3 / other AWS services for a few details and initial constraints. I'm not aware of any self contained workaround - within a cooperative environment, you might be able to approximate what you want by applying respective monitoring and automation as follows: 1) you need to mandate that users only ever run EC2 instances with some kind of tagging scheme, e.g. owner=<username> 2) with that scheme in place, you could apply a ${aws:username} based policy as outlined above, resp. a slight variation based on tags - the AWS security blog has a comprehensive post Resource-level Permissions for EC2 – Controlling Management Access on Specific Instances illustrating this approach - your policy might look as follows: 3) please note that this means a user won't be able to manage his instances if he forgets to start them with the correct tag, so in addition you could use something like the Netflix' Conformity Monkey to enforce the policy on a heuristic basis, i.e. once an instance is detected without the required tag, whoever is in charge gets a notification and can try to enforce this by inquiring the users or shutting down the instance (which could also be done automatically of course).
563e339261a8013065267ff2	X	I have a site that uses OpenSeadragon to display deep zoom images. I have it working with some test images, but the images I need are on a domain that I cannot access from my webpage due to a security issue (No "Access-Control-Allow-Origin" header). For the record, the page hosting the DZI images is owned by my company, but because it is an Amazon S3 site, I am unable to add the header to the site because Amazon does not provide that capability. I've created a Proxy controller, and the controller successfully gets the XML data it needs. Here is my controller code: The controller returns the XML data correctly. For example, when I go to http://mysite.mvc/api/test?bucket=66&guid=e41de95d-6235-4581-b823-4887b50eb8ad, I get a page with correct looking XML data. I have also tested this on the DHC chrome extension. On my webpage, I make an Ajax call to the proxy controller, and use the returned XML to open Seadragon: The alert in my Ajax success function displays the XML I expect. However, in my Seadragon viewer, the tiles do not display (but the viewer is open, and the nav buttons are there). In my console, I have this response: for every tile in the image. I suspect this is because my Seadragon viewer is being opened with static XML that is not actually linked to the webpage where it is from, but I have no idea what to do about this. Is there something I can do to fix it, or is my Proxy controller simply not going to work? And if it doesn't work, what else can I do to display these images? edit: another thought I have is that maybe the tilesources aren't loading because the data is being passed in as a string and not as an XML document? I also tried this instead of OpenSeadragon() but got a 400 bad request error in my Seadragon viewer. I also tried slicing off the heading of the XML by using data.substring(38), but same error. My XML looks like this:
563e339361a8013065267ff3	X	Unfortunately OpenSeadragon does not yet support passing the XML directly; you'll have to break apart the info. See the answer here: https://github.com/openseadragon/openseadragon/issues/460
563e339461a8013065267ff4	X	how do you run this, running directly from s3? can you show the code that call elb.describeInstanceHealth(elbRequestParams).on('success', describeInstanceHealthCallback).send();
563e339461a8013065267ff5	X	Yes, it runs directly from S3. It's quite difficult to un-pick the specific code but it's essentially just define elb to be an instance of AWS.ELB() then call that line. If I disable CORS in my browser that line successfully completes as expected. Is there some specific code you'd like to see? The callback perhaps?
563e339461a8013065267ff6	X	CORS issue is annoying for sure, and they are not syntax error that is right or wrong, they can be different. Can you show the callback code and also full request header/detail from chrome devtool
563e339461a8013065267ff7	X	For clarity; AWS must support CORS header for ELB for my requests to work? If this is the case, is there a way to check that easily? Trying to find request/header detail now.
563e339461a8013065267ff8	X	I think it is something to do with the region. what region is your s3 bucket and what region is your elb? non US region will have trouble using domain-style to access s3. you see the request elasticloadbalancing.eu-west-1.amazonaws.com/(domain style eu region) is redirect to aws.amazon.com/elasticloadbalancing (US region). no harm trying changing region setting.
563e339461a8013065267ff9	X	I'm working on a ELB monitoring application that I'd like to build with JS and host directly in S3. I have no experience of JS and I'm struggling to get to grips with CORS. I have successfully created a build of the SDK for my browser (http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-building.html) which includes ELB API support. I have integrated Amazon login (http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-configuring-wif.html) with my script and I have attached it to an IAM WebIdentity role. When I call I get an error in Javascript console I can't find any clear documentation that what I'm trying to do won't work, but, I'll admit I'm confused by some of the terms in the documentation and I have no experience of CORS in previous applications to fall back on. I would think the whole JS-SDK for the browser is a bit redundant if the majority of services aren't CORS aware and need to be. I can get this working by disabling web-security in my Chrome browser, obviously this isn't a good workaround but would indicate to me that CORS is the issue. Is anyone familiar with this approach, is it a problem that I'm hosting on S3, or, would I have this problem from any server? Should I be configuring my S3 bucket with CORS, or, is it that there's no CORS policy on the 'elasticloadbalancing.eu-west-1.amazonaws.com' endpoint? Thanks Andrew
563e339461a8013065267ffa	X	I have successfully created a build of the SDK for my browser (http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-building.html) which includes ELB API support. The referenced page already provides the first clue towards the lack of CORS support for elasticloadbalancing.eu-west-1.amazonaws.com you correctly suspect to be the cause here: If you are working with the SDK outside of an environment that enforces CORS in your browser and want access to the full gamut of services provided by the AWS SDK for JavaScript, it is possible to build a custom copy of the SDK locally by cloning the repository and running the same build tools used to generate the default hosted version of the SDK. That is, the main reason for the official AWS SDK for JavaScript in the Browser excluding all but the currently 5 services listed in section Supported Services within Working with Services in the Browser is their lack of CORS support: It is possible to use the SDK with other services if CORS security checking is disabled in your environment. In this case, you can build your own custom version of the SDK. See the Building the SDK section of the guide for more information on how to create a custom build of the SDK. Unfortunately neither Amazon EC2 nor Elastic Load Balancing currently offers CORS support, see e.g. this recent Feature Request (CORS support for EC2 service), where the author rightfully reaches the same conclusion as you did already (while also hinting on Node.js to be another major use case, which supports all available AWS services, see Working with Services in Node.js): From my point of view if AWS is providing a aws-sdk-js library all AWS services supported by this library should support CORS so that the aws-sdk-js iibrary is not just useable in a node.js environment but also in a browser. While I agree in principle and would have expected faster turnaround times on this myself, AWS is well known for an agile approach to product development, i.e. start early with a small feature set and improve over time based on customer feedback. Notably the SDK is labeled Developer Preview only, i.e. not even BETA yet and I'd hope this to be addressed in the future accordingly - admittedly this is easily overlooked and a more prominent warning might go a long way in sparing users the time to figure out this limitation by themselves.
563e339661a8013065267ffb	X	more specifically (and run using hadoop jar code.jar {classname}) org.apache.hadoop.fs.FileSystem.get(new org.apache.hadoop.mapred.JobConf()).create(new org.apache.hadoop.fs.Path("however.file"));
563e339661a8013065267ffc	X	I need to write data in to Hadoop (HDFS) from external sources like a windows box. Right now I have been copying the data onto the namenode and using HDFS's put command to ingest it into the cluster. In my browsing of the code I didn't see an API for doing this. I am hoping someone can show me that I am wrong and there is an easy way to code external clients against HDFS.
563e339661a8013065267ffd	X	Install Cygwin, install Hadoop locally (you just need the binary and configs that point at your NN -- no need to actually run the services), run hadoop fs -copyFromLocal /path/to/localfile /hdfs/path/ You can also use the new Cloudera desktop to upload a file via the web UI, though that might not be a good option for giant files. There's also a WebDAV overlay for HDFS but I don't know how stable/reliable that is.
563e339661a8013065267ffe	X	There is an API in Java. You can use it, by including the Hadoop code in your project. The JavaDoc is quite helpful in general, but of course you have to know, what you are looking for *g * http://hadoop.apache.org/common/docs/ For your particular problem, have a look at: http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/fs/FileSystem.html (this applies to the latest release, consult other JavaDocs for different versions!) A typical call would be: Filesystem.get(new JobConf()).create(new Path("however.file")); Which returns you a stream you can handle with regular JavaIO.
563e339661a8013065267fff	X	For the problem of loading the data I needed to put into HDFS, I choose to turn the problem around. Instead of uploading the files to HDFS from the server where they resided, I wrote a Java Map/Reduce job where the mapper read the file from the file server (in this case via https), then write it directly to HDFS (via the Java API). The list of files is read from the input. I then have an external script that populates a file with the list of files to fetch, uploads the file into HDFS (using hadoop dfs -put), then start the map/reduce job with a decent number of mappers. This gives me excellent transfer performance, since multiple files are read/written at the same time. Maybe not the answer you were looking for, but hopefully helpful anyway :-).
563e339761a8013065268000	X	About 2 years after my last answer, there are now two new alternatives - Hoop/HttpFS, and WebHDFS. Regarding Hoop, it was first announced in Cloudera's blog and can be downloaded from a github repository. I have managed to get this version to talk successfully to at least Hadoop 0.20.1, it can probably talk to slightly older versions as well. If you're running Hadoop 0.23.1 which at time of writing still is not released, Hoop is instead part of Hadoop as its own component, the HttpFS. This work was done as part of HDFS-2178. Hoop/HttpFS can be a proxy not only to HDFS, but also to other Hadoop-compatible filesystems such as Amazon S3. Hoop/HttpFS runs as its own standalone service. There's also WebHDFS which runs as part of the NameNode and DataNode services. It also provides a REST API which, if I understand correctly, is compatible with the HttpFS API. WebHDFS is part of Hadoop 1.0 and one of its major features is that it provides data locality - when you're making a read request, you will be redirected to the WebHDFS component on the datanode where the data resides. Which component to choose depends a bit on your current setup and what needs you have. If you need a HTTP REST interface to HDFS now and you're running a version that does not include WebHDFS, starting with Hoop from the github repository seems like the easiest option. If you are running a version that includes WebHDFS, I would go for that unless you need some of the features Hoop has that WebHDFS lacks (access to other filesystems, bandwidth limitation, etc.)
563e339761a8013065268001	X	It seems there is a dedicated page now for this at http://wiki.apache.org/hadoop/MountableHDFS: These projects (enumerated below) allow HDFS to be mounted (on most flavors of Unix) as a standard file system using the mount command. Once mounted, the user can operate on an instance of hdfs using standard Unix utilities such as 'ls', 'cd', 'cp', 'mkdir', 'find', 'grep', or use standard Posix libraries like open, write, read, close from C, C++, Python, Ruby, Perl, Java, bash, etc. Later it describes these projects I haven't tried any of these, but I will update the answer soon as I have the same need as the OP
563e339761a8013065268002	X	You can now also try to use Talend, which includes components for Hadoop integration.
563e339761a8013065268003	X	you can try mounting HDFS on your machine(call it machine_X) where you are executing your code and machine_X should have infiniband connectivity with the HDFS Check this out, https://wiki.apache.org/hadoop/MountableHDFS
563e339761a8013065268004	X	You can also use HadoopDrive (http://hadoopdrive.net). It's a Windows shell extension.
563e339761a8013065268005	X	I am in process of hosting a dynamic website on Amazon EC2. I have created the environment and deployed war on ElasticStalkBean. I can connect to mysql database too. But I am not sure how my web application will read/write to the disk and at which path? As per my understanding, Amazon provides 3 options for file storage S3 EBS (Persistant) instance storage I could upload files on s3 creaing bucket but how can my web application read or write to S3 bucket path on differnt server? I am not sure how should i upload files or write file to EBS. Connecting to EC2, I cannot cd /dev/sd* directory for my EBS attached to my environment instance. How can I configure my web app to use this as directory for images etc Instance storage is lost if I stop or recreate env. and is non persistant. So not interested to store files here. Can you help me on this? Where to upload file that are read by application? Where can my application write files?
563e339761a8013065268006	X	Your question: "how can my web application read or write to S3 bucket path on different server? I'm a newbie user of AWS too, so can only offer limited help, but this is what I understand: The webapp running in the EC2 instance can access the S3 storage using with the REST or SOAP APIs. Here's the link to the reference guide for using the REST GET function to get a file from S3: GET object documentation I guess the idea is that the S3 storage bucket that Amazon create for your EBS "environments" provides permanent storage for your application and data files (images etc.). When a EC2 instance is created or rebooted, it should get any additional application files from an S3 bucket and 'cache' them on the file system ("volume") attached to the EC2 "instance".
563e339761a8013065268007	X	If you set permissions in advanced, you don't have to audit later j. Currently you can't audit API calls.
563e339761a8013065268008	X	As of the latest update (June 30 2014) they still do not support S3 logging, which was the OP's request and seems like the common one.
563e339861a8013065268009	X	I am trying to see which user was responsible for changes in S3 (at buckets level). I could not find a audit trail for actions done at S3 bucket level or EC2 who created instances. Beanstalk has a log of the actions the machine performed, but not which user. Is there a way around AWS that we can see this information in IAM or any other location ? P.S: I am not interested to know about S3 log buckets which provide access logs
563e339861a801306526800a	X	AWS has just announced AWS CloudTrail, finally making auditing API calls available as of today (and for free), see the introductory post AWS CloudTrail - Capture AWS API Activity for details: Do you have the need to track the API calls for one or more AWS accounts? If so, the new AWS CloudTrail service is for you. Once enabled, AWS CloudTrail records the calls made to the AWS APIs using the AWS Management Console, the AWS Command Line Interface (CLI), your own applications, and third-party software and publishes the resulting log files to the Amazon S3 bucket of your choice. CloudTrail can also issue a notification to an Amazon SNS topic of your choice each time a file is published. Each call is logged in JSON format for easy parsing and processing. Please note the following (temporary) constraints: This is a long standing feature request, but unfortunately AWS does not provide (public) audit trails as of today - the most reasonable way to add this feature would probably be a respective extension to AWS Identity and Access Management (IAM), which is the increasingly ubiquitous authentication and authorization layer for access to AWS resources across all existing (and almost certainly future) Products & Services. Accordingly there are a few respective answers provided within the IAM FAQs along these lines:
563e339961a801306526800b	X	I have been trying to understand how different a map-reduce job is executed on HDFS vs S3. Can someone please address my questions: Typically HDFS clusters are not only storage oriented, but also contain horsepower to execute MR jobs; and that is why the jobs are mapped on several data nodes and reduced on few. To be exact, the mapping (filter etc) is done on data locally, whereas the reducing (aggregation) is done on common node. Does this approach work as it is on S3? As far as I understand, S3 is just a data store. Does hadoop has to COPY WHOLE data from S3 and then run Map (filter) and reduce (aggregation) locally? or it follows exactly same approach as HDFS. If the former case is true, running jobs on S3 could be slower than running jobs on HDFS (due to copying overhead). Please share your thoughts.
563e339a61a801306526800c	X	If you have a Hadoop cluster in EC2 and you run a MapReduce job over S3 data, yes the data will be streamed into the cluster in order to run the job. As you say, S3 is just a data store, so you can not bring the computation to the data. These non-local reads could cause a bottleneck on processing large jobs, depending on the size of the data and the size of the cluster.
563e339b61a801306526800d	X	Performance of S3 is slower than HDFS, but it provides other features like bucket versioning and elasticity and other data recovery schemes(Netflix uses a Hadoop cluster using S3). Theoretically, before the split computation, the sizes of input files need to be determined, so hadoop itself has an filesystem implementation on top of S3 which allows higher layers to be agnostic of the source of the data. Map-Reduce calls the generic file listing API against each input directory to get the size of all files in the directory. Amazons EMR have a special version of the S3 File System that can stream data directly to S3 instead of buffering to intermediate local files this can make it faster on EMR.
563e339b61a801306526800e	X	Any reason for migrating from Django to Grails?
563e339b61a801306526800f	X	I still can't seem to get this working correctly. 1) I know the filters are set up correctly, because in an action, "request" has a string representation of "grails.util.http.MultiReadHttpServletRequest@43e1542f". 2) I've tried generating the request with both wget --post-data='xxx' and curl -d @file, and verified the request is properly formed from both using Wireshark. 3) However, no matter what I try, request.reader.readLine() returns null. 4) I also added a "bodyToString()" which uses the body byte array, but it always returns a empty string. Any ideas?
563e339b61a8013065268010	X	I've changed the code of the multireadhttpservletrequest to work more reliable. You might want to try that.
563e339b61a8013065268011	X	Great solution! Helped me when catching errors and sending error report.
563e339c61a8013065268012	X	it also is not working for me. I set a breakpoint at both getInputStream and getReader and it appears that it is not hit when request.getParameter reads in the body and therefore the parameter array never is properly initialized.
563e339c61a8013065268013	X	Which would make sense, since the HttpServletRequestWrapper delegates getParameter() to the original request's getParameter(), which only knows about the original ServletInputStream (which will be empty after the first call to getInputStream() on the wrapper). I can't seem to find any solution to this issue, apart from hand-writing overrides for getParameter() and its friends as well. It's odd that no one blogging about or documenting servlet filters seems to have run into this. Unless I'm missing something...
563e339c61a8013065268014	X	So to clarify, I just tried this and it works, however it's crucial you don't import grails.converters.* (which is what anders probably meant by 'turning off grails automatic handling of XML')
563e339c61a8013065268015	X	I actually meant 'turning off grails automatic handling of XML' as in Graemes comment jira.grails.org/browse/…
563e339c61a8013065268016	X	I am implementing a RESTful API in Grails, and use a custom authentication scheme that involves signing the body of the request (in a manner similar to Amazon's S3 authentication scheme). Therefore, to authenticate the request, I need to access the raw POST or PUT body content to calculate and verify the digital signature. I am doing authentication in a beforeInterceptor in the controller. So I want something like request.body to be accessible in the interceptor, and still be able to use request.JSON in the actual action. I am afraid if I read the body in the interceptor using getInputStream or getReader (methods provided by ServletRequest), the body will appear empty in the action when I try to access it via request.JSON. I am migrating from Django to Grails, and I had the exact same issue in Django a year ago, but it was quickly patched. Django provides a request.raw_post_data attribute you can use for this purpose. Lastly, to be nice and RESTful, I'd like this to work for POST and PUT requests. Any advice or pointers would be greatly appreciated. If it doesn't exist, I'd prefer pointers on how to implement an elegant solution over ideas for quick and dirty hacks. =) In Django, I edited some middleware request handlers to add some properties to the request. I am very new to Groovy and Grails, so I have no idea where that code lives, but I wouldn't mind doing the same if necessary.
563e339c61a8013065268017	X	It is possible by overriding the HttpServletRequest in a Servlet Filter. You need to implement a HttpServletRequestWrapper that stores the request body: src/java/grails/util/http/MultiReadHttpServletRequest.java A Servlet Filter that overrides the current servletRequest: src/java/grails/util/http/MultiReadServletFilter.java Then you need to run grails install-templates and edit the web.xml in src/templates/war and add this after the charEncodingFilter definition: You should then be able to call request.inputStream as often as you need. I haven't tested this concrete code/procedure but I've done similar things in the past, so it should work ;-) Note: be aware that huge requests can kill your application (OutOfMemory...)
563e339c61a8013065268018	X	As can be seen here http://jira.codehaus.org/browse/GRAILS-2017 just turning off grails automatic handling of XML makes the text accessible in controllers. Like this Best, Anders
563e339c61a8013065268019	X	It seems that the only way to be able to have continued access both to the stream and request parameters for POST requests is to write a wrapper that overrides the stream reading as well as the parameter access. Here is a great example: Modify HttpServletRequest body
563e339c61a801306526801a	X	I'm deploying (a .NET app) to Elastic Beanstalk from scratch using PowerShell. I'm building a PS script because it's going to run on a CI server and I want to fully understand the process. The documentation doesn't want to make it easy, finding the expected package format seems an impossible task. Amazon want you to use VS and many details are not forthcoming in the documentation. To test things, I'm able to set up an app, environment, S3 bucket/key with source (a text file so I can force it to fail), and a version. I use this to monitor deployment progress: The results are not super explanatory (newest log entry at the top): It fails - fine. But how should I get more detailed logs about the failure? I checked the bucket in S3 that EB created, but it has no logs. I can't find anything in the docs about using either the API, CLI, or PowerShell cmdlets to get logs from an instance. In short: how do I get more detailed deployment logs via the AWS API? I can find/adapt/create a PowerShell script for it if needed, just need to know the mechanics! (Edit: from the management console, I can request logs - these are perfect and in the correct format. What is the API I can use to do this?)
563e339d61a801306526801b	X	Looks like I wanted Request-EBEnvironmentInfo then Get-EBEnvironmentInfo. I have no idea how I didn't find them before. http://docs.aws.amazon.com/powershell/latest/reference/items/Request-EBEnvironmentInfo.html http://docs.aws.amazon.com/powershell/latest/reference/items/Get-EBEnvironmentInfo.html
563e339d61a801306526801c	X	I neither can open this one (long) in browser!
563e339d61a801306526801d	X	The image is not a valid URL. I just gave an example to show the format of the urls I am seeing.
563e339d61a801306526801e	X	Thanks for your answer. However, my site works fine with users profile images. Download works well. It is only specific circumstances when it breaks.
563e339d61a801306526801f	X	My app creates a video montage of your friend's Facebook Images. The app asks for the permissions: The user chooses the pictures to add to the montage with Javascript in the browser. Then I pass the URL of the chosen image to the server, which downloads the image from the URL and stores it on Amazon S3. Everything works on the client and always works for friends profile images. But for other images i.e. ones from an album or previous profile pics, my server cannot download the image at all. Using cURL, the status code returned is 0. For other images, the code is 200. How can I get the access to these images? Does the server need to pass some other data to be able to access CDN images? The image URLs passed from my JS are long links and look like this: http://fbcdn-sphotos-a.akamaihd.net/hphotos-ak-ash4/s720x720/408158_894440624792_192203432_41161655_469457024_n.jpg (This is not a real image link though) I can open the images in my browser fine, but cannot open them server-side. I am using PHP-SDK, so the server is authenticated with Facebook for the logged in user. I know this is correct as I have tried multiple accounts and checked the current logged in user id. To download, I have tried cURL, file_get_contents and a couple of other methods. All receive nothing, therefore I don't think Facebook is giving my server access to the picture. Strangely, this issue seems to only affect some users, not everyone. Also - my server is PHP 5.2 and is running on IIS. Any help would be fantastic! Further info: I tried visiting the URL when logged in with another user account, someone not a friend of the owner of the photo and they could also see the image in the browser. Odd that my server cannot.
563e339d61a8013065268020	X	Because this issue only occurs for a subset of Facebook users, I have a feeling that there may be a bug somewhere in the way image permissions are granted. This then means there is a slight discrepancy between the JS API and the PHP API, only for some images and only for some users. However, I have now adjusted my code to get the images another way and so no longer have this issue.
563e339d61a8013065268021	X	i'm not able to see the photo you linked but i used this php function to do same with facebook photos http://www.w3schools.com/php/func_filesystem_copy.asp
563e339e61a8013065268022	X	With S3, the file can be directly downloaded to the browser, without going through EC2. That is potentially faster, too.
563e339e61a8013065268023	X	How many and how much size of video files we are talking here ?
563e339e61a8013065268024	X	max 5 - 10 mb .. it shouldnt exceed that
563e339e61a8013065268025	X	S3 is very slow.
563e339e61a8013065268026	X	Thanks Ben for the information. I felt that S3 is quite slow.
563e339e61a8013065268027	X	@Deepak If you make sure your s3 account and your ec2 account are in the same region (ie. eu-west or us-east) transfer will be super fast as they are in the same datacenter.
563e339e61a8013065268028	X	s3 is indeed slow but you can pair it with cloudfront to serve the files. Its easy to setup and hyperfast when you pair the two.
563e339e61a8013065268029	X	In my application; users can upload videos and I want to keep them on file system and not in database. If I use Amazon Web Services ( AWS ) and use only one EC2 instance with EBS ( for storage ) its fine. But if I use auto-scaling or multiple EC2 instances ; then if user uploads the video it gets saved on one of the EC2 ( associated with one of EBS ) . Next time if user logs in ( or if session stickiness is not there; then if the user's next request goes to another EC2 instance ) how will it access his video ? What is the best solution for this problem ? Is using S3 the only solution but for that I cant simply do java.io.File .. I think then I will have to use AWS SDK api to access the uploaded videos.. but wont that be slower ? Is there any better solution ?
563e339f61a801306526802a	X	I would use Amazon S3. It's fast, relatively cheap and works well if you are using other Amazon Web Services. You could upload to the EC2 instance first and use Simple Workflow Servie to transfer the files centrally automatically. This is also useful if you want to re-encode the video to a multiple bit-rates later.
563e339f61a801306526802b	X	You need to setup NAS. It can be NFS or something like GlusterFS. The later is modern and it scales well...
563e339f61a801306526802c	X	I did test it a few times, but as I mentioned in my post I had mixed results depending on the directory sizes. You are absolutely correct that the Parallel.ForEach returned prior to each of the MyUploadMethodAsync methods. I saw this in my tracing, but that was expected since they're async operations. I'll try this version and see if it improves performance.
563e339f61a801306526802d	X	@JNYRanger, what you're passing to Parallel.ForEach is async void lambda, not even async Task lambda. When Parallel.ForEach returns, you have no way of tracking the completion status, check the result and handle any exceptions possibly thrown by each MyUploadMethodAsync (and most of them will still be pending).
563e339f61a801306526802e	X	Thanks a lot, this did the trick. I was able to not only handle errors better, but improved performance by over 25%! I went against my own rules making async void methods. Never again.
563e339f61a801306526802f	X	I have a few different ways of upload entire directories to Amazon S3 within my application depending on what options are selected. Currently one of the options will perform an upload of multiple directories in parallel. I'm not sure if this is a good idea as in some cases it sped up the upload and other cases it slowed it down. The speed up appears to be when there are a bunch of small directories, but it slows down if there are large directories in the batch. I'm using the parallel ForEach loop seen below and utilizing the AWS API's TransferUtility.UploadDirectoryAsync() method as such: Where the TransferUtility.UploadDirectoryAsync() method is within MyUploadMethodAsync(). The TransferUtility's upload methods all perform parallel uploads of parts a single file (if the size is big enough to do so), so performing a parallel upload of the directory as well may be overkill. Obviously we are still limited to the amount of bandwidth available so this might be a waste and I just should just use a regular foreach loop with the UploadDirectoryAsync() method. Can anyone provide some insight on if this is bad case for parallelization?
563e339f61a8013065268030	X	Did you actually test this? The way you're using it, Parallel.ForEach may return well before any of MyUploadMethodAsync is completed, because of the async lambda: Parallel.ForEach is suited for CPU-bound tasks. For IO-bound tasks, you are probably looking for something like this:
563e339f61a8013065268031	X	possible duplicate of How can I update files on Amazon's CDN (CloudFront)?
563e33a061a8013065268032	X	as a sidenote, I don't think it's stupid to name static files like that. We've been using it a lot and having automated renaming as per file version in version control has saved us a lot of headaches.
563e33a061a8013065268033	X	@eis unless the file you need to replace has been linked to 1000 different places online. Good luck getting all those links updated.
563e33a061a8013065268034	X	@Jakobud why should the links be updated in that case? they're referring to specific version, which is not the latest, if the file has been changed. If the file has not been changed, it'll work as it did before.
563e33a061a8013065268035	X	In some cases a company may make a mistake in posting the wrong image for something or some other type of item where they receive a takedown notice from a law firm and have to replace the file. Simply uploading a new file with a new name isn't going to fix that kind of problem, which is unfortunately a problem that is more and more common these days.
563e33a061a8013065268036	X	Sweeeeeeeeeeeeeet!
563e33a061a8013065268037	X	Please note that invalidation will take some time (apparently 5-30 minutes according to some blog posts I've read).
563e33a061a8013065268038	X	If you do not want to make an API request yourself, you can also log in to the Amazon Console and create an Invalidation request there: docs.amazonwebservices.com/AmazonCloudFront/latest/…
563e33a061a8013065268039	X	For those of you using the API to do the invalidation, approximately how long is it taking for the invalidation to take effect?
563e33a061a801306526803a	X	Remember this costs $0.005 per file after your first 1,000 invalidation requests per month aws.amazon.com/cloudfront/pricing
563e33a061a801306526803b	X	Where is this setting in the new AWS Console UI? I can't find it.
563e33a061a801306526803c	X	I found the setting for an individual file, but is there a setting to make it so that anything uploaded to my bucket has a TTL of 0?
563e33a061a801306526803d	X	While I would also definitely be interested in a bucket-wide setting, I found this a quicker/better solution. Invalidation requests (along with the rest of the API) are very confusing and poorly documented, and I spun my wheels for 3 hours before this instantly worked.
563e33a061a801306526803e	X	Call me crazy but setting the TTL to 0 and max-age to 0 is really using CloudFront without caching, wouldn't that forward all requests to the origin constantly checking for updates? Essentially making the CDN useless?
563e33a161a801306526803f	X	This is exactly what I was looking for. I am going to hook this in Beanstalkapp's web-hooks when auto deploying from git! Thanks for the link!
563e33a161a8013065268040	X	Thanks, super helpful!
563e33a161a8013065268041	X	sorry, but even "you say" the credentials not stored or leeked ... one should never give his credential to a 3rd party. May be implement a remote amazon authentication or something ?
563e33a161a8013065268042	X	You should put this behind https at the least.
563e33a161a8013065268043	X	Actually cfadmin is a very helpful tool, especially if you need to reset CloudFront cache from the console\bash\travis ci deployment script. BTW here is the post how to reset\invalidate CoudFront cache during the travis deployment to aws
563e33a161a8013065268044	X	You just saved my life!
563e33a161a8013065268045	X	I'm using Amazon's CloudFront to serve static files of my web apps. Is there no way to tell a cloudfront distribution that it needs to refresh it's file or point out a single file that should be refreshed? Amazon recommend that you version your files like logo_1.gif, logo_2.gif and so on as a workaround for this problem but that seems like a pretty stupid solution. Is there absolutely no other way?
563e33a161a8013065268046	X	Good news. Amazon finally added an Invalidation Feature. See the API Reference. This is a sample request from the API Reference:
563e33a161a8013065268047	X	As of March 19, Amazon now allows Cloudfront's cache TTL to be 0 seconds, thus you (theoretically) should never see stale objects. So if you have your assets in S3, you could simply go to AWS Web Panel => S3 => Edit Properties => Metadata, then set your "Cache-Control" value to "max-age=0". This is straight from the API documentation: To control whether CloudFront caches an object and for how long, we recommend that you use the Cache-Control header with the max-age= directive. CloudFront caches the object for the specified number of seconds. (The minimum value is 0 seconds.)
563e33a261a8013065268048	X	With the Invalidation API, it does get updated in a few of minutes. Check out PHP Invalidator.
563e33a261a8013065268049	X	Bucket Explorer has a UI that makes this pretty easy now. Here's how: Right click your bucket. Select "Manage Distributions." Right click your distribution. Select "Get Cloudfront invalidation list" Then select "Create" to create a new invalidation list. Select the files to invalidate, and click "Invalidate." Wait 5-15 minutes.
563e33a261a801306526804a	X	Just posting to inform anyone visiting this page (first result on 'Cloudfront File Refresh') that there is an easy-to-use+access online invalidator available at swook.net This new invalidator is: Full disclosure: I made this. Have fun!
563e33a261a801306526804b	X	If you have boto installed (which is not just for python, but also installs a bunch of useful command line utilities), it offers a command line util specifically called cfadmin or 'cloud front admin' which offers the following functionality: You invaliate things by running:
563e33a261a801306526804c	X	using web console: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html Yii: https://github.com/iamfrankhe/Yii-CloudFrontAssetManager Very simple PHP script: https://gist.github.com/claylo/1009169 Hope it helps many!
563e33a261a801306526804d	X	Set TTL=1 hour and replace http://developer.amazonwebservices.com/connect/ann.jspa?annID=655
563e33a261a801306526804e	X	In ruby, using the fog gem even on invalidation, it still takes 5-10 minutes for the invalidation to process and refresh on all amazon edge servers
563e33a261a801306526804f	X	one very easy way to do it is FOLDER versioning. So if your static files are hundreds for example, simply put all of them into a folder called by year+versioning. for example i use a folder called 2014_v1 where inside i have all my static files... So inside my HTML i always put the reference to the folder. ( of course i have a PHP include where i have set the name of the folder. ) So by changing in 1 file it actually change in all my PHP files.. If i want a complete refresh, i simply rename the folder to 2014_v2 into my source and change inside the php include to 2014_v2 all HTML automatically change and ask the new path, cloudfront MISS cache and request it to the source. Example: SOURCE.mydomain.com is my source, cloudfront.mydomain.com is CNAME to cloudfront distribution. So the PHP called this file cloudfront.mydomain.com/2014_v1/javascript.js and when i want a full refresh, simply i rename folder into the source to "2014_v2" and i change the PHP include by setting the folder to "2014_v2". Like this there is no delay for invalidation and NO COST ! This is my first post in stackoverflow, hope i did it well !
563e33a261a8013065268050	X	Those scala links are great! It looks very close to what I need. Is it possible to do similar thing in Java or is that exclusive to Scala?
563e33a261a8013065268051	X	Iteratees are not available in Play Java, but you should be able to produce something quite close with Akka. You can also mix Java and Scala in your project (a way to learn softly).
563e33a261a8013065268052	X	I'm planning a web application where users will be able to upload and process their files. The specifics of the application are irrelevant to my questions, but lets assume that the application will deal with mp3 audio files. I'm going to split my application in two distinct parts: the front-end and the back-end. The front-end application will be a usual web application serving html pages to users. Typically a user will upload his file and fill an html form to specify which operations he would like to perform on the file. The files will be initially uploaded to a storage facility, such as Amazon S3, and later processed by a back-end server. I'm using Play 2.0.4 framework to develop the front-end application and this is going very well for me. I managed to implement user authorization, drafted most of the UI and also implemented file upload to S3. The application is currently deployed on Heroku without any problems. For my back-end server I'm considering to use Play 2 framework once again. The back-end server will receive notification (http request) from the front-end server about creation of a new job. Job specification will include a link to the original user file in the storage and arguments describing the job. The job should be added to a queue. Now the most important part is to delegate the actual processing job to a third party program, which most certainly will be a compiled command line utility, such as SoX for the case of audio processing, written by good people using a programming language of their choice. As far as I know it is possible to call an external program from java, pass command line arguments and collect the result. After processing is done, the back-end server will upload processed file back to storage, and send notification (http request) to the front-end application, which will store a link to the processed file and display it to the user at some later time. To be able to use command line utility I'm going to deploy the back-end application to a Amazon EC2 instance with a Typesafe stack installation. Here are some questions about this basic plan: I would appreciate any advice! Thanks! Note: I'm using Java api for Play 2 framework, since I'm not familiar with Scala language.
563e33a361a8013065268053	X	You may consider Akka for processing and it's built in Play2. It will help you to manage tasks easily, and even saving hardware ressources if used with advanced features. There is a Java API that should cover all your needs. And it's not necessary in a backend APP, if you need more power you can scale even better with two same instancies. Play and Akka are stateless, you can just add new instances to scale. To make it run on EC2, just use the play dist command. And yes, you can install whatever you want in EC2 and call it from your app. You may like: also, but in scala
563e33a361a8013065268054	X	so what you mean is that one of 100 requests from some one ip x.x.x.x takes > 0.5 seconds ?
563e33a361a8013065268055	X	Yeah, I think the inefficient routing would cause longer than expected queuing times. Our queuing times are always really short though. Our traffic is about 100 rpm.
563e33a361a8013065268056	X	What webserver process are you using?
563e33a361a8013065268057	X	We're stumped by this, been trying to solve it for a month. It looks like every once in a while (maybe one in 100 requests) our connection time to RDS or RabbitMQ takes > 0.5 seconds. We're using Heroku with Django, amazon RDS and S3 (we do tons of image uploads). Here's a sample slow connection (fast connections are really fast, like < 20 ms):  It's worth mentioning both Heroku and RDS are in the us-east region. Also when we use Redis Cloud (Heroku add-on) we see slow connection speeds to that too. UPDATE: I've also found that these same delays exist for Memcached. In fact, on average Memcached takes roughly the same amount as a request to the database. I'm just using Memcached for storing API keys and there's plenty of memory left. 
563e33a361a8013065268058	X	It resembles something I read earlier on a blog where it seemed that on Heroku a request might be routed to a dyno that is handling a long running request. That way the old request blocks the execution of the next request. The routing isn't quite as "intelligent" as it is advertised. I'm not 100% sure that this is the problem you are experiencing as is seems that the slowness shouldn't be in the connection but in the handling of the request to Heroku. You can find more details on the problem describes here: http://news.rapgenius.com/James-somers-herokus-ugly-secret-lyrics
563e33a361a8013065268059	X	good question - afaik, MPMoviePlayerController is not able to play m3u8 stream from a local URL (filesystem) as the networking part of MPMoviePlayerController will always try to negotiate the "right" bandwidth/profile from the m3u8 snippet - that part does not seem to work locally and becomes a showstopper. If that is correct (as I assume), then you will have to provide and alternative file on your server -> MP4.
563e33a361a801306526805a	X	thanks, I'm doomed to download the mp4. It 'll be 2 downloads, one full mp4 files and on other segmented file depending on client wishs
563e33a361a801306526805b	X	I've m3u8 file with all the TS files. MPMoviePlayerController play them fine via http request on the streaming server. But I'd like to get the files localy in order to play them again later without any connection. I managed to download m3u8 file and all the TS files localy on my device, I edited m3u8 files to point to local .ts instead of http ones, but I can't read them from this emplacement. (VLC can do it well) Is there a way to download the segments while playing (to avoid 2 downloads) and then to play them localy with MPMoviePlayerController or else.
563e33a361a801306526805c	X	.m3u8 is Apple HTTP Live Streaming, right? I think what you're trying to do simply goes against the design of that technology. You should expose the original file and allow it to be downloaded. From what I understand, it's in the design of streaming that you don't get explicit access to the pieces in order to put them back together. For instance, Netflix uses streaming via Silverlight, and one of the benefits (to Netflix) is that it protects the data from being saved as if it were downloaded. Also, since HTTP Live Streaming allows a stream to switch bitrates on the fly, it's designed such that each time slice can be encoded at any number of bitrates, and none of them is canonical. In theory, there might be a way to collect all the slices for a particular bitrate and re-encode them into a single video. But Apple's playback APIs are not going to give you that opportunity. Instead of HTTP Live Streaming, consider progressive download. Just serve the original video file (transcode it to something the iPhone likes if necessary). If your server is configured properly, the playback APIs will do small requests to get particular chunks of the file, rather than the whole thing in one go, and it's a close second to proper streaming. I wish I could find where I read about this so I could give the proper name for it. Amazon S3 is set up to serve this way, if you need a quick solution. But beware, Apple's docs say, If your app delivers video over cellular networks, and the video exceeds either 10 minutes duration or 5 MB of data in a five minute period, you are required to use HTTP Live Streaming. (Progressive download may be used for smaller clips.)
563e33a461a801306526805d	X	groups.google.com/d/msg/solrnet/26gAjZVMSA0/4XRHN0KOTYYJ
563e33a461a801306526805e	X	With Amazon's Cloud Search being powered by Solr, I have certain questions before we proceed. May be someone who has experience with both can guide us. The reason we're asking is, we need to migrate one application from Solr to Amazon Cloud Search and before we proceed we need some idea on how does this work? I checked with Amazon Cloud Search documentation, but unable to find any details on this particular thing!
563e33a461a801306526805f	X	Amazon cloudsearch is based on solr, so conceptually they work same way, but amazon has written its own wrapper on top of solr api. Answers to the questions in same order below: Amazon cloud search has two endpoints, one for search and other for indexing documents. Provides with multiple ways of indexing like S3 documents, JSON, XML etc. Amazon cloudsearch has four different query parsers, if you place queries using Lucence as query parser, query syntax and functionality is same. The only difference I observed functionally is cloudsearch doesn't support hierarchy in fields data, it only provides with text and literal datatypes and their arrays for multiple values. There are migration tools available like http://www.8kmiles.com/blog/apache-solr-to-amazon-cloudsearch-migration-tool/ which support migration from solr to cloudsearch.
563e33a461a8013065268060	X	Remember that with Amazon S3 you only pay for your consumption - also they have an iOS SDK page to get you started.
563e33a461a8013065268061	X	That's true, especially since my application won't have much traffic. However, this page outlines the pricing (aws.amazon.com/s3), and I just don't understand what category editing a text file would fall into.
563e33a461a8013065268062	X	@boopyman: There's no concept of editing in S3. You just retrieve it and later store a new version. You pay for one GET, one PUT and the outgoing bytes. If you cache the document on the client, you may also avoid most of the GETs and bytes.
563e33a461a8013065268063	X	Ah, I see - so it could end up being costly... See my edit above ^.
563e33a461a8013065268064	X	Perhaps. If the document isn't shared, caching can make this relatively cheap (a single PUT per edit). If it is shared, you might still benefit from caching if you use Cache-Control to avoid transferring data unnecessarily. There's no way you'll get to use S3 for free, though.
563e33a561a8013065268065	X	I'm developing a Mac application in Objective-C, and I'd like to store either a database (spreadsheet type of file pretty much) or a text file online. Every time a user runs the app, I'd like for it to retrieve that file, modify it, and upload it back up to the server. I've been thinking of a couple of options, which I've listed below: -Amazon S3: the advantage is it seems relatively easy to implement. Inconvenients: the fees, even as small as they are, could add up, and it's probably way overkill. -Dropbox: the main advantage is it's free, and probably quite simple to implement. The disadvantage to this service is it's probably not tailored for the type of action I'm trying to run. -Google Docs: best guess on my part so far, because it's free, and easy to implement (I think) with Google's Objective-C APIs. I was thinking it might have the same problem as Dropbox (it's not made for this type of thing), but I'm not really sure. So, to resume, does anyone know of a simple, free, and easy way to modify a text file stored online from an Objective-C application? Out of these three, which one would you suggest, and why, or do you have any other suggestions? EDIT: I'm not sure I've made this clear, but ideally, I'm looking for a free service that doesn't necessarily offer much space (I don't need that much), but lets me make a good number of requests.
563e33a561a8013065268066	X	So, after realizing there are several issues for me regarding Amazon S3 (lack of credit card, etc, no Paypal option, etc...), I have decided to use Google Docs to store a 'database' in a spreadsheet. Since I don't have a big user base, this has many advantages, including: free, easy to implement, and really fast. Using Google's GData libraries (in particular the Docs and Spreadsheet libraries), I only took me a couple of hours to have the whole system set up. My only regret is that the Spreadsheet library doesn't let you set sorting and formatting for a particular spreadsheet.
563e33a561a8013065268067	X	Thanks a lot for your answer :)
563e33a561a8013065268068	X	Thanks a lot for your answer. What do you mean by MS?
563e33a561a8013065268069	X	Thank you for your answer. :)
563e33a561a801306526806a	X	Thank you for your answer :)
563e33a561a801306526806b	X	We are planning to start an ecommerce startup and are evaluating scalability options for choosing between (PrestaShop/WooCommerce/OpenCart) or our own custom ecommerce solution. We have thought of the following optimization techniques for scalability: 1) CDN for static resources. 2) Load balancer for horizontal scaling once the traffic goes high. 3) MemCached or APCU cache for caching database queries. 4) APC Cache for PHP ByteCode Caching. 5) Making sure all images are compressed losslessly. 6) Minifying CSS and JS of theme. 7) Enabling mod_deflate or mod_gzip for compression. 8) Master Slave Replication once DB starts to become a bottleneck. 9) Making sure unnecessary Apache modules are disabled. 10) Making sure unnecessary Prestashop modules are disabled. What would you recommend? A custom eCommerce solution or we can optimize one of these frameworks(PrestaShop, WooCommerce, OpenCart etc) ?
563e33a561a801306526806c	X	My recommendation is PrestaShop: 1) It has CDN support 2) No "special" support (it supports master/slave DB servers) 3 & 4) Has MemCached, APC & xcache 5) Not supported by default, but has Smush.it paid module 6) Full suppoert - CCC i.e. Combine (all .js in one file, etc), Compress (minify js, css, html & Cache - the combined files in cache folder with timestamp based expiration) 7) Integrated mod_deflate, you can always enable mod_gzip at the .htaccess file 8) You can configure master (this is the default one) & slave servers, and the core PrestaShop queries support master/slave (i.e. some queries are passed to slave and they have specified which exactly). Most of the 3rd party modules does not use that feature. 9 & 10) These are the thing the administrator/developer must take care. Custom solution is a worst case, unless if you have 1+ year and lot money to invest. I don't like Magento & OpenCart and that's why 5 years ago I chose PrestaShop for eCommerce developement. Magento has unnecessarily complex class tree and of course the developers charge usually a lot more, because they have a lot of work :), and OpenCart is a way below the others - having not a single comment in the code is a just not professional, no indexes at all on the database tables, it does not even uses template engine. Regarding the "WooCommerce" - using CMS system for eCommerce is just not serious. My advice is to check PrestaShop - get the latest version, test it, check at addons.prestashop.com (The official Marketplace) for the modules you will need. Also, there's a newly released "PrestaShop Cloud" - you can take a look on it as well.
563e33a561a801306526806d	X	First of all its not frameworks its cms. Frameworks: laravel, symphony and etc.. And u can do all things with all cms. But to my mind the best - prestashop. 2) Lot of ways to optimize your server, your cms, write correct modules . 3) In prestashop you can use memcached 4) You can install APC on server and enable it in prestashop performance 5) You can edit compression settings or write/buy powerful module to get such effect 6) Minifying CSS / JS / HTML in performance (settings) 7) mod_gzip in server settings 8) Disable overrides or non prestashop modules. Do profiling to check MS and bad modules.
563e33a561a801306526806e	X	If you're looking at developing on top of any of the existing open source carts out there, have a good look at the code first. Just a quick look and I can make these comments: WooCommerce -- OK if you're used to the wordpress style of code I guess but it locks you into using that specific CMS as your development framework. PrestaShop -- Coding standards are a bit outdated (no PSR compliance), no use of namespaces, code has some but not comprehensive API documentation. OpenCart -- code has almost no comments, no use of namespaces, limited PSR compliance, no API documentation. Have you considered Magento 2.0 which is in beta? Magento 1 had the limitation of no namespaces because it relied on Zend Framework 1 which was pre-namespace but Magento 2.0 has namespace support while not throwing the baby out with the bathwater (Zend 1 classes have been kept and wrapped in namespaced classes). If you're looking at extreme flexibility and the ability to code things your own way, you may be better off starting from scratch on top of one of the existing PHP frameworks (Laravel, Yii2, etc). In terms of performance you're not likely to gain much -- you're apt to make just as many performance mistakes building your own code as you will find in someone else's code. However this will be a lot of work! eBay bought Magento for $180 million and that wasn't because it was knocked together by a couple of guys in a week or two -- there is some serious programming work in all of these systems.
563e33a561a801306526806f	X	Since others have given their comments on each solutions I will give you more overall idea. PrestaShop/WooCommerce/OpenCart - These products are somewhat mature according to my knowledge. Advantages Disadvantages own custom e-commerce solution - There are pros and cons using your own e-commerce solution rather than an existing products. Advantages Disadvantages To find out whether it’s a proper decision for you too, you need to answer the following questions: Is it reasonable to make it? If you are going to use PrestaShop/WooCommerce/OpenCart, I would advice you to check out Magento as well. Hope this answers your question. Also note that your considered optimisation techniques for scalability aspects are good but there is a lot more to be considered if you are willing to learn. I can help you with them as well.
563e33a561a8013065268070	X	Building your own custom eCommerce solution from scratch can be a real nightmare for a startup and should generally be avoided. Usually what happens a few months later is that the startup ends up having to maintain code, fix bugs and create new features internally. This all adds up and eats into time that could be better spent working on other more important aspects of your startup. There's no point re-inventing the wheel! Eventually the startup decides to bite the bullet and scrap what they built out for several months for an off the shelf solution. They then choose a downloaded platform like Prestashop/WooCommerce/OpenCart etc.. that they feel they can then customise. This again takes time to both learn, implement and tailor to your specific needs; taking you away from other more important activities. If you're looking for a lightweight and scalable solution that is quick to integrate, low maintainance, with no bloated code base and is super customisable you could look at more modern methodologies like eCommerce APIs. These services are usually already heavily optimized for increased performance. They are usually available globally in multiple regions, load balanced, provide asset CDNs and some allow for custom data structures etc... The beauty with this approach is that you can pick the components you need to integrate, without having to disable modules. You can also decide in the future that you need to change or add to your frontend technology stack and even pick a different programming language. You could even build static sites that talk to these APIs and host a few files that make up your site in an Amazon S3 bucket for a few cents a month!?
563e33a561a8013065268071	X	About the AWS SDK being not async: Couldn't that be solved by uploading each filepart in a different thread?
563e33a661a8013065268072	X	Yes, you could/should/must execute that on a separate thread pool/execution context or else it may block you entire play server.
563e33a661a8013065268073	X	Is it possible to use Play's Iteratees from within Java? I have not been able to find any examples nor doco on using Iteratee in Java, only Scala. I'm guessing getting Iteratees working in Java with the PLay API is a little more messy code wise (lots of anon Funtion1<?,>s)... If it is possible I would like to create an App controller that can accept multi-part file uploads uploaded via HTTPs chunked transfer encoding and parse these message chunks downstream to an S3 store. Any ideas on how I can approach this in Java? Cheers.
563e33a661a8013065268074	X	I think it is possible to implement iteratees in Java. There is an example of doing this in Scala written by Sadache in this question: Play 2.x : Reactive file upload with Iteratees Note though that there is no async api library available for S3, so you will be blocking in that end of the upload if you for example use the official amazon api java libraries.
563e33a661a8013065268075	X	The Java SDK contains the class TransferManager to perform asynchronous uploads. It contains an own configurable ThreadPool. Iteratees written in Java might be able to push the bytes of the uploaded file directly to S3 but the code would looking akward and hard to configure. For a lot of use cases it would be good enough to stream a file from the browser to a temporary file (so it is not fully in memory) and then stream it to S3. I created an example project on Github how to do that: An example log output for uploading two files in different browser windows:
563e33a661a8013065268076	X	Thank you for your reply, it was much appreciated. What would be your opinion for the following: I would like to leverage S3 to store photos and DynamoDB for user profile keeping, thereby using the Amazon SDK methods (after authentication via Cognito). I can use the Amazon SDK to do that and the signed requests will be transparent to my application code. For the custom REST services call, what would you recommend in terms of security, OAuth2? What do most of the AWS customer do in this regard?
563e33a661a8013065268077	X	The most scalable way to design an application using S3 or DynamoDB is to let the client directly read / write to the S3 and DynamoDB backend using the JavaScript (for web app) or iOS/Android SDK (for mobile apps). This would leverage the AWS backend and offload your server side infrastructure. When an API is needed I would make it stateless and would just accept the AK/SK generated by Cognito on the client side to ensure your API runs with your client's privileges. When more priviledges are required, EC2 roles are a clean solution.
563e33a661a8013065268078	X	I am considering using AWS for a project that will have an IOS application as a client and a server side using a custom developed REST API using Java Spring. I have been reading about the need to sign all requests to AWS services using a signature version (version 4 for most services) and would like to leverage the mechanism in order to secure my REST services. There is plenty of documentation for using the REST wrappers in the AWS SDK for services such as S3 or DynamoDB, however I am having trouble getting a clear answer on how to validate signature from a custom REST API running on Elastic Bean stalk (for example a WAR deployed on Tomcat implementing Spring REST) 1) IOS client calls a REST service using RestKit (or can I use a class in Amazon SDK for IOS that I can use instead). As part of the call it specifies the token string and the AWS access key. 2) Server side, a Java program running on Tomcat on Elastic Bean stalk, receives the REST call and processes it by first validating the signature. If the signature corresponds to the re computed signature then allow the request, otherwise reject it. Could anyone point me into the right direction in terms of what is available in the AWS SDK for ObjC and Java to do this REST signature validation (again not using pre boxed services such as S3)? Thank you much.
563e33a661a8013065268079	X	This is an excellent question and a very popular feature request from our customers. As of today, there is no AWS API to validate an AWS Access Key / Secret Key based signatures for your custom web services. However, everything AWS does is based on customer feedback and your feedback helps to setup our development priorities. We are hearing that requirement a lot.
563e33a661a801306526807a	X	Hi this is a very noob question, but I am trying to deply my Node JS API server on AWS. Everything is working fine with one m1.large instance that my Front End running on S3 connects to. Now I want to Scale and put my EC2 instance and possibly many more behing and ELB and an Auto Scaling Group. Do I need to duplicate my server code on every EC2 instance? If so , I assume I'll have to create a seperate DB server which all of the EC2 instances will connect to. Am I right,anyone experienced in Amazon AWS can answer this, I tried googling but most of the links point to detailed tutorials which however don't answer my question. Any help would be much appreciated. Thanks
563e33a661a801306526807b	X	yep. that's basically correct. the code needs to be on all instances fronted by the load balancer. for the database you may want to look into RDS.
563e33a661a801306526807c	X	On ruby on rails I have written a code where I can upload a file on the amazon console, when i run the code from the localhost i am able to upload the file successfully . But i try to upload it from the swagger i am getting a error called : internal error 500. I have checked the log file and found the following error: /2013/12/23 09:34:05 [crit] 1705#0: *315335 open() "/tmp/passenger-standalone.1627/client_body_temp/0000000007" failed (2: No such file or directory), client: 10.29.36.248, server: _, request: "POST /v1/models/GTAG2/modelfirmware.json?api_key=5rx2mR3muK1mCydYerw3 HTTP/1.1", host: "dev-api-3.elasticbeanstalk.com" Can anyone tell me how to fix this bug? In my S3 the folder and bucket is available.
563e33a761a801306526807d	X	Sounds like you may be encountering issues similar to one of the following: Phusion-Passenger Issue #654 where something (a daemon) is deleting (cleaning) /tmp files while they are still in use. Issue Uploading Files from Rails app hosted on Elastic Beanstalk I'd check to make sure nothing is running on your system that could be deleting/cleaning files under /tmp while Passenger is running.
563e33a761a801306526807e	X	setting localStorage to an empty object doesn't do anything - window.localStorage always remains null for me.
563e33a761a801306526807f	X	I'm poking around with this example on writing to an S3 bucket from Javascript in a chrome app, but seem to be getting blocked by permissions. I must include a local copy of the aws sdk: <script src="assets/third-party/js/aws-sdk-2.0.22.min.js"></script> Otherwise I get a: <script src="https://sdk.amazonaws.com/js/aws-sdk-2.0.22.min.js"></script> Refused to load the script <script path> because it violates the following Content Security Policy The AWS SDK library then fails with: window.localStorage is not available in packaged apps. Use chrome.storage.local instead. Am I out of luck? @Amazon, can you make a version of your SDK for chrome apps?
563e33a761a8013065268080	X	I just tested embedding the SDK into the hello-world sample app provided by Google and I was able to successfully load it and list objects in an Amazon S3 bucket I own with CORS enabled. I did get the window.localStorage warning, but note that it is a warning and should only affect you if you are using AWS.CognitoIdentityCredentials (and can be worked around, see the end of this post for more). I must include a local copy of the aws sdk: <script src="assets/third-party/js/aws-sdk-2.0.22.min.js"></script> That's right, you must embed external content when writing a Chrome app. This is due to Chrome's Content Security Policy, which has strict rules about where executable code comes from. I would recommend reading through the "Handling External Content" section of the Chrome App developer guide to learn more about how to handle these kinds of resources, as well as how to sandbox content that cannot be embedded. edit: This workaround will no longer be necessary after v2.0.23 due to this change. If you happen to require AWS.CognitoIdentityCredentials, you can work around the window.localStorage issue by adding a script tag prior to the SDK like so: The localStorage.js file would simply redefine localStorage and have the following single line: Note that AWS.CognitoIdentityCredentials relies on window.localStorage to cache your Cognito identity ID when possible. The need to cache this ID should be less important with single page applications, and therefore is okay to disable using the above technique. If you do need to cache, you can see the AWS.CognitoIdentityCredentials API docs for getting the identity ID and storing it for later. You may also want to look at writing a sandboxed app, which would allow you to make use of window.localStorage as well as referencing external content and a more relaxed CSP, if needed.
563e33a761a8013065268081	X	Hi, like you've mentioned Campaign Monitor will immediately download and parse both the HTML and text content of your campaign. When parsing the HTML content we'll download all linked assets with some degree of parallelism (depending on load at the time) which is why you're still seeing timeouts with only 2 or 3 dynos. Obviously this is just the current implementation and may change, but suffice to say, we'll always try and download your assets in parallel.
563e33a761a8013065268082	X	@Toby I have updated my answer to say that a Heroku app hosting assets on a CDN requires a minimum of 3 web dynos to use Campaign Monitor. This fact should probably be near the top of your docs somewhere, as it implies substantial fees.
563e33a761a8013065268083	X	I'm using the CreateSend API gem, and in my local dev environment I am able to successfully create a draft campaign. In production (staging), on Heroku, the exact same code instead returns with an error: I am definitely passing the HTML content URL, as well as all the other required arguments to Campaign.create (I've visually verified the fact by logging the variables). Here's the API call: Furthermore, in local dev the API call returns in less than 3 seconds, whereas on Heroku it times out for 30 seconds and returns that error. Why is this happening on Heroku?
563e33a761a8013065268084	X	Because their API requires a second request to your app to happen in the middle of a first ongoing request, I suggest you move the logic off to a worker process:
563e33a761a8013065268085	X	Short answer: look at @kch's answer. I was getting the error 310: HTML Content URL Required when running one web dyno. Here are the relevant logs: Seeing the Request timeout error on GET my-app-1111.herokuapp.com/newsletters/1?cm=1 made me recall that Campaign Monitor was immediately calling back my app for all the newsletter assets. When I increased the number of web dynos to two, sometimes the error changed to 312: Text Content URL Required and it would timeout on GET my-app-1111.herokuapp.com/newsletters/1.txt; other times (still with two dynos) the API would not error out (the campaign draft was actually created), but the app timed-out on other assets (and the site still produced the stock Application Error). Here are some of those asset timeouts: When I increased the number of web dynos to three, there were still asset timeouts. With four web dynos, the whole process sometimes completed successfully (i.e. not always!) -- anyway, even if it was always successful, four dynos would be too expensive just to create Campaign Monitor newsletters. assets on CDN I have converted my app to use a CDN asset host (Amazon S3) thanks to asset_sync and fog, and I have returned to this problem: using Campaign Monitor from Heroku. Since my app no longer serves assets (they are served by Amazon S3), essentially all it is responsible for is returning HTML. In terms of Campaign Monitor, the moment you query their API to create a campaign based on your newsletter, they immediately call you back to download your newsletter. This actually means that they send two parallel requests: one for the HTML version of the newsletter, and one for the TEXT version of the newsletter. With one dyno blocked by the original call to their API, and with all assets hosted on a CDN, you'd think that a second dyno would do the trick. In fact, I have seen a single second dyno handle the "parallel" requests (for the HTML and TEXT versions of the newsletter). But it is unreliable, as there still are random timeouts (Error H12 (Request timeout)): Heroku's support says that this is because they dispatch each request independently, on the expectation that eventually the receiving dyno will be able to handle it. What's happening here is that in some cases, the same dyno that is making the post is the one that's receiving the second request, but it seems it needs the result from the second request to complete the post request, so it deadlocks. solutions One possible interim solution is to use unicorn for your app server, with two unicorn worker_processes. That way even if Heroku dispatches to the busy dyno, the second unicorn listener will respond to the request. Ultimately though, and in addition to using unicorn with two worker_processes, the correct solution is to handle the request asynchronously, by something like delayed_job, and use workless to spin up Heroku worker dynos on a per-need basis, so you only pay for what you use.
563e33a761a8013065268086	X	I am currently developing an iOS application that will need to be able to both upload and download images from a remote database; in addition, it will need to be able to cite and update stats associated with said images - e.g. comments, etc. (That is, i think it will need to be a relational database.) Any suggestions on how I might go about this/which frameworks/apis are best to use? it seems that Amazon's s3 service, while it is great for uploading and downloading, would not be as easy to store information along with images that would be easily searched; likewise, it 'parse' seems to be a good service for the backend, but it has the same issues. Please correct me if I am wrong!
563e33a861a8013065268087	X	A way to do that is to have a php server with a web service that have the connection with the relational DB. Your application will connect to this web service to GET/POST/DELETE/PUT data. The server will send this information and you will parse it with a JSON tool. I recommend you to use Restkit for the IOS side, it will make your life very easy: http://www.Restkit.org You can use Symphony on the server side, it's a php framework very easy to use: http://www.symfony-project.org/ I hope this will help you!
563e33a861a8013065268088	X	With a little extra work, you can even load data direct from an SQL database, or any other data source.
563e33a861a8013065268089	X	Instead of using App Engine Console one would be better of with remote_api_shell.py script. For more information look here: developers.google.com/appengine/docs/python/tools/…
563e33a961a801306526808a	X	I have about 4000 records that i need to upload to the datastore. They are currently in CSV format. I'd appreciate if someone would point me to or explain how to upload data in bulk to GAE. Thank you very much. Help appreciated.
563e33a961a801306526808b	X	You can use the bulkloader.py tool: The bulkloader.py tool included with the Python SDK can upload data to your application's datastore. With just a little bit of set-up, you can create new datastore entities from CSV files.
563e33a961a801306526808c	X	I don't have the perfect solution, but I suggest you have a go with the App Engine Console. App Engine Console is a free plugin that lets you run an interactive Python interpreter in your production environment. It's helpful for one-off data manipulation (such as initial data imports) for several reasons: I suggest something like the following: You should find that after one iteration through #5, then you can either copy and paste, or else write simple functions to speed up your import task. Also, with fetching and processing your data in steps 5.1 and 5.2, you can take your time until you are sure that you have it perfect. (Note, App Engine Console currently works best with Firefox.)
563e33a961a801306526808d	X	By using remote API and operations on multiple entities. I will show an example on NDB using python, where our Test.csv contains the following values separated with semicolon: First we need to import modules: Then we need to create remote api stub: For more information on using remote api have a look at this answer. Then comes the main code, which basically does the following things: Main code: The put_multi operation also takes care of making sure to batch appropriate number of entities in a single HTTP POST request. Have a look at this documentation for more information:
563e33a961a801306526808e	X	That's a great list you've got there
563e33a961a801306526808f	X	I'd love force.com if they did managed site hosting and I could get my data, not just the extras, or through some API, but a nightly incr. backup of my Oracle dataset. Do the salesforce guys offer this? I've never been able to get a straight answer out of their sales guys, which I always take as a no.
563e33aa61a8013065268090	X	They don't give you such "raw" access to your data. There is a backup service that gives you zipped CSVs of your org on a regular basis. There's also a replication api that allows you to keep your own side by side backup in pseudo real time.
563e33aa61a8013065268091	X	@Jeremy out of curiousity... how much time do you spend in the eclipse ide plugin vs just setting things up in the "setup" menu's within a salesforce application?
563e33aa61a8013065268092	X	I personally spend 90% of my time in either eclipse or a text editor (TextMate, in my case). But that's because someone else usually does a lot of the basic data configuration. The configuration of custom objects and fields is done in salesforce.com, not code, because there is no DDL in the force.com world. There is a metadata api, but I never use it during data design.
563e33aa61a8013065268093	X	More that 2 years after this answer, what about the platform these days? Has it improved, some of this cumbersome issues are solved or at least aliviated?
563e33aa61a8013065268094	X	Bump, im also want to know if things are changed during those 2 years.
563e33aa61a8013065268095	X	I can't comment on the AppExchange, but I found this thread after mashing "salesforce.com sucks" into google in frustration with triggers and governor limits and jumping through hoops to deal with very simple data....just lots of it. Take that as you will ;)
563e33aa61a8013065268096	X	RESTful API is now available for force
563e33aa61a8013065268097	X	JSON Serialization and De-Serialization is available for Non SObject.
563e33aa61a8013065268098	X	How did you integrate your Amazon document storage with Salesforce (assuming that you did)?
563e33ab61a8013065268099	X	+1 for mentioning version control.
563e33ab61a801306526809a	X	you spend a lot of time being underwhelmed with force.com
563e33ab61a801306526809b	X	it will alleviate some of the issues, but you will still be bound to the Force.com database which is terrible and you'll not really get true control over your deployments. It's still early days and this may change going forward, but right now it doesn't look to be an overly compelling alternative.
563e33ab61a801306526809c	X	VMForce is dead: siliconangle.com/blog/2011/09/01/…
563e33ab61a801306526809d	X	We're currently looking at using the Force.com platform as our development platform and the sales guys and the force.com website are full of reasons why it's the best platform in the world. What I'm looking for, though, is some real disadvantages to using such a platform.
563e33ab61a801306526809e	X	Here are 10 to get you started. Disclaimers/Disclosures: There are lots of benefits to a hosted platform such as force.com. Force.com does regularly enhance the platform. There are plenty of things about it I like. I make money building on force.com
563e33ab61a801306526809f	X	I see you've gotten some answers, but I would like to reiterate how much time is wasted getting around the various governor limits on the platform. As much as I like the platform on certain levels, I would very strongly, highly, emphatically recommend against it as a general application development platform. It's great as a super configurable and extensible CRM application if that's what you want. While their marketing is exceptional at pushing the idea of Force.com as a general development platform, it's not even remotely close yet. The efficiency of having a stable platform and avoiding big performance and stability problems is easily wasted in trying to code around the limits that people refer to. There are so many limits to the platform, it becomes completely maddening. These limits are not high-end limits you'll hit once you have a lot of users, you'll hit them almost right away. While there are usually techniques to get around them, it's very hard to figure out strategies for avoiding them while you're also trying to develop the business logic of your actual application. To give you a simple sense of how developer un-friendly the environment is, take the "lack of debugging environment" referred to above. It's worse than that. You can only see up to 20 of the most recent requests to the server in the debug logs. So, as you're developing inside the application you have to create a "New" debug request, select your name, hit "Save", switch back to your app, refresh the page, click back to your debug tab, try to find the request that will house your debug log, hit "find" to search for the text you're looking for. It's like ten clicks to look at a debug output. While it may seem trivial, it's just an example of how little care and consideration has been given to the developer's experience. Everything about the development platform is a grafted-on afterthought. It's remarkable for what it is, but a total PITA for the most part. If you don't know exactly what you are doing (as in you're certified and have a very intimate understanding of Apex), it will easily take you upwards of 10-20x the amount of time that it would in another environment to do something that seems like it would be ridiculously simple, if you can even succeed at all. The governor limits are indeed that bad. You have a combination of various limits (database queries, rows returned, "script statements", future calls, callouts, etc.) and you have to know exactly what you are doing to avoid these. For example, if you have a calculated rollup "formula" field on an object and you have a trigger on a child object, it will execute the parent object triggers and count those against your limits. Things like that aren't obvious until you've gone through the painful process of trying and failing. You'll try one thing to avoid one limit, and hit another in a never ending game of "whack a limit". In the process you'll have to drastically re-architect your entire app and approach, as well as rewrite all of your test code. You must have 75% test code coverage to deploy into production, which is actually very good thing, but combined with all of the other limits, it's very burdensome. You'll actually hit governor limits writing your test code that wouldn't come up in normal user scenarios, but that will prevent you from achieving the coverage. That is not to mention a whole host of other issues. Packaging isn't what you expect. You can't package up your app and deliver it to users without significant user intervention and configuration on the part of the administrator of the org. The AppExchange is a total joke, and they've even started charging 5K just to get your app listed. Importing with the data loader sucks, especially if you have any triggers. You can't export all of your data in one step that includes your relationships in such a way that it can easily be re-imported into another org in a single step (for example a dev org). You can only refresh a sandbox once a month from production, no exceptions, and you can't include your data in a refresh by default unless you have called your account executive to get that feature unlocked. You can't mass delete data in custom objects. You can't change your package names. Certain things can take numerous days to complete after you have requested them, such as a data backup before you want to deploy an app, with no progress report along the way and not much sense of when exactly the export occurred. Given that there are synchronicity issues of data if there are relationships between the data, there are serious data integrity issues in that there is no such thing as a "transaction" that can export numerous objects in a single step. There are probably some commercial tools to facilitate some of this, but these are not within reach to normal developers who may not have a huge budget. Everything else the other people said here is true. It can take anywhere from five seconds to a minute sometimes to save a file. I don't mean to be so negative because the platform is very cool in some ways and they're trying to do things in a multi-tenant environment that no one else is doing. It's a very innovative environment and powerful on some levels (I actually like VisualForce a lot), but give it another year or two. They're partnering with VMware, maybe that will lead to giving developers a bit more of a playpen rather than a jail cell to work in.
563e33ab61a80130652680a0	X	Here's a few things I can give you after spending a fair bit of time developing on the platform in the last fortnight or so: There's no RESTful API. They have a soap based api that you can call, but there is no way of making true restful calls There's no simple way to take their SObjects and convert them to JSON objects. The visual force pages are ok until you want to customize them and then it's a whole world of pain. Visual force pages need to be bound to SObjects otherwise there's no way to get the standard input fields like the datepicker or select list to work. The eclipse plugin is ok if you want to work by yourself, but if you want to work in a large team with the eclipse plugin forget it. It doesn't handle synchronizing to and from the server, it crashes and it isn't really helpful at all. THERE IS NO DEBUGGER! If you want to debug, it's literally debug by system.debug statements. This is probably the biggest problem I've found Their "MVC" model isn't really MVC. It's a lot closer to ASP.NET Webforms. Your views are tightly coupled to not only the models but the controllers as well. Storing a large amount of documents is not feasible. We need to store over 100gb's of documents and we were quoted some ridiculous figure. We've decided to implement our document storage on amazons S3 infrastructure Even tho the language is java based, it's not java. You can't import any external packages or libraries. Also the base libraries that are available are severely limited so we've found ourselves implementing a bunch of stuff externally and then exposing those bits as services that are called by force.com You can call to external SOAP or REST based services but the message body is limited to 100kb's so it's very restrictive in what you can call. In all honesty, whilst there are potential benefits to developing on something like the force.com platform, for me, you couldn't use the force.com platform for true enterprise level apps. At best you could write some basic crud style applications but once you move into anything remotely complicated I'd be avoiding it like the plague.
563e33ab61a80130652680a1	X	Wow- there's a lot here that I didn't even know were limitations - after working on the platform for a few years. But just to add some other things... The reason you don't have a line-by-line debugger is precisely because it's a multi-tenant platform. At least that's what SFDC says - it seems like in this age of thread-rich programming, that isn't much of an excuse, but that's apparently the reason. If you have to write code, you have "System.debug(String)" as your debugger - I remember having more sophisticated server debugging tools in Java 1.2 about 12 years ago. Another thing I really hate about the system is version control. The Spring framework is not used for what Spring is usually used for - it's really more off a configuration tool in SFDC rather than version control. SFDC provides ZERO version-control. You can find yourself stuck for days doing something that should seem so ridiculously easy, like, say, scheduling a SFDC report to export to a CSV file and email to a list of recipients... Well, about the easiest way to do that is create a custom object with a custom field, with a workflow rule and a Visualforce email template... and then for code you need to write a Visualforce component that streams the report data to the Visualforce email template as an attachment and you write anonymous APEX code schedule field-update of the custom object... For SFDC developers, this is almost a daily task... trying to put about five different technologies together to do tasks that seem so simple.... And this can cause management headaches and tensions too - Typically, you'd find this out after getting a suggestion to do something that doesn't work in the user-community (like someone already said), and then trying many things that, after you developed them you'd find they just don't work for some odd-ball reason - like "you can't schedule a VisualForce page", or "you can't call getContent from a schedulable context" or some other arcane reason. There are so many, many maddening little gotcha's on the SFDC platform, that once you know WHY they're there, it makes sense... but they're still very bad limitations that keep you from doing what you need to do. Here's some of mine; You can't get record owner information "out of the box" on pretty much any kind of record - you have to write a trigger that links the owner on create of the record to the record you're inserting. Why? Short answer because an owner can be either a "person" or a "queue", and the two are drastically different entities... Makes sense, but it can turn a project literally upside down. Maddening security model. Example: "Manage Public Reports" permission is vastly different from "Create and Customize Reports" and that basically goes for everything on the platform... especially folders of any kind. As mentioned, support is basically non-existent. If you are an extremely self-sufficient individual, or have a lot of SFDC resources, or have a lot of time and/or a very forgiving manager, or are in charge of a SFDC system that's working fine, you're in pretty good shape. If you are not in any of these positions, you can find yourself in deep trouble. SFDC is a very seductive business proposition... no equipment footprint, pretty good security, fixed price, no infrastructure, AND you get web-based CRM with batchable, and schedualble processing... But as the other posters said, it is really quite a ramp-up in development learning, and if you go with consulting, I think the lowest price I've seen was $200/hour. Salesforce tends integrate with other things years after some technologies become common-place - JSON and jquery come to mind... and if you have other common infrastructures that you want to do an integration with, like JIRA, expect to pay a lot extra, and they can be quite buggy. And as one of the other posters mentioned, you are constantly fighting governor limits that can just drive you nuts... an attachment can NOT be > 5MB. Period. And sometimes < 3MB (if base64 encoded). Ten HTTP callouts in a class. Period. There are dozens of published governor limits, and many that are not which you will undoubtedly find and just want to run out of your office screaming. I really, REALLY like the platform, but trust me - it can be one really cruel mistress. But in fairness to SFDC, I'd say this: the biggest problem I find with the platform is not the platform itself, but the gargantuan expectations that almost anyone who sees the platform, but hasn't developed on it has.... and those people tend to be in positions of great authority in business organizations; marketing, sales, management, etc. Huge disconnects occur and heads roll, or are threatened to roll daily - all because there's this great platform out there with weird gotchas and thousands of people struggling daily to get their heads around why things should just work when they just don't and won't. EDIT: Just to add to lomaxx's comments about the MVC; In SFDC terminology, this is closely related to what's known as the "viewstate" -- aand it can be really buggy, in that what is on the VF page is not what is in the controller-class for the page. So, you have to go throught weird gyrations to synch whats on the page with what the controller is going to write to SF when you click your "save" button (or make your HTTP callout or whatever).... man, it's annoying.
563e33ac61a80130652680a2	X	I think other people have covered the disadvantages in more depth but to me, it doesn't seem to use the MVC paradigm or support much in the way of code reuse at all. To do anything beyond simple applications is an exercise in frustration compared to developing an application using something like ASP.Net MVC. Furthermore, the tools, the data layer and the frustration of trying to refactor code or rename fields during the development process doesn't help. I think as a CMS it's pretty cool but as a platform for non CMS applications, it's doesn't make sense to me.
563e33ac61a80130652680a3	X	The security model is also very very restrictive... but this isn't the worst part. You can't currently assert whether a user has the ability to perform a particular action. You can check to see what their role is, but you can't check if that role has permissions to perform the current action. Even worse is the response from tech support to "try the action and if there's an exception, catch it"
563e33ac61a80130652680a4	X	Considering Force.com is a "cloud" platform, its ability to act as a client to an external WSDL-defined service is pretty underwhelming. See http://force201.wordpress.com/2010/05/20/when-generate-from-wsdl-fails-hand-coding-web-service-calls/ for what you might end up having to do.
563e33ac61a80130652680a5	X	To all above, I am curious how the release of VMforce, allowing Java programmer to write code for Force.com, changes the disadvantages above? http://www.zdnet.com/blog/saas/vmforcecom-redefines-the-paas-landscape/1071
563e33ac61a80130652680a6	X	I guess they are trying to address these issues. At dreamforce they mentioned they we're trying to drop the Governor limits to only 4. I'm not sure what the details are. They have a REST API for early access, and they bought heroku which is a ruby development in the cloud. They split out the database, with database.com so you can do all your web development on and your db calls using database.com. I guess they are trying to make it as agnostic as possible. But right about now these are all announcements and early access so like their Safe Harbor statements don't purchase on what they say, only on what they currently have.
563e33ad61a80130652680a7	X	Or, to but it bluntly: SimpleDB is a database, S3 is a filesystem.
563e33ad61a80130652680a8	X	Indeed. That is the tl;dr.
563e33ad61a80130652680a9	X	I am wondering what's the real technical difference between Amazon S3 and Amazon SimpleDB. From my little understanding, both can be used to store and retrieve key/value pairs. So in what scenario would I need SimpleDB instead of S3? How is SimpleDB actually architected to deliver better performance (is it a file system, database backend or other)? Additional thought: I know that SimpleDB is about storing key/value pairs. Is there any info as to how it generates the hash index from the key for a lookup?
563e33ad61a80130652680aa	X	Amazon describes the differences between S3 and SimpleDB on the SimpleDB overview page. They store S3 objects in slower storage and SimpleDB objects on faster storage, meaning that it's more expensive to store the same amount of data in SimpleDB than it is in S3. If you're simply storing large binary objects as values and you're storing metadata on your own or can easily derive the key for the object you desire, then you'd probably just use S3. SimpleDB would be used over S3 in the case where you want to store multiple key-value pairs associated with an item and want to retain the ability to find items based on any of the key-value pairs. S3 allows you to store key-value metadata along with the object, but to find an object based on the metadata, you'd have to retrieve the metadata for each object in your bucket on your own and then decide which item (or items) you want to get. That can be slow and costly for large buckets. Additionally, there is a limit to the amount of metadata you can store and retrieve if you're using the REST API. SimpleDB is more flexible with the stored metadata. The key-value pairs are indexed, so querying can be quick. You can add and modify key-value pairs that are already in SimpleDB, where you'd need to delete and recreate objects in S3 to update metadata. However, there is a 1024-byte limit to the size of key and values and an overall limit to the amount of data in a domain (bucket analogue). All of SimpleDB's limits are listed in the SDB Developer Guide. If you're storing large objects with a lot of key-value metadata, you'd probably use a hybrid approach. Amazon's overview page suggests storing metadata in SimpleDB with one of the key-value pairs being a pointer to S3 for the object's data.
563e33ae61a80130652680ab	X	You make this sound like it's going to take a team of 10 engineers to pull it off :D
563e33ae61a80130652680ac	X	I'm listing all the technololgies to look into. If you have a simple app, then one engineer can pull it off with your technologies of choice. If you have complex requirements and high scale, then, yes, a complex backend can take many engineers to pull off and maintain. It's a function of your requirments and scale, not your tech choice necessarily.
563e33ae61a80130652680ad	X	Of course. I understand what you've said, but it just doesn't seem like it needs to require so many technologies at once just to get a simple query from iPhone to database. What I'm trying to do is the equivalent of an early-stage Twitter iOS app.
563e33af61a80130652680ae	X	It doesn't require all the techs. If you roll your own, pick a front end and back end tech. For example, you mentioned twitter, I think they started with Rails and MySql, then I think casandra, then Java for backend storage etc...
563e33af61a80130652680af	X	If you pick a combo like asp.net, SQL and azure, you can get off the ground quickly. Or combos like Node.js and mongodb - off the ground quickly. You don't need all these techs. Search for tutorials on these backend techs.
563e33af61a80130652680b0	X	I'm trying to create a back-end in which I can have many users communicate with each other amongst an iPhone app I'm creating. I've tried working with Core Data, Google App Engine, Google Cloud Storage, and Amazon Web Services (RDS & Elastic Beanstalk). Unfortunately, after weeks of trying to get any of this working, none of it will! I've been trying to get in touch with someone who would know how startups (when they were little) like Instagram, Path, and Pinterest have managed to do this. But everyone out there seems to despise this stuff as much as I'm growing to... I would love for someone to simply map out EXACTLY how I need to create a back-end database that I can save and query data to and from that many users can see. That means that just SQLite, Core Data, or Parse by itself isn't going to work here! A tutorial of some kind would be incredible.
563e33af61a80130652680b1	X	First off, technologies like CoreData and sqlite are typically local device storage. Local device storage is not going to get you shared cloud storage. Parse.com is a fast way for devices to access cloud storage and get going fast. Especially useful for games and other mobile apps to access cloud data via an app id and app key. It's simple storage to avoid creating your own backend if it fills all your needs and requirements. When you get to a multi-tenant cloud backend where you roll your own services and multiple devices accessing your cloud application you need to look into exposing your web API. Exposing RESTful API over http is great for devices and web clients. Exposing the data as JSON is especially conventient for the web and easily consumed by devices. Those web service end points in the cloud access some sort of backend storage which is optimized for concurrent access by mutliple clients. This is typically a SQL backend like MySQL, SQLServer etc... or a NoSQL solution like mongodb, couchDB, etc... Some front end web api technologies to look into: Some back end storage technologies to look into: If the data is used by many many multi-tenant clients, the backends can scaled up (larger and larger) or get sharded. Sharding is where the data for multiple users is split into many databases or datastores with some sort of lookup algorithm for requests to find where that users data is stored. The front end web api servers abstract the backend storage. Finally, you'll end up needing some sort of caching/fast lookup technology (if you're successful :): Your question is an open ended up broad question so start by googling many of these terms and technologies. Each of these links will have resources and tutorials. Get a cloud VM, play with each and decide which fits your needs best. There is no one size fits all solution.
563e33b061a80130652680b2	X	Following on to the idea to use the "consolidated billing" workaround approach, a person who wants to remain the sys admin in control could retain ownership of all accounts (create one for each user) and just use IAM in each account to grant an individual user the abilities they need. That way you as sys admin can still see when they leave machines running (otherwise they rack up a huge bill without you knowing it till the end of the month. Combine this approach with some 'alert' functionality built into AWS, or use the EC2 APIs to check across all accounts what is running.
563e33b061a80130652680b3	X	@dmohr - Good point; maybe a bit extreme for larger user bases, but it might sufficiently scale still depending on your needs and, as of today, is also the only option to gain sort of an audit trail (see Logs for actions on amazon s3 / other AWS services for more on this); if you are only concerned about running instances, you might also want to look into the new Amazon CloudWatch - Alarm Actions, see my related answer for details.
563e33b061a80130652680b4	X	What the title says. Within the master AWS account, I have several personal accounts, i.e. AWS Identity and Access Management (IAM) users. I would like to assign certain IAM users to groups and prevent them from terminating certain Amazon EC2 instances, de-registering certain Amazon Machine Images (AMIs), etc. I don't mind if they're playing with their own stuff, but I don't want them to touch my stuff. Is that possible?
563e33b061a80130652680b5	X	AWS has just announced Resource-Level Permissions for Amazon EC2 and Amazon RDS to address this long standing shortcoming of IAM support within EC2 and RDS (in comparison to other AWS services, see my original answer below for details/background): Today we are making IAM even more powerful with the introduction of resource-level permissions for Amazon EC2 and Amazon RDS. [...] On the EC2 side, you can now construct and use IAM policies to control access to EC2 instances, EBS volumes, images, and Elastic IP addresses. [...] Here are just a few of things that you can do: This solves a plethora of security complications and enables quite a few new use cases as well. Furthermore, EC2 policy statements can include reference to tags on EC2 resources, which allows to use the same tagging model and schema for permissions and for billing reports. Finally, there's an expanded set of condition tags [...] including ec2:Region, ec2:Owner, and ec2:InstanceType, see Condition Keys for Amazon EC2 for details. Here's a variation of Example 3: Allow users to stop and start only particular instances for the use case at hand, which allows users to start and stop [and terminate] only the instances that have the tag "department=dev": Support for resource-level permissions is restricted to the following set of actions on the indicated resources still, which excludes parts of the use case (e.g. de-registering AMIs) - the confidence in the foundation for this complex and far-reaching feature is apparently high enough though to announce that they plan to add support for additional APIs throughout the rest of 2013 (AWS doesn't usually publish any roadmaps): I'm afraid this isn't possible the way you'd like to do it (and many others for that matter, including myself). You want to restrict access to a particular service's resources rather than its actions - while AWS Identity and Access Management (IAM) supports both in principle, not every AWS product/service offers restrictions based on resources yet; unfortunately Amazon EC2 is one of these and even featured as an example for this very difference, see Integrating with Other AWS Products: The following table summarizes whether you can grant IAM permissions that control access to a service's actions, resources, or both. For example, you can use IAM to control which Amazon EC2 actions users have access to, but you can't use IAM to control users' access to AMIs, volumes, instances, etc. [emphasis mine] Depending on the needs of the other accounts, you might still be able to at least limit their ability to perform those actions considered destructive - you can explore the available actions via the AWS Policy Generator, for example: By default, you can terminate any instances you launch. If you want to prevent accidental termination of the instance, you can enable termination protection for the instance. That is, once you've enabled termination protection, anyone without permission to use ec2:ModifyInstanceAttribute cannot terminate these instances at all. Obviously the respectively restricted accounts won't be able to facilitate those calls for their own resources anymore. Furthermore this won't prevent them from running a fancy Cluster Compute Eight Extra Large Instance or so, incurring respective costs in turn ;) Depending on your setup/environment you might want to look into Consolidated Billing instead, which essentially provides a way to gather one or many AWS accounts under another one, which is paying for the resources used by the others. While this is primarily an accounting feature, it can be used to separate areas of concern as well - for example, it's quite common to facilitate separate development and production accounts to achieve respectively independent operation, not the least regarding IAM rights and the like. The introductory blog post New AWS Feature: Consolidated Billing provides a good overview, and here is the relevant topic from the AWS Consolidated Billing Guide regarding your apparent use case: The paying account is billed for all costs of the linked accounts. However, each linked account is completely independent in every other way (signing up for services, accessing resources, using AWS Premium Support, etc.). The paying account owner cannot access data belonging to the linked account owners (e.g., their files in Amazon S3). Each account owner uses their own AWS credentials to access their resources (e.g., their own AWS Secret Access Key). [emphasis mine] Obviously this functionality is targeted at larger customers, but depending on your scenario you might be able to come up with a solution to separate your AWS accounts and resources as needed still.
563e33b161a80130652680b6	X	Thanks for pointing out the stuff. I am also wondering it this pass the security review for salesforce appexchange
563e33b161a80130652680b7	X	@Shinya Koizumi Yes, There is a 'Force.com Toolkit for Amazon Web Services' App available on AppExchange.
563e33b161a80130652680b8	X	The tool is in a way different because it still pass through the salesforce. In my app the file is posted to aws directoy with Jquery file uploader using javascript. However we saved the url to the custom object so that it should be retrieved from s3.
563e33b161a80130652680b9	X	In the tool, there are two ways of uploading file. You are write about one, look at another. I have used it multiple times.
563e33b161a80130652680ba	X	Our app directory upload images to S3 from client side and on success we store the url to custom object in salesforce. However I saw a lot of people are indirectly upload images to S3 through salesforce using AWS API. I wonder if my approach pass through the salesforce security review.
563e33b161a80130652680bb	X	1. Its about Salesforce Limits. If you upload files to AWS via Salesforce, you will not be able to upload files more than 5MB as salesforce can accept files upto 5MB only. By directly uploading to AWS S3, you dont need to worry about this limit anymore. 2. Its Easy. Uploading files to AWS is very easy science Amazon provides required APIs to do it. You must be knowing difficulties of working in Salesforce Environment while doing this. 3. Its More Secure S3 provides all possible kind of security features possible. Directing files from Salesforce will make process slow and less secure.
563e33b261a80130652680bc	X	Hi Brad, The goal is: to make the user able to listen to the file, but to avoid he downloads it. Because the music is on selling on iTunes and it doesn't cost a fortune. The method I would like to apply, but i have not clue how, it's similare to the one on Reverbnation. I can't find their files nor download them. As far as I don't use special programs to record the stream, as you also told. The solution of the temporary URL (Token?) it's quite interesting. I need to develop something or to find some documentation to see if there is already something ready or if I have to start from scratch
563e33b261a80130652680bd	X	I need to stream audio files from my server, but I no want that: the content is "savable" on the user's PC (I mean the user shouldn't download the file and save it on his PC, due Copyright issues) the address & name of the original file, shouldn't be revealed. I tried the interesting solution here proposed: Streaming an MP3 on stdout to Jplayer using PHP I created a file called music.php with the proposed code. after have called the script music.php, calling it from the URL bar of any browser, it appears the multimedia reader. The play starts excellently. Then just right click and save as... and it saves a file called music.php But the content of this file, is the audio file itself. Thus just renaming it into music.mp3 and I downloaded the original MP3 :-/ which is what I no want it occurs. Please, do you have any suggestion? Thank ou so much
563e33b261a80130652680be	X	but I no want that: the content is "savable" on the user's PC This isn't possible. If you can play it, you can record and save it. the address & name of the original file, shouldn't be revealed Why? If you offer a script for downloading the file, then you can download that just as easily. If your intention is to prevent folks from directly linking to files, then you can check the Referer header to ensure that the request came from your site. However, anyone could still download the files if they wanted to by not sending that header. You might also consider how signed URLs for Amazon S3 work. HMAC is used to create a signed URL that expires after a certain period of time. The URL works, but only if the application granted access for the user to use it, and only for a few minutes or whatever you specify. In your case, simple session data could help if you are only allowing signed in users to get at your music. Your question doesn't say entirely what you're trying to do, so what works in your case is up to you. But the content of this file, is the audio file itself. Thus just renaming it into music.mp3 and I downloaded the original MP3 :-/ which is what I no want it occurs. Again, that isn't something you can prevent. Even if you go way out of your way to write your own streaming methods and use the Web Audio API or whatever to play it, it's still trivial to capture that data or even record the audio digitally.
563e33b361a80130652680bf	X	I am looking for a way to create functionality, similar to when you post a link to the existed web-site in facebook. If this statement is rather ambiguous, I will try to elaborate. When you paste your link and submit your post, facebook together with you link gives a small preview of the page, you are posting (text and may be a small image) What are the ways to achieve this? I read the similar post, but the thing is that I do not need an image so much, text will be sufficient. Working in PHP, but language is not important, because I am looking for a high level idea. Previously I was thinking about parsing content of the link with cURL but the thing is that in a lot of situations the text returned by facebook is not available on the page. Is there other ways?
563e33b361a80130652680c0	X	From what I can tell, Facebook pulls from the meta name="description" tag's content attribute on the linked page. If no meta description tag is available, it seems to pull from the beginning of the first paragraph <p> tag it can find on the page. Images are pulled from available <img> tags on the page, with a carousel selection available to pick from when posting. Finally, the link subtext is also user-editable (start a status update, include a link, and then click in the link subtext area that appears). Personally I would go with such a route: cURL the page, parse it for a meta tag description and if not grab some likely data using a basic algorithm or just the first paragraph tag, and then allow user editing of whatever was presented (it's friendlier to the user and also solves issues with different returns on user-agent). Do the user facing control as ajax so that you don't have issues with however long it takes your site to access the link you want to preview. I'd recommend using a DOM library (you could even use DOMDocument if you're comfortable with it and know how to handle possibly malformed html pages) instead of regex to parse the page for the <meta>, <p>, and potentially also <img> tags. Building a regex which will properly handle all of the myriad potential different cases you will encounter "in the wild" versus from a known set of sites can get very rough. QueryPath usually comes recommended, and there are stackoverflow threads covering many of the available options. Most modern sites, especially larger ones, are good about populating the meta description tag, especially for dynamically generated pages. You can scrape the page for <img> tags as well, but you'll want to then host the images locally: You can either host all of the images, and then delete all except the one chosen, or you can host thumbnails (assuming you have an image processing library installed and turned on). Which you choose depends on whether bandwidth and storage are more important, or the one-time processing of running an imagecopyresampled, imagecopyresized, Gmagick::thumbnailimage, etc, etc. (pick whatever you have at hand/your favorite). You don't want to hot link to the images on the page due to both the morality of it in terms of bandwidth and especially the likelihood of ending up with broken images when linking any site with hotlink prevention (referrer/etc methods), or from expiration/etc. Personally I would probably go for storing thumbnails. You can wrap the entire link entity up as an object for handling expiration/etc if you want to eventually delete the image/thumbnail files on your own server. I'll leave particular implementation up to you since you asked for a high level idea. but the thing is that in a lot of situations the text returned by facebook is not available on the page. Have you looked at the page's meta tags? I've tested with a few pages so far and this is generally where content not otherwise visible on the rendered linked pages is coming from, and seems to be the first choice for Facebook's algorithm.
563e33b361a80130652680c1	X	Full disclosure upfront, I'm a developer at ThumbnailApp.com. It's an JSON API service with an optional Javascript SDK which I think does exactly what you're after: It will parse a string to detect any urls and return the title, description and thumbnail of the asset. If the page has OpenGraph tags, it will use those for the image thumbnail. It's currently in private beta but we're adding more accounts each week. If you feel that you really need a do-it-yourself solution: Checkout the python based Webkit2Png and the headless browser PhantomJs. They can render webpages to an image (default size is 800x600), then you'll have to write some code to resize and crop the image like taswyn mentioned. Ideally you would then upload the resized image to Amazon S3 and then get it hosted on a CDN such as CloudFront. To get the title and description, first get the URL content (cURL or whatever) and you will need to check the content-type header to make sure it's a webpage. If it is, you can then use a HTML parser such as the SimpleHTMLDOM PHP library to grab the title and description meta data. If you want it exactly like Facebook you will also need to check for any OpenGraph tags specifically the og:image tag. Also don't forget about caching. The first render and description parsing can take a long time. Even if your site is fast, the webpage you're rendering could be slow and the best approach is to render / parse it once, then just save and return the resized image and meta data for subsequent requests. Depending on what your requirements are you may need to refresh the cached data every hour or you could get away with refreshing it once a day. To do it yourself takes quite a bit of work and lots of server configuration. I feel using a 3rd party service is a better way to go, but obviously I have a biased opinion :)
563e33b361a80130652680c2	X	What are your requirements ACID-wise? Apart from NoSQL solutions, have you investigated the use of SQL Server 2008 (and newer) FILESTREAM?
563e33b361a80130652680c3	X	why you wanna store file itself in a database? why not store the file on S3 and store the path to database? any particular reason? One of the problem with storing files in data is, it makes it difficult to manipulate , ie: resize, compress them on the fly.
563e33b361a80130652680c4	X	@Lucero Thanks for your reply. We would be streaming thumbnail from MongoDB using HttpHandler or Web API to the web based application. So it should be available all the time. We do use SQL Server 2008R2, but have not evaluated for storing it as BLOB. How does it scale up where we do have thousands of images? Also, we would not be using same server as a database and digital asset, so we have to get several sql server licenses, which could add up the cost.
563e33b361a80130652680c5	X	@DarthVader Thanks for your reply. All the database and file servers are needs to be hosted in client environment. That is why S3 is not an option.
563e33b361a80130652680c6	X	@user1810385, it does scale pretty well, this seems like a very close fit to what FILESTREAM was designed for. All data is then stored in the file system and it scales as well as your file system does (typically NTFS, but SANs are also supported), which is pretty good with many files. It also supports SQL Server replication. Have a look at this Whitepaper.
563e33b361a80130652680c7	X	We are evaluating our options for alternatives to the static file storage (which is hosted among multiple geographic location). We are looking into MongoDB for this. Does anyone could suggest pros & cons based above assumptions or any other alternatives? Some of MongoDB research reveals... I have prototyped to read digital asset from static file system and store it to MongoDB GridFS in default chunk. What is the better approach in storing thumbnail and originals to MongoDB? As thumbnail would always be less than 16MB, but original could/not be more than 16MB, so by default should I store all image asset on GridFS? I could envision to create different DB based upon content type, for example: one for PDF, Excel, Word, another for Image. I would really appreciate any input. Thank you.
563e33b361a80130652680c8	X	Some of MongoDB research reveals... Disk space consumption is 3 times larger than size of raw data Could cut down space consumption by -oplogSize parameter If We try to read chunk and stream to the browser speed could be 6 times slower than reading it from static file store. Replication is not bidirectional and it works as Master and Slave. Did you tried to store data or just found some info somewhere? There is always an overhead if you are using a database (no matter which) than a plain filestorage. Why? Well, you have indexes and meta information. mongodb is a shared nothing strong consistent db. So you write your data to one node and it then gets replicated. But you can use WriteConcerns (http://docs.mongodb.org/manual/core/write-operations/#write-concern) to wait and so make sure that your data is been written to a number, majority etc of nodes in a replicaset. With replication you can do rolling upgrades without downtime and it is also very easy to scale using sharding. And using shard-tags to 'pin' documents to specific shards. see here: http://www.kchodorow.com/blog/2012/07/25/controlling-collection-distribution/
563e33b461a80130652680c9	X	Hi, Scott, thx your pointing, it is clear. I will follow yours go in depth
563e33b461a80130652680ca	X	There is a basic inquiry, I just suppose I am a Mobile Game vendors, And I have some game player(users). I want to confirm whether the game player will be the IAM User ? If so, how to authenticate the IAM user by using Amazon congnito ? Document is not mention it ..
563e33b461a80130652680cb	X	There is no API to login/authenticate (verify passwords) for AWS IAM users. Amazon Cognito allows you to authenticate your players/users when they login with Facebook, Twitter, Digits, Amazon, Google or any OpenID Connect compatible provider (see External Identity Providers in the documentation). Alternatively, if you store your users and their credentials in your own backend database, you can use Cognito's Developer Authenticated Identities feature to authenticate your players (docs, sample). Whether using an external provider or Developer Authenticated Identities, Cognito enables you to provide your users AWS Credentials so that they can directly access your resources in AWS (e.g. such as S3 or DynamoDB).
563e33b461a80130652680cc	X	Can you use canvas to make the image smaller?
563e33b461a80130652680cd	X	How would I go about doing that? Could you post an answer?
563e33b461a80130652680ce	X	This might help: stackoverflow.com/questions/2303690/…
563e33b461a80130652680cf	X	Post your link as an answer if you want reputation points.
563e33b461a80130652680d0	X	You can use use canvas to make the image smaller. See stackoverflow.com/questions/2303690/…
563e33b461a80130652680d1	X	I have a Phonegap application that uses the camera Cordova API to take a picture and upload it to my server upon successful capture. The only problem is that the quality of the image is too good. It will really be a problem when I reach a higher user count. Here is my javascript: The file being uploaded is about 240KB in size and more then 1200 pixels wide. I tried adding the "quality" and "targetWidth" within the options to change the quality of the image, but to no avail. Any help would be appreciated, thanks!
563e33b461a80130652680d2	X	So I took @Gerben advice and I used html5 canvas to modify my image before using javascript (jQuery) to post it to the server. Here is the link.
563e33b461a80130652680d3	X	Utilizing a service like Amazon Web Services (AWS)S3 and the corresponding API, you could store your images elsewhere once they get to your server, and simply store the resulting links to the uploaded pictures. You still will eventually run into space issues, but expanding should be trivial. To start however, you can get a free trial account. This way you get blazin hi def pics for display, and the hosting server for your site won't get bogged down.
563e33b561a80130652680d4	X	I have a Postfix mail server running on a Amazon instance. Is there a way to interact with Postfix and save every mail that comes to server in a file on Amazon S3 and to save email details in a Amazon SQS queue (mail details: to, from, subject, etc, and Amazon S3 file name)? I'm new in Linux and Postfix. Maybe a script or something? Thank you.
563e33b561a80130652680d5	X	Yeah, it looks like you can use this: http://jeroensmeets.net/setup-postfix-to-forward-incoming-email-to-php/ To forward the mail to a php script (or language of your choice). You can then use amazon's API to upload to S3. There are command line tools and most languages have libraries as well (for python use boto). So you'd just have to paste everything together.
563e33b661a80130652680d6	X	This is a shopping question. You've already successfully identified the products that could work for you, but anything that we could add would be subjective opinion. Here's mine: If you're looking for a key-value file store, do Ceph. If you're looking for a filesystem to treat as a key-value store, Gluster will do just as well, but Ceph can do that also.
563e33b661a80130652680d7	X	@Charles, I agree, but I still think other could benefit from it. Thanks for your opinion!
563e33b661a80130652680d8	X	I use Ceph - it's been working for us so far.
563e33b661a80130652680d9	X	I'd like to have recommendations for a distributed key-value storage, for avg. entry size of up to 50KB, to be installed on a Linux environment (dedicated servers). A file-system solution would do. I found a few solutions: Ceph, Cassandra, Riak, and a few more. I'm looking for a storage solution for one of our components, it should be a key-value storage, flat namespace. The read/write patterns are very simple: Once a key-value is written, there are a few reads within the next hours. After that, nothing touches the given key-value. We'd like to keep the data for future purposes, "Storage mode". The "base" requirements are: I'd appreciate any recommendation on any of the tools I mentioned above (with total capacity of more than 50TB), or on a tool you think is sufficient.
563e33b661a80130652680da	X	I'm new to Openstack and I would understand very well the storage part, because I have to integrate Ceph as backend of Cinder and Swift for educational purposes ( we have to modify the crush algorithm to stress the reliability and another algorithm ti stress the performance). I've read all the tutorials, tutorials show what is an object storage and what is a block storage. I understand that the block storage (Cinder) is a storage for volume and the volume is attached to the vm when it is launched; and I understand that the object storage (swift) stores the images ( like cow2) of the image that are used to launch the virtual machine. But, do I need both block storage and object storage? The files contained in the volume are stored inside Cinder or Swift? I don't understand very well where files and other things are stored.
563e33b661a80130652680db	X	You do not need Swift with Nova. You can use Ceph for images and block storage with Glance and Cinder. See the following for directions: http://ceph.com/docs/master/rbd/rbd-openstack/ . Using Ceph with Nova is the easiest way to do storage.
563e33b661a80130652680dc	X	Swift = object storage for storing objects accessible over a rest api or http get. This is akin to S3 at Amazon Cinder = block storage, a volume, like a disk, or san allocation, or lvm volume. This is like EBS in Amazon Ceph = a suite of storage services for block, network, and object storage. You do not need to use swift if you do not have a need for object storage -or- you can use Ceph for that purpose. ref: https://www.mirantis.com/blog/object-storage-openstack-cloud-swift-ceph/
563e33b661a80130652680dd	X	400 Bad request?
563e33b661a80130652680de	X	Thanks for this!
563e33b761a80130652680df	X	Arguably, the server could have a logic branch that follows your suggestion 2 without executing code on the server.
563e33b761a80130652680e0	X	The CORS spec is silent on how servers should respond to invalid CORS requests. For example, if the request Origin is invalid, the CORS spec states: "terminate this set of steps. The request is outside the scope of this specification." There is similar language for other error cases, such as requesting invalid methods or headers. What should the expected response be in the case of CORS errors? I understand that different servers may require different behavior. But I'm looking for a standard response or responses that could be acceptable if the server owner doesn't care.
563e33b761a80130652680e1	X	Yes, I realize I'm answering my own question, but here it goes anyway. I did some research into this and it seems like behavior falls into two camps: 1) Return an error if the CORS request is invalid. This is the route taken by the Java CORS filter project. This library returns 2) Return CORS headers in the response and let the browser sort out the access details. This seems to be the far more common approach, and is used by the APIs for Amazon S3, SoundCloud, FourSquare and Spotify (the latter two APIs benefit from only supporting simple CORS requests). In this case, the server doesn't do any error checking, and just returns the CORS headers for the supported origin, method and headers. The browser still makes the request, but if the CORS response headers don't match the user's request, the browser withholds the response from the user. Each of these methods has their pros and cons. Method #1 is more closely aligned to the CORS spec, but provides no useful debugging information to the user. Method #2 provides more insight into what the supported methods and headers are. It is also easier to implement, since the server is returning the same response headers regardless of the request headers. However this comes at the expense of actually executing the request on the server-side, even though the browser will block the response from the user. This may not be desirable for a method with side-effects, such as POST, PUT or DELETE and highlights the fact that CORS should not be used as an authentication mechanism. (Note that my research above is by no means exhaustive. It was tough to see the true behavior for many APIs, since they either required auth, or blocked the request at a different level for some other error, such as an unsupported method.)
563e33b761a80130652680e2	X	I would expect that it would depend on the nature of the error, but I would expect some 4xx error such as 403 (if the request fails some required criteria) or 404 (if the method isn't available).
563e33b761a80130652680e3	X	There are similar questions: Looking for a simple and minimalistic way to store small data packets in the cloud Mine is this one: 1) it needs to be reliable 2) it needs to be secure Thanks
563e33b761a80130652680e4	X	Nasuni has a free white paper on the state of cloud storage and you can see the benchmarks of the major cloud providers (Amazon, Azure, Google). How reliable they are, performance etc. http://www6.nasuni.com/the-state-of-cloud-storage-in-2013-nasuni-industry-report.html All major providers provide: free ingress to their cloud, global CDNs, https security based on certificates and 3x/6x redudancy backups. (it is the smaller/cheaper providers to watch out for that do not provide everything mentioned). All major cloud providers provide high level APIs to their cloud platform. For sending small data packets. However, I would be interested in low level API features. For example, you can have a 40 gig file in Azure and download/manipulate the last 16 bits if you want to rather than downloading the entire file locally. Another thing that some cloud providers don't all provide is custom domains behind your storage URL that sometimes is important. The API language if you need to do automation may be important to you. If you look at Google, they support Java, Python, Go and PHP...if you need .NET Azure has much better support there. In my opinion, you can't really go wrong with any of the major providers and the best part is to test the upload/storage of Azure/Amazon/Google is really cheap even for a large amount of files.
563e33b761a80130652680e5	X	Your best option is to use object storage on one of the big cloud providers. The key thing to note is to not use a CDN and use an API to access your files (all the big providers have clients in Python, Java, .NET, PHP, Ruby, node.js). Here's a little list: Rackspace's CloudFiles, built off of OpenStack Swift. Amazon's Simple Storage Service (S3) Least Authority's Simple Secure Storage Service Microsoft's Azure Blob Storage
563e33b761a80130652680e6	X	Take a look at this: https://www.dropbox.com/developers/datastore/tutorial/http You can use it either from a JSON or a HTTP posting point of view. (I recently asked about it: Temporary cloud storage and sending script)
563e33b861a80130652680e7	X	I have an EC2 instance and an S3 bucket in different region. The bucket contains some files that are used regularly by my EC2 instance. I want to programatically download the files on my EC2 instance (using python) Is there a way to do that?
563e33b861a80130652680e8	X	Lots of ways to do this from within python Boto has S3 modules which will do this. http://boto.readthedocs.org/en/latest/ref/s3.html You could also just use the python requests library to download over http AWS Cli also give you an option to download from the shell:
563e33b861a80130652680e9	X	Adding to what @joeButler has said above... Your instances need permission to access S3 using APIs. So, you need to create IAM role and instance profile. Your instance needs to have instance profile assigned when it is being created. See page 183 (as indicated on bottom of page. The topic name is "Using an IAM Role to Grant Permissions to Applications Running on Amazon EC2 Instances") of this guide: AWS IAM User Guide to understand the steps and procedure.
563e33b861a80130652680ea	X	As mentioned above, you can do this with Boto. To make it more secure and not worry about the user credentials, you could use IAM to grant the EC2 machine access to the specific bucket only. Hope that helps.
563e33b961a80130652680eb	X	I can save my app and data to S3. But how should I say to a new EC2 instance to run my app when it's started?
563e33b961a80130652680ec	X	You can setup whatever you want on an instance, and configure your app to run at startup. Then 'bundle' the instance (docs.amazonwebservices.com/AWSEC2/latest/GettingStartedGuide/…) all your settings get saved into an AMI that gets saved to S3. You can then start/stop your customised AMI whenever you want.
563e33b961a80130652680ed	X	What I need is a powerful machine that will run my .NET code one hour a day. I can't use EC2 cause it will loose all my data on shutdown. I need a virtual PC that I can start at specific time, and this PC should start my .exe/service/whatever automatically. Can I ask Amazon MapReduce to start a Windows instance and execute my code?
563e33b961a80130652680ee	X	Amazon MapReduce is a webservice for processing big chunk of data - not somewhere to run your .net code. EC2 is virtual server hosting - can you save your data to an external webservice, on your own machine, or S3? This library is available from .net: http://developer.amazonwebservices.com/connect/entry.jspa?externalID=129
563e33b961a80130652680ef	X	Amazon Elastic MapReduce is designated to be integrated with EC2 and S3, providing the infrastructure for intense data processing applications. MapReduce is centered around the concept of Job Flow, where each Job can contain one or more steps. Each step takes some data from S3, distributes it to the EC2 instances configured and then writes the results back to S3. So basically you're supposed to upload your application and data to S3, then configure how many and what type of EC2 instances you want, specify the location of your application and your data on S3 and then start the job. There are several ways in which you can start the job: either logging in to the AWS Management Console, or using the exisitng APIs or the Command Line tools. There is also a C# Library for MapReduce available from Amazon that can help. However, you should note that your application needs to employ to MapReduce programming model to be able to run distributedly, so you can't really just run any .Net code. There's another post here with some good answers relating to .Net frameworks in regards to the MapReduce implementation.
563e33ba61a80130652680f0	X	What's your reasoning for not wanting to use the local images directory?
563e33ba61a80130652680f1	X	May be this link would help stackoverflow.com/questions/20911776/…
563e33ba61a80130652680f2	X	I'm somewhat new to S3. I'm wondering if it's a good idea to put public/images in S3 bucket and if so is there an easy way to do the move?
563e33ba61a80130652680f3	X	If you look in config/environments/production.rb, you'll see the following line of code (commented out): If you want to keep your assets (images, stylesheets, etc.) on S3, just put the correct url there. You might be interested in this from Amazon: http://developer.amazonwebservices.com/connect/message.jspa?messageID=159916 and this in the Rails API: http://api.rubyonrails.org/classes/ActionView/Helpers/AssetTagHelper.html Asset hosts are good if you are serving up a bunch of static assets or storing bulky ones on expensive servers. If you're low volume or are serving up small assets, then S3 might not make sense. By the way, Paperclip understands S3, so it's a natural attachment-handling gem for this kind of thing.
563e33ba61a80130652680f4	X	Unless you're storing very large or extremely-frequently-accessed assets in public/, there's no reason to put that on an external asset host. It'd be a needless increase in the complexity of your app. If you do have large or frequently-accessed assets, you probably want to be using Amazon's CloudFront content distribution network in addition to S3. Steve Ross's answer is good for the Rails-side how-to.
563e33ba61a80130652680f5	X	Welcome to Stack Overflow. You're taking the wrong approach for asking a question on Stack Overflow. You're asking for opinions, without telling what you've tried and explaining why it didn't work, resulting in a very open-ended question where we have to supply a textbook's worth of information to cover all the possible situations. Instead, you need to be as accurate and exact as possible, detailing what you've done and why it doesn't work for you. THEN we can help you. And, lose the opening paragraph. That'll only act as a red-flag.
563e33ba61a80130652680f6	X	As for pulling in images from Facebook, you'll never achieve the sort of storage savings you want by pulling in the images vs. storing IDs or URLs to images. Also, there are a slew of copyright issues you can encounter by storing the images on your site vs. leaving them on Facebook. IF I were to do it, I'd use CSS to set the image size, let the browser scale it during the render to the page, and not try downloading or storing them.
563e33ba61a80130652680f7	X	If you have huge number of image to show, then go with " resize - store - show". If less then go with on fly process.
563e33ba61a80130652680f8	X	@theTinMan Thank you, I appreciate the feedback and suggestions. I have a much better idea of what I need to do now
563e33ba61a80130652680f9	X	Thank you for showing me these resources
563e33ba61a80130652680fa	X	I have a Ruby on Rails application where part of the core purpose involves displaying images that I pull from the Facebook Graph API. And I want to be able to display the images in a uniform size. What I'm hoping I can do: I'm trying to just store the id of the image from Facebook to my database and resize the image after I pull it from Facebook's servers. I was thinking that this would save on storage costs. I've been looking at the RMagick gem that binds Ruby to the ImageMagick library, specifically their resize method. Another option I've considered: I'm thinking about adding a cropping feature. To do this though, I think I would need to set up a storage service, like Amazon's S3. Then I would pull from the image storage service where the cropped images are stored instead of pulling from Facebook and resizing client-side. Are there any options I might not be considering? And how accurate are my examples to approaches that could be taken.
563e33bb61a80130652680fb	X	If you have a huge number of images to show, then go with "resize - store - show". If less then go with an on-the-fly process. Here are a few well-known gems that help you process images, but choose according to your requirement: See "Compare CarrierWave ,Paperclip and Dragonfly in rails" and "Refile: Fixing Ruby File Uploads".
563e33bb61a80130652680fc	X	OK, can ypu provide a piece of code how to retry a part of the upload upon failure?
563e33bb61a80130652680fd	X	@Alexey - I've added a sample to hopefully put you on the right track. The basic idea is to retry a part if it fails instead of just cancelling the entire upload.
563e33bb61a80130652680fe	X	Thanks, this resolved my problem. A good idea is to also use several threads to allow faster upload
563e33bb61a80130652680ff	X	@Alexey - Great. Happy to help:) Yes, being able to now use multiple threads will be a big win. Previously uploads to S3 were slow and unreliable as you've seen.
563e33bb61a8013065268100	X	One file of size 11 GB. There are no 5 GB limit now, because if you upload it through Amazon web interface (with Enable Large files option) it allows you to upload files up to 5TB. I just want to do this from Java program
563e33bb61a8013065268101	X	Quote from their docs (link is in my post above): "Upload objects in parts—Using the Multipart upload API you can upload large objects, up to 5 TB."
563e33bb61a8013065268102	X	This is a new feature of S3.
563e33bb61a8013065268103	X	Thanks guys for correcting me. :)
563e33bb61a8013065268104	X	I tried to use Amazon SDK (Java) sample code called S3TransferProgressSample.java to upload large files to Amazon s3 storage (also posted here http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?HLuploadFileJava.html). But when I am trying to upload 11 GB files the upload is getting stuck at different points with the error message "Unable to upload file to Amazon S3: Unable to upload part: Unable toexecute HTTP request: Unbuffered entity enclosing request can not be repeated " (attached screesnhot). It looks like after IOException occurs SDK is not able to retry the request (see below). Does anyone encounter this and what is the best-practise to resolve this? Any code is appreciated INFO: Received successful response: 200, AWS Request ID: 2B66E7669E24DA75 Jan 15, 2011 6:44:46 AM com.amazonaws.http.HttpClient execute INFO: Sending Request: PUT s3.amazonaws.com /test_file_upload/autogenerated.txt Parameters: (uploadId: m9MqxzD484Ys1nifnX._IzJBGbCFIoT_zBg0xdd6kkZ4TAtmcG0lXQOE.LeiSEuqn6NjcosIQLXJeKzSnKllmw--, partNumber: 1494, ) Jan 15, 2011 6:45:10 AM org.apache.commons.httpclient.HttpMethodDirector executeWithRetry INFO: I/O exception (java.net.SocketException) caught when processing request: Connection reset by peer: socket write error Jan 15, 2011 6:45:10 AM org.apache.commons.httpclient.HttpMethodDirector executeWithRetry INFO: Retrying request Jan 15, 2011 6:45:12 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:12 AM org.apache.commons.httpclient.HttpMethodDirector executeWithRetry INFO: I/O exception (java.net.SocketException) caught when processing request: Connection reset by peer: socket write error Jan 15, 2011 6:45:12 AM org.apache.commons.httpclient.HttpMethodDirector executeWithRetry INFO: Retrying request Jan 15, 2011 6:45:13 AM org.apache.commons.httpclient.HttpMethodDirector executeWithRetry INFO: I/O exception (java.net.SocketException) caught when processing request: Connection reset by peer: socket write error Jan 15, 2011 6:45:13 AM org.apache.commons.httpclient.HttpMethodDirector executeWithRetry INFO: Retrying request Jan 15, 2011 6:45:13 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:14 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:14 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:14 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:15 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:16 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:16 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:17 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:19 AM com.amazonaws.http.HttpClient execute WARNING: Unable to execute HTTP request: Unbuffered entity enclosing request can not be repeated. Jan 15, 2011 6:45:19 AM com.amazonaws.http.HttpClient execute .... Jan 15, 2011 6:45:21 AM com.amazonaws.http.HttpClient handleResponse INFO: Received successful response: 204, AWS Request ID: E794B8FCA4C3D007 Jan 15, 2011 6:45:21 AM com.amazonaws.http.HttpClient execute ... Jan 15, 2011 6:45:19 AM com.amazonaws.http.HttpClient execute INFO: Sending Request: DELETE s3.amazonaws.com /test_file_upload/autogenerated.txt Parameters: ... Jan 15, 2011 6:47:01 AM com.amazonaws.http.HttpClient handleErrorResponse INFO: Received error response: Status Code: 404, AWS Request ID: 0CE25DFE767CC595, AWS Error Code: NoSuchUpload, AWS Error Message: The specified upload does not exist. The upload ID may be invalid, or the upload may have been aborted or completed. Thank you! -Alexey
563e33bb61a8013065268105	X	Try using the low level API. This will give you far more control when things go wrong, as they are likely to do with an 11GB file. Requests to and from S3 do fail from time to time. With the low level API, you'll be able to retry a part of the upload if it fails. Refactoring the example in the Amazon docs a bit: Note: I am not a java developer so I could have messed things up syntactically, but hopefully this gets you going in the right direction. Also, you'll want to add in a 'retry counter' to prevent an endless loop if the upload repeatedly fails.
563e33bb61a8013065268106	X	I think you should try Multipart API supported by AWS. Check this out : http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/transfer/TransferManager.html
563e33bb61a8013065268107	X	Are you trying to upload a single file of size 11GB? Or the size of all your files combined is 11GB? Because Maximum File Size limit on S3 is 5GB.
563e33bb61a8013065268108	X	As a side note, 404 errors can be thrown if you try to do a multipart upload to a key that is already under a multipart upload.
563e33bb61a8013065268109	X	The answer of Geoff Appleford works for me. However, I would add a && retryCount < MAX_RETRIES to the while loop control statement and increment of the retryCount on every exception caught inside the while. Aviad
563e33bb61a801306526810a	X	I wanted to add a comment to Geoff Appleford's answer but SO wouldn't allow me to. In general his answer to use low level API works fine but even if we do now have a do-while loop the way for loop is designed there is in-built retry logic. In his code snippet the file position increases only when there is a success otherwise you are uploading the same part again.
563e33bc61a801306526810b	X	Am I right in assuming that mysql_affected_rows() can be directly read from the last request and does not do another request to the mysql server? No. =\
563e33bc61a801306526810c	X	I really searched a lot but didnt find anything like this out there that. Of course I would re use an existing solution. Preferrably open source. Do you know of anything like that? What you're trying to do seems a lot like a RapidShare-like share-locker service. Have you looked into the hundreds of clone scripts out there?
563e33bc61a801306526810d	X	@Alix Axel. As i said I have not found anything. A sharehoster would be a clients use case. I want to do a storage management network that enables better usage of servers so everyone participating gets a better deal in the end. The focus is on scaling, balancing, distributing, redundancy durability. I dont need some script with an upload form... I have indeed found some interesting stuff but its all based on block device repliaction with cannot be used here because it is too much of an constraint.
563e33bc61a801306526810e	X	Also, I heard that Lighttpd had memory faults, can't tell if they are still present in the latests releases. Nginx seems way harder to get used to.
563e33bc61a801306526810f	X	yeah you are right about nginx, thats why i wanted to avoid it. i also wanted to be everything to be common so others can use, modify contribute. therefore apache would be nice. lighttpd runs perfectly already. the issues were resolved years ago. its a relly nice fast bitch.
563e33bc61a8013065268110	X	+1, I came across this one a while ago.
563e33bc61a8013065268111	X	en.wikipedia.org/wiki/…
563e33bc61a8013065268112	X	I am designing a file download network. The ultimate goal is to have an API that lets you directly upload a file to a storage server (no gateway or something). The file is then stored and referenced in a database. When the file is requsted a server that currently holds the file is selected from the database and a http redirect is done (or an API gives the currently valid direct URL). Background jobs take care of desired replication of the file for durability/scaling purposes. Background jobs also move files around to ensure even workload on the servers regarding disk and bandwidth usage. There is no Raid or something at any point. Every drive ist just hung into the server as JBOD. All the replication is at application level. If one server breaks down it is just marked as broken in the database and the background jobs take care of replication from healthy sources until the desired redundancy is reached again. The system also needs accurate stats for monitoring / balancing and maby later billing. So I thought about the following setup. The environment is a classic Ubuntu, Apache2, PHP, MySql LAMP stack. An url that hits the currently storage server is generated by the API (thats no problem far. Just a classic PHP website and MySQL Database) Now it gets interesting... The Storage server runs Apache2 and a PHP script catches the request. URL parameters (secure token hash) are validated. IP, Timestamp and filename are validated so the request is authorized. (No database connection required, just a PHP script that knows a secret token). The PHP script sets the file hader to use apache2 mod_xsendfile Apache delivers the file passed by mod_xsendfile and is configured to have the access log piped to another PHP script Apache runs mod_logio and an access log is in Combined I/O log format but additionally estended with the %D variable (The time taken to serve the request, in microseconds.) to calculate the transfer speed spot bottlenecks int he network and stuff. The piped access log then goes to a PHP script that parses the url (first folder is a "bucked" just as google storage or amazon s3 that is assigned one client. So the client is known) counts input/output traffic and increases database fields. For performance reasons i thought about having daily fields, and updating them like traffic = traffic+X and if no row has been updated create it. I have to mention that the server will be low budget servers with massive strage. The can have a close look at the intended setup in this thread on serverfault. The key data is that the systems will have Gigabit throughput (maxed out 24/7) and the fiel requests will be rather large (so no images or loads of small files that produce high load by lots of log lines and requests). Maby on average 500MB or something! The currently planned setup runs on a cheap consumer mainboard (asus), 2 GB DDR3 RAM and a AMD Athlon II X2 220, 2x 2.80GHz tray cpu. Of course download managers and range requests will be an issue, but I think the average size of an access will be around at least 50 megs or so. So my questions are: Do I have any sever bottleneck in this flow? Can you spot any problems? Am I right in assuming that mysql_affected_rows() can be directly read from the last request and does not do another request to the mysql server? Do you think the system with the specs given above can handle this? If not, how could I improve? I think the first bottleneck would be the CPU wouldnt it? What do you think about it? Do you have any suggestions for improvement? Maby something completely different? I thought about using Lighttpd and the mod_secdownload module. Unfortunately it cant check IP adress and I am not so flexible. It would have the advantage that the download validation would not need a php process to fire. But as it only runs short and doesnt read and output the data itself i think this is ok. Do you? I once did download using lighttpd on old throwaway pcs and the performance was awesome. I also thought about using nginx, but I have no experience with that. But What do you think ab out the piped logging to a script that directly updates the database? Should I rather write requests to a job queue and update them in the database in a 2nd process that can handle delays? Or not do it at all but parse the log files at night? My thought that i would like to have it as real time as possible and dont have accumulated data somehwere else than in the central database. I also don't want to keep track on jobs running on all the servers. This could be a mess to maintain. There should be a simple unit test that generates a secured link, downlads it and checks whether everything worked and the logging has taken place. Any further suggestions? I am happy for any input you may have! I am also planning to open soure all of this. I just think there needs to be an open source alternative to the expensive storage services as amazon s3 that is oriented on file downloads. I really searched a lot but didnt find anything like this out there that. Of course I would re use an existing solution. Preferrably open source. Do you know of anything like that?
563e33bc61a8013065268113	X	MogileFS, http://code.google.com/p/mogilefs/ -- this is almost exactly thing, that you want.
563e33bc61a8013065268114	X	I am playing around with the boto library to access an amazon s3 bucket. I am trying to list all the file and folders in a given folder in the bucket. I use this to get all the file and folders: This gives me all the files and folders within the root , along with the sub-folders it has the files within them, like this: How can I list only the contents of say folder1, where in it will list something like: I can navigate to a folder using the but in that case it lists the files in folder2 as files of folder1 because I am trying you use string manipuations on the bucket path. I have tried every scenario and it still breaks in case there are longer paths and when folders have multiple files and folders( and these folders have more files). Is there a recursive way to deal with this issue?
563e33bc61a8013065268115	X	S3 has no concept of "folders" as may think of. It's a single-level hierarchy where files are stored by key. If you need to do a single-level listing inside a folder, you'll have to constraint the listing in your code. Something like if key.count('/')==1
563e33bc61a8013065268116	X	What I found most difficult to fully grasp about S3 is that it is simply a key/value store and not a disk or other type of file-based store that most people are familiar with. The fact that people refer to keys as folders and values as files helps to lend to the initial confusion of working with it. Being a key/value store, the keys are simply just identifiers and not actual paths into a directory structure. This means that you don't need to actually create folders before referencing them, so you can simply put an object in a bucket at a location like /path/to/my/object without first having to create the "directory" /path/to/my. Because S3 is a key/value store, the API for interacting with it is more object & hash based than file based. This means that, whether using Amazon's native API or using boto, functions like s3.bucket.Bucket.list will list all the objects in a bucket and optionally filter on a prefix. If you specify a prefix /foo/bar then everything with that prefix will be listed, including /foo/bar/file, /foo/bar/blargh/file, /foo/bar/1/2/3/file, etc. So the short answer is that you will need to filter out the results that you don't want from your call to s3.bucket.Bucket.list because functions like s3.bucket.Bucket.list, s3.bucket.Bucket.get_all_keys, etc. are all designed to return all keys under the prefix that you specify as a filter.
563e33bd61a8013065268117	X	All of the information is the other answers is correct but because so many people store objects with path-like keys in S3, the API does provide some tools to help you deal with them. For example, in your case if you wanted to list only the "subdirectories" of root without listing all of the objects below that you would do this: which should produce the output: You could then do: and get: And so forth. You can probably accomplish what you want with this approach.
563e33bd61a8013065268118	X	I may have a very large-scale computational task coming up in the next few months (Intensive processing of ~400 GB of data, probably in the few-thousands of CPU-hours). I am likely to do this either on a cluster at my school or Amazon's cloud computing service. It's a machine learning task, so it will be generally more of a scientific nature than software/business. I have done more than trivial parallel programming. What are good resources to learn about Amazon's cloud service and learn how to effectively harness it so I don't waste my money? More generally, what are good cloud computing resources?
563e33bd61a8013065268119	X	If you're just getting started, I would research EC2 and S3 - they're pretty much the intro-level services that Amazon offers. They have a free tier if you're making a new account, but if you want to mess around with a few machines and not worry about racking up a big bill while you're still testing your code, Eucalyptus offers an API-compatible version to Amazon's EC2 and S3 that you can mess around with. They normally offer software you can install on your own boxes, but it looks like in your case you may not want to install it on your own boxes, so you can use their Community Cloud to test everything out for free.
563e33bd61a801306526811a	X	Some of the useful stuff that get you started with Amazon Web Services: http://paulstamatiou.com/how-to-getting-started-with-amazon-ec2 Some interesting facts on EC2 & SQS - for building a scalable application: http://sqs-public-images.s3.amazonaws.com/Building_Scalabale_EC2_applications_with_SQS2.pdf A whole bunch of useful information for EC2, EBS, S3, SQS,... http://kenneth.kufluk.com/blog/2010/03/getting-started-with-amazon-ec2/
563e33bd61a801306526811b	X	s3 upload stream module?
563e33bd61a801306526811c	X	I would like to upload files of users to S3. I have a web client/NodeJS Express solution. So far in my research i found that using multipart uploads, i can do either one of these: 1.) If streaming is enabled directly using pipes, then i cannot control the size of the file. 2.) If size needs to be controlled, pipe/streaming has to be disabled and file has to be written to disk or memory buffer to check for file size then transfer it to S3. Is there any way i can get both of these ? I am looking at the following middle wares : busboy multiparty multer formidable
563e33bd61a801306526811d	X	At least with busboy, you can set various request-wide limits. So you could set a file size limit for all files (or do it on a per-file basis by doing the counting yourself and draining any extra data over your custom file size limit), then stream directly to S3 (using Amazon's multipart API). If busboy notifies you that the file size limit was reached, then you'd simply have to delete the partial file you just streamed to S3. Otherwise the alternative is like you said, to save to disk first and then upload to S3. Both have their advantages and disadvantages, but because of the nature of multipart streams, there really are no other solutions to this particular problem.
563e33bd61a801306526811e	X	I have written an archival system with Python Boto that tar's several dirs of files and uploads to Glacier. This is all working great and I am storing all of the archive ID's. I wanted to test downloading a large archive (about 120GB). I initiated the retrieval, but the download took > 24 hours and at the end, I got a 403 since the resource was no longer available and the download failed. If I archived straight from my server to Glacier (skipping S3), is it possible to initiate a restore that restores an archive to an S3 bucket so I can take longer than 24 hours to download a copy? I didn't see anything in either the S3 or Glacier Boto docs. Ideally I'd do this with Boto but would be open to other scriptable options. Does anyone know how given an archiveId, I might go about moving an archive from AWS Glacier to an S3 Bucket? If this is not possible, are there other options to give my self more time to download large files? Thanks! http://docs.pythonboto.org/en/latest/ref/glacier.html http://docs.pythonboto.org/en/latest/ref/s3.html
563e33be61a801306526811f	X	The direct Glacier API and the S3/Glacier integration are not connected to each other in a way that is accessible to AWS users. If you upload directly to Glacier, the only way to get the data back is to fetch it back directly from Glacier. Conversely, if you add content to Glacier via S3 lifecycle policies, then there is no exposed Glacier archive ID, and the only way to get the content is to do an S3 restore. It's essentially as if "you" aren't the Glacier customer, but rather "S3" is the Glacier customer, when you use the Glacier/S3 integration. (In fact, that's a pretty good mental model -- the Glacier storage charges are even billed differently -- files stored through the S3 integration are billed together with the other S3 charges on the monthly invoice, not with the Glacier charges). The way to accomplish what you are directly trying to accomplish is to do range retrievals, where you only request that Glacier restore a portion of the archive. Another reason you could choose to perform a range retrieval is to manage how much data you download from Amazon Glacier in a given period. When data is retrieved from Amazon Glacier, a retrieval job is first initiated, which will typically complete in 3-5 hours. The data retrieved is then available for download for 24 hours. You could therefore retrieve an archive in parts in order to manage the schedule of your downloads. You may also choose to perform range retrievals in order to reduce or eliminate your retrieval fees. — http://aws.amazon.com/glacier/faqs/ You'd then need to reassemble the pieces. That last part seems like a big advantage also, since Glacier does charge more, the more data you "restore" at a time. Note this isn't a charge for downloading the data, it's a charge for the restore operation, whether you download it or not. One advantage I see of the S3 integration is that you can leave your data "cooling off" in S3 for a few hours/days/weeks before you put it "on ice" in Glacier, which happens automatically... so you can fetch it back from S3 without paying a retrieval charge, until it's been sitting in S3 for the amount of time you've specified, after which it automatically migrates. The potential downside is that it seems to introduce more moving parts.
563e33be61a8013065268120	X	Using document lifecycle policies you can move files directly from S3 to Glacier and you can also restore those object back to S3 using the restore method of the boto.s3.Key object. Also, see this section of the S3 docs for more information on how restore works.
563e33be61a8013065268121	X	Where would I need to host an xml file in order to use it in Google maps for GEORss ?? From google api docs... var georssLayer = new google.maps.KmlLayer('http://api.flickr.com/services/feeds/geo/?g=322338@N20&lang=en-us&format=feed-georss'); georssLayer.setMap(map); this seems to do a great job of creating the info popouts and everything... It would seem i cannot have one locally... So i guess i need an aspx or ashx to push the xml...but does it have to be web accessible? Thx
563e33be61a8013065268122	X	Yes, it needs to be web accessible because Google converts the GeoRSS to KML on the server side: "The Google Maps API supports the KML and GeoRSS data formats for displaying geographic information. These data formats are displayed on a map using a KmlLayer object, whose constructor takes the URL of a publicly accessible KML or GeoRSS file. The Maps API converts the provided geographic XML data into a KML representation which is displayed on the map using a V3 tile overlay." http://code.google.com/apis/maps/documentation/javascript/overlays.html#KMLLayers If you are hosting a static XML file, you might use Amazon Web Service's Simple Storage Service: http://aws.amazon.com/s3/
563e33be61a8013065268123	X	That question adresses encoding the string. I want to encode an integer.
563e33be61a8013065268124	X	I am using NodeJS to interact with Amazon Web Services (specifically s3). I am attempting to use Server side encryption with customer keys. Only AES256 is allowed as an encryption method. The API specifies that the keys be base64 encoded. At the moment I am merely testing the AWS api, I am using throwaway test files, so security (and secure key generation) are not an issue at the moment. My problem is as follows: Given that I am in posession of a 256bit hexadecimal string, how do I obtain a base64 encoded string of the integer that that represents? My first instinct was to first parse the Hexadecimal string as an integer, and convert it to a string using toString(radix) specifying a radix of 64. However toString() accepts a maximum radix of 36. Is there another way of doing this? And even if I do this, is that a base64 encoded string of 256bit encryption key? The API reference just says that it expects a key that is "appropriate for use with the algorithm specified". (I am using the putObject method).
563e33be61a8013065268125	X	To convert a hex string to a base64 string in node.js, you can very easily use a Buffer; ...which will set str to the base64 equivalent of the hex string passed in ('112233...') You could also of course combine it to a one liner;
563e33be61a8013065268126	X	Ok, you highlight that Lambda leverage "STATELESSNESS" like it would be a problem. I'd like to remind, that correct REST is stateless.
563e33be61a8013065268127	X	As I gone over the AWS Lambda documentation, there are references on triggering services based on AWS events. I do not see references on hosting services in Lambda. Would like to understand whether is it possible to create RESTful services using AWS Lambda for web sites to consume or not? I can use NodeJs to develop the service.
563e33bf61a8013065268128	X	Correction : Amazon has launched - Amazon API Gateway which used Lambda What Is Amazon API Gateway? API Gateway helps developers deliver robust, secure, and scalable mobile and web application back ends. API Gateway allows developers to securely connect mobile and web applications to business logic hosted on AWS Lambda, APIs hosted on Amazon EC2, or other publicly addressable web services hosted inside or outside of AWS. With API Gateway, developers can create and operate APIs for their back-end services without developing and maintaining infrastructure to handle authorization and access control, traffic management, monitoring and analytics, version management, and software development kit (SDK) generation. API Gateway is designed for web and mobile developers who are looking to provide secure, reliable access to back-end APIs for access from mobile apps, web apps, and server apps that are built internally or by third-party ecosystem partners. The business logic behind the APIs can either be provided by a publicly accessible endpoint API Gateway proxies call to, or it can be entirely run as a Lambda function. https://aws.amazon.com/api-gateway As of today; AWS Lambda is focused on working out / responding to events like S3 put, DynamoDB Streams and also custom events [ there could be more event sources expected from Amazon ] - with heavily leveraging the STATELESSNESS style of programming. To build out a complete RESTful Service backend using AWS Lambda wouldn't be possible or in other words AWS Lambda would be a bad choice to build the RESTful Service. You can continue to use the NodeJS and make it run on top of EC2 or ElasticBeanstalk.
563e33bf61a8013065268129	X	As of the last month, the answer has changed. AWS Lambda functions can now return a synchronous response and AWS is now encouraging the use of Lambda as a mobile backend or potentially a full REST API. Documentation is a bit sparse at the moment, but you can start reading about it here for now: http://aws.amazon.com/lambda/whatsnew/
563e33bf61a801306526812a	X	Amazon now supports this directly with the API Gateway service. This post is a great rundown of how to get started.
563e33c061a801306526812b	X	The answer is Yes it is possible to build RESTful services using AWS lambda for websites to consume. Your question has been answered by Tim Wagner of Amazon at a conference presentation where an audience member asks a very similar question to yours. Question "If you want to trigger a lambda function based on a regular old web request coming in from a user on the internet ... they hit an address and they sent in a bunch of query parameters ... So theres just this one stateless kind of thing that comes in with a bunch of data and then you want to trigger a lambda function off of that? ... what are the options there? ... to fire off that node.js workload?" Answer 1 "... use something like beanstalk to create a web app. And then inside that webapp invoke the lambda functions for the portions of your workload or scenario that make sense to do" Answer 2 "... If you're able to ... constrain your calls to match lambda's request model then you can skip that (beanstak webapp) and simply call us as if we were a web service proxy for you" https://youtu.be/copO_JQQsBs?t=50m53s As for the node part of your question. The answer is yes you can develop your Lambda function using node.js Please refer to the Webinar from Amazon below. "The language that we have support for today is node.js" https://youtu.be/YEWtQsqIYsk?t=25m22s
563e33c061a801306526812c	X	Great explanation, thanks. S3 suggestion is interesting too.
563e33c061a801306526812d	X	@user882209 The play-s3 library is very good. It has helpers for creating signed upload forms for the client-side, and provides an asynchronous server-side API for handling the files on S3 once they've been uploaded.
563e33c061a801306526812e	X	It's not really akka that is key here but the fact that it is using netty and NIO. Especially if moveto is using zero copy which I'm sure it does or hope so.
563e33c061a801306526812f	X	@AdamGent Seems like it doesn't. You may need to use nio yourself instead of moveTo. But this is only after the upload has completed.
563e33c061a8013065268130	X	@m-z Ahh that is unfortunate (I didn't bother to look because I don't use Play). However my point still stands that its not Akka but Netty that is abstracting it since to your point: "this is only after the upload". You need Netty (NIO) to have the nonblocking not Akka... You could almost do the exact same code with out having Akka as a dispatcher (ie Vert.x).
563e33c161a8013065268131	X	Given this example code from Play documentation:
563e33c161a8013065268132	X	How 100 simultaneous slow upload requests will be handled (number of threads)? It depends. The number of actual threads being used isn't really relevant. By default, Play uses a number of threads equal to the number of CPU cores available. But this doesn't mean that if you have 4 cores, you're limited to 4 concurrent processes at once. HTTP requests in Play are processed asynchronously in a special internal ExecutionContext provisioned by Akka. Processes running in an ExecutionContext can share threads, so long as they are non-blocking--which is abstracted away by Akka. All of this can be configured in different ways. See Understanding Play Thread Pools. The Iteratee that consumes the client data must do some blocking in order to write the file chunks to disk, but done in small (and fast) enough chunks, this shouldn't cause other file uploads to become blocked. What I would be more worried about is the amount of disk I/O your server can handle. 100 slow uploads might be okay, but you can't really say without benchmarking. At some point you will run into trouble when the client input exceeds the rate that your server can write to disk. This will also not work in a distributed environment. I almost always choose to bypass the Play server entirely and direct uploads to Amazon S3. Will be uploaded file buffered in memory or streamed directly to disk? All temporary files are streamed to disk. Under the hood, all data sent from the client to the server is read using the iteratee library asynchronously. For multipart uploads, it is no different. The client data is consumed by an Iteratee, which streams the file chunks to a temporary file on disk. So when using the parse.temporaryFile BodyParser, request.body is just a handle to a temporary file on disk, and not the file stored in memory. It is worth noting that while Play can handle those requests in a non-blocking manner, moving the file once complete will block. That is, request.body.moveTo(...) will block the controller function until the move is complete. This means that if several of the 100 uploads complete at about the same time, Play's internal ExecutionContext for handling requests can quickly become overloaded. The underlying API of moveTo is also deprecated in Play 2.3, as it uses FileInputStream and FileOutputStream to copy the TemporaryFile to a permanent location. The docs advise you to use the Java 7 File API, instead, as it is much more efficient. This may be a little crude, but something more like this should do it:
563e33c161a8013065268133	X	I'm following this tutorial and 100% works like a charm : http://docs.aws.amazon.com/gettingstarted/latest/wah-linux/awsgsg-wah-linux.pdf but, in that tutorial, it use Amazon EC2 and RDS only. I was wondering what if my servers scaled up into multiple EC2 instances then I need to update my PHP files. do I have to distribute it manually across those instances? because, as far as I know, those instances are not synced each other. so, I decided to use S3 as replacement of my /var/www so the PHP files is now centralised in one place. so, whenever those EC2 scaled up, the files remains in one place and I don't need to upload to multiple EC2. is this the best practice to have centralised file server (S3) for /var/www ? because currently I still having permission issue when it's mounted using s3fs. thank you.
563e33c161a8013065268134	X	You have to put your /var/www/ in S3 and when your instances scaled up have to make 'aws s3 sync' from your bucket, you can do that in the userdata. Also you have to select a 'master' instance where you make changes, a sync script upload changes to S3 and with rsync it copy changes to your alive FE. This is because if you have 3 FE that downloaded /var/www/ from S3 and you want to make a new change you would have to make a s3 sync in all your instances. You can manage changes in your 'master' instance with inotify. Inotify can detect a change in /var/www/ and exec two commands, one could be aws s3 sync and then a rsync to the rest of your instances. You can get the list of your instances from the ELB through the AWS API. The last thing is check the instance terminate protection in your 'master' instance. Your architecture should look like here http://www.markomedia.com.au/scaling-wordpress-in-amazon-cloud/ Good look!!
563e33c161a8013065268135	X	I'm trying to download a file from my AWS bucket and store it in my temp folder. The below code returns this error: OpenURI::HTTPError (301 Moved Permanently (Invalid Location URI)): @filename is the filename within the bucket, including the extension, as a string. Here's what I've found out so far: The first BUCKET call returns the correct object, the second returns this: When you go to that url this is what is returned This (http://www.sitefinity.com/developer-network/forums/set-up-installation/amazon-s3---must-be-addressed-using-the-specified-endpoint) states "there's a limitation in S3, whereby if you create a bucket that is not "US Standard", you cannot use path-style syntax in the bucket address." However, I've checked this bucket and it's def. region US Standard. Edit: I'm foolish; I had read that the AWS default was US Standard. But west isn't standard, east is so I guess it does not default to US Standard. After fixing that foolishness with I'm now receiving this error: However, the url created from the below code does lead to the sound file. So the issue must lie with Thoughts?
563e33c161a8013065268136	X	There appear to be two things going on here. First, the US standard region is actually either Virginia or Oregon. You just need to realize that you are being redirected because Amazon has chosen Oregon for your bucket. The URL explaining the regions is currently here: http://docs.aws.amazon.com/AmazonS3/latest/dev/LocationSelection.html The second thing going on is redirection. You are being redirected to Oregon when you connect, because the typical S3 endpoint you will see is the closest one by latency DNS. When you later hard code in Virginia, you don't see your bucket as it is in Oregon. Amazon has some documentation here regarding how S3 will redirect you and how to handle this. http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTRedirect.html Since you are using a toolkit and rather than an API, I didn't see anything specifically in the ruby forge or Amazon documentation that would tell why the redirect is not followed. It looks as though other people with the AWS ruby SDK had this problem last year. It may be fixed in the newer versions. The documentation that I am referring to is here: http://amazon.rubyforge.org/doc/ Amazon appears to have newer documentation here: http://docs.aws.amazon.com/AWSRubySDK/latest/ Another thing is that I saw complaints that the redirect URL was malformed. While I was not able to duplicate that behavior, I would try to send the request to the Oregon endpoint to see if this resolves your issue.
563e33c161a8013065268137	X	This is no longer the case. See rhonda's answer below: stackoverflow.com/a/21836343/1101095
563e33c161a8013065268138	X	To all the upvoters of the above comment: the OP doesnt indicate whether they are wanting to search the file names or the key contents (e.g. file contents). So @rhonda's answer still might not be sufficient. It appears that ultimately this is an exercise left to the consumer, as using the S3 Console is hardly available to your app users and general users. Its basically only revant to the bucket owner and/or IAM roles.
563e33c261a8013065268139	X	This is exactly what I was looking for. Terrible user experience design to have zero visual cues
563e33c261a801306526813a	X	Cool I was looking for exactly that, thanks for sharing
563e33c261a801306526813b	X	Thanks for your answer. AWS should really have a think about the discoverability of this feature. Perhaps a simple search box/button would be helpful for their customers.
563e33c261a801306526813c	X	Need to select a file in the bucket, then start typing.
563e33c261a801306526813d	X	This should be the accepted answer!
563e33c261a801306526813e	X	docs for using prefix in ruby
563e33c261a801306526813f	X	I have a bucket with thousands of files in it. How can I search the bucket? Is there a tool you can recommend?
563e33c261a8013065268140	X	S3 doesn't have a native "search this bucket" since the actual content is unknown - also, since S3 is key/value based there is no native way to access many nodes at once ala more traditional datastores that offer a (SELECT * FROM ... WHERE ...) (in a SQL model). What you will need to do is perform ListBucket to get a listing of objects in the bucket and then iterate over every item performing a custom operation that you implement - which is your searching.
563e33c261a8013065268141	X	Just a note to add on here: it's now 3 years later, yet this post is top in Google when you type in "How to search an S3 Bucket." Perhaps you're looking for something more complex, but if you landed here trying to figure out how to simply find an object (file) by it's title, it's crazy simple: open the bucket, select "none" on the right hand side, and start typing in the file name. http://docs.aws.amazon.com/AmazonS3/latest/UG/ListingObjectsinaBucket.html
563e33c261a8013065268142	X	There are (at least) two different use cases which could be described as "search the bucket": Search for something inside every object stored at the bucket; this assumes a common format for all the objects in that bucket (say, text files), etc etc. For something like this, you're forced to do what Cody Caughlan just answered. The AWS S3 docs has example code showing how to do this with the AWS SDK for Java: Listing Keys Using the AWS SDK for Java (there you'll also find PHP and C# examples). List item Search for something in the object keys contained in that bucket; S3 does have partial support for this, in the form of allowing prefix exact matches + collapsing matches after a delimiter. This is explained in more detail at the AWS S3 Developer Guide. This allows, for example, to implement "folders" through using as object keys something like 
563e33c261a8013065268143	X	There are multiple options, none being simple "one shot" full text solution: Key name pattern search: Searching for keys starting with some string- if you design key names carefully, then you may have rather quick solution. Search metadata attached to keys: when posting a file to AWS S3, you may process the content, extract some meta information and attach this meta information in form of custom headers into the key. This allows you to fetch key names and headers without need to fetch complete content. The search has to be done sequentialy, there is no "sql like" search option for this. With large files this could save a lot of network traffic and time. Store metadata on SimpleDB: as previous point, but with storing the metadata on SimpleDB. Here you have sql like select statements. In case of large data sets you may hit SimpleDB limits, which can be overcome (partition metadata across multiple SimpleDB domains), but if you go really far, you may need to use another metedata type of database. Sequential full text search of the content - processing all the keys one by one. Very slow, if you have too many keys to process. We are storing 1440 versions of a file a day (one per minute) for couple of years, using versioned bucket, it is easily possible. But getting some older version takes time, as one has to sequentially go version by version. Sometime I use simple CSV index with records, showing publication time plus version id, having this, I could jump to older version rather quickly. As you see, AWS S3 is not on it's own designed for full text searches, it is simple storage service.
563e33c361a8013065268144	X	Another option is to mirror the S3 bucket on your web server and traverse locally. The trick is that the local files are empty and only used as a skeleton. Alternatively, the local files could hold useful meta data that you normally would need to get from S3 (e.g. filesize, mimetype, author, timestamp, uuid). When you provide a URL to download the file, search locally and but provide a link to the S3 address. Local file traversing is easy and this approach for S3 management is language agnostic. Local file traversing also avoids maintaining and querying a database of files or delays making a series of remote API calls to authenticate and get the bucket contents. You could allow users to upload files directly to your server via FTP or HTTP and then transfer a batch of new and updated files to Amazon at off peak times by just recursing over the directories for files with any size. On the completion of a file transfer to Amazon, replace the web server file with an empty one of the same name. If a local file has any filesize then serve it directly because its awaiting batch transfer.
563e33c361a8013065268145	X	Given that you are in AWS...I would think you would want to use their CloudSearch tools. Put the data you want to search in their service...have it point to the S3 keys. http://aws.amazon.com/cloudsearch/
563e33c361a8013065268146	X	Take a look at this documentation: http://docs.aws.amazon.com/AWSSDKforPHP/latest/index.html#m=amazons3/get_object_list You can use a Perl-Compatible Regular Expression (PCRE) to filter the names.
563e33c661a8013065268147	X	The way I did it is: I have thousands of files in s3. I saw the properties panel of one file in the list. You can see the URI of that file and I copy pasted that to the browser - it was a text file and it rendered nicely. Now I replaced the uuid in the url with the uuid that I had at hand and boom there the file is. I wish AWS had a better way to search a file, but this worked for me.
563e33c661a8013065268148	X	Yeah the new cli tools are based on botocore, which appears to be the groundwork for boto v3. Overall botocore is a great interface if you know the structure of the AWS API.
563e33c661a8013065268149	X	Excellent, succinct answer @James van Dyke. But I still have doubts about how secure this is, illustrated in my similar question here: stackoverflow.com/questions/29932355/…
563e33c661a801306526814a	X	What is the "key k"? is it a file with temporary credentials?
563e33c661a801306526814b	X	For future users, the key k is the file name you want from S3. If you have folder structure then the key will be, foldername/file.sh
563e33c661a801306526814c	X	I'm using CloudFormation to manage a Tomcat webserver stack but am tired of doing raw AMI management for new application versions. I'd like to move in the direction of Chef but don't have the time right now. Instead, I'm trying to conquer a simple problem in webserver instantiation: How can I download a "current" WAR when new machines spin-up? My thought was to utilize a private S3 bucket and cloudinit, but I'm a little stumped by what to do with IAM credentials. I could put them in the template's user data, but I'm loathe to do so, particularly because I'm version controlling that file. The only alternative I can think of is to use environment variables in the AMI itself. They'd have to be plaintext, but... eh, if you can break into my instance, you could zip up and download my entire webserver. As long as the IAM user isn't reused for anything else and is rotated regularly, it seems like a reasonable way to solve the problem. Am I missing anything? How can I securely download a private S3 asset using cloudinit?
563e33c661a801306526814d	X	Amazon recently announced a new feature where you can give "IAM roles" to your EC2 instances. This makes it fairly easy to allow specific instances to have permission to read specific S3 resources. Here's their blog post announcing the new feature: http://aws.typepad.com/aws/2012/06/iam-roles-for-ec2-instances-simplified-secure-access-to-aws-service-apis-from-ec2.html Here's the section in the EC2 documentation: http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/UsingIAM.html#UsingIAMrolesWithAmazonEC2Instances Here's the section in the IAM documentation: http://docs.amazonwebservices.com/IAM/latest/UserGuide/WorkingWithRoles.html IAM roles make the credentials available to the instance through HTTP, so any users or processes running on the instance can see them.
563e33c761a801306526814e	X	To update the answers to this question a bit: Along with IAM roles, the new AWS command-line client makes fetching these assets trivial. It will automatically pull AWS credentials bestowed via IAM from the environment and handle the refreshing of those credentials. Here's an example of fetching a single asset from a secure S3 bucket in a user-data script: Simple as that. You can also copy entire directory contents from S3 and uploaded, etc. See the reference material for more details and options.
563e33c761a801306526814f	X	An instance with an IAM Role has temporary security credentials that are automatically rotated. They're available via http at http://169.254.169.254/latest/meta-data/iam/security-credentials/RoleName, where RoleName is whatever you called your role. So they're easy to get from your instance, but they expire regularly. Using them is a bit tough. CloudFormation can't use temporary credentials directly. The Amazon Linux AMI has Python boto installed, and it's now smart enough to find and use those credentials for you automatically. Here's a one-liner you can put in a script to fetch a file from S3 bucket b, key k to local file f: boto finds and uses the role's temporary credentials for you, which makes it really easy to use.
563e33c761a8013065268150	X	@steefen ,which category 'Delete EBS Snapshots and Delete RDS Snapshots' fall in link ,modify actions or the other one ?
563e33c761a8013065268151	X	Also ,please look for the Queries Response Time
563e33c761a8013065268152	X	@anoop - good question: while it is not explicated, I would expect 'Delete EBS Snapshots and Delete RDS Snapshots' to fall within the Modify actions indeed, because they sort of modify resources too (by deleting them) and will consequently have a lower request limit than describe calls as well.
563e33c761a8013065268153	X	@steffen-thanks for reply, can you please write your answer in much detail into Queries Response Time ,so that anyone in future do not need to search more for such kind of throttleErrors problems in aws APIs?
563e33c761a8013065268154	X	I am wondering if there are any EC2 API Throttling Limits like EMR that would limit the number of calls you can make to EC2 per second or min? If so, what are they these limits? are they published anywhere?
563e33c761a8013065268155	X	Amazon EC2 has meanwhile documented their general Query API Request Rate handling: We throttle Amazon EC2 API requests for each AWS account to help the performance of the service. We ensure that all calls to the Amazon EC2 API (whether they originate from an application, calls to the Amazon EC2 command line interface, or the Amazon EC2 console) don't exceed the maximum allowed API request rate. Note that API requests made by IAM users are attributed to the underlying AWS account. The Amazon EC2 API actions are divided into the following categories: [...] If an API request exceeds the API request rate for its category, the request returns the RequestLimitExceeded error code. To prevent this error, ensure that your application doesn't retry API requests at a high rate. You can do this by using care when polling and by using exponential back-off retries. While the details might vary between the plethora of their services, I think it is safe to assume similar patterns in place usually (not the least due to EC2 backing many of the other services too). Both the aforementioned explanation from EC2 as well as a few indirect quotes from users interacting with AWS support seem to suggest that the limits can vary by service status and per account even, i.e. AWS is capable of raising/reducing limits for individual accounts depending on dedicated high performance use cases, suspected abuse etc., see e.g.: RequestLimitExceeded within Client Error Codes, hinting on per account handling: The maximum request rate permitted by the Amazon EC2 APIs has been exceeded for your account. [...] How do we get the EBS API rate limit increased?: Rightscale support have said we need to contact Amazon to get the EBS API rate limit increased as it seems we are exceeding calls to this API as we have a large number of MySQL databases in this account and they are all having EBS snapshots taken for backups. The AWS team's response to Error: Request limit exceeded using boto to launch and terminate instances: The rate limit that you will see with EC2 can vary depending on system load. Saul's answer to Amazon Web Services S3 Request Limit: However, after posting this question I received an email from AWS that said the had capped my LIST requests to 10 requests per second because I had too many going to a specific bucket. Of course their are usually exception to most rules, I'm aware of the following two services documenting specific limits: Amazon Route 53 is documenting its specific Limits on Amazon Route 53 API Requests and Entity Counts: All requests: Five requests per second per AWS account. [...] Amazon SES also documents a specific limit of one request/second for most API actions (presumably all but SendEmail and SendRawEmail), see e.g. GetSendQuota: This action is throttled at one request per second. As outlined on Troubleshooting API Request Errors, the correct approach for handling this on the client side is to implement Error Retries and Exponential Backoff in AWS (most AWS SDKs meanwhile apply this guidance automatically, including options to adjust the default retry policy or add a custom implementation even).
563e33c761a8013065268156	X	Is there a way to start / stop an Amazon EC2 instance from Google App Engine? I read this question In python, how do you launch an Amazon EC2 instance from within a Google App Engine app? and it seems to me that the answer there is not correct. Basically I can't install "boto" on the app engine.
563e33c761a8013065268157	X	Have you looked into KOALA? It's (more or less) a rewrite of boto for EC2, S3, EBS, and ELB that works in App Engine. From their page: KOALA (Karlsruhe Open Application (for) cLoud Administration) is a software service, designed to help you working with your Amazon Web Services (AWS) compatible cloud services and infrastructures (IaaS). The Amazon AWS public cloud and private cloud services based on Eucalyptus, Nimbus or OpenNebula are supported. The storage services Google Storage and Host Europe Cloud Storage can be used with KOALA too KOALA helps interacting with cloud services that implement the APIs of Elastic Compute Cloud (EC2) Simple Storage Service (S3) Elastic Block Store (EBS) Elastic Load Balancing (ELB) With KOALA the users can start, stop and monitor their instances, volumes and elastic IP addresses. They can also create and erase buckets inside the S3-compatible storage services S3, Google Storage and Walrus. It's easy to upload, check and modify data that is stored inside these storage services, the same way it can be done with S3Fox and the Google Storage Manager. KOALA itself is a service that is able to run inside the public cloud platform (PaaS) Google App Engine and inside Private Cloud platforms with AppScale or typhoonAE.
563e33c761a8013065268158	X	see Running Boto on Google App Engine (GAE) just copy the boto files into the root of your project.
563e33c761a8013065268159	X	Suggest moving to dba.stackexchange.com, as this is more infrastructure than programming.
563e33c861a801306526815a	X	I don't think dba.se would be the correct site, since this doesn't seem to be about the actual database, but rather about ec2 & ebs snapshots.
563e33c861a801306526815b	X	Thanks. Sounds like black magic, stuff going on behind the scenes. It would be very nice if Amazon set up a backup facility for EBS, as they have with RDS. I will experiment some with this then. Again, thanks for the very thorough reply.
563e33c861a801306526815c	X	You might also take a look at Eric Hammond's ec2-consistent-snapshot utility. It will freeze your filesystem while you take the snapshot which "should" get you the most reliable possible snapshots. As always, remember that unless you actually do successful test recoveries using your backup strategy, you don't actually have a backup strategy. :)
563e33c861a801306526815d	X	Ah yes, I have seen that before. It ended up going badly. Going to be testing the entire thing, including deletions of the snapshots, before we go much further. Thanks again.
563e33c861a801306526815e	X	This has ended up working fine, at least in test. I deleted the old snapshots, and was still able to restore from the most recent. Not going to work as sell if I move the journaling elsewhere, or shard. It's not simple stuff.
563e33c861a801306526815f	X	I'm a bit confused with how snapshots work as backups. I am currently running a script that does a snapshot every night of our mongo data and logs. But I'm seeing information that says that snapshots are incremental, which confuses me. Are they incremental? If so, which is the base one, and how would I reset the base? I'd rather just take a complete snapshot every time. Also, anyone know of a way to autoexpire old snapshots? They just keep building up.
563e33c861a8013065268160	X	Every EBS snapshot is a standalone snapshot, which, if restored onto a new volume, will give you a volume that is identical to the volume as it existed at the time of the snahshot. However, snapshots are stored in S3 and the way they are stored (and the way you are billed for the storage of them) is incremental. Amazon EBS snapshots are incremental backups, meaning that only the blocks on the device that have changed since your last snapshot will be saved. If you have a device with 100 GBs of data, but only 5 GBs of data has changed since your last snapshot, only the 5 additional GBs of snapshot data will be stored back to Amazon S3. Even though the snapshots are saved incrementally, when you delete a snapshot, only the data not needed for any other snapshot is removed. So regardless of which prior snapshots have been deleted, all active snapshots will contain all the information needed to restore the volume. In addition, the time to restore the volume is the same for all snapshots, offering the restore time of full backups with the space savings of incremental. — http://aws.amazon.com/ebs/ So behind the scenes, this snapshot contains only the blocks that changed from the prior snapshot... but restoring the snapshot does not mean you have to put the incremental pieces back together. EBS does that for you automatically, all behind the scenes. So, let's say you have a 100 GB EBS volume, and snapshots A, B, and C, taken in that order, and no other snapshots of the volume. Snapshot A would be 100GB in size (possibly less, since space you've never written to might be eliminated from the shapshot). If 20GB changed, then you took snapshot B, that snapshot would be 20GB in size, but if you restored it, the resulting volume would contain the full 100GB, because it has pointers back to the unchanged data from shapshot A. Then another 10GB changed, and you took snapshot C. That would be a 10GB snapshot, with pointers back to B for the previous data, and pointers back to A for the rest. Again, restoring this one would get you the full volume at the time you took snapshot C. Now, if you delete snapshot B, the blocks changed in snapshot B but not subsequently changed in shapshot C would roll forward into snapshot C so that you could still do a restoration of the entire volume at the point of snapshot C, and snapshot C would be a 30 GB snapshot. This is an oversimplification, because it's likely that some of the same blocks would have changed from A -> B and B -> C making the final version of C somewhat smaller than 30 GB but it does convey the general idea. Every snapshot stands alone for restoration purposes, but the inner workings of EBS store only the differences from the prior snapshot, and you only pay storage for the amount of data the snapshot contains. Unfortunately, at this time, there's no way to find out via the API how large each snapshot actually is, because this information isn't exposed... they always show to be the same size as the volume. There is no way to automatically purge snapshots. For my systems, I have written a script that runs once a day, looking for volumes to snapshot, based on their tags. Then it considers which volumes have sufficient snapshots based on my retention policy, and deletes any other snapshots -- but it will only delete snapshots that it, itself, created, and again this is based on tags that the snapshot script applies to snapshots it creates.
563e33c861a8013065268161	X	What checks are you doing in the service layer on the token and how come they are so expensive?
563e33c861a8013065268162	X	@Henrik The token is composed by some user info + secret salt + date emitted
563e33c861a8013065268163	X	Who is the salt secret for? Does each service share knowledge of the salt, or only the authentication service?
563e33c861a8013065268164	X	@Henrik It's implemented at the business level. This secret is handled by authentication manager. By the way, maybe I'm lost, but it seems that this detail has nothing to do with my question.
563e33c861a8013065268165	X	Salts are for increasing entropy so that you can't use rainbow tables on a whole range of digests/hashes, but only on a single hash + salt at a time. I'm not sure this is what you're after. So if your services don't know the salt, they don't know how to recreate the hash, and so then you need each service to talk to the authentication manager each time, because that's where the knowledge is.
563e33c861a8013065268166	X	Thanks for your answer. Let's see if others have other opinions. Completely stateless authentication is a good option.
563e33c861a8013065268167	X	I've a question about your answer. How service verifies that a token is valid? I mean that because ok, authentication service emits a token and a Web client stores it as a token. Next requests authenticates the application or user using this token. Since a token is a hash you cannot revert it to know what's its source plain text. How you authenticate this token in the service? Do you consider any token received from some application with a valid "AppKey" (client secret) eligible for authenticating the request?
563e33c861a8013065268168	X	(continuation) ... Since connection uses HTTPS/SSL, and application does requests to the service using an unique secret (the AppKey), the pair of secret (appkey) + token should be secure enough to understand that some request is authenticated. In the other hand, in a Web client token is stored in a cookie and token expiration is equal to the cookie expiration. If cookie expires, token too. As you said in your answer, cookie expiration would be increased after each successfully authenticated request to the service layer. Am I wrong?
563e33c961a8013065268169	X	We're talking about two different things. What I am suggesting in my response above is that you introduce public key infrastructure, because it can solve three problems for you; authentication, verification of message contents and encryption. The public key of C in the above example is the only identifier that the authentication service needs in order to find the identity of the caller. The token is signed by the authenticator, thereby making it intractable for any other agent to falsify the validity of when the token expires or who it is for. ...
563e33c961a801306526816a	X	If you were doing it with hashes, then you need to have a shared secret amongst your services, but in order to reverse the hash, to see what it means, you will need a lookup of some sort; this is what you were talking about being slow (an extra call for every request that S receives, to A or a lookup dictionary; token -> (expiry, uid); so I explained a different way, using signatures.
563e33c961a801306526816b	X	This isn't what I wrote though ;). I will write some code instead and include that, because that's unambiguous.
563e33c961a801306526816c	X	@Henrik It's a summary of my conclusions based on your answer and my own investigation. I believe that your answer is a good one, but it's too techie, as I said in some comment. Don't think that anyone will understand it. I think Stackoverflow should provide direct, easy to understand answers. That's why I tried to summarize everything in a single text :D
563e33c961a801306526816d	X	Yes, that's good, but you've changed some pieces of it, which is why I'd rather improve my own answer... :)
563e33c961a801306526816e	X	@Henrik And, why not? It's not about creating a competition but contributing to the community and share our knowledge !!
563e33c961a801306526816f	X	I mean, you've changed some important pieces that breaks the protocol (or changes protocol). But I don't think we'll get further in comments right now.
563e33c961a8013065268170	X	I'm implementing a set of RESTful services for some developments and one of these is an authentication service. This authentication service authenticates two kinds of identities: These RESTful services are stateless. When a client application authenticates against the authentication service, or when a human or machine authenticates as an identity using credentials, both operations generates an AppToken and UserToken respectively. These tokens are a salted hash so subsequent requests to the RESTful infrastructure will be authenticated without sharing AppKeys and credentials. Form the point of view of a fully stateless approach, these tokens should be stored no where in the service layer but in some kind of client-side state (f.e., a Web client would store it using HTTP cookies). This is how my current implementations are working right now. Because re-authenticating each request using these tokens and let the service layer receive the token coming from the client so it can compare what token comes from the client and check if it's a valid token re-generating it in the service layer and compare with the one owned by the client is too expensive, I've implemented a service layer AppToken and UserToken, both having an expiration date and an owner (the application or user for which the token have been created for), in order to check if the token coming from the client exists in the token store. How does clients interactively unauthenticate? Just dropping client-side security state. If it's a Web client, it drops the authentication cookie and just refreshing the page, client detects no authentication cookie and user is redirected to the login page. From the point of view of RESTful services, this is a stateless unauthentication: clients aren't aware about the trick of having a service layer pseudo-authentication state. It's just a service implementation detail - a performance optimization -. I'm not going to list the pros of stateless services because I'm absolutely sure that this approach is the way to go, but I find a problem: stateless authentication/unauthentication means that clients don't notify server that they close their session, so the security store ends with a lot of useless records. This isn't a great problem if service clients are ones that would have limited time sessions (f.e., 1 hour, 3 hours, a day...), but what happens if an user must be authenticated forever (8 months, a year)?. How do you distinguish what's an an expired token? There're some approaches in order to solve this situation: Whenever the service layer receives a request, it updates token expiration date, so an automated process may drop those tokens that have expired defining an arbitrary expiration of tokens (f.e. 24 hours). Compromise stateless nature of the architecture and let clients notify service layer that they don't want to be authenticated anymore, so service can drop the associated token to the client session (But wait... what happens if client closes a Web client? User will never actively notify service that the token must be dropped... So... Zombie tokens are there yet, so an automated process should drop them, but... what's a zombie token? I don't like this approach). Completely stateless authentication, no store, per-request authentication. This is the question! What's your suggested approach - even if it's not 1., 2. or 3. - and why? Thanks for this long reading - I honestly believe that question's conclusions are going to be extremely useful to anyone -!
563e33ca61a8013065268171	X	Stateless authentication, token-based. Assuming transport-level encryption. Aim: Client C wishes to speak to service S. Client C sends its shared secret or public key PKC for authentication to service A, for which C knows the endpoint and public key (PKA). A [now + interval | user-id or PKC]SA -> C Explained: Service A adds an interval to the current date/time as an expiration. In the buffer-to-be-sent is now the expiry date and the user id, PKC (assuming you have a valid Identity Provider). [now + interval | user-id or PKC] = T A signs it; [T]SA Client C wishes to talk to backend service S. C [[M|[T]SA]SC -> S C sends message M plus the token it got signed from A, to service S. S cares that C really did send it and verifies C's signature SC that it reads from the envelope. S verifies signature SA of token. Failure means request is denied. S verifies token [T]SA: user-id/PKC correct and token date >= now. Expired token means to send 'token expired' message to client C. Permission denied if token has faulty signature. (Optional; S authorizes C, digression) S performs work and sends back [M2]SS to client C. This wouldn't be too much overhead; verifying the signature is a pretty fast operation. The question 'C# Sign Data with RSA using Bouncy Castle' shows how you sign and verify a piece of string, the message that you are sending. You'll need the certificates; if you are using a configuration manager (YOU SHOULD BE DOING THAT! ;)), like puppet, then you create a certificate signing request (CSR) and then sign it using puppet. There's something called a certificate revocation request, which basically is a laundry list of public keys that have been withdrawn and are not to be trusted/used. Place PKC there and broadcast the revocation, and it basically acts by requiring the client to do another certificate signing request round. Also, if you want the ability to expire specific tokens, add a unique ID (UUID/GUID) to the token T when you create it, and have a token revocation list, similarly broadcasted when changed, that you purge the token UUIDs from when they expire. So a service would then also check the token revocation list if the received T is in it. Have a look at the software giants are doing. E.g. Amazon's REST interface, that uses shared secret keys: The Amazon S3 REST API uses a custom HTTP scheme based on a keyed-HMAC (Hash Message Authentication Code) for authentication. To authenticate a request, you first concatenate selected elements of the request to form a string. You then use your AWS Secret Access Key to calculate the HMAC of that string. Informally, we call this process "signing the request," and we call the output of the HMAC algorithm the "signature" because it simulates the security properties of a real signature. Finally, you add this signature as a parameter of the request, using the syntax described in this section. Read more on Amazon's scheme.
563e33ca61a8013065268172	X	Chosen approach: COMPLETELY STATELESS AUTHENTICATION AND UNAUTHENTICATION Finally, I got a conclusion and a protocol in order to switch to the whole completely stateless token-based authentication and unauthentication. How to achieve it? First of all, this is what you need to have stateless token-based authentication for applications (but user authentication would work in the same way, excluding this inventory): This is the flow of authenticating an application: Client sends an authentication request to the authentication service. This request must include the Application Key (AppKey). Authentication service receives the previously sent request. Now authentication service creates an application token (AppToken), which is a self-describing concatenation of the necessary information to track a concrete authenticated client to the services relying on authentication service. AppToken is a compound string (this composition can be an object serialized using JSON) of: Authentication service encrypts step #4 result (the JSON-serialized object). **Use AppKey as the key or password for a symmetric cipher. In my case, I'll use Rijndael for that. Subsequent request will include this token in order to avoid sending plain text credentials. Those request will always include the AppKey too, so authentication service will be able of identify what application is trying to authenticate the request. After some time, a token becomes expired or invalid, and client requests for a new AppToken. Or the client was closed by the user and there's no persistent storage that would save security tokens, so next client session will request new ones when needed. Some hints and details about .NET implementation of such authentication method: I've used System.Security.Cryptography.RijndaelManaged class for symmetric encryption. Both AppKey and AppToken (and in case of token-based user authentication it's almost the same solution) are generated using RijndaelManaged class. Encrypted text is converted to an HEX string. This is sent with the authentication response. In our case (a RESTFul API), the HEX string representing the AppToken will be sent as a response header. Whenever a request includes this HEX string, authentication process will reconvert it to the original encrypted text, and later it'll get decrypted in order to evaluate if the token is valid. Thanks Henrik for your effort. I've taken some of concepts in your own answer and I've mixed them with my own conclusions.
563e33ca61a8013065268173	X	Check out webthumb.bluga.net/home and snippets.dzone.com/posts/show/3621
563e33ca61a8013065268174	X	Also, just out of curiosity, what's your end goal here? What will these site screenshots be used for? And do you need to store the images or can they be rendered on the fly, returning asynchronously (like what Litmus does with their email previews)?
563e33ca61a8013065268175	X	Both. I'd like to store the images for caching, but initially I'm going to need to render them on the fly and return them asynchronously
563e33ca61a8013065268176	X	maybe code.google.com/p/wkhtmltopdf is more up2date - you'll need to convert to an image though. Rendering was just fine for us (javascript, canvas and all ..)
563e33ca61a8013065268177	X	cool thanks! so far I'm doing something very similar to what you've suggested. I am currently using PhantomJS, a Webkit engine with a javascript API, to generate the images in parallel Resque background tasks. However, it's not specifically suited for Rails, so I'm definitely going to look into IMGKit to see if I like it better. I was worried about not being able to use PhantomJS on Heroku, and it seems like the library you suggested would work on it.
563e33cb61a8013065268178	X	I also was not going to originally store anything except the file path of the stored image on the filesystem, and I was checking if the file existed (File.exist?("path")) to see if I should serve it from there. However, this probably wouldn't allow me to perform any logic for updating the cache, which a created_at field would help me do. Great idea! I'll do this.
563e33cb61a8013065268179	X	Oh, and I'm in NY as well :)
563e33cb61a801306526817a	X	@neezer IMGKit is giving me trouble when trying to generate the image.
563e33cb61a801306526817b	X	@Justin Meltzer - Did you install wkhtmltoimage? Though it may be that this might not work on Heroku either, as wkhtmltoimage is a separate compiled binary. You could fire up a minimal slice and host your "image processing" (either IMGKit or PhantomJS) there and communicate to it as an external API from your Heroku app. Also, you might want to update your question to say that you're hosting on Heroku.
563e33cb61a801306526817c	X	This is really nice. A bit pricey though :/
563e33cb61a801306526817d	X	Yeah, I think there's a discount code on the blog post to make it slightly cheaper. In the end it really depends on how much screenshots will mean to your product. For my colleague its a major feature so he's prepared to pay. If it's not that big of a deal then using one of the mentioned current tech's will suffice.
563e33de61a801306526817e	X	Please note that PhantomJS already moves away from using stock QtWebKit, expect more bleeding-edge WebKit features coming to it in the future releases.
563e33de61a801306526817f	X	Also URL2PNG uses Chromium 11, which is a release from a year ago (hence, soon will be in the "outdated" WebKit category as well, unless the backend gets upgraded).
563e33df61a8013065268180	X	I'm using Google's Custom Search API to dynamically provide web search results. I very intensely searched the API's docs and could not find anything that states it grants you access to Google's site image previews, which happen to be stored as base64 encodes. I want to be able to provide image previews for sites for each of the urls that the Google web search API returns. Keep in mind that I do not want these images to be thumbnails, but rather large images. My question is what is the best way to go about doing this, in terms of both efficiency and cost, in both the short and long term. One option would be to crawl the web and generate and store the images myself. However this is way beyond my technical ability, and plus storing all of these images would be too expensive. The other option would be to dynamically fetch the images right after Google's API returns the search results. However where/how I fetch the images is another question. Would there be a low cost way of me generating the images myself? Or would the best solution be to use some sort of site thumbnailing service that does this for me? Would this be fast enough? Would it be too expensive? Would the service provide the image in the correct size for me? If not, how could I change the size of the image? I'd really appreciate answers that are comprehensive and for any code examples to be in ruby using rails.
563e33df61a8013065268181	X	So as you pointed out in your question, there are two approaches that I can see to your issue: I'm no expert in field, but my Googling has so far only returned services that allow you to generate thumbnails and not full-size screenshots (like the few mentioned here). If there are hosted services out there that will do this for you, I wasn't able to find them easily. So, that leaves #2. For this, my first instinct was to look for a ruby library that could generate an image from a webpage, which quickly led me to IMGKit (there may be others, but this one looked clean and simple). With this library, you can easily pass in a URL and it will use the webkit engine to generate a screenshot of the page for you. From there, I would save it to wherever your assets are stored (like Amazon S3) using a file attachment gem like Paperclip or CarrierWave (railscast). Store your attachment with a field recording the original URL you passed to IMGKit from WSAPI (Web Search API) so that you can compare against it on subsequent searches and use the cached version instead of re-rendering the preview. You can also use the created_at field for your attachment model to throw in some "if older than x days, refresh the image" type logic. Lastly, I'd put this all in a background job using something like resque (railscast) so that the user isn't blocked when waiting for screenshots to render. Pass the array of returned URLs from WSAPI to background workers in resque that will generate the images via IMGKit--saving them to S3 via paperclip/carrierwave, basically. All of these projects are well-documented, and the Railscasts will walk you through the basics of the resque and carrierwave gems. I haven't crunched the numbers, but you can against hosting the images yourself on S3 versus any other external provider of web thumbnail generation. Of course, doing it yourself gives you full control over how the image looks (quality, format, etc.), whereas most of the services I've come across only offer a small thumbnail, so there's something to be said for that. If you don't cache the images from previous searches, then your costs reduces even further, since you'll always be rendering the images on the fly. However I suspect that this won't scale very well, as you may end up paying a lot more for server power (for IMGKit and image processing) and bandwidth (for external requests to fetch the source HTML for IMGKit). I'd be sure to include some metrics in your project to attach some exact numbers to the kind of requests you're dealing with to help determine what the subsequent costs would be. Anywho, that would be my high-level approach. I hope it helps some.
563e33df61a8013065268182	X	Screen shotting web pages reliably is extremely hard to pull off. The main problem is that all the current solutions (khtml2png, CutyCapt, Phantom.js etc) are all based around QT which provides access to an embedded Webkit library. However that webkit build is quite old and with HTML5 and CSS3, most of the effects either don't show, or render incorrectly. One of my colleagues has used most, if not all, of the current technologies for generating screenshots of web pages for one of his personal projects. He has written an informative post about it here about how he now uses a SaaS solution instead of trying to maintain a solution himself. The TLDR version; he now uses URL2PNG to do all his thumbnail and full size screenshots. It isn't free, but he says that it does the job for him. If you don't want to use them, they have a list of their competitors here.
563e33df61a8013065268183	X	You have kind of dived straight into the details without giving any overview of what you are trying to achieve... Why are you sending images? Where from? Are they coming back? What dimensions are they? Are they JPEG or PNG or other? Are you trying to send as many images per second to AWS/S3 as possible? Or to parallelise some processing? Or re-assemble something? Why must you tile them?
563e33df61a8013065268184	X	I have added some background in the original post. Thanks for your interest.
563e33df61a8013065268185	X	As a side-node. I can run gm.crop() in parallel (eachLimit(10)) under OSX. But the same code crashes on Ubuntu. On OSX it takes 6 seconds instead of 25 seconds when I run it in parallel.
563e33df61a8013065268186	X	I've tried a couple of Imagemagick wrapper libraries and some S3 libraries. I'm having trouble choosing the best concept due to big performance differences. I have settled with the node library "gm", which is a joy to work with and very well documented. As for S3 I have tried both Amazon's own AWS library as well as "S3-Streams" Edit: I just discovered that the AWS library can handle streams. I suppose this is a new function s3.upload (or have I just missed it?). Anyway, I ditched s3-streams which makes use of s3uploadPart which is much more complicated. After switching library streaming is equal to uploading buffers in my test case. My testcase is to split a 2MB jpg file into approx 30 512px tiles and send each of the tiles to S3. Imagemagick has a really fast automatic way of generating tiles via the crop command. Unfortunately I have not found any node library that can catch the multi file output from the autogenerated tiles. Instead I have to generate tiles in a loop by call the crop command individually for each tile. I'll present the total timings before the details: A: 85 seconds (s3-streams) A: 34 seconds (aws.s3.upload) (EDIT) B: 35 seconds (buffers) C: 25 seconds (buffers in parallell) Clearly buffers are faster to work with than streams in this case. I don't know if gm or s3-streams has a bad implementation of streams or if I should have tweaked something. For now I'll go with solution B. C is even faster, but eats more memory. I'm running this on a low end Digital Ocean Ubuntu machine. This is what I have tried: A. Generate tiles and stream them one by one I have an array prepared with crop information and s3Key for each tile to generate The array is looped with "async.eachLimit(1)". I have not succeeded in generating more than one tile at once, hence limit(1). As the tiles are generated, they are directly streamed to S3 Pseudo code: B. Generate tiles to buffers and upload each buffer directly with AWS-package Pseudo code: C. Same as B, but store all buffers in the tile array for later upload in parallell Pseudo code: ..this next step is done after finalizing the first each-loop. I don't seem to gain speed by pushing limit to more than 10. Edit: Some more background as per Mark's request I originally left out the details in the hope that I would get a clear answer about buffer vs stream. The goal is to serve our app with images in a responsive way via a node/Express API. Backend db is Postgres. Bulk storage is S3. Incoming files are mostly photos, floor plan drawings and pdf document. The photos needs to be stored in several sizes so I can serve them to the app in a responsive way: thumbnail, low-res, mid-res and original resolution. Floor plans has to be tiles so I can load them incrementally (scrolling tiles) in the app. A full resolution A1 drawing can be about 50 MPixels. Files uploaded to S2 spans from 50kB (tiles) to 10MB (floor plans). The files comes from various directions, but always as streams: I'm not keen on having the files temporarily on local disk, hence only buffer vs stream. If I could use the disk I'd use IM's own tile function for really speedy tiling. Why not local disk?
563e33df61a8013065268187	X	After some more tinkering I feel obliged to answer my own question. Originally I used the npm package s3-streams for streaming to S3. This package uses aws.s3.uploadPart. Now I found out that the aws package has a neat function aws.s3.upload which takes a buffer or a stream. After switching to AWS own streaming function there is no time difference between buffer/stream-upload. I might have used s3-streams in the wrong way. But I also discovered a possible bug in this library (regaring files > 10MB). I posted an issue, but haven't got any answer. My guessing is that the library has been abandoned since the s3.upload function appeared. So, the answer to my own question: There might be differences between buffers/streams, but in my test case they are equal, which makes this a non issue for now. Here is the new "save"-part in the each loop: Thank you for reading.
563e33e061a8013065268188	X	Sorry I forgot to say. Thnking about the future you could outsource your site hosting completely to Windows Azure as long as you understand the implications of data access etc...
563e33e061a8013065268189	X	Good call, something to look out for. Thanks.
563e33e061a801306526818a	X	Google likely hosts their AJAX APIs on a unique domain so the least amount of data is sent over the line. If you access something.google.com, any cookies for *.google.com will also be sent by the client. This may seem irrelevant but saving time over the wire is one of the reasons people use Google's hosted copy.
563e33e061a801306526818b	X	Ahh, good point. Still, if it's a choice between slower or blocked, slower wins. I'll have to try loading up my browser with microsoft.com cookies and see how the CDN compares to my company's servers.
563e33e061a801306526818c	X	Here's what I mean. In developing my ASP.NET MVC based site, I've managed to offload a great deal of the static file hosting and even some of the "work". Like so: jQuery for my javascript framework. Instead of hosting it on my site, I use the Google CDN Google maps, obviously "offloaded" - no real work being performed on my server - Google hosted jQueryUI framework - Google CDN jQueryUI CSS framework - Google CDN jQueryUI CSS framework themes - Google CDN So what I'm asking is this, other than what I've got listed... What aspects of your sites have you been able to offload, or embed, from outside services? Couple others that come to mind... OpenAuth - take much of the authentication process work off your site Google Wave - when it comes out, take communication work off of your site
563e33e061a801306526818d	X	In the past I've used Amazon AWS. Their S3 service was cheap for hosting Images and video. The EC2 service is also good for additional computational power or just removing load from your server. In additon to Pay for hosted services you can use Youtube or Vimeo to host videos and they API will allow you to upload and host videos. There are also APIs for may other services depending on exactly what you're wanting to do. If you looking at adding functionality to your site but without hosting the service it would be worth checking out http://www.programmableweb.com/
563e33e061a801306526818e	X	Even though Google's CDN has smaller files and faster response times, I'm now using Microsoft's CDN for jQuery. Why? Big Brother. In some high-security companies, they only allow access to known domains. Users at those companies had problems because their firewalls didn't know googleapis.com, and blocked jQuery. They knew microsoft.com, so ajax.microsoft.com worked. I've suggested to google that they change their URL from ajax.googleapis.com to something.google.com to avoid the issue in the future.
563e33e061a801306526818f	X	The replay attack that you are talking about can be avoided by adding a timestamp along with the public key while hashing at the time the request is sent. So, I think I can implement that. The only use of my authentication is to make sure that only authorized users are using my api links. So, isn't OAuth too much for that simple thing? Sorry, I don't know much, just want to know. And Yes, I am finding it really annoying to learn OAuth. Could you please suggest some good documentations?
563e33e161a8013065268190	X	My Requirement I am making a website which will have mobile version as well. So, I am making it API centric. Now I want to make my API secure without the complexities of OAuth because the security I need is quite simple. I don't want anyone with access to the api links to be able to access my data. So, I came across this article http://www.thebuzzmedia.com/designing-a-secure-rest-api-without-oauth-authentication/ which is quite amazing and cleared most of my doubts. Right now, I am trying to recreate whatever is there in the article. I am using Laravel 5 framework for PHP for development. I want to make sure that the API is being used by the mobile app and the web version only and no one else. I have seen api links like Now, I understand that this key is generated by using hash_hmac() function in php. My Approach My Confusion I am not sure if this is the right way to do this. Can we get the data that has been used to generate the hash using hash_hmac() by decrypting the hash?
563e33e161a8013065268191	X	That HashKey in the URL is generated by hashing the privateKey and the publicKey in the client side and then sent to the server. So, i send the generated Hash along with the publicKey to the server. Close, but not quite. As you just described it, a user with a given public key would send the same hmac with every request. That's no better than "username and password." Side note: if you aren't using https, you're already insecure and whatever else you do to secure the site is of relatively little value. The point of generating an hmac signature is that it not only authenticates the user as being in possession of the secret key, it also authenticates the specific request as being made by that user and being made during a specific window of time. Two different requests back to back should have a different hmac. One request today and an identical request tomorrow should also have a different hmac. Otherwise, you're in for replay attacks. This means information about the current time or expiration time of the signature, and information about the request itself, must be included in the information that's passed through the hmac algorithm or you're not accomplishing much. For any given request, by a specific user, at a specific time, there can only be one possible valid signature. HMAC is not reversible, so you can't take the signature apart at the server end and figure out the attributes of the request. Of course, of you're thinking about embedding that secret key in your app, remember that such tactics can be relatively trivial to reverse-engineer. Is it a viable authentication mechanism? Of course. As the article points out, Amazon Web Services uses hmac signatures on their APIs, and they have a massive potential attack surface... but does that mean you will implement it in a meaningfully secure fashion? Not necessarily. There is always someone more clever, devious, and determined than you can imagine. Even Amazon apparently realizes that their Signature Version 2 is not as strong as it could be, so they now have Signature Version 4, which has a much more complex algorithm, including several rounds of hashing and generation of an intermediate "Signing Key" that is derived from your secret, the current date, the specific AWS service, AWS region, and other attributes. Regions where Amazon S3 was first deployed in 2014 or later don't have support for the original Sig V2 at all -- and it seems like it can only be security-consciousness that drove that decision, since the old algorithm is computationally less expensive, by far. Use caution in rolling your own security mechanisms. If you are primarily trying to avoid the learning curve with OAuth, which I agree is quite annoying at first, you could be on a fool's errand.
563e33e161a8013065268192	X	If this method works for you it should definitely be fine, and undoubtedly it is secure. Regarding decryption - HMAC is not supposed to be decrypted due to its nature (hash). HMAC is considered to be very secure and you should have no problems with it. You can read a bit more about How and when do I use HMAC? [SE Security]
563e33e161a8013065268193	X	I want to make sure that the API is being used by the mobile app and the web version only and no one else. This is a problem that neither OAuth nor AWS-style signature authentication really help with. Both are about authenticating users, not applications. You can certainly implement either approach if you have a bunch of time to sink into it, but in both cases you're going to need to embed a "secret" in your apps, and once you give that app to a user your secret's not really a secret any more... There's no great way to do what you're looking for. If someone's going to take the time to reverse-engineer your app to learn about how to directly hit the underlying API, anything else you do client-side to "authenticate" the calling application can be reverse-engineered as well. I'd recommend not even bothering, and spending the time you save polishing your app so no one wants to bypass it and hit your API directly. :)
563e33e161a8013065268194	X	Your server is in South Africa. Are most of your users?
563e33e161a8013065268195	X	I am considering trying out Amazons CloudFront CDN, which utilizes their S3 service for file storage and springs data to servers closest to the browser, however, we have a dedicated server in South Africa, Johannesburg to be exact, so my question is this: Amazons CloudFront seems to give you the option to have your base server in EU, America, Japan but nowhere near South Africa - I guess EU is the closest? So, will I still benefit from using the CDN to server static files (css, images, javascript - when not using Googles ajax API - and media files) rather than calling them from the same, dedicated server? Bear in mind that although we have a dedicated server, it is STILL a shared hosting environment as we host multiple clients websites on the server. Secondly, if I DO use a CDN like Amazons CloudFront, can I benefit from caching my content and using far future expires headers, compression etc? Many thanks
563e33e161a8013065268196	X	Well, the CDN would take load off your main dedicated server. This is quite advantageous, because it means that you can downgrade your server to something a little cheaper. It also provides many, many more levels of redundancy than small-medium business could ever hope to achieve (especially with only a single server). The fact that a CDN has mirrors all over the world is only 1 feature of such a network. It's a good one, but it is by no means the only one. Really, you have to think about what you are hoping to achieve by using the CDN, and what you are using it for. If your application relies on the response time and nothing else matters, then it might be worth keeping your dedicated server running everything because of the proximity to your location. That said, quite often applications that require quick response time only require it for a very small subsection of the overall functionality of the server, so you could unload only the non-time-critical elements to the CDN and still get the best of both worlds. In short, CDNs have many functions. They save businesses time, money and provide infrastraucture that is otherwise unattainable to the large majority of developers and administrators.
563e33e161a8013065268197	X	I don't think it matters where your server is. You're still uploading stuff to Amazon (although, yes you'd probably want to make use of the EU servers so it's quicker if you're sending stuff from South Africa) but the idea of a CDN is that it has end points around the world so that you don't have to worry about having servers in those locations. In short, yes a CDN such as Amazon's CloudFront is what you want to be able to serve static content quickly.
563e33e261a8013065268198	X	+1 for a going the extra mile on this answer!
563e33e261a8013065268199	X	+1 just what I needed!
563e33e261a801306526819a	X	+1 because it was what I was looking for
563e33e261a801306526819b	X	I've been doing a lot of studying and work recently related to WCF, web services and distributed computing in general, but most of the security concepts go over my head. Transport security, message security, encryption, certificates, etc. I understand the basics of symmetric and asymmetric encryption, but I don't really understand the real world application of them in a SOAP conversation. I'd read the specs, but they seem a bit dense. Can anyone point me to resources that start with the basics and work up from there? I'm tempted to fish out the textbook from my networking course in college to get a better understanding of what's happening at the lowest level, but I don't know if this is massively inefficient or not. I'd prefer not to have to read a small library full of stuff - I just want to solidly grok the concepts and be able to explain them to the rubber duck on my desk.
563e33e261a801306526819c	X	Edit: It's been several years since I first wrote the answer and the list is getting old. There have been some wide adoption of web-enabled APIs and token-based trust relaying. I haven't read it, but Windows Communication Foundation Security would be a good place to start, if you're looking for something specific to WCF. Also keep your eyes open for what major players like Facebook, Google, and Twitter are doing. They are using open protocols like OpenID and OAuth. At first, OAuth looks complicated, but you should understand the mechanism. In my opinion earlier OAuth reinvents a lot of wheels that SSL has already solved, and leaves some security holes open. An interesting read is Compromising Twitter's OAuth security system. Facebook's OAuth 2.0 implementation and Google's OAuth 2.0 implementation simplify many of these issues by using https where it makes sense. These are must reads.  The basic concept around OAuth is trust relaying. You would want third-party developers to make apps against your API, but the end users cannot always trust these apps. Giving password to them, is like giving the keys to the kingdom. So the user types in the password into your UI, and your UI redirects to the third party with an access token. Building Secure ASP.NET Applications: Authentication, Authorization, and Secure Communication is a good introduction to ASP.NET's security models. You can skip over the details because much of the technology is now obsolete. A good overview specific to Web Services is Web Service Security: Scenarios, Patterns, and Implementation Guidance for Web Services Enhancements (WSE) 3.0. It says WSE, but basic concepts still remain the same. To get more details on WS-Security, read Securing Web Services with WS-Security: Demystifying WS-Security, WS-Policy, SAML, XML Signature, and XML Encryption.  After reading above, what really helped me was looking at existing implementations like Amazon S3's authentication:  Flickr Authentication API: Each authentication frob is specific to a user and an application's api key, and can only be used with that key. Authentication frobs are valid for 60 minutes from the time it is created, or until the application calls flickr.auth.getToken, whichever is sooner. Only one authentication frob per application per user will be valid at any one time. Applications must deal with expired and invalid authentication frobs and know how to renew them. Twitter REST API Many Twitter API methods require authentication. All responses are relative to the context of the authenticating user. For example, an attempt to retrieve information on a protected user who is not friends with the requesting user will fail. For the time being, HTTP Basic Authentication is the only supported authentication scheme. When authenticating via Basic Auth, use your registered username or email address as the username component. Session cookies and parameter-based login are known to work but are not officially supported. The OAuth token-based authentication scheme will shortly be offered as an experimental beta release. So it's nice to know the complicated certs and PKI stuff, but the world seems to operate without it just fine.
563e33e361a801306526819d	X	Additionally, there's also the WCF Security Guidance by Microsoft's Patterns & Practices group. Check it out. Marc
563e33e361a801306526819e	X	I want to build ... something (website? app? tool of some variety?) that searches other sites -- such as Amazon -- for specific items and then lists whether or not those items exist. Ideally it could also pull prices, but that's secondary. I'd like to be able to enter a (very specific, an identification number) search term into the thing that I build and then have the thing return whether or not the searched item exists on the sites that it checks (a predetermined list). I'd also like it to take a list of ID numbers and search them all at once. I have no idea where to begin. Can anyone point me in the right direction? What do I need to learn to make this happen?
563e33e361a801306526819f	X	You will need to learn a few key languages in order to start working on a program like this. Once you learn the basics, search stackoverflow for specific questions relating to a specific problem.
563e33e361a80130652681a0	X	This is certainly a too broad question, but as OP asks to point in some direction here are few suggesstions- Well this seems to be a big projects. You'll need to find if there is some official api given the other sites from where you want to fetch the product info, if yes use the api to retrieve the product info or else use web scrapping where you retrieve the data by parsing the page and storing into your local database. Amazon provides EC2 instances, where you can hourly rent specific configuration server as needed e.g. Linux with apache/mysql/php, or linux/java. Amazon has a set of other tools like the S3 storage where you can host your images/docs/video and link them on the site. Hope this helps in someway.
563e33e361a80130652681a1	X	I would question the wisdom of doing this. Isn't the network latency of accessing S3 going to outweigh any advantage gained by page caching?
563e33e461a80130652681a2	X	@Scott: I wouldn't do this. You'd need a cache for your cache :)
563e33e461a80130652681a3	X	I was specifically looking at using the CloudFront CDN front end for S3, which should be faster than serving locally cached html files.
563e33e461a80130652681a4	X	I am looking for a simple and automated way to store the page cache on S3 (and of course cloudfront.) I know of a plugin that does this with the fragment cache, but before I create it myself, wanted to tap into the collective wisdom here :) Edited: To make myself more clear, I am not looking to use S3 to serve the files, but rather, the CloudFront CDN.
563e33e461a80130652681a5	X	In order to put something in CloudFront, you have to first have it in S3. See Amazon's introduction for all the steps. Basically, you put the document in a bucket on S3 and then make an API call to register your bucket for distribution (you do this using a perl script they provide). At that point, they transfer the contents of your bucket out to the edge servers for high performance distribution. You can change the contents of your bucket once an hour. Anyway, in order to use CloudFront, what you really need to do is get the contents of your rendered pages into S3. Once you've gotten your distribution up and running this is how you manage your content in CloudFront. The simplest way to manage a cache in S3 would probably be to create a model for your cache and use the attachment_fu plugin to store the page contents in s3. Then, you could use ActiveRecord's Observer functionality to invalidate and re-populate the cache as appropriate for your application. The only other tricky bit would be reaching into ActionView to access the result of rendering a page, but I bet you could crib some of that code from the default page caching system itself. If you really wanted to, you could probably wrap all of this functionality up into a plugin that would make it easy to re-use across applications for you and others.
563e33e461a80130652681a6	X	I read the headline and was going to tell you Amazon started doing a CDN a few weeks back. But obviously, you already know that. :) There's a Python package that looks like it wraps CloudFront, botto. But that's all I can find. I think you're the first... go forth and start it. Let me know where it is, I'll probably fork it.
563e33e461a80130652681a7	X	Why so many question marks????
563e33e561a80130652681a8	X	Is your question actually, "Why isn't there a standardised API for cloud computing?" If so, I'd put it in those terms. I don't see that SQL is "regulated" which suggests government legislation etc.
563e33e561a80130652681a9	X	@Jon-Skeet, ya when I first opened the question I was thinking it was about legislation. Anyway, to the question itself, I think we are still in the hype stage of "cloud computing." We should wait for everyone to realize this isn't anything new and then move forward.
563e33e561a80130652681aa	X	thanks for your answer.. you are right :-)
563e33e561a80130652681ab	X	yes you are right it should be S3. thanks you very much
563e33e561a80130652681ac	X	I was involved in couple of cloud computing platform recently. First of all please note that I am not trying to criticize any platform. Cloud computing is large area but to make my point simple and understandable. Let me come up with very simple scenario and that is data storage services hosted on the cloud. If you take any storage service like Amazon EC2, SQL Data Service(SDS), Salesforce.com services. If you want to consume any of such data storage service platform goal of all such service are same and that is to serve requested data on demand. Without warring about how it store and where it stored and who is maintaining it etc... (all cloud goodies) Now my area of concern is the way ANSI-SQL regulated platform venders to make sure they follow similar language across all the product can’t they regulate similar concept across service providers? Why no such initiatives?? Any thoughts appreciated
563e33e561a80130652681ad	X	It seems to me like you're worried about vendor lock-in with cloud computing. I may be naive but I would normally choose technologies and then go look for cloud vendors that'd be able to deliver these technologies. And if I was aiming for a "write once run anywhere approach" I'd have to select technology that'd make this as realistic as possible. With the fairly rapid speed of development I really think standardization committees would struggle to keep up. ANSI-SQL has had 20 + years of history. It seems to me like you're requesting for standardization long before we even know what the cloud is up to....
563e33e661a80130652681ae	X	I think that this emerging cloud computing initiative is just too young in order to have standards. Service providers right now just worry about rushing into the market, rather than interoperability and standards. Later on, when the situation is more established, some common guidelines may emerge. But there is still a long way to go.
563e33e661a80130652681af	X	You seem to be asking specifically about cloud storage services, rather than cloud computing in general. So your Amazon example would be S3, not EC2. I think the field is a little young to be standardising on an API just yet. The services differentiate themselves in ways which rule this out. For example, S3 trades sophistication for scalability/reliability/performance: you can't do a complex SQL LIKE query. You can store and retrieve blobs of data based on a key, and that's about it. I think as such services become more and more the mainstream way to do things, standards will emerge. Users will want the freedom to switch providers on a whim, move their data around, test against free local storage, etc. The APIs used are all based on Web Standards already. Making an abstraction layer to make them look the same is fairly trivial.
563e33e661a80130652681b0	X	Do you have SSH access to the box?
563e33e661a80130652681b1	X	it is a "normal" server. If you want to set up ftp, SSH to the box and set it up. You are asking too many questions at once. Each of your points should be a separate question, apart from 4 and 5 which are totally OT for stack.
563e33e661a80130652681b2	X	Google App Engine is not a "normal" server. There is no writable filesystem (unless you like juggling with the GAE API), there's no SSH access, no FTP access and so on. Google Compute Engine on the other hand is a "traditional" VPS which has those options.
563e33e661a80130652681b3	X	Thank you very much. I have to learn much more than I thought so for now I have to stay with my traditional servers. The costs looks higher as well.
563e33e661a80130652681b4	X	The costs on GAE might actually become less than with traditional servers, but for small sites and applications there might be no or little difference. Additionally the work that goes into configuring and upkeeping a GAE app might be higher at first, but that is a workflow question.
563e33e661a80130652681b5	X	Thank you for Your comments. I still test it and I think this will be the next thing I want to learn
563e33e661a80130652681b6	X	I have just started playing with Google cloud. I used to work on normal servers so I need advice. I created my first instance and deployed Wordpress. I installed woocommerce plugin. The shop is quite fast and I am happy (with the lowest settings) but now: SNI SSL certificate slots are offered for no additional charge for accounts that have billing activated. Free accounts are limited to 5 certificates. I have no experience with ssl but I plan run shop so what it means. Free certificates for 5 instances or 5 deployement ? How many certificates do I need to run one shop? I know there are many questions but I wanted to go further and all advise on internet is outdated because are for older versions of google cloud. Please help me to understand this all.
563e33e761a80130652681b7	X	I assume you're attempting to use WordPress on Google App Engine. GAE has no real filesystem, so you cannot write to it (unless you juggle with the API GAE offers). Editing happens locally using the GAE SDK development server and you deploy your changes to the App Engine ecosystem using the SDK interface (GUI or CLI). All application writes should go to Google Cloud Storage (which is similar to Amazon S3 and the like). I'm not certain whether the Google Cloud Storage can be accessed via traditional FTP. There might be some middleware required. You can see and browse the contents of your buckets in the developer project console (https://console.developers.google.com/). The databases are on a separate "server" when using GAE. MySQL instances are spawned into the Google Cloud SQL ecosystem, which are available for App Engine and Compute Engine instances (and why not other places too). You can define the GCSQL address and port to wp-config.php like normally. You need to create a local MySQL database for your local installation. More: https://cloud.google.com/appengine/docs/php/cloud-sql/ When working with Google App Engine you should deploy the whole WordPress installation (wp-config.php, wp-includes/, wp-admin/, wp-content/, etc.) in order for it to work in the GAE system. For a "better" deployment system you should do some searching or ask a new question dedicated for that issue. The certificates themselves on GAE are not free, but the "slots" you put the certificates into are. Free projects (no billing enabled) offer 5 free slots where you can put your purchased certificates. SSL SNI means that you can use multiple different domain/host certificates under a single listening IP address (which some years back was not that simple to do). What this all means that GCP offers a way to use certificates with their services, but you still need to get the certificates themselves elsewhere. Have you seen the GAE starter project offered by Google: https://googlecloudplatform.github.io/appengine-php-wordpress-starter-project/ ? It makes your live a bit easier when developing WP sites for Google App Engine. If you're working with Google Compute Engine instances, then they should operate just like regular VPS machines, with some Google restrictions applied. I have not used them so I do not know the specifics.
563e33e761a80130652681b8	X	Here's the scenario: I have JBoss serving a web service with JBossWS providing me with a wsdl. I have connected and used it from both .NET and Java so far (and it has been quite easy once I figured it out). I am now trying to do the same with R. Is there anything out there considered to be "the right way" for doing this? I am not that familiar with R, and my searches have not turned up much, so I figured I'd ask and maybe spare my head and the wall a bit of damage.
563e33e761a80130652681b9	X	I have had good luck using rJava to recreate in R something that works in Java. I use this method for connecting to Amazon's AWS Java SDK for their API with R. This allows me, for example, to transfer files to/from S3 from R without having to recreate the whole connection/handshake/boogieWoogie from R. If you wanted to go more "pure R" I think you'll have to use some combination of RCurl and the XML package to grab and parse the wsdl.
563e33e761a80130652681ba	X	There are a number of ways: You could retain your Java approach and use the rJava package around it You could use RCurl which is used to power a few higher-level packages (accessing Google APIs, say) I believe there is an older SSOAP package on Omegahat which may help too.
563e33e761a80130652681bb	X	You mean other than aws.amazon.com ?
563e33e761a80130652681bc	X	I know about the AWS site, problem is they don't have any information about the e-commerce, store, or book APIs. Let alone Kindle and related SDKs or such. That's why I'm looking for a site that has links to all of these disparate SDK/APIs and such. :/
563e33e761a80130652681bd	X	There seem to be about a zillion Amazon APIs, but it is hard to figure out what all is available from a single point on their site(s). Does anyone know of anything written up regarding their APIs?
563e33e861a80130652681be	X	Basically, you need to go here: http://aws.amazon.com/ and check out all they have. They break it down by product so you can figure out which APIs they have in the space you are looking for. They also have demo code to help you get started using the APIs. You aren't going to find a list of Amazon APIs on the web because they cover so many different areas. There are APIs for S3, EC2, shopping, etc. that all have different purposes. The Amazon link I listed above takes you to their Web Services page where you can drill into the area you are looking to program against. Inside each area is a list of APIs that you can take advantage of, including source code that demonstrates how to do so. I will say that the documentation for the Amazon APIs isn't the greatest. Sometimes even inside the specific areas they struggle putting all the information in one place. You have to do some legwork to get what you want.
563e33e861a80130652681bf	X	Basically it looks like Amazon doesn't provide a central place that lists all of their respective APIs. Kindle, Cloud Player (if it ever exists), AWS, and all the others appear on differents domains/sites throughout various Amazon Web Properties. To Summarize, you have to dig for them. :(
563e33e861a80130652681c0	X	I am trying to use undesigned s3 class to create a folder. I can create just a folder in my amazon s3 account but just running the follow. I can do this. Which will create a folder in my bucket but when i run a loop it doesn't reference it as a folder Any suggestions why???
563e33e861a80130652681c1	X	S3 doesn't actually have folders. Each bucket just contains your files which are referenced by their key/filename. There is a convention that if these filenames contain /'s then the text before each / is considered a folder and many of the GUI tools use this to display a folder hierachy. eg A file with the name folder1/folder2/file.txt will appear to be 2 levels deep in folder1 and subfolder folder2. Amazon also makes it easier to search you virtual folders using the delimiter and prefix parameters. See http://docs.amazonwebservices.com/AmazonS3/latest/API/index.html?RESTBucketGET.html
563e33e861a80130652681c2	X	While S3 doesn't support directories they way they are generally thought of, they do support an interesting alternative. If you PUT an object with no content (0 bytes), and the key (name) ends with a "/", and the "content-type" = "binary/octet-stream", it will be treated by S3 as a directory (folder). Keep in mind, it is still just a file; however, the console will treat it as a directory (folder) and many SDK clients will treat it as a directory (folder). In order to get your 0 bytes, you can simply use the file "/dev/null" instead of "license.txt". In order to rename/copy/delete a directory, keep in mind, you will want to search for everything with that key prefix (you will get the directory object and any other objects with that directory path as the key prefix) -- You will need to copy those objects to objects with the new prefix (in cases of rename/delete, you will need to delete the old prefixes).
563e33e961a80130652681c3	X	I'd like this feature, too. I don't think it exists now, though.
563e33e961a80130652681c4	X	What are you trying to accomplish by mirroring S3 to Glacier?
563e33e961a80130652681c5	X	@EricHammond I'm trying backup my S3 files on Glacier.
563e33e961a80130652681c6	X	I don't think Glacier is generally an appropriate place to create backup copies of S3 objects (where you keep copies in both places). I explain more in my answer here: stackoverflow.com/questions/15231733/…
563e33e961a80130652681c7	X	I'd would like that feature also in order to increase the availability of the data stored in S3.
563e33e961a80130652681c8	X	I'd like to mirror an S3 bucket with Amazon Glacier. The Glacier FAQ states: Amazon S3 now provides a new storage option that enables you to utilize Amazon Glacier’s extremely low-cost storage service for data archiving. You can define S3 lifeycycle rules to automatically archive sets of Amazon S3 objects to Amazon Glacier to reduce your storage costs. You can learn more by visiting the Object Lifecycle Management topic in the Amazon S3 Developer Guide. This is close, but I'd like to mirror. I do not want to delete the content on S3, only copy it to Glacier. Is this possible to setup automatically with AWS? Or does this mirroring need be uploaded to Glacier manually?
563e33e961a80130652681c9	X	Amazon doesn't offer this feature through its API. We had the same problem, and solved the problem by running a daily cron job that re-uploads files to Glacier. Here is a snippet of code you can run using Python and boto to copy a file to a Glacier vault. Note that with the code below, you do have to download the file locally from S3 before you can run it (you can use s3cmd, for instance) - the following code is useful for uploading the local file to Glacier.
563e33e961a80130652681ca	X	This is done via Lifecycle policy, but the object is not available in S3 anymore. You can duplicate it into separate bucket to keep it.
563e33e961a80130652681cb	X	I had the same issue but could not afford to endure the long latency to restore from Glacier which is typically 3-5 hours. In my case I created a commercial product that can synchronize and create snapshots of my buckets amongst other things. It can also utilize S3 Reduced Redundancy Storage to better approximate the cost savings benefits of Glacier. You can try a full featured 2 week trial version absolutely free at bucketbacker.com
563e33ea61a80130652681cc	X	I read both links. But I did not find where I used a wrong mapping of HTTP methods to CRUD operations. The only new information to me is that PUSH can be used as a "fallback" method for anything because it is has no predetermined standarized semantic. Anyway, I decided to go for option 1 in order to avoid too much double URIs and will use "GET /person/new" to get an empty form. It best matches to "GET /person/{id}" which returns the same form, but with user data.
563e33ea61a80130652681cd	X	after years of absence from web programming I now start to write a new web application from scratch. I learned the ropes of REST and found some helfful presentations and webinars about good and bad "URI styles". But all these tutorials seems to assume that there are only ressources that can be mapped to entitities that persist in a database. If one needs to GET a HTML (user-friendly) version of something, then the prevalent answer is to use content negiotation via HTTP header fields. The ultimate goal seems to have only URIs that follow the REST paradigm. But what is the "correct" way to display web pages that cannot be mapped to an entity but are required anyway? Perhaps I make an example to clarify what I mean. Assume we have a collection Persons and the entity Person. Then we have With respect to the GET operations I generally found the advice to use the Accept and X-Requested-With header fields of the request to create a response that either returns the "bare" person object in a computer-readable respresentation (i.e. JSON, XML, etc.) or that returns a fully-fledged web page for a browser. But what is about the PUT operation. Ultimately, this operation will send a bare person object (i.e. JSON, XLM) that is going to be created, but before this step I need to collect the data from the user. This means I need some empty web form that is just "eye-candy" for the human user. Of course, I could habe something like GET /newPersonForm/ but it seems that this contradict the REST philosophy, because /newPersonForm/ is an URI that only points to some user interface element. At the moment I see to options: Use the same name space for both kind of URIs: Make separate name spaces: With the first approach I feel that it is somebit "unclean". Although the second approach seems cleaner, I see two problems. Firstly, if one follows this approach cleanly, one gets much double URIs, because one dispense with content negiotation and has an UI web page for every REST function. I have GET /api/Person/{id}/ to return a JSON object and GET /ui/Person/{id} to return a browser version. Secondly, I feel that this approach contradict REST philosophy because search egines and web crawlers cannot understand the structure of the site. Any recommendations what the best practice is?
563e33ea61a80130652681ce	X	First of all, let's get a few misconceptions out of the way. Anything for which you have semantics identifiable by an URI is a resource. The HTTP methods don't map to CRUD operations like that. Read this answer for more on that. I recommend you read it before continuing reading this answer. This one is probably going to be helpful too. There's no such thing as an URI that follows the REST paradigm. The only constraints REST imposes on URIs is that they must identify one and only one resource, and they must be treated as atomic identifiers. The semantics of the URI is irrelevant, although obviously you should design URIs that make sense for the developers and users. As you already figured out, the correct way to return an user-friendly representation of something is through negotiation via the Accept header. It doesn't matter if it's not something that maps to a database. That's an implementation detail only the server knows, and that's what REST is about. When you retrieve something from a REST API, it doesn't matter if it's coming from the application server, from a cache somewhere, from a static file served by Amazon S3, or even an FTP link. The client should simply follow links, like when you click a link on a webpage and you don't care where the result comes from. Both options you present are valid, but that has nothing to do with REST. Separating them in api and ui is a nice way to organize things for you, but what really matters to REST is how the client obtains those links. If they are implementing clients by reading those URIs in documentation and filling up values, that's not REST. Think about it from a web browsing perspective. How do you reach that /newPersonForm html page? You followed a link somewhere else that had a label telling you to click on it to create a new Person. If you are the one clicking, it doesn't matter if it's /newPersonForm or /forms/newperson or simply /persons. REST works in the exact same way.
563e33ea61a80130652681cf	X	The login function does not fit in the REST paradigm.
563e33ea61a80130652681d0	X	@leftclickben Thanks for your reply. In such a way, how should I implement a login/authentication feature using the REST Api? Does this apply to every general aspect as well? Such as having the need to create some action/functions for a particular resource (other than insert/update/delete/retrieve information directly from the resource?
563e33ea61a80130652681d1	X	Well, see answer from Neville K, but in my opinion using REST for login is trying to shoe-horn something that doesn't fit. Use REST when you need it, don't force it on yourself in every circumstance :-)
563e33ea61a80130652681d2	X	@leftclickben Does it means that the solution will end up to have various forms of requesting for the api? Like using REST for retrieve/insert/update/delete for the resource, while actions using the usual methods like passing in parameters in the url or post and a function name which executes the action?
563e33ea61a80130652681d3	X	Yes you are right, but I think this is inherent in what login is and what REST is. In REST, you are accessing resources, and you have 8 different actions depending on which method you are using and whether you are hitting a collection or an element. All 8 of these methods do not map to login, in fact none of them map particularly cleanly. Login and logout are plainly actions and while the implementation of those actions requires resources, the action itself does not map directly to any resource.
563e33ea61a80130652681d4	X	Thanks for your reply! Yeah I was thinking of using my own auth tokens and sending it for every request to make it stateless. However, I would still require some "actions" for some resources. Does that still render it as stateless?
563e33ea61a80130652681d5	X	Thanks for your reply. I hope I have understand the question you have linked correctly. It explains the two different methods of passing in parameters to the server and which practice would be more applicable for whichever purpose. Please correct me if I have interpreted it wrongly. However, I am curious to find out how should actions be performed/executed through the REST concept. Or perhaps like what leftclickben and you mentioned, some actions are hard to model the true RESTful way and should be done in the normal way instead?
563e33eb61a80130652681d6	X	I'll update the answer to clarify - but many people would argue that RESTful is the normal way!
563e33eb61a80130652681d7	X	Thanks! Probably the word "normal" here is a little ambigious. Because the usual way I have been trying to work on is to send post/get request accordingly with a function parameter in the url stating which function in the script it should execute.
563e33eb61a80130652681d8	X	Would this also mean that in the user controller file, I would have to listen to the various types of parameters being passed in and react accordingly?
563e33eb61a80130652681d9	X	Updating the answer - but yes, you do.
563e33eb61a80130652681da	X	Thanks for your reply. I was just curious, by sending the client a form instead or a link, wouldn't it still require an function to accept the return values from the form in the action field?
563e33eb61a80130652681db	X	Yeah, I was considering to use the auth token concept where I will store the id and auth token of the user on the client side(cookie) where he will send in this two necessary information before probing the api for data.
563e33eb61a80130652681dc	X	Updated answer.. Hope that covers it?
563e33eb61a80130652681dd	X	I have read several tutorials to introduce myself to know more about the rest API recently. However, I have got some doubts here and there and hope someone can help me out with this. Reading the Beginner's Guide to HTML and REST, which states: "Resources are best thought of as nouns. For example, the following is not RESTful: 1 /clients/add This is because it uses a URL to describe an action. This is a fairly fundamental point in distinguishing RESTful from non-RESTful systems." As such, I was wondering if for such cases where I have a user resource and to access it to do the usual insert/update/delete/retrieve would be as follow: www.example.com/users [get] <-- to retrieve all records www.example.com/users/1 [get] <-- to retrieve record with id of 1 www.example.com/users/1 [put] <-- to update record with id of 1 www.example.com/user/1 [delete] <-- to delete record with id of 1 www.example.com/user [post] <-- to insert a new user record This would have used up the 4 common verbs to make request. What if I were to require a function such as login or perhaps in general any other types of action commands? How should the url be formed and how should the router redirect in such cases? EDIT: After looking at the various comments and answers. My take away from them is that the final solution would be somewhere along "use rest principles whenever possible and use the query string method with functions whenever not." However, I was thinking of a slight variant of the implementation (not a restful implementation anymore, but following similar concepts) and wondering if it could have work out this way. Hope you guys can advice me on this. Using the same authenticate/login function I would require to implement, could it be something along this instead: www.example.com/users [get] <-- to retrieve all records www.example.com/users/1 [get] <-- to retrieve record with id of 1 www.example.com/users/1 [put] <-- to update record with id of 1 www.example.com/user/1 [delete] <-- to delete record with id of 1 www.example.com/user [post] <-- to insert a new user record as usual and if I were to require an action to be performed it will be as such: [controller]/[action] --- user/authenticate [post] --- to login [controller]/[id]/[action] --- user/1/authenticate [put] --- to logout Will this work? Will there be any foreseen problems that I would face and are there similar implementations out there like this already? Please kindly advice!
563e33eb61a80130652681de	X	REST is stateless so you need to put all the needed information into all queries. The idea is to work with the HTTP Verbs (GET, PUT, DELETE, POST - as you already descripted). If you want an user authentification for your REST API, use something like HTTP Basic Auth, or your own Authentification. You have to send the Auth Information for every Request to the Server (stateless). If you don't want an HTTP Basic Auth you can try some Token Authentification or any other auth. Edit: If you want an "Check Login" Resource, build your own. For Example GET /account/checklogin with http basic auth header informations. The Result of this Request depends on your Authinformations.
563e33eb61a80130652681df	X	There are some actions that are hard to model in a true RESTful way - but login, for instance, can be implemented using the following pseudo code: See this question for how to retrieve the user rights. The point in this question is that you usually need multiple ways of accessing your resources. Some are based on IDs or well-know attributes, for instance: However, some ways of accessing users - especially when you need to combine two or more filtering attributes - are easier to manage using query string parameters. For instance: www.example.com/users?department=xxx&role=yyy&status=active [get] So, your REST API might expose a URL along the lines of: www.example.com/users?userName=xxxx&password=yyy [get] This URL would match the username and password parameters against the user database, and return either a 404 (if they don't match a known user), or a document representing the user, with their access rights. Your client code then manages the current user's session - i.e. by setting the status to "logged in ", and associating the session with that user profile. The key to making this work is assigning responsibility to the right layer - the API should not have to manage user sessions, that is the responsibility of the client application. There are cases where that doesn't work particularly well - not sure if yours is one, though. If you really want to use a POST request, you can, of course, consider the "login" method the start of a session for that user. You could, therefore, do something like this: www.example.com/session [POST] with parameters userID and password. This would return a representation of the user profile and rights; it might also create documents accessible under the URLs www.example.com/session/sessionID www.example.com/session/user/ID/session However, in general, it is a very dangerous idea to manage session state within the API - nearly always, you want the client session to be managed by the application interacting with the client, not by the API it talks to.
563e33eb61a80130652681e0	X	What if I were to require a function such as login or perhaps in general any other types of action commands? How should the url be formed and how should the router redirect in such cases? It's not RESTful to have a login-action resource, but it is RESTful to provide a login-form resource: The HTML-form you return in the response functions as code-on-demand; you are supplying a configured piece of software to help the user supply their login credentials. There would be nothing wrong with identifying the resource as just /login - I added the form-part to make the example clear. You should avoid redirects where auth is required because it breaks the interface for clients other than web-browsers; instead you might either: provide a link to the login-form; or actually supply the login-form code in the response. If you want to manage authentication, I prefer the approach of creating auth-tokens; in the case of Web-browsers I consider it acceptable to overload a single cookie for the purpose of helping the client supply the token with each request since they will have no other reasonable way to control the Auth header they send; obviously if you're writing your own client-application this is not a concern. Answering your comments below, the purpose of the login form in an auth-token scenario is to create a new authentication token. So, thinking RESTfully, you model the users list of auth-tokens and POST a representation of the auth-token. This representation might contain the user's username and password. You might let the user choose their own token, or you might choose it for them and return this in the response. There is no action-URI required, and setting any cookies happens following successful creation of the new auth-token. I recommend studying Amazon S3 REST API. It's slightly different than your requirement but its the best in-depth description of a potential REST authentication system I've seen set out: http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html Your thoughts on managing users RESTfully are accurate. Hope it helps :)
563e33eb61a80130652681e1	X	CPU usage is definitely something you would have to watch out for if you are running on a shared hosting environment. Most plans offer plenty of disk space and bandwidth, but are very stingy on the CPU power you use.
563e33eb61a80130652681e2	X	I know about FFMpeg, but i'm not sure i can make a shell call on my webserver (especially because i don't know the OS it runs on and i'd like my app to be OS-independent), i was hoping to find something pure-java
563e33ec61a80130652681e3	X	FFMpeg is unfortunately the only REAL player in the game for free. FFmpeg is commonly used on linux, but there's also a Windows binary if you are so inclined.
563e33ec61a80130652681e4	X	make sure you get/compile a binary with LAME support, as FLV typically uses MP3 for its audio stream
563e33ec61a80130652681e5	X	FFMPeg is the way to go... As someone has mentioned in another answer, there are java wrappers you can use.
563e33ec61a80130652681e6	X	+1 and many more upvotes for your answers, which are undervalued. Art, big kudos for your work in Xuggler project.
563e33ec61a80130652681e7	X	Xuggler is not a pure java executable - You need to install it on the OS. This means that if I'd like to use it in Android - I cannot.
563e33ec61a80130652681e8	X	i am currently working on a web application that needs to accept video uploaded by users in any format (.avi, .mov, etc.) and convert them to flv for playing in a flash-based player. Since the site is OpenCms-based, the best solution would be a ready-made plugin for OpenCms that allowed to upload and play videos doing the transcode operation in background, but just a set of Java classes to do the transcode would be great and then i could make the uploading form and playback part on my own.
563e33ed61a80130652681e9	X	You basically have two choices if you want to host, transcode and stream flv files (and don't want to buy a video transcoding application): you can call out to FFMpeg/MEncoder or you can use an external Web service. You could also sidestep the problem completely by allowing them to embed YouTube videos on your site. If you go the 'local FFMpeg route' I would suggest simply using ProcessBuilder and constructing a command-line to execute FFMpeg. That way you get full control over what gets executed, you avoid JNI, which is an absolute nightmare to work with, and you keep OS-specific code out of your app. You can find FFMPeg with all the bells and whistles for pretty much any platform. There's a good chance it's already on your server. The nice thing about the 'Local FFMPeg' route is that you don't have to pay for any extra hosting, and everything is running locally, although your hosting admin might start complaining if you're using a crazy amount of disk and CPU. There are some other StackOverflow questions that talk about some of the gotchas using FFMpeg to create flvs that you can actually play in the flash player. The Web service route is nice because there is less setup involved. I have not used Hey!Watch but it looks promising. PandaStream is easy to set up and it works well, plus you get all your videos on S3 with no additional effort.
563e33ed61a80130652681ea	X	There's a great open source tool call FFmpeg that I use to transcode my videos. I use PHP making shell calls to make it come to life, but I can't imagine that it would be too hard to get it to play nice with Java. (Maybe this could be a good starting point for you.) I feed my installation 30+ gig batches on a weekly basis and it always comes out as quality material. The only tricky part for me has been getting it compiled to handle a wide variety of video formats. On the bright side, this has provided me with heavy lifting I need.
563e33ed61a80130652681eb	X	You can encode video in Java using Xuggler, which is a Java API that natively uses FFmpeg's C code behind the scenes.
563e33ed61a80130652681ec	X	This can be slightly tangential, but I have found Panda Stream to be a very useful solution to all kinds of video encoding problems. All you have to do is to upload the video file to an Amazon EC2 instance running Panda and it will encode the video to your desired formats and quality and will issue a callback to your application server with the details when it's done. You can then use the bundled Flash Video player or your own player to play the videos on your site. It's a very scalable (thanks to Amazon EC2 & S3), cost-effective and customisable solution compared to rolling your own. Highly recommended. Update: The architecture of Panda is as follows: 
563e33ed61a80130652681ed	X	There is an open source library used by MPlayer, called mencoder, wich supports FLV, as well as a lot of other codecs. There is a Java GUI you could see how was made This could help too. I don't seem to be able to find any example not called from the console, so it may not be usefull for you. :S Edit Also take a look at this question.
563e33ee61a80130652681ee	X	You could try using an online service like HeyWatch to convert your video. Never used it but they claim "transparent upload, send videos transparently from your website" Not a java solution, but you wouldn't have to worry about what OS your web application is on. If OS wasn't an issue I agree with the answer theBadDawg gave. I don't know of and have had not any luck finding a pure java solution.
563e33ee61a80130652681ef	X	Encoding files in one format to another takes a lot of development time to get right, which is why there is so little in terms of decoders/encoders that are able to accomplish those feats. The ones that are well known and used the most are ffmpeg and mencoder. What you may want to look into is to see if the platform you are running on (Windows/Mac OS X/Other unix) has an underlying set of API calls you can use that is able to decode the files, and re-encode them. Windows has DirectShow and Mac OS X has Quicktime. Not sure if you can access those API's using Java though. FFMpeg does have a Java wrapper available: FFMPEG Java, and there is also FOBS which has a JNI available for their C++ wrapper around ffmpeg. The last one that I found jFFmpeg, however there are some posts that I found with Google suggesting that the project may not be alive any longer. Your best bet would be either mencoder from mplayer and or ffmpeg. Ffmpeg can be installed as a separate binary and then called from other code using the default "shell" commands. If you are however not able to execute commands you may need to look at using an online conversion website like Mark Robinson suggested.
563e33ee61a80130652681f0	X	FFMpeg is the best when it comes to video transcoding. You can use java wrappers for ffmpeg - http://fmj-sf.net/ffmpeg-java/getting_started.php http://sourceforge.net/projects/jffmpeg/
563e33ee61a80130652681f1	X	If you want to do it with java, you can do it very easily using Xuggle. They have a great website explaining how to do everything the documentation is here: http://build.xuggle.com/view/Stable/job/xuggler_jdk5_stable/javadoc/java/api/index.html and an excellent tutorial telling you how to do what you want is here: http: //blog.xuggle.com/2009/06/05/introduction-to-xuggler-mediatools/ They provide an easy way to do what you want in the first tutorial, which is simple trans-coding. I've found that it works alright for encoding to flv. What it does behind the scenes is use ffmpeg, so anything that will trip up ffmpeg will also fail with xuggle. The relevant sample java code is: Which I got from http ://wiki.xuggle.com/MediaTool_Introduction If you want some fully working clojure code... here it is :) now all you have to do is something like: and you're done!
563e33ee61a80130652681f2	X	You might also be interested in hearing that we've now released Panda as a hosted service as well, which makes the setup and scaling easier :) http://pandastream.com
563e33ee61a80130652681f3	X	yea, ffmpeg is the best for this work...We use ffmpeg to convert video for a long time and it works with all video formats..numerous options are there..
563e33ee61a80130652681f4	X	Confirmed. You just put the url of your CloudFront distribution in your POST request instead of the url of your S3 bucket. Fortunately I don't need chunked requests or authorization headers, so those weren't issues.
563e33ef61a80130652681f5	X	Hi @yndolok - could you update with your POST - I'm struggling to get it to work myself - how are you doing this?
563e33ef61a80130652681f6	X	@waxical Hi and sorry for the late reply. Here are a couple of articles I found helpful: aws.amazon.com/articles/1434, pjambet.github.io/blog/direct-upload-to-s3. Do everything exactly the same as you would with a browser-based upload to S3, but use the url of your CloudFront distribution instead of your S3 bucket url.
563e33ef61a80130652681f7	X	It looks like the Authorization header is now preserved: docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/…
563e33ef61a80130652681f8	X	CF is a mess though. There are a number of issues when uploading to S3 via CF via the REST API. Some headers are still stripped. I ended up using a different CDN to deal with S3.
563e33ef61a80130652681f9	X	Now that CloudFront supports POST requests, is it possible to do browser-based uploads directly to CloudFront?
563e33ef61a80130652681fa	X	In theory, yes. However, there are very serious issues with support for anything but the most simple upload requests from a browser. For example, support for uploads to S3 via CloudFront using the multipart upload API is not possible due to the fact that CloudFront rips off the Authorization header from all of these requests. Amazon appears to be unwilling to fix this issue. This means you cannot support chunked requests from the browser if you target a CF endpoint or make any REST calls that require an Authorization header. There is also an issue the prevents you from targeting differing CloudFront distribution endpoints (in order to more easily target different S3 from the same CF domain).
563e33ef61a80130652681fb	X	I am developing a new website where users can upload files to an Amazon S3 bucket. After evaluating different upload libraries for jQuery I finally chose Dropzone JS. I was able to integrate Dropzone into my application in order to upload files directly to an Amazon S3 bucket. Everything is working fine with the upload. However I am having troubles reading the response from Amazon using jQuery. In particular, I'd like to get the Location header that comes as a response from Amazon. This location header has the information I need to process the uploaded file, but I am unable to get it using Dropzone. Anyone can advice how to get the XHR response headers? Checking the code I don't think this is possible, seems we can only get the response text but not the headers.
563e33ef61a80130652681fc	X	see How to get read data from response header in jquery/javascript for obtaining the response info. Assuming you are using the AWS POST operation http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html I suspect that the URI it returns is the one with amazon's domain: https://s3.amazonaws.com/Bucket/Object if you are using a "web" bucket and want to use your custom domain you will have to figure that out for yourself. You already have the bucket name since you provided it in the call. Another wrinkle could be the permissions of the file after upload. Be sure to set a policy on the paths for the uploads appropriately.
563e33ef61a80130652681fd	X	Here is my vote because of the article that explains everyting.
563e33ef61a80130652681fe	X	One of my clients has a site which displays media that has been uploaded from a client application. This application initially used FTP, but we're moving to S3 for various data storage and performance reasons. What I would like to be able to do is have this client upload a file directly to our central S3 store (ala dropbox/jungledisk etc etc), but I can't see a way of doing this without handing over our keys and embedding them in the application - not ideal! Is there any way to furnish a client application with a session key / temporary upload URL / something? This could be done via our website's api - which of course has full access to any required S3 secret keys. Suggestions?
563e33ef61a80130652681ff	X	Yes, this should be possible. What you need to do is create a signed policy file per-upload or per-user. That policy file, the signature, and some other data must be sent by the client program using a POST request to the bucket you'd like them to use. Amazon will examine the request, check that the parameters are within the limits of the policy file that accompanies the request, and then allow the post. Note that this policy should not be confused with the bucket policy. This is, in fact, a policy which could change per request if you wanted, and it is submitted by the client program (after the client program obtained a signed copy from you). Full details on this can be found in the Browser Based Uploads Using POST section of the S3 documentation. I'd recommend a detailed review of the HTML Forms section, and a review of how to get the POST parameters to your client (for a browser you can send it HTML, which is how the documentation is worded, for a non-browser program you probably need an API call of some sort followed by the client submitting a POST to S3). You can also check out this web page which can give you an idea about how to setup the parameters: http://s3.amazonaws.com/doc/s3-example-code/post/post_sample.html
563e33ef61a8013065268200	X	Its possible to do it using form based HTML upload: This article does a good job explaining how it is possible. After reading the article you can use my scripts mentioned below to make life easier. Here is a python script I use for generating my policy, its signature and base64 string. IMP: Make sure the policy.json file is in the same directory And here is the corresponding policy.json file I use: The HTML form for this code is as such:
563e33f561a8013065268201	X	And, given that everything is happening over https, do you think it is even necessary to check the MD5 hash on the client side?
563e33f561a8013065268202	X	The algorithm has NOT been cracked. It has just been proven that there are possible collisions. That does in not mean that it's easy to produce a collision. - Yet, I still agree that SHA or even better PGP is more suitable for anything in the legal sector.
563e33f661a8013065268203	X	Hi Guys - thanks for the messages. I saw too that it is possible to craft documents that have the same md5 hash. The only reason I am using it is simply because that is what Amazon S3 reports on files. Our caching server will sit between the client and S3. The client will upload files to the caching server and, subsequently, the caching server will upload files to S3.
563e33f661a8013065268204	X	@Halfdan - Your response makes me think you don't quite understand how hashing algorithms work. The pigeon hole principle dictates that all hashing algorithms will have collisions. Once an algorithm exists that can predict collisions in a hashing algorithm it's considered broken. MD5 has been considered broken for about 3 years now ever since it was proven that MD5 SSL certs could be faked. See: en.wikipedia.org/wiki/MD5#Collision_vulnerabilities
563e33f861a8013065268205	X	@Ross - They're probably intentionally using MD5 hashes so that you don't do exactly what you're trying to do. MD5 hashes are only good for determining if a file was accidentally corrupted through transmission or some other issue. Relying on it for some kind of pseudo security mechanism is at best silly and at worst irresponsible. Figure out another way.
563e33fc61a8013065268206	X	I'm writing a web application that will store files on the Amazon S3 cloud. The application needs to be able to show that files that are uploaded to it have not been altered at any time. Files will be uploaded to the web application server where they will be cached, and then uploaded to the Amazon S3 Cloud. At each point in the process (file selected for upload on the client side, file stored on the caching server, file stored in the cloud) I want to take and compare md5 checksums to show the file has not been changed in any way. The application is part of a larger project in the legal sector, hence the need for the file reliability and validation. Comparing the checksum of the file stored on the caching server, and the file stored in Amazon S3 is easy, but I am looking for a file uploader that will calculate a checksum on the client side and report this with the file upload. File sizes will likely be no more than 20MB in size so checksumming will not be too hard on the client machine. Any ideas anyone? For compatibility purposes, I would prefer a flash or java implementation, although I understand that html5 will allow client side file access and this is already adopted in the latest firefox. Any ideas extremely appreciated!! Thanks Ross
563e33fc61a8013065268207	X	MD5 file hashes are only useful for determining unintentional file alterations. That is to say, if you're worried someone might intentionally alter the files an MD5 hash can't assure that because the algorithm has been cracked. You should look into using one of the SHA hashing capabilities. Additionally I can't determine if you're asking for an app that can upload to your server or an app that will upload to Amazon. Clearly these clients would be very different as the S3 system already has an API for uploading files whereas your server does not.
563e33fc61a8013065268208	X	Thanks. I feel like I understand the pieces on simple terms but hooking everything up still intimidates me a bit.
563e33fc61a8013065268209	X	I am looking at the data.seattle.gov data sets and I'm wondering in general how all of this large raw data can get sent to hadoop clusters. I am using hadoop on azure.
563e33fd61a801306526820a	X	It looks like data.seattle.gov is a self contained data service, not built on top of the public cloud. They have own Restful API for the data access. Thereof I think the simplest way is to download interested Data to your hadoop cluster, or to S3 and then use EMR or own clusters on Amazon EC2. If they (data.seattle.gov ) has relevant queries capabilities you can query the data on demand from Your hadoop cluster passing data references as input. It might work only if you doing very serious data reduction in these queries - otherwise network bandwidth will limit the performance.
563e33fd61a801306526820b	X	In Windows Azure you can place your data sets (unstructured data etc..) in Windows Azure Storage and then access it from the Hadoop Cluster Check out the blog post: Apache Hadoop on Windows Azure: Connecting to Windows Azure Storage from Hadoop Cluster: http://blogs.msdn.com/b/avkashchauhan/archive/2012/01/05/apache-hadoop-on-windows-azure-connecting-to-windows-azure-storage-your-hadoop-cluster.aspx You can also get your data from the Azure Marketplace e.g. Gov Data sets etc.. http://social.technet.microsoft.com/wiki/contents/articles/6857.how-to-import-data-to-hadoop-on-windows-azure-from-windows-azure-marketplace.aspx
563e33fd61a801306526820c	X	can I pass arguments to s3.copy(...) to specify the permissions of the new file?
563e33fd61a801306526820d	X	I am currently developing a rails application that tries to copy/move videos from one bucket to another in s3. However i keep getting a proxy error 502 on my rails application. In the mongrel log it says "failed to allocate memory." Once this error occurs the application dies and we must restart is.
563e33fd61a801306526820e	X	Seems like your code is reading the entire resource into memory, and that out-of-memories your application. A naïve way to do this (and from your description, you're doing something like this already) would be to download the file and upload it again: just download it to a local file and not into memory. However, Amazon engineers have thought ahead and provide APIs that can deal with this specific case, as well. If you're using something like the RightAWS gem, you can use its S3Interface like so: And if you're using the naked S3 HTTP interface, see amazon's object copy docs for a solution that uses only HTTP to copy one object from one bucket to another.
563e33fd61a801306526820f	X	try to stream files instead of loading whole file into memory and then working with it. for example, if you're using aws-s3 gem, do not use: Use following instead: not sure how exactly to "stream-download" the file though.
563e33fd61a8013065268210	X	boto works well. See this thread. Using boto, you copy the objects straight from one bucket to another, rather than downloading them to the local machine and then uploading them to another bucket.
563e33fe61a8013065268211	X	Thanks for the answer. I had seen that post - I had really dug into this a lot. We went with the option of clients cannot see their own buckets, which is a shame, but is the best option given our primary concerns.
563e33fe61a8013065268212	X	I've been able to generate a user policy that only gives access to a specific bucket, however after trying everything (including this post: S3 policy limiting access to only one bucket (listing included)). The problem: I am unable to restrict the listing of the buckets down to just one bucket. For a variety of reasons, I do not want the listing to show any buckets other than the one specified. I've tried a variety of policies, to no avail. Here's my latest policy JSON which is working as far as restricting operations, but not listing: Any help would be greatly appreciated. I'm beginning to wonder if it's even possible.
563e33fe61a8013065268213	X	It is not currently possible to restrict the list of buckets to show only one bucket. The AWS console relies on the ListAllMyBuckets action to get the list of buckets owned by the user, but the returned list can not be restricted by using an Amazon Resource Name (or ARN; the only ARN that's allowed for ListAllMyBuckets is arn:aws:s3:::*). This limitation isn't clearly explained in the official AWS docs, but ListAllMyBuckets is a service level API call (it's also called GET Service in the REST API), not a bucket level API call and its associated ARN in the IAM policy refers to the S3 service an not to a specific bucket. For possible workarounds, see this answer on StackOverflow:
563e33fe61a8013065268214	X	The free "S3 Browser" (this works on my version 3-7-5) allows users with the proper permissions to "Add External Bucket" for the account, all they need to know is the name of the bucket. This allows them to "see" their bucket and the contents (and what ever abilities they've been given inside that bucket), they won't see any of the other buckets. To make the bucket "play nice" with the S3 Browser behavior, I suggest the following IAM Policy for the User or Group: It's a work around, and it's okay if the user only needs to do it once. But if the buckets your user is accessing are changing around a lot then this work around isn't very practical.
563e33fe61a8013065268215	X	What is StreamingMessage?
563e33fe61a8013065268216	X	It is a java bean containing the message fields. Basically a bunch of String or Number fields and Maps of key/values (String).
563e33fe61a8013065268217	X	could you post the code for StreamingMessage ? probably its equals and|or hashCode are incorrectly implemented.
563e33fe61a8013065268218	X	You were right, equals and compareTo were inconsistent in the class used for sorting. In one special case, compareTo returned 0 but equals would return false. After fixing this bug, the job is working as expected.
563e33fe61a8013065268219	X	I am testing the Spark Streaming API. The application is deployed on an Amazon EMR cluster with Spark 1.4.0 I am sorting data and saving files in S3. The code of the pipeline (except the sort algorithm) is detailed below : When the application is run on a cluster, it fails fast with the following error. 15/07/17 13:17:36 ERROR executor.Executor: Exception in task 0.1 in stage 8.0 (TID 90) java.lang.IllegalArgumentException: Comparison method violates its general contract! at org.apache.spark.util.collection.TimSort$SortState.mergeLo(TimSort.java:776) at org.apache.spark.util.collection.TimSort$SortState.mergeAt(TimSort.java:507) at org.apache.spark.util.collection.TimSort$SortState.mergeCollapse(TimSort.java:435) at org.apache.spark.util.collection.TimSort$SortState.access$200(TimSort.java:307) at org.apache.spark.util.collection.TimSort.sort(TimSort.java:135) at org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37) at org.apache.spark.util.collection.PartitionedPairBuffer.partitionedDestructiveSortedIterator(PartitionedPairBuffer.scala:70) at org.apache.spark.util.collection.ExternalSorter.partitionedIterator(ExternalSorter.scala:690) at org.apache.spark.util.collection.ExternalSorter.iterator(ExternalSorter.scala:708) at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:64) at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:90) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69) at org.apache.spark.rdd.RDD.iterator(RDD.scala:242) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63) at org.apache.spark.scheduler.Task.run(Task.scala:70) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745 The error happens on the foreachRDD step but I'm still searching why it fails...
563e33fe61a801306526821a	X	The class used for sorting had a bug in the compareTo implementation. The javadoc for Comparable recommend to implement compareTo in a consistent way with equals(). After fixing this bug, the spark job works as expected.
563e33fe61a801306526821b	X	Facing the same issue, did you ever resolve this? I suspect there is a way to upload EC2 specific config.
563e33fe61a801306526821c	X	I'm afraid not :( this problem and acouple others led us to switch to Microsoft Azure, which ended up being better suited to .NET applications.
563e33fe61a801306526821d	X	I am currently building a C# WebApi 2 application that I will be uploading to an Amazon Elastic Beanstalk instance to deploy. I am having success so far, and on my local machine, I just finished testing the file upload capability in order for clients to upload images. The way it goes is I accept the multipart/formdata in the Web Api and save the temp file (with a random name like BodyPart_24e246c7-a92a-4a3d-84ef-c1651416e667) to the App_Data folder. The temporary file is put into an S3 Bucket and I create a reference in my SQL Server database to it. Testing works fine with single or multiple file uploads locally but when I deploy the application to Elastic Beanstalk and try to upload I get errors like "Could not find a part of the path 'C:\inetpub\wwwroot\sbeAPI_deploy\App_Data\BodyPart_8f552d48-ed9b-4ec2-9986-88cbffd673ee'" or a similar one saying access is denied altogether. I have been trying to find the solution online for a few hours now, but the AWS documentation is all over the place and tutorials/other questions seem to be outdated. I believe it has something to do with not having permission to write the temporary files on the EC2 server, but I can't figure out how to fix it. Thanks very much in advance.
563e33ff61a801306526821e	X	This is already possible since April 2013, see also here: Basically the steps you need to perform are the following: All set!! Be sure there's a file within App_Data otherwise Visual Studio won't publish it. { "containercommands": { "01-changeperm": { "command": "icacls \"C:/inetpub/wwwroot/MyApp_deploy/App_Data\" /grant DefaultAppPool:(OI)(CI)" } } }
563e33ff61a801306526821f	X	hadoop -> the old way, Spark -> the new way.
563e340061a8013065268220	X	Do you have enough RAM in your cluster? Spark loves RAM. M/R goes to disk for everything, supports larger datasets.
563e340061a8013065268221	X	You said "But, a lot of companies are still using MapReduce Framework on Hadoop for batch processing instead of Spark." Do you have any data to support this claim ?
563e340061a8013065268222	X	Yes, I do. Based on the data, I asked this question. But, data cannot be shared. I am very much convinced that Spark is better and it will become mainstream in future. So, companies not adapting Spark at this point of time is because of stability or any other reasons?
563e340061a8013065268223	X	Well, my question is why MR is preferred over spark for batch processing by many companies today. All new projects (batch) are implemented in MR only. Why not Spark? Is it because of spark is not as stable as MR? Or any other challenges?
563e340061a8013065268224	X	I'm afraid that's a hard question to answer either way. Where is the data that shows that companies are selecting MR over Spark? IMO, many have MR because it existed long before Spark did. Spark is still not main stream yet but I don't see any other reason why I won't become mainstream. In my answer above I've tried to give as key "technical" reasons why I think Spark is better than MR and why it is likely to replace MR in the future.
563e340161a8013065268225	X	I agree with Soumya that time is probably most important factor when it comes to the adoption of Apache Spark. When 64 bit architectures were introduced, people also didn't immediately switch their operating system because migrations are always complicated and risky. The same applies to companies which started with MR and now slowly start to look into other alternatives. I don't know if there are many companies today which decide to use MR over other next generation runtimes like Spark.
563e340161a8013065268226	X	"Apache Spark runs in memory" - I don't think this is completely correct. Spark performs the best if your dataset fits in memory. But it can handle operations even when your dataset doesn't fit in memory. Spark in most cases is as fast as Hadoop map-reduce.
563e340161a8013065268227	X	I know that MapReduce is a great framework for batch processing on Hadoop. But, Spark also can be used as batch framework on Hadoop that provides scalability, fault tolerance and high performance compared MapReduce. Cloudera, Hortonworks and MapR started supporting Spark on Hadoop with YARN as well. But, a lot of companies are still using MapReduce Framework on Hadoop for batch processing instead of Spark. So, I am trying to understand what are the current challenges of Spark to be used as batch processing framework on Hadoop? Any thoughts?
563e340161a8013065268228	X	Spark is an order of magnitude faster than mapreduce for iterative algorithms, since it gets a significant speedup from keeping intermediate data cached in the local JVM. With Spark 1.1 which primarily includes a new shuffle implementation (sort-based shuffle instead of hash-based shuffle), a new network module (based on netty instead of using block manager for sending shuffle data), a new external shuffle service made Spark perform the fastest PetaByte sort (on 190 nodes with 46TB RAM) and TeraByte sort breaking Hadoop's old record. Spark can easily handle the dataset which are order of magnitude larger than the cluster's aggregate memory. So, my thought is that Spark is heading in the right direction and will eventually get even better. For reference this blog post explains how databricks performed the petabyte sort.
563e340161a8013065268229	X	I'm assuming when you say Hadoop you mean HDFS. There are number of benefits of using Spark over Hadoop MR. Performance: Spark is at least as fast as Hadoop MR. For iterative algorithms (that need to perform number of iterations of the same dataset) is can be a few orders of magnitude faster. Map-reduce writes the output of each stage to HDFS. 1.1. Spark can cache (depending on the available memory) this intermediate results and therefore reduce latency due to disk IO. 1.2. Spark operations are lazy. This means Spark can perform certain optimizing before it starts processing the data because it can reorder operations because they have executed yet. 1.3. Spark keeps a lineage of operations and recreates the partial failed state based on this lineage in case of failure. Unified Ecosystem: Spark provides a unified programming model for various types of analysis - batch (spark-core), interactive (REPL), streaming (spark-streaming), machine learning (mllib), graph processing (graphx), SQL queries (SparkSQL) Richer and Simpler API: Spark's API is richer and simpler. Richer because it supports many more operations (e.g., groupBy, filter ...). Simpler because of the expressiveness of these functional constructs. Spark's API supports Java, Scala and Python (for most APIs). There is experimental support for R. Multiple Datastore Support: Spark supports many data stores out of the box. You can use Spark to analyze data in a normal or distributed file system, HDFS, Amazon S3, Apache Cassandra, Apache Hive and ElasticSearch to name a few. I'm sure support for many other popular data stores is comings soon. This essentially if you want to adopt Spark you don't have to move your data around. For example, here is what code for word count looks in Spark (Scala). I'm sure you have to write a few more lines if you are using standard Hadoop MR. Here are some common misconceptions about Spark. Spark is just a in-memory cluster computing framework. However, this is not true. Spark excels when your data can fit in memory because memory access latency is lower. But you can make it work even when your dataset doesn't completely fit in memory. You need to learn Scala to use Spark. Spark is written in Scala and runs on the JVM. But the Spark provides support for most of the common APIs in Java and Python as well. So you can easily get started with Spark without knowing Scala. Spark does not scale. Spark is for small datasets (GBs) only and doesn't scale to large number of machines or TBs of data. This is also not true. It has been used successfully to sort PetaBytes of data Finally, if you do not have a legacy codebase in Hadoop MR it makes perfect sense to adopt Spark, the simple reason being all major Hadoop vendors are moving towards Spark for good reason.
563e340261a801306526822a	X	Apache Spark runs in memory, making it much faster than mapreduce. Spark started as a research project at Berkeley. Mapreduce use disk extensively (for external sort, shuffle,..). As the input size for a hadoop job is in order of terabytes. Spark memory requirements will be more than traditional hadoop. So basically, for smaller jobs and with huge memory in ur cluster, sparks wins. And this is not practically the case for most clusters. Refer to spark.apache.org for more details on spark
563e340261a801306526822b	X	You can't because you can't execute code on S3 servers. You need to download, concat, upload.
563e340261a801306526822c	X	concating files can be done in unix shell by using cat pipes | and greater than >. Putting together two mp3s is a different question. Please differentiate and describe your problem better.
563e340261a801306526822d	X	yeah concatenating content of mp3 is different business then just concatenating files
563e340261a801306526822e	X	Concatenating text files is one thing...you'd just download concat and then upload. However, binary media files, such as an MP3 cannot just be concatenated as that's actually a fairly complicated process. So is this question regarding MP3s or how to concatenate generic text files from S3. Please edit your question appropriately.
563e340261a801306526822f	X	You can use the audiosprite node module to join mp3 files with ffmpeg. https://github.com/tonistiigi/audiosprite. As others have mentioned. you will generally need to download the mp3 files from S3 and then join them. Depending on your ultimate goal, it is possible to join and serve the joined file to your users without downloading to your server but doing the processing with AWS lambda based on a user event (e.g. join button). lambda and S3 are close together so no added double latency or extra server load. If that's what you'd like to know, please update the question.
563e340361a8013065268230	X	I want to concatenate the files uploaded on Amazon S3 server. How can I do this. Concatenation on local machine i can do using following code.  
563e340361a8013065268231	X	You can achieve what you want by breaking it into two steps: Since s3 is a remote file storage, you can't run code on s3 server to do the operation locally (as @Andrey mentioned). what you will need to do in your code is to fetch each input file, process them locally and upload the results back to s3. checkout the code examples from amazon: at this stage you'll run your concatenation code, and upload the results back: Since MP3 file include a header that specifies some information like bitrate, simply concatenating them together might introduce playback issues. See: http://stackoverflow.com/a/5364985/1265980 what you want to use a tool to that. you can have one approach of saving your input mp3 files in tmp folder, and executing an external program like to change the bitrate, contcatenate files and fix the header. alternatively you can use an library that allows you to use ffmpeg within node.js. in their code example shown, you can see how their merge two files together within the node api.
563e340361a8013065268232	X	Here's my quick take on the problem of downloading and processing S3 objects. My example is focused mostly on getting the data local and then processing it once it's all downloaded. I suggest you use one of the ffmpeg approaches mentioned above.
563e340361a8013065268233	X	Are you sure that the put request is successful? There is usually a clear cache call you can make with these services and you can often do it manually. If you do that you might see that the PUT request was successful or not.
563e340361a8013065268234	X	Yes, i see that the data was changed in the S3 bucket
563e340361a8013065268235	X	I am using CloudFront to access my S3 bucket. I perform both GET and PUT operations to retrieve and update the data. The problem is that after i send PUT request with new data, GET request still returns the older data. I do see that the file is updated in S3 bucket. I am performing both GET and PUT from iOS application. However, i tried performing GET request using regular browsers and i still receive older data. Do i need to do anything in addition to make CloudFront refresh its data?
563e340361a8013065268236	X	Cloudfront caches your data. How long depends on the headers the origin serves content with and the distribution settings. Amazon has a document with the full results of how they interac, but if you haven't set your cache control headers, not changed any cloudfront settings, then by default data is cached for upto 24 hours. You can either: set headers indicating how long to cache content for (e.g. Cache-Control: max-age=300 to allow caching for up to 5 minutes). How exactly you do this depends on how you are uploading the content, at a pinch you can use the console Use the console / api to invalidate content. Beware that only the first 1000 invalidations a month a free - beyond that amazon charges. In addition, invalidations take 10-15 minutes to process. Change the naming strategy for your s3 data so that new data is served under a different name (perhaps less relevant in your case)
563e340361a8013065268237	X	When you PUT an object into S3 by sending it through Cloudfront, Cloudfront proxies the PUT request back to S3, without interpreting it within Cloudfront... so the PUT request changes the S3 object, but the old version of the object, if cached, would have no reason to be evicted from the Cloudfront cache, and would continue to be served until it is expired, evicted, or invalidated. "The" Cloudfront cache is not a single thing. Cloudfront has over 50 global edge locations (reqests are routed to what should be the closest one, using geolocating DNS), and objects are only cached in locations through which they have been requested. Sending an invalidation request to purge an object from cache causes a background process at AWS to contact all of the edge locations and request the object be purged, if it exists. What's the point of uploading this way, then? The point has to do with the impact of packet loss, latency, and overall network performance on the throughput of a TCP connection. The Cloudfront edge locations are connected to the S3 regions by high bandwidth, low loss, low latency (within the bounds of the laws of physics) connections... so the connection from the "back side" of Cloudfront towards S3 may be a connection of higher quality than the browser would be able to establish. Since the Cloudfront edge location is also likely to be closer to the browser than S3 is, the browser connection is likely to be of higher quality and more resilient... thereby improving the net quality of the end-to-end logical connection, by splitting it into two connections. This feature is solely about performance: http://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/ If you don't have any issues sending directly to S3, then uploads "through" Cloudfront serve little purpose.
563e340461a8013065268238	X	Probably you should ask this question on superuser.com
563e340461a8013065268239	X	Thanks for the very thorough answer! Guess I'll just use RDS for now, and worry about load balancing and auto-scaling later. One question I have is: How do I counter the volatility of an instance? Meaning: If I go through the trouble of installing an configuring all the software I want on an instance, then it gets terminated, how do I prevent starting from scratch the next time I want to fire up an instance?
563e340461a801306526823a	X	The images that EC2 instances boot are called AMIs. There are a few for standard linux distributions, and you can make your own reasonably easy. You can also boot from EBS snapshots. A third option, more often found in larger setups, is to boot from a standard AMI, but run a script to set up the server on the fly, usign something like Chef or Puppet . Best of luck.
563e340461a801306526823b	X	I'm a web developer just now getting interested in sysadmin stuff. I've set up a server before on Linode.com (Ubuntu 10.04 LTS, nginx, Ruby on Rails, PostgreSQL), but there were some issues. Everything was on one machine, so whenever something went wrong with Linode or I got a lot of traffic, my site would go down. Now I'm interested in setting up a personal blog, and deploying it across Amazon AWS. This is a good opportunity for me to learn to how to use multiple servers with load balancing, auto-scaling, failover, etc. The only problem is I'm not quite sure where to start. I've read a litany of documentation from Amazon and blog posts elsewhere, but as a sysadmin newbie I have a few questions: Thanks for any answers you can provide!
563e340461a801306526823c	X	I've set up my fair share of AWS deployments; here's basics: If you have frequently accessed data, as you likely know, it is best to use a database. This is one of the hairier parts of AWS hosting. Your options are, roughly in increasing order of complexity/cost: Amazon pretty much enumerates these options, plus a few more which are not applicable to you at http://aws.amazon.com/running_databases/ Infrequently changed data should be stored in S3; there's plenty of ruby gems for accessing this easily. If your website is entirely static on the server side, you can even run your entire site off S3 Amazon "Elastic Load Balancing" is quite effective at the typical web load balancing requirements. It is usually a no-brainer choice, unless you have exotic requirements. It will not scale your cluster for you, however. For auto-booting and shutting down of instances, you should look to Amazon's own auto-scaling solution Be sure to note which "Availability Zone" (aka datacenter) you're in. In some cases, you cannot share AWS resources across availability zones. There are plenty of tutorials, but in my brief search, none that I found to be really great or up to date. However, check out https://github.com/wr0ngway/rubber , which is a ruby tool for deploying apps to EC2. It will get you most of the way there.
563e340461a801306526823d	X	If the client knows how to manipulate your URIs (by appending .xml or otherwise) then your API is not REST.
563e340461a801306526823e	X	this is a good Oauth 2.0 Server library for ruby github.com/Lelylan/rest-oauth2-server
563e340461a801306526823f	X	many people are going down the API key + signing path, anyone know if there is a 'better way'?!
563e340461a8013065268240	X	Nice answer - this finally cleared it up for me with the different levels. Thanks!
563e340461a8013065268241	X	There are two big problems with HTTP "Authorization:". The first is that the server decides on the auth scheme, which means that Digest authentication doesn't prevent password-compromise by a MITM (just pretend that you only support Basic). The second is that Basic authentication isn't limited to a path, so anyone on a shared webserver can get anyone else's password (a lot of people are switching to subdomains for this reason, along with cookies and generally more sane Apache config).
563e340561a8013065268242	X	What is not clear is whether this creates an inconsistency in websites that support (the now ubiquitous) OAuth-style delegated authentication. If my site promotes Twitter or Facebook login, then I do not provide any user name and password for browser access. I simply delegate it to Twitter/Fb via oauth. But when I need to provide programmatic access to my users, I have to generate and provide an API_KEY to the same users. IMO, this is equivalent to providing user name and password even when browser access to your website relies on delegated authentication.
563e340561a8013065268243	X	FYI, it is generally safe to not have csrf for API calls as the particular vulnerability can only be executed through a web browser.
563e340561a8013065268244	X	I'm getting started on building a REST API for a project I'm working on, and it led me to do a little research as to the best way to build an API using RoR. I find out pretty quickly that by default, models are open to the world and can be called via URL by simply putting a ".xml" at the end of the URL and passing appropriate parameters. So then the next question came. How do I secure my app to prevent unauthorized changes? In doing some research I found a couple articles talking about attr_accessible and attr_protected and how they can be used. The particular URL I found talking about these was posted back in May of '07 (here). As with all things ruby, I'm sure that things have evolved since then. So my question is, is this still the best way to secure a REST API within RoR? If not what do you suggest in either a "new project" or an "existing project"scenario?
563e340561a8013065268245	X	There are several schemes for authenticating API requests, and they're different than normal authentication provided by plugins like restful_authentication or acts_as_authenticated. Most importantly, clients will not be maintaining sessions, so there's no concept of a login. HTTP Authentication You can use basic HTTP authentication. For this, API clients will use a regular username and password and just put it in the URL like so: I believe that restful_authentication supports this out of the box, so you can ignore whether or not someone is using your app via the API or via a browser. One downside here is that you're asking users to put their username and password in the clear in every request. By doing it over SSL, you can make this safe. I don't think I've ever actually seen an API that uses this, though. It seems like a decently good idea to me, especially since it's supported out of the box by the current authentication schemes, so I don't know what the problem is. API Key Another easy way to enable API authentication is to use API keys. It's essentially a username for a remote service. When someone signs up to use your API, you give them an API key. This needs to be passed with each request. One downside here is that if anyone gets someone else's API key, they can make requests as that user. I think that by making all your API requests use HTTPS (SSL), you can offset this risk somewhat. Another downside is that users use the same authentication credentials (the API key) everywhere they go. If they want to revoke access to an API client their only option is to change their API key, which will disable all other clients as well. This can be mitigated by allowing users to generate multiple API keys. API Key + Secret Key signing Deprecated(sort of) - see OAuth below Significantly more complex is signing the request with a secret key. This is what Amazon Web Services (S3, EC2, and such do). Essentially, you give the user 2 keys: their API key (ie. username) and their secret key (ie. password). The API key is transmitted with each request, but the secret key is not. Instead, it is used to sign each request, usually by adding another parameter. IIRC, Amazon accomplishes this by taking all the parameters to the request, and ordering them by parameter name. Then, this string is hashed, using the user's secret key as the hash key. This new value is appended as a new parameter to the request prior to being sent. On Amazon's side, they do the same thing. They take all parameters (except the signature), order them, and hash using the secret key. If this matches the signature, they know the request is legitimate. The downside here is complexity. Getting this scheme to work correctly is a pain, both for the API developer and the clients. Expect lots of support calls and angry emails from client developers who can't get things to work. OAuth To combat some of the complexity issues with key + secret signing, a standard has emerged called OAuth. At the core OAuth is a flavor of key + secret signing, but much of it is standardized and has been included into libraries for many languages. In general, it's much easier on both the API producer and consumer to use OAuth rather than creating your own key/signature system. OAuth also inherently segments access, providing different access credentials for each API consumer. This allows users to selectively revoke access without affecting their other consuming applications. Specifically for Ruby, there is an OAuth gem that provides support out of the box for both producers and consumers of OAuth. I have used this gem to build an API and also to consume OAuth APIs and was very impressed. If you think your application needs OAuth (as opposed to the simpler API key scheme), then I can easily recommend using the OAuth gem.
563e340561a8013065268246	X	How do I secure my app to prevent unauthorized changes? attr_accessible and attr_protected are both useful for controlling the ability to perform mass-assignments on an ActiveRecord model. You definitely want to use attr_protected to prevent form injection attacks; see Use attr_protected or we will hack you. Also, in order to prevent anyone from being able to access the controllers in your Rails app, you're almost certainly going to need some kind of user authentication system and put a before_filter in your controllers to ensure that you have an authorized user making the request before you allow the requested controller action to execute. See the Ruby on Rails Security Guide (part of the Rails Documentation Project) for tons more helpful info.
563e340561a8013065268247	X	I'm facing similar questions as you at the moment because i'm also building out a REST api for a rails application. I suggest making sure that only attributes that can be user edited are marked with attr_accessible. This will set up a white list of attributes that can be assigned using update_attributes. What I do is something like this: All my models inherit from that, so that they are forced to define attr_accessible for any fields they want to make mass assignable. Personally, I wish there was a way to enable this behaviour by default (there might be, and I don't know about it). Just so you know someone can mass assign a property not only using the REST api but also using a regular form post.
563e340561a8013065268248	X	Another approach that saves building a lot of the stuff yourself is to use something like http://www.3scale.net/ which handles keys, tokens, quotas etc. for individual developers. It also does analytics and creates a developer portal. There's a ruby/rails plugin ruby API plugin which will apply to policies to traffic as it arrives - you can use it in conjunction with the oAuth gem. You can also us it by dropping varnish in front of the app and using the varnish lib mod: Varnish API Module.
563e340561a8013065268249	X	stackoverflow.com/questions/30565422/…
563e340561a801306526824a	X	A good guess, but sadly, I went down that alley too. Looked into it and they fixed the bug last month :(. Thank you for helping me look though!
563e340561a801306526824b	X	the ssl write failure still seems to be the issue here. Not sure the aws sdk is causing this. it may be a cert of library issue. Do you use keep-alive for your connections? What happens if you turn that off?
563e340561a801306526824c	X	Currently it is a really simple app that just makes that one call. I am not running any services or alarms to run it via a service. It is running on AsyncTask (So off the main thread). I am unsure of what the cert library issue is you mentioned is, I will have to research it a bit. / One thing I did figure out is that I think it is an issue before even the credentials hit. I changed the key and secret key to random letters and received the same error. Furthermore, I know those keys work as they upload just find via Eclipse/ Java.
563e340561a801306526824d	X	yup. seems something goes wrong when the connection is established / tries to be established.
563e340661a801306526824e	X	I have spent the past 2 days struggling with Amazon's S3 SDK for Android. I was able to get the Java one (in Eclipse) working without any problems whatsoever; I could upload pictures, download them, and it would be no problem. Changing gears to Android, however, and I have had no luck. Currently, with this selected code: I am getting multiple errors, the most common of which is this: I have done a ton of research and had no luck finding WORKING code. I used this link from Amazon: https://aws.amazon.com/articles/SDKs/Android/3002109349624271 Without it working for me at all. They say up top it is deprecated, but I cannot find any links to working code. If you follow the SDK links to 'android sample code' files, their github repo (here: https://github.com/awslabs/aws-sdk-android-samples) contains zero code on the topic of uploading files (namely pictures). Does anyone have ANY idea where I can find some working code that shows how to just upload a stupid picture to my bucket?!??! (Wish I knew why this was so simple in Java/ Eclipse and not so in Android / Studio). PS: I have my api_key in the correct assets folder, my credentials are correct for login, the image is under 5mb, and this is being run on a background (async) thread so as to not be on the main thread. -Pat
563e340661a801306526824f	X	Have you tested to see that file:///storage/emulated/0/MyDir/image_1437585138776.jpg is a useable file? A content URI is Android does not typically map to a file and is typically used with content resolvers. I would double check that is actually a file path and not a content resolver uri path, which is what it looks like. If that pans out, then double check the internet connection of the device. Can you call any AWS api? Are you behind some kind of firewall or protected wifi? Finally it is not secure to use embedded credentials in an Android app (understandable if you are just testing locally, but never ship an app with embedded credentials, instead use Amazon Cognito to authenticate). For other example with S3 you can see the Getting Started Guide http://docs-aws.amazon.com/mobile/sdkforandroid/developerguide/getting-started-android.html which has a bunch of S3 example. And the S3 sample on GitHub (which was updated on July 22nd) has an S3 uploader sample using the Transfer Utility, and a step-by-step tutorial along with it. Hope that helps!
563e340661a8013065268250	X	Figured out the answer, though the why still eludes me. Turns out, the issue was with the Buckets on the back-end. One of my buckets was renamed slightly (whitespace) which prevented any uploads to it. I deleted and recreated by bucket and then re-posted it, and it seemed to work just fine. WestonE, you bring up some excellent tips in your post, making sure this does not go to production with local credentials is a very good call. And to answer your Q, yes, surprisingly, file:///storage/emulated/0/MyDir/image_1437585138776.jpg is indeed a valid file. It may be as simple as the fact that HTC phones have really weird storage location systems.
563e340661a8013065268251	X	Pretty much covered in stackoverflow.com/questions/1775816/… , although @svetianov's answer looks a bit more complete.
563e340661a8013065268252	X	Thanks you ! :)
563e340661a8013065268253	X	i developed a iphone app to upload videos to amazon , and i encoded file in to base64 encoded 128-bit MD5, upon file upload completion i receive " ErrorCode:BadDigest, Message:The Content-MD5 you specified did not match what we received." does this means i cant match md5 values with my pohone and s3 server ?
563e340661a8013065268254	X	I try to verify the integrity of a file that was uploaded to a bucket but I don't find any information of this. In the file's headers, there is a "E-tag" but I think its not a md5 checksum. So, how can I check if the file that I uploaded on Amazon S3 is the same that I have on my computer ? Thanks. :)
563e340661a8013065268255	X	If you are using the REST API to upload an object (up to 5GB) in a single operation, then you can add the Content-MD5 header in your PUT request. According the the S3 documentation for PUT, the Content-MD5 header is: The base64 encoded 128-bit MD5 digest of the message (without the headers) according to RFC 1864. This header can be used as a message integrity check to verify that the data is the same data that was originally sent. Although it is optional, we recommend using the Content-MD5 mechanism as an end-to-end integrity check. Check this answer on how to compute a base64 encoded 128-bit MD5 digest. If you are using s3curl, you can include the computed digest in your request headers using the --contentMd5 option. If the md5 digest computed by Amazon upon upload completion does not match the md5 digest you provided in the Content-MD5 header, Amazon will respond with a BadDigest error code. If you are using multipart upload, the Content-MD5 header serves as an integrity check for each part individually. Once the multipart upload is finalized, Amazon does not currently provide a way to verify the integrity of the assembled file.
563e340661a8013065268256	X	Possible Duplicate: database for huge files like audio and video I'm seeking for the best (or at least good enough) way of storing large sets of binary data (images, videos, documents, etc.). The solution has to be scalable and can't get stuck after X amount of data. I would like to have a one place for example MySQL database where all the data is kept. When one of web front ends needs it (on request) It can acquire it from the the DB and cache it permanently for later. From this what I can see on http://dev.mysql.com/doc/refman/5.0/en/table-size-limit.html MySQL table can't store more then 4TB per table. Is there something more appropriate like perhaps nosql databases or perhaps it's better to store everything in files on one server and propagate it to all web frontends?
563e340661a8013065268257	X	You typically don't want to store large files in a relational database -- it's not what they're designed for. I would also advise against using a NoSQL solution, since they're also typically not designed for this, although there are a few exceptions (see below). Your last idea, storing the files on the filesystem (do note that this is what filesystems are designed for ;) is most likely the right approach. This can be somewhat difficult depending on what your scalability requirements are, but you will likely want to go with one of the following: SAN. SANs provide redundant, highly-available storage solutions within a network. Multiple servers can be attached to storage provided by a SAN and share files between each other. Note that this solution is typically enterprise-oriented and fairly expensive to implement reliably (you'll need physical hardware for it as well as RAID controllers and a lot of disks, at minimum). CDN. A content delivery network is a remote, globally distributed system for serving files to end users over the Internet. You typically put a file in a location on your server that is then replicated to the CDN for actual distribution. The way a CDN works is that if it doesn't have the file a user is requesting, it'll automatically try to fetch it from your server; once it has a copy of the file once, it caches the file for some period of time. It can be really helpful if you're normally constrained by bandwidth costs or processing overhead from serving up a huge number of files concurrently. Cloud offering (Amazon S3, Rackspace Cloud Files). These are similar to a CDN, but work well with your existing cloud infrastructure, if that's something you're using. You issue a request to the cloud API to store your file, and it subsequently becomes available over the Internet, just like with a CDN. The major difference is that you have to issue any storage requests (create, delete, or update) manually. If the number of files you're serving is small, you can also go with an in-house solution. Store files on two or three servers (perhaps have a larger set of servers and use a hash calculation for sharding if space becomes an issue). Build a small API for your frontend servers to request files from your storage servers, falling back to alternate servers if one is unavailable. One solution that I almost forgot (although I haven't ever used beyond research purposes) is Riak's Luwak project. Luwak is an extension of Riak, which is an efficient distributed key/value store, that provides large file support by breaking the large files into consistently-sized segments and then storing those segments in a tree structure for quick access. It might be something to look into, because it gives you the redundancy, sharding, and API that I mentioned in the last paragraph for free.
563e340761a8013065268258	X	I work as a (volunteer) developer on a fairly large website - we have some 2GB of images in 14000 images [that's clearly nowhere near a "world record"], and a database of 150MB of database. Image files are stored as separate files instead of as database objects, partly because we resize images for different usages - thumbnails, medium and large images are created programattically from the stored image (which may be larger than the "large" size we use for the site). Whilst it's possible to store "blobs" (Binary Large Objects) in SQL databases, I don't believe it's the best solution. Storing a reference in the database, so that you can make a path/filename combination for the actual stored file [and possibly hiding the actual image behind some sort of script - php, jsp, ruby or whatever you prefer] would be a better solution.
563e340761a8013065268259	X	Have a look at gweb.io. It allows you to specify a custom domain. It requires API access to your drive though.
563e340761a801306526825a	X	I can't get this to work. With just http I get the Google error "The requested URL /host/0B716ywBKT84AcHZfMWgtNk5aeXM/ was not found on this server", although the link works in the browser since it follows the redirect to https, which the proxy doesn't seem to do. When adding https:// I get the error "proxy: HTTPS: failed to enable ssl support". After having enabled SSL as per serverfault.com/a/84828/49363 I get "Bad Gateway - The proxy server received an invalid response from an upstream server" and "proxy: pass request body failed to 173.194.69.132:443 (googledrive.com)".
563e340761a801306526825b	X	I also tried removing "SSLProxyCheckPeerCN on", but then I get the Google error: "503. That's an error. The service you requested is not available at this time. Service error -27."
563e340761a801306526825c	X	But the downside of this would be a second roundtrip to resolve the URL, right?
563e340761a801306526825d	X	It's just a matter of one extra request, so the difference is minimal.
563e340761a801306526825e	X	Not if you are using it for CDN, it makes a big difference, especially on mobile. I would advise against it.
563e340761a801306526825f	X	Google announced it's now possible to publish a website through Google Drive. Is it possible to connect a custom domain to a public folder like you can do on Amazon S3 with a CNAME in the DNS? So to have something more like http://example.com instead of https://googledrive.com/host/0B716ywBKT84AcHZfMWgtNk5aeXM
563e340761a8013065268260	X	As of November 30th 2012 it seems to be that there is no way to publish the contents of Google Drive Publish directly under a custom domain. The SDK docs do not mention such a function [1] and the popular web blog webmonkey.com [2] reports it as apparently missing as well. [1] also shows how to retrieve the some what randomly generated URL for your published site from the API. It looks like follows: "webViewLink": "https://googledrive.com/host/A1B2C3D4E5F6G7H8J9" Amazon AWS S3 in contrast is enabling the user to use a subdomain to access the s3 bucket [3] which can be pointed at from a DNS CNAME record. To enable virtual hosting (delivering the contents under a differen domain) the user needs to name the bucket the same as the domain under which the contents should be available. i.e. johnsmith.net for http:// johnsmith.net. So johnsmith.net needs to have a CNAME record which points to johnsmith.net.s3.amazonaws.com. [1] https://developers.google.com/drive/publish-site [2] http://www.webmonkey.com/2012/11/google-drives-new-site-publishing-takes-on-amazon-dropbox/ [3] http://docs.amazonwebservices.com/AmazonS3/latest/dev/VirtualHosting.html
563e340761a8013065268261	X	Not exactly what you want, but a couple fairly trivial options to publish a site directly to Google Drive: Using apache httpd (or a similar server, maybe nginx) to reverse proxy, it would be fairly trivial to forward the root of a domain to a webViewLink. You can do this with something along the lines of a VirtualHost with basically just: If you then point an A record for the given domain to that apache instance, it should work okay. Given the low resource need, this could be a pretty weak box. You still need httpd, but you can publish straight to Google Drive. I believe that Akamai also provides a service to change areas of a CDN'ed domain to go to an alternative resource, I believe with a prefix. So going directly to the Google Drive, no, but you can do it fairly easily without the need to configure a full server.
563e340761a8013065268262	X	You cannot use your own domain, but you can create a free alias using G Drives, which masks a long URL containing folder ID.
563e340861a8013065268263	X	if what you want are common dependencies for every project, i think what you want is Maven
563e340861a8013065268264	X	What if they are custom created classes? Is it used for that?
563e340861a8013065268265	X	+1 for excellent discussion, but a warning: using pre-built object files is very hard when all the pieces are under development at the same time. It works well when those .jar/.so/.dll files are fairly stable and static, and somewhat when they have dedicated teams maintaining them. But if you're developing all the parts together, and your team does not have a strong commitment to reuse, my experience is that The Stinky Bad Way is what still works best for the code that changes a lot. Ease into better reuse with the pieces that very seldom change, and then expand reuse as you learn and mature.
563e340861a8013065268266	X	@RobNapier See my answer above...
563e340861a8013065268267	X	"You might have different versions of your common code so you probably want to tag it and use external to reference a tagged version." - Shit. RTFM SVN Book - Peg and Operative Revisions
563e340861a8013065268268	X	For my day job, we have a CVS repository where we are supposed to store our code. The problem is, it's such a mess that no one wants to use it because over the years people have put things in there in so many different ways. I am trying to convince my boss to let us start from scratch and do it right. Here's my question: How do you store projects so that they are easy for a new person to come along and pull down the code, yet have a place for common code? For example, should we be storing our code like this: Now I want to add a new project. Should I create my new project and take what's in the common directory and copy and paste it into my new project and then upload that whole project as /project3 so that way if a new guys comes along, he can just check out project3 and have everything, or should I create my new project and have most of it linked to /project3 in CVS and then my common stuff linked to /common in CVS? The problem now being that if a new guy comes along, he has to spend days trying to figure out where all the code for project 3 is located in the repo
563e340861a8013065268269	X	There are several ways to handle common stuff. One is to copy it over your repository. This is technically known as The Stinky Bad Way. The reason is quite simple: If you change the common module for one place, but don't do it in the other places. After a while, you don't have a common any more. In Subversion, you can use svn:externals to automatically import common code across directories. This is technically called Depending upon a proprietary mechanism to manage code that doesn't work all that well. I've tried using svn:externals for years, and never got it working the way I want. The problem is that when I tag my code or create a branch, my svn:external links don't automatically move over. For example, imagine I depend upon a common project stored in http://repos/svn/common. Because there are changes in common that are required in my project, we decide to create a 2.1 branch in common at http://repos/svn/common/branches/2.1, and my svn:externals will point there. After I finish my changes, I first have to create a http://repos/svn/common/tags/2.0 tag in commons, then I have to change my svn:external to point to this new URL, and then finally create my tag in my project. And, if I depend upon dozens of common projects, I'll have dozens of these externals to track. The best way is to treat your common dependencies as pre-compiled third party libraries. If you use Java, they'll become .jar files. If you use C++, they'll become *.so or *.dll. You then store these compiled objects in a release repository, and during the build process, you can fetch the right version of these dependencies in each project. The good news is that there's already an open source, reliable technology that does this, so you don't have to invent anything. The bad news is that it's Maven. However, even if you aren't a Java shop, or you use Ant instead of Maven, you can still use the same mechanism that Maven uses to pull in your common pre-compiled dependencies. You need to use a Maven release repository software package like Nexus or Artifactory. If you aren't a Java shop, you don't connect these repositories to the outside world. Simply use them to store your releases. During the build process, you download the dependencies using either standard wget or curl or Ivy, if you're using Ant, or if your using Maven, Maven handles this automatically. To upload the artifacts during the build, you can use the Maven deploy:deploy-file plugin. This last way is the trickiest to setup, but is well worth the effort. You now know your dependencies, and the version of that dependency. You also have everything only stored once in your source repository since you're not copying source all around. And, compiled code shouldn't be stored in your repository anyway. +1 for excellent discussion, but a warning: using pre-built object files is very hard when all the pieces are under development at the same time. It works well when those .jar/.so/.dll files are fairly stable and static, and somewhat when they have dedicated teams maintaining them. But if you're developing all the parts together, and your team does not have a strong commitment to reuse, my experience is that The Stinky Bad Way is what still works best for the code that changes a lot. Ease into better reuse with the pieces that very seldom change, and then expand reuse as you learn and mature. – Rob Napier The Sticky Bad Way (SBW) is the easiest way to do components. This is especially true if you're creating your component code while you're creating your programs that use the code. The problem is that writing the initial program is only 10% of programming. The other 90% is maintaining that program and making sure it remains relevant. That's the hardest part of programming. Imagine if I decide to use Amazon's S3 service for storage, and I write what you could call an API or maybe a driver to work between my program and S3. Let's say you call it The Foundation Package, and all of your programs will use it. The simplest thing to do is the SBW -- just copy the Foundation code to each module. If there's a problem or a module needs a new feature that's not in Foundation, I can just modify the Foundation until it does what I want. This works out great for a few years. Then, Amazon announces a new API and that the old API will be deprecated. Not only that, the new API has features your customers want. Now, you have a problem. You have this problem because this Foundation has no real owner. The separate development teams never really took the time to learn the code. If there was a change, they employed the HAIUIW 1 method of development. Now, you have a half dozen separate and incompatible Foundation modules that no one really understand. The SBW isn't an issue if your developers understand from the very start that the code they're using isn't a common module, but is part of their code. They'll learn how it works and use it as such. But, you're not going to get the benefits of having a common module. In programming, 10% involves writing that first bit of code. The other 90% is attempting to maintain that code. We've learned long ago that we try to find errors as early as possible when we code. In many programming paradigms, we learn to write tests and documentation first, then code. It's hard and it's not fun. It makes doing that first 10% really, really hard to do. I could write it in a couple of days, but now I'm spending a couple of weeks doing all of this thinking. Yet, we know that doing this makes doing the other 90% of our programming job much, much easier. The same is true with components. It is so easy to copy the code from one place to another and HAIUIT. It is much more difficult to create a separate component with its own team. That team must work with the other development teams. There will be arguments, conflicts, and shouting matches. People will call each other names. Each group has its own goals. Now, imagine doing this while attempting to setup the release repository and creating the whole infrastructure to get everything working. That first 10% is really difficult to do. But, when Amazon makes that announcement for their new API, or you find that you could greatly increase sales if you could get your software working with Microsoft Azure too, making that separate component makes doing that other 90% much, much easier. 1 Hack At It Until It Works.
563e340861a801306526826a	X	First, I strongly discourage anything that includes "let us start from scratch and do it right." In the vast majority of cases, it is better to pick a new direction and migrate towards it over time. This requires some commitment from the team, but all code-reuse requires commitment from the team. Whether you should clone common classes or not depends very heavily on your team again. Sharing common code in a common directory means that anytime you change the common code, you potentially have to fix every project. This often puts a very high barrier on improving the common code, unless you have a team that is highly committed to managing common (typically this means having a separate group who does only this). If you are like many teams, you will find that this becomes a mess in practice if your number of projects are large, and your commitment to code reuse is weak to moderate (this being the common case). This leads you towards cloning the common code for each project, which will quickly fork the common code, making it not "common" at all. So what's a team to do? The first step is total honesty with yourself and your team. Is your team actually committed to code reuse? It can't be one or two guys. It has to be the team, including the manager. No process will change your culture. You cannot just say "we'll reuse code because we have to" and expect it to happen. Nothing happens that way. Instead, I recommend taking easy gains as you can get them. Identify what's common, and within that what's really stable and seldom changes. That's the stuff to put into common. For SVN, use svn:external for this so that a single checkout gets everything (svn:external is a royal pain if the code changes a lot, but we agreed above that this is for code that seldom changes). For CVS, switch to SVN. :D You may be forced in CVS to create a "checkout-dependencies" script in the top of the project directory. I hate those, but if they're consistent, then it's not the worst possible solution. Really stable code can be checked-in as libraries to speed up build times. Stuff that is going to change a lot, and you're not committed to keeping shared, clone it into the projects. Ideally you should be talking among the projects to that you can integrate fixes, pick up new versions from each other, etc. If you don't talk to each other, then the common code will fork. (But if you can't even talk to each other informally, then sticking it all in one directory won't "make" you talk. It'll just cause it to explode.) If, after doing this for a while you discover that hey, you guys actually talk to each other a lot and keep sharing code and the integration has become a hassle, then you'll have justification to task specific people with maintaining the common code full time. But no matter which way you go, your gut feeling that developers should be able to just "checkout, build, run" without having to read three wiki pages and ask six people is absolutely correct. That's critical IMO.
563e340961a801306526826b	X	Use svn:externals to check out the common code along with the project that requires it. You might have different versions of your common code so you probably want to tag it and use external to reference a tagged version. Set the property: svn:externals common http://your.svn.provider/repo/common on each of your projects. That will cause common to get checked out with the project.
563e340961a801306526826c	X	Doing so means that you get to decide when to pull down a different snapshot of external information, and exactly which snapshot to pull. Besides avoiding the surprise of getting changes to third-party repositories that you might not have any control over, using explicit revision numbers also means that as you backdate your working copy to a previous revision, your externals definitions will also revert to the way they looked in that previous revision, which in turn means that the external working copies will be updated to match the way they looked back when your repository was at that previous revision. For software projects, this could be the difference between a successful and a failed build of an older snapshot of your complex codebase. It means also what you can eliminate excessive tagging of code in your externals repository and save some time
563e340961a801306526826d	X	What storage service are you using? Most services use hashes for concurrency purposes but the way you retrieve them can vary
563e340961a801306526826e	X	the files in question are youtube video thumbnails
563e340961a801306526826f	X	Duplicate of Best way to tell if two files are the same?. Also, your question is a oneliner that does not show you understand the problem (have you researched any ways of comparing files and why didn't they suffice?) or that you have tried anything.
563e340961a8013065268270	X	@CodeCaster the question is valid if not perfectly phrased. The post you point to does not apply for web-hosted files. This is more of an HTTP question
563e340961a8013065268271	X	@PanagiotisKanavos while the mere question itself is valid, its current format shows no research whatsoever of OP himself, which is required on SO. If OP actually explained his actual problem (why compare YouTube thumbnails) and explained what has been tried, it would be a better question.
563e340961a8013065268272	X	none, The idea is the user is browsing a list of images on the internet (the list is not mine, nor the place where they are held is mine) and I want to limit bandwidth usage, i.e if the user already has this image, don't download it, just load it from the local storage. The images in question are youtube video thumbnails.
563e340961a8013065268273	X	This may be an easier case. You can use HTTP GET with the If-XXX headers to get a file only if it has changed
563e340961a8013065268274	X	The advice is good, but I get a null ETag in my response :/
563e340961a8013065268275	X	This may be a problem with the server or your code. Please post the code. What about Last-Modified? What URL are you hitting? Is it publicly available?
563e340961a8013065268276	X	pastebin.com/3CRdRnSV - this is the code. The url is publicly available
563e340961a8013065268277	X	Unless the online file already has an associated checksum available, the Op would still need to download the file in order to run the check.
563e340961a8013065268278	X	Most storage services already use hashes (NOT checksums) for this purpose. Typically they are stored as a file's ETag value
563e340a61a8013065268279	X	BTW the lenght and the hash are just headers, usually returned by the same call.
563e340a61a801306526827a	X	-1 link to a file on the internet
563e340a61a801306526827b	X	Please keep in mind that this is a windows store app - no FileStream
563e340a61a801306526827c	X	@SriramSakthivel this is just an idea to compute the HashCode. OP might have the two links obviously, otherwise his first question will be against the extraction of the Windows Store App link
563e340a61a801306526827d	X	@MarioStoilov see updaed
563e340a61a801306526827e	X	And now you downloaded the file ........
563e340a61a801306526827f	X	Here is my scenario - I have a windows store app. I have a local file, and a link to a file on the internet. Is there a way I can check if these two files are the same, WITHOUT downloading the file from the link? The code used to get the file is this:
563e340a61a8013065268280	X	The usual solution is to keep a hash of the cloud file somewhere, usually in the file's metadata and compare it with the hash of your local file. Checksums are unsuitable for this operation because they have a very high chance of collision (ie different files having the same checksum). Most storage services (Azure Blob storage, Amazon S3, CloudFiles) actually use a file's MD5 or SHA hash as its ETag, the value used to detect changes to a file for caching and concurrency purposes. Typically, a HEAD operation on the file will return its headers and ETag value. If you have the option of picking your own algorithm, choose SHA256 or higher as these algorithms are highly optimized and their large block size means that calculating hashes for large files is much faster. SHA256 is actually much faster than the older MD5 algorithm. What storage service are you using? EDIT If you only want to check files to avoid downloading them again, you can use the ETag directly. ETag was created for exactly this purpose. You just have to store it together with your file when you download it the first time. That's how proxies and caches know to send you a cached version of a picture instead of hitting the destination server. In fact, you can probably just do a GET on the file with the ETag/If-None-Match headers. The intermediate proxies and the final web server will return a 304 status code if the destination file hasn't changed. This will halve the number of requests you need to download all images in your list. An alternative is to store the Last-Modified header value for the file and use the If-Modified-Since header in GET EDIT 2 You mention that the ETag header is null, although your code doesn't show how you retrieve it. HttpResponseMessage has multiple Headers properties, both on the message itself and its Content. You need to use the proper property to retrieve the ETag value. You can also check using Fiddler to ensure the server does actually return an ETag. EDIT 3 Finally found a way to get an ETag from Youtube! The answer comes from "How to get thumbnail of YouTube video link using YouTube API?" Doing a HEAD or GET on a YouTube thumbnail from ytimg.com does NOT return the ETag or Last-Modified headers. Using YouTube's Data API and doing a GET on gdata.youtube.com on the other hand, returns a wealth of information about the video. An ETag value is included, although I suspect it changes whenever the video changes. This may be OK though, if you only want to download an image when the video changes, or you don't want to download the image a second time again. The code I used was:
563e340b61a8013065268281	X	If you want to do a comparison without downloading and you are the one who has placed the file over the internet. Then ideally you should place a checksum of the file uploaded. Then before uploading a new one you can just check the checksum of local file and the one on the server. if it is not equal proceed with the upload else cancel it.
563e340b61a8013065268282	X	You could calculate a hash of the file contents like git does. Use MD5 or similar. Then you only need to check if files have the same hash.
563e340b61a8013065268283	X	Directly? No. If the file online is also provided with a Hash, you can get a high probability of successfully checking the equality of the files, though.
563e340b61a8013065268284	X	Now with your update, it's kind of clear what your code does: it downloads an image from a given URL and stores it in your application data folder under the given filename. You want to download any image only once. It's still unclear to me how you call this code, but the solution to me looks like you just need an "URL to filename" translation. So, in psuedo: This does not account for images that have been updated on the server. If you want to do that, you need to save metadata in the DownloadAndSaveImage() call, for example the mentioned ETag or last-modified date. Then to save bandwidth, you can do a HEAD or conditional GET request with an if-none-match or if-modified-since header before the call to ReadImageFile() to check if a newer version is available.
563e340b61a8013065268285	X	Here is the small help. For exactly same file you need to check MD5 or Hashchecks Now you have calculated the hashcode of the files now you can compare it. For those who dont know how to convert link to stream:
563e340b61a8013065268286	X	You mentioned in your Question that you used "a static HTML player" is this possible! I thought the only way to play the the rtmp is to use a Flash based player. please reply with the tutorial to do so if possible. Thanks
563e340c61a8013065268287	X	This is a great start. I can barely wait for the second half!
563e340c61a8013065268288	X	Just to reiterate, now you've finished the post, this is really great. I'm shocked at how pathetic the web UI is and how much heavy lifting they expect users to do to accomplish simple, advertised features. I'll try this tomorrow and get back to you.
563e340c61a8013065268289	X	Well. Hooray! Permissions set and tested, streaming URL generated, working player, Oli happy. I will add that for some players --JW Player in my case-- you have to remove the extension of the file from the filename so video.flv%3... becomes video%3.... No idea why it needs that or why it even works... But there you go. Bounty well earned.
563e340c61a801306526828a	X	Nice, one of the the most comprehensive SO answers I've seen. Thanks
563e340c61a801306526828b	X	S3 supports restrictions based on HTTP Referer however cloudfront currently only supports time and source IP address. For hotlinking, generating time-expiring URLs as described here should work. Anything that links directly to your content will stop working after a few minutes. For crawlers, I recommend a robots.txt
563e340c61a801306526828c	X	I have created a S3 bucket, uploaded a video, created a streaming distribution in CloudFront. Tested it with a static HTML player and it works. I have created a keypair through the account settings. I have the private key file sitting on my desktop at the moment. That's where I am. My aim is to get to a point where my Django/Python site creates secure URLs and people can't access the videos unless they've come from one of my pages. The problem is I'm allergic to the way Amazon have laid things out and I'm just getting more and more confused. I realise this isn't going to be the best question on StackOverflow but I'm certain I can't be the only fool out here that can't make heads or tails out of how to set up a secure CloudFront/S3 situation. I would really appreciate your help and am willing (once two days has passed) give a 500pt bounty to the best answer. I have several questions that, once answered, should fit into one explanation of how to accomplish what I'm after: In the documentation (there's an example in the next point) there's lots of XML lying around telling me I need to POST things to various places. Is there an online console for doing this? Or do I literally have to force this up via cURL (et al)? How do I create a Origin Access Identity for CloudFront and bind it to my distribution? I've read this document but, per the first point, don't know what to do with it. How does my keypair fit into this? Once that's done, how do I limit the S3 bucket to only allow people to download things through that identity? If this is another XML jobby rather than clicking around the web UI, please tell me where and how I'm supposed to get this into my account. In Python, what's the easiest way of generating an expiring URL for a file. I have boto installed but I don't see how to get a file from a streaming distribution. Are there are any applications or scripts that can take the difficulty of setting this garb up? I use Ubuntu (Linux) but I have XP in a VM if it's Windows-only. I've already looked at CloudBerry S3 Explorer Pro - but it makes about as much sense as the online UI.
563e340c61a801306526828d	X	You're right, it takes a lot of API work to get this set up. I hope they get it in the AWS Console soon! UPDATE: I have submitted this code to boto - as of boto v2.1 (released 2011-10-27) this gets much easier. For boto < 2.1, use the instructions here. For boto 2.1 or greater, get the updated instructions on my blog: http://www.secretmike.com/2011/10/aws-cloudfront-secure-streaming.html Once boto v2.1 gets packaged by more distros I'll update the answer here. To accomplish what you want you need to perform the following steps which I will detail below: 1 - Create Bucket and upload object The easiest way to do this is through the AWS Console but for completeness I'll show how using boto. Boto code is shown here: 2 - Create a Cloudfront "Origin Access Identity" For now, this step can only be performed using the API. Boto code is here: 3 - Modify the ACLs on your objects Now that we've got our special S3 user account (the S3CanonicalUserId we created above) we need to give it access to our s3 objects. We can do this easily using the AWS Console by opening the object's (not the bucket's!) Permissions tab, click the "Add more permissions" button, and pasting the very long S3CanonicalUserId we got above into the "Grantee" field of a new. Make sure you give the new permission "Open/Download" rights. You can also do this in code using the following boto script: 4 - Create a cloudfront distribution Note that custom origins and private distributions are not fully supported in boto until version 2.0 which has not been formally released at time of writing. The code below pulls out some code from the boto 2.0 branch and hacks it together to get it going but it's not pretty. The 2.0 branch handles this much more elegantly - definitely use that if possible! 5 - Test that you can download objects from cloudfront but not from s3 You should now be able to verify: The tests will have to be adjusted to work with your stream player, but the basic idea is that only the basic cloudfront url should work. 6 - Create a keypair for CloudFront I think the only way to do this is through Amazon's web site. Go into your AWS "Account" page and click on the "Security Credentials" link. Click on the "Key Pairs" tab then click "Create a New Key Pair". This will generate a new key pair for you and automatically download a private key file (pk-xxxxxxxxx.pem). Keep the key file safe and private. Also note down the "Key Pair ID" from amazon as we will need it in the next step. 7 - Generate some URLs in Python As of boto version 2.0 there does not seem to be any support for generating signed CloudFront URLs. Python does not include RSA encryption routines in the standard library so we will have to use an additional library. I've used M2Crypto in this example. For a non-streaming distribution, you must use the full cloudfront URL as the resource, however for streaming we only use the object name of the video file. See the code below for a full example of generating a URL which only lasts for 5 minutes. This code is based loosely on the PHP example code provided by Amazon in the CloudFront documentation. 8 - Try out the URLs Hopefully you should now have a working URL which looks something like this: Put this into your js and you sould have something which looks like this (from the PHP example in Amazon's CloudFront documentation): Summary As you can see, not very easy! boto v2 will help a lot setting up the distribution. I will find out if it's possible to get some URL generation code in there as well to improve this great library!
563e340c61a801306526828e	X	Thanks Paulpro. In the past I tried taking two snapshots and store them in S3, but each turned out to be a full image, I must have done something wrong. Also, I am not familiar with cron jobs. How do I create them and where do they run? BTW the instance I use is a Windows AMI.
563e340c61a801306526828f	X	Ah, in that case you could create a scheduled task. The task / cron job does not need to be executed on the instance though. You could have any server doing the snapshots, so long as it has the credentials to access your AWS account on it. That's just to avoid doing them manually every day.
563e340c61a8013065268290	X	I am running a web-app on an AWS EC2 instance that uses EBS storage as its local drive. The web app runs on an Apache/Tomcat server, handles uploaded files in local storage and uses a MySQL database, all on this local drive. Does AWS guarantee the integrity and availability of EBS data or should I back it up to S3? If so, how do I do that? I need to have daily incremental backups (i.e. I can only afford to loose recent transactions/files performed today). Note: I am not worried about human caused errors (accidental deletes, etc.) rather system crashes, underlying service failure, etc. Thanks..
563e340d61a8013065268291	X	Amazon does not guarantee the integrity of your EBS volumes, but they are very easy to back up. Simply take a daily snapshot (You could set up a cron using ec2-api-tools to do the daily snapshot). EBS snapshots are stored in S3. They are not in your own bucket and the details are handled by Amazon, but the infrastructure that the snapshots are stored on is S3. The snapshots are incremental, and back up the entire volume. Each snapshot stores the changes on the device since the last snapshot, so taking them often will reduce how long they take to make, but you can only have a limited number of snapshots at once per AWS account. I think it is 250. You need to delete your old snapshots eventually to deal with that. You could also do that with a cron job. Deleting old snapshots does not invalidate the newer ones even though they are stored as incremental, because it will actually update the next newest snapshot to contain the information from the previous one upon deletion.
563e340d61a8013065268292	X	Thank Kgu87 for the book suggestion. Going to get it this weekend. Also I will look into Amazon EMR as suggested. I wonder how does Twitter do it? They use Lucene for search, I get it... but how do they come up with their stats so quickly baffles me.
563e340d61a8013065268293	X	Check out the company called Backtype (backtype.com) recently acquired by Twitter and Nathan Marz's slideshare presentation on how they did it - slideshare.net/nathanmarz/… . In a nutshell - incremental, micro-batch. Should give you a general idea :)
563e340d61a8013065268294	X	Excellent @kgu87. Thanks once again!
563e340d61a8013065268295	X	Beginner questions. I read this article about Hadoop/MapReduce http://www.amazedsaint.com/2012/06/analyzing-some-big-data-using-c-azure.html I get the idea of hadoop and what is map and what is reduce. The thing for me is, if my application sits on top of a hadoop cluster 1) No need for database anymore? 2) How do I get my data into hadoop in the first place from my ASP.NET MVC application? Say it's Stackoverflow (which is coded in MVC). After I post this question, how can this question along with the title, body, tags get into hadoop? 3) In the above article, it collects data about "namespaces" used on Stakoverflow and how many times they were used. If this site stackoverflow wants to display the result data from mapreducer in real time, how do you do that? Sorry for the rookie questions. I'm just trying to get a clear pictures here one piece at a time.
563e340d61a8013065268296	X	1) That would depend on the application. Most likely you still need database for user management, etc. 2) If you are using Amazon EMR, you'd place the inputs into S3 using .NET API (or some other way) and get the results out the same way. You could also monitor your EMR account via API, fairly straight-forward. 3) Hadoop is not really a real-time environment, more of a batch system. You could simulate realtime by continuous processing of incoming data, however it's still not true real-time. I'd recommend taking a look at Amazon EMR .NET docs and pick up a good book on Hadoop (such as Hadoop in Practice to understand the stack and concepts and Hive (such as Programming Hive) Also, you can, of course, mix the environments for what they are best at; for example, use Azure Websites and SQLAzure for your .NET app and Amazon EMR for hadoop/hive. No need to park everything in one place, considering cost models. Hope this helps.
563e340d61a8013065268297	X	Thx. That is useful. One question though based on what you said, if i put 1000 records in a one shard stream. Then after 500 have been consumed, i split the stream into 2. Assuming i use the KCL (not reading with the native API), what is going to be the behavior of the KCL. Will it make sure i will have 3 RecordProcessors for a while until the 500 remaining are consumed, or will it just have 2 record processors for the child shards, and basically the 500 remaining are lost? Thx in advance.
563e340d61a8013065268298	X	Your consumption has no effect whatsoever on the disposition of the records. The shard is selected at input time. If you put 1000 records in one shard, then split, you will have 1000 records in the original shard and no records in the the child shards. It doesn't matter when you consume anything.
563e340d61a8013065268299	X	And to clarify, the library will spawn two more processors for the child shards, but they will do nothing until more records are added. All 1000 records will be processed by the original parent shard processor, because that's where those records exist.
563e340d61a801306526829a	X	Sorry i wasn't clear,true consumption has no effect, that is just an example of the potential data loss. I'll clarify more, when a shard is split, i am seeing clearly that the a shutdown method is called on my record processor. I interpret this shutdown as : "No more records will be sent to you". Question is: is it guaranteed that the KCL will call the shutdown method after it sent all the pending records to my processor, or will the shutdown be called immediately on split in which case i basically lost the remaining 500 records that were never consumed since my processor is being shutdown?
563e340e61a801306526829b	X	You are shut down when the end of the closed shard data is reached (when the shard iterator returns null), not when the shard is closed. You will not lose the records.
563e340e61a801306526829c	X	When splitting a shard into 2 child shards, the parent shard is shutdown. It is expecting that the record processor(KCL is being used here) would checkpoint when this happens as the following KCL source code shows: The questions are: Is this checkpoint indispensable? What happens if the record processor does not checkpoint and absorbs the exception? The reason I am asking is because in my use case I want to make sure that every record from the stream has been processed to s3, now if the shard is shutdown, there might be items which have not been flushed yet and therefore i want to make sure they would be resent to the new consumer/worker of the child-shard? They wouldn't be resent if I checkpoint. Any ideas? Thx in advance.
563e340e61a801306526829d	X	Items don't move between shards. After re-sharding, new records are put into new shards, but old records are never transferred from the parent shard, and no more new records are added to the (now closed) parent shard. Data persists in the parent shard for its normal 24 hour lifespan even after it is closed. Your record processor would only be shutdown after it has reached the end of the data from the parent shard. http://docs.aws.amazon.com/kinesis/latest/dev/kinesis-using-sdk-java-after-resharding.html BTW, as you probably know the SDK API is difficult, and the client library isn't a whole lot better. Try the connector library, which is a much better API and includes an example of an S3 archiving application. https://github.com/awslabs/amazon-kinesis-connectors
563e341061a801306526829e	X	Best practice is to upload it to a service that does that for you (like Vimeo, Youtube), since there are many things you need to take into consideration - there are whole bunch of video and audio codecs each supported by different browsers. Look here for starters: w3schools.com/html/html5_video.asp
563e341061a801306526829f	X	Hi Miha, sorry I cannot upload the video to external web site. So, I want to confirm if client browser does not support the video codec, I have to encode the video using different codec? And HTML5 will not help to adaptive making client browser adapt video at server side, is that correct?
563e341061a80130652682a0	X	I am wondering if I am using HTML5 to develop UI for both mobile (for both Android and iPhone) and PC. If I have a video recorded for PC (high bandwidth, screen size and Windows codec, etc.), and I am not sure if HTML5 could automatically generate adaptive video stream for mobile client (so that on server side, I only need to keep one copy of video)? If not, what is the best practices to support video rendering from both PC and mobile (do I have to encode into different formats using different codec)? Any related documents or code samples are appreciated. thanks in advance, Lin
563e341061a80130652682a1	X	You have many, many options. Typically you convert all possible formats at once. Amazon web services offers a pay-as-you-go video encoding service, but they're not the only game in the market. They are typically the cheapest since other processing farms usually sit on their stack. We usually convert to mp4/h264 and ogg/theora and occasionally webm/vp8. You can use the AWS conversion api to put files in the queue, then pull them down from S3 when ready, or leave them there as your video source URL. You can even make them private with the api, and login to your service with your app or webserver depending on the application. I've also run FFMepg on the server to output various formats, but then you need to manage your own queue and can be a headache.
563e341161a80130652682a2	X	Does Amazon Web Service provide support for building Hybrid Mobile Apps? Is it possible to use their services along with the app?
563e341161a80130652682a3	X	As AWS provides PAAS / SAAS as a backendservice and an app can be build as a client that consumes these services: YES! You want to set up a node.js / express server with some kind of DB access? -> Set it up on AWS (Elastic Beanstalk) , if you like. You need cloud storage? Get some from Amazon.. Ofcourse, this is not specific to AWS, there are (a lot of) other service providers out there in the internet.
563e341161a80130652682a4	X	"Does Amazon Web Service provide support for building Hybrid Mobile Apps?" You can get yourself an EC2 instance, install everything you need to build on there and run Jenkins on it. This isn't going to help with iOS builds that need to run on Mac OS X (for which services like PhoneGap Build or Mac in the cloud leasing companies such as MacStadium are available). If you're building Android / Blackberry then AWS would work for this. "Is it possible to use their services along with the app?" Absolutely. Most can be accessed via standard APIs from any mobile platform, hybrid or fully native application. For example you might use S3 for uploading images from devices to the cloud, EC2, Elastic Beanstalk or Lambda to do server side processing / API work, and RDS as a data store.
563e341161a80130652682a5	X	S3 is a flat file structure, so is it possible to programmatically fracture the namespace as you've mentioned?
563e341161a80130652682a6	X	Sure. It's technically flat but the IAM permissions behave as if its not, and most things that interact with S3 treat the namespace prefixes as folders. Check out this discussion for some working examples of IAM policies.
563e341161a80130652682a7	X	I have a rails app setup with Devise, AWS S3 and Highcharts. Currently the users can log in and upload text files to S3. Rails then requests the data directly from AWS and passes it to Highcharts for processing – spitting out a nice and pretty graph. However, users can currently see every piece of data that's been uploaded. I’m not sure how to setup a relationship between users and their respective uploaded objects. What is the best way to ‘scope’ data within S3 so users can only see the data that they have uploaded? I am assuming this is done through an AWS ACL?
563e341161a80130652682a8	X	The S3 bucket ACLs are designed to control bucket access to AWS accounts and anonymous requests as a whole, as documented here. As such, I don't think the ACLs will work for your use case. A better solution could be IAM policies. The idea, here, would be to create a new IAM user for every account registered in your app. This can be done both easily and programmatically. Then, fracture your bucket's namespace along some line, perhaps account_id: On account creation, construct an IAM policy that grants RW access to just that account's folder. I haven't actually tried this for your use case, but I'm fairly confident it'd work. IAM also comes at no cost and its API is trivially easy to use once you figure out how amazon resources are named. More details are in the API docs.
563e341161a80130652682a9	X	The reason you would host it on a server like Heroku or some other server that has Node.js support is in case you have dynamic data in your website. DocPad can be completely static, on the other hand it can be completely dynamic. Or it can be both of them. But if anything on your website is dynamic or you need your website to regenerate from time to time then you need a server with Node.js support. Does that answer your question? If yes say so so I can put it in an answer. :)
563e341161a80130652682aa	X	Thanks for the answer. I forgot to mention about file upload. How does Docpad handle it in Heroku's ephemeral file system? Ephemeral file system doesn't write anything permanently. Sorry I'm still a bit confused with the "completely file based" from Docpad website.
563e341261a80130652682ab	X	You might want to understand what a static website is first. That's what DocPad is, a static website generator. It generates a static website which you can deploy anywhere. Basically the content doesn't change, it is always served the same way, this makes it a way faster way to serve websites, since there's no server side logic going on. The idea of "completely file based" means that there's no database going on in the server.
563e341261a80130652682ac	X	Thx again. I think Docpad is suffering "identity crisis". Is it a web application framework or simply a website generator for non-programmgers? If this is about serving static contents (in bloody truth nothing is static:(), what's the draw of streamlining "designers and developers"? My impression of developer is someone that writes back end logic code?? In my first experiment, Docpad is heavy. I can see it will soon hit by some performance issue.
563e341261a80130652682ad	X	I am pretty confused with the architecture behind how data is persisted in Docpad. From blogs and forums, I got to know in-memory (and/or out directory) is used for generated contents. But one of the selling points of Docpad is "completely file based". From the sound of it, hosting it on Heroku or any ephemeral file system doesn't seem logical. Can anyone give some explanation/clarification?
563e341261a80130652682ae	X	DocPad is pitched as a next generation web architecture. This mindmap showcases why we call it that perfectly:  The workflow being like so: In that sense, DocPad is a next generation web architecture that has static site generation abilities, as well as dynamic site generation abilities. What separates DocPad from traditional web architectures, is that traditional web architectures consider the content and templating separate beings, where DocPad considers them the same and just separated by their extension. Traditional web architectures also are dynamic by defaults, with static site generation accomplished via caching, rather than the other way round of being static by default. Because of this load everything in the in-memory database situation, we are suffering some from growing pains with performance during generation and post-generation. Discussion here. However there is nothing there that can't be fixed with enough time and resources. Regardless of this, DocPad will still be faster than your traditional web architecture due to the static nature (faster requests) as well as the asynchronous nature (faster generations). In terms of how you would handle file uploads: If you are doing a static website with DocPad, you would have a backend API server somewhere else that you would do the upload too and load the data into DocPad as a single page application style. If you are doing a dynamic website with DocPad, you would host DocPad on a server like Heroku, and extend the server to handle the file upload to a destination like Amazon S3, Dropbox, or into MongoDB or the like. You can then choose to expose the file via templateData as a link, or inject the file into the DocPad in-memory database as a file. Which one you chose is whether or not you just want to reference the upload or treat it as a first class citizen in the DocPad universe (it gets it's own URL and page). For dynamic sites, I would say I really go with the static site + single page application approach. You get benefits like responsive design, offline support, really fast UX which without doing it that way, you struggle a bit accomplishing it with the dynamic site approach regardless of which web architecture you build it on.
563e341261a80130652682af	X	Well, I can't top off Benjamin's excellent explaination, but if you want a TLDR explaination: docpad is used to (biggest-use-case) generate STATIC websites, a-la github pages or old websites of 1990s. You can write your pages in whatever you like (Jade, eco, coffeescript, etc) and it will compile the pages and output HTML files. Think of it as a "Compile-once-server-forever" thing. On the other hand, if you want Dynamic content on your site, you'd like to use Nodejs for pulling in the dynamic data from other sites, or generating it on the fly. As for your concern about Heroku's ephemeral file system, (I don't know exactly how what works) you can use Amazon's S3 for storage. Check out this
563e341261a80130652682b0	X	I need to making multiple(more than 100) HTTP requests to Google Scholar, from a Java code to collect data. However, the site prevents this after around 20 requests or so, and produces a captcha. I have heard of 'Amazon Spot Instances' letting the IP address of requesting system change periodically and thus avoid the occurence of captcha, by ensuring that the requests do not come from a single IP. Can anyone help me through this, with further details?(an alternate method other than Amazon EC2 spot instances is also fine)
563e341361a80130652682b1	X	Changing IP addresses periodically aren't a unique feature of Spot Instances within the Amazon Environment (it's also available on the On demand and Reserved Instances), the Amazon CLI will allow you to assign, attach, deattach and release IP addresses as well. Amazon's SDK will allow you to call the creation of a Spot Instance and attach an IP address, for the latter http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-RunInstances.html is a good starting point. It's well supported across a wide range of languages. For Java I would look at http://aws.amazon.com/sdkforjava/ and get your feet wet, it's a powerful API! Depending on how much experience you have with the AWS environment there is a bit of extra stuff to keep in mind, especially with Spot Instances. Spots can terminate at any time (literally mid-query) so you should build your app to be stateless, a good solution is to send the results into an S3 bucket. It has the added benefit of being able to deploy multiple instances at one time and have a single endpoint of data collection.
563e341361a80130652682b2	X	Thanks, I will check into that. I would eventually like to create a very small 4 player online game. I have been reading and got the impression that sockets were the way to go for something like that. I have used JSON before between Java and PHP with some Android applications and that worked fine. I don't see that it would be very practical if I need to continuously update a players movement or actions.
563e341361a80130652682b3	X	Ok. Games are one particular exception to my suggestions, but only if this game is going to be realtime or continuous. If it's turn based then my suggestions still hold. But, now that you are saying you're doing a game Amazon EC2 probably serve you much better.
563e341361a80130652682b4	X	I appreciate your help. I'm glad to be heading in the right direction now. I just signed up for EC2 and will try it out.
563e341361a80130652682b5	X	Thanks. I was actually just trying out that PHP socket server tutorial.
563e341361a80130652682b6	X	I have been searching for hours now and just need some guidance about my situation. I want to create a simple client/server program. I was originally planning on doing them both in java, but I bought a shared hosting account from godaddy a couple of weeks ago and they have disabled java for new accounts. So, I guess my next best choice, and in a language that I'm somewhat familiar with, is PHP. I have been following the tutorial on sockets from the java site, and have the java client made. I've been trying to convert the server part of the example to PHP. Apparently, godaddy will let you use fsockopen() for sockets on a shared server. I guess what I need to know is, is this possible, and how do I run the php file once it is made? The example says I need to start the server program before I run the client. I'm not sure of how to do that though.
563e341361a80130652682b7	X	I would NOT use raw sockets to do this. Instead use JSON over HTTP because PHP supports processing HTTP without any special consideration. It's simple to run your PHP pages on the hosted apache instance on GoDaddy or Amazon EC2. Sure you can use sockets, but very very few people actually do that. Massively more people process and respond to HTTP with PHP. That means you'll find vastly more people who can help answer your questions if you follow the herd here. Also there are API libraries on both sides for doing this easily. Using sockets comes with plenty of things you'll have to do yourself or suffer through all the strange bugs that comes with working with raw sockets for the first time. Also JSON processing is easily supported by both Java and PHP to it's very easy to send the data to the client and server using that. Well you can certainly use PHP on the backend and Java on the frontend if you like, but I'd suggest canceling your Go-Daddy account and get an Amazon EC2/S3 account because you get a full machine dedicated to whatever you want to put on it. So if you want to do Java on the backend you can just by installing the JDK, Tomcat, etc yourself on the amazon instance and you're good to go. You can also host PHP on there too. There is even plenty of AMI instances pre-installed for Java or PHP stacks.
563e341461a80130652682b8	X	While I can't be 100% sure on this, I am going to go ahead and say that it probably won't work. Sockets can be on the more expensive side for a hosting company, so chances are they will be among the first things to be cut. When you need something acting as a socket server, you generally need a long-running process which has access to certain ports on the machine running the connection. Because most hosting companies create a chrooted environment for each of their clients, the clients are denied the security access which they would need to be able to run genuine socket servers. My experiences with GoDaddy as a hosting company, is that they seem to operate along that line of thought. From what I can tell, GoDaddy will allow PHP to read sockets and act as a socket client. That is consistent with their allowance of fsockopen. I sincerely doubt that is what you need. Unless you have an actual need to have the server push information to the client (instead of having the client request data from the server), then you really are much better off using the standard HTTP request and having either XML or JSON go over the wire (XML and JSON are both supported rather masterfully in PHP). If you really have to have a socketserver, then I would go to webmasters.stackexchange.com and see if you can find recommendations similar to this one. If you do decide that you would like to create a PHP Socket Server after all that, there is a tutorial from one of the people at Zend. They're pretty sharp so hopefully it will be enough to help you convert the script (if that is the way you decide to go)
563e341461a80130652682b9	X	Have you tried giving admin rights to IIS? This is a frequent source of errors.
563e341461a80130652682ba	X	I'm fairly certain that isn't the issue here, since the parent application is running on our production servers currently with no problems. (Unless it's a specific permissions issue with one of the related files here, perhaps.)
563e341461a80130652682bb	X	Have you used native IE9, or just the quirks-mode switch in IE11? There are discrepancies, and quirks mode is known to cause problems.
563e341461a80130652682bc	X	I believe you need to ensure the content-type is text/html for the returned iframe too.
563e341461a80130652682bd	X	You'll need to show your client side code to receive any useful assistance. You'll Aldo need to tell us if the file is actually making it to S3 or not (successfully ).
563e341461a80130652682be	X	Note that the iframeSupport option is not needed and thus ignored in IE10+. In fact, this is only used in IE7-9.
563e341561a80130652682bf	X	@RayNicholus Makes sense, I figured as much. Thanks for the clarification.
563e341561a80130652682c0	X	Yep, turns out some Javascript earlier in the page was indeed setting the domain to something else (the base domain, without a subdomain part). I posted my solution below. I'm not sure if it's "best", but it works.
563e341561a80130652682c1	X	Short/Generic Version: I'm working on an application that (unfortunately, for other reasons), sets document.domain at the top of every page to a substring of the "true" domain: for subdomains like sub.app.local, document.domain = "app.local". I'm also creating an iframe dynamically and adding it to the page. The iframe loads a file residing on the same server as the parent page. Later on, some Javascript needs to access the contentDocument property of the iframe. This is fine in modern browsers, but causes problems in IE9 due to this bug. (See the top answer for a good explanation of why this is.) AFAIK, every browser newer than IE9 automatically inherits document.domain for programmatically-created iframes, so this is IE9-specific. Because of some of the unique requirements of my scenario (tldr: the iframe src needs to change), the answer in the above post didn't work for me. Longer/Application-Specific Version: I'm using FineUploader in an application running on IIS, uploading files to S3. Everything's working just fine in modern browsers, but IE9 support is giving me trouble, even after following the docs (Supporting IE9 and older) to configure the iframeSupport.localBlankPagePath option. I'm a little stumped! The error that I get in the console is: UPDATE I've determined that the reason it's not working out-of-the-box is because the application sets document.domain on page load, and it's different than (a subset of) location.host. FineUploader's 303 redirect mechanism to the blank page is working fine, but the document.domain of the iframe differs from the parent (due to IE9 not inheriting the property) and so, access denied. The actual S3 upload is working. It's just the final step of verifying the upload that fails. Here's my client-side code: FineUploader.js
563e341561a80130652682c2	X	(If anyone stumbles across this answer outside of the context of FineUploader, this idea is what I based my solution on.) To implement this, I made FineUploader's blank.html slightly non-blank: This gives me a way to feed the correct document.domain value from the parent page when the iframe is generated. Slight modification to the FineUploader configuration object: This doesn't seem to interfere with the arguments that are prepended by AWS. We're still using FineUploader 4.0.3 in this application, but this should work with latest as well. tl,dr; It works! Tested in IE11 Document mode and also native IE9.
563e341561a80130652682c3	X	The error suggests that the page served by the iframe is indeed not the same domain as the page hosting the uploader. Either that, or you have some plug-in/extension causing trouble. According to the error, Fine Uploader is simply not able to access any content in the iframe, which happens when the domain of the iframe doesn't match the domain of the frame/page hosting the uploader.
563e341561a80130652682c4	X	I have a zip archive uploaded in S3 in a certain location (say /foo/bar.zip) I would like to extract the values within bar.zip and place it under /foo without downloading or re-uploading the extracted files. How can I do this, so that S3 is treated pretty much like a file system
563e341561a80130652682c5	X	S3 isn't really designed to allow this; normally you would have to download the file, process it and upload the extracted files. However, there may be a few options: You could mount the S3 bucket as a local filesystem using s3fs and FUSE (see article and github site). This still requires the files to be downloaded and uploaded, but it hides these operations away behind a filesystem interface. If your main concern is to avoid downloading data out of AWS to your local machine, then of course you could download the data onto a remote EC2 instance and do the work there, with or without s3fs. This keeps the data within Amazon data centers. You may be able to perform remote operations on the files, without downloading them onto your local machine, using AWS Lambda. You would need to create, package and upload a small program written in node.js to access, decompress and upload the files. This processing will take place on AWS infrastructure behind the scenes, so you won't need to download any files to your own machine. See the FAQs. Finally, you need to find a way to trigger this code - typically, in Lambda, this would be triggered automatically by upload of the zip file to S3. If the file is already there, you may need to trigger it manually, via the invoke-async command provided by the AWS API. See the AWS Lambda walkthroughs and API docs. However, this is quite an elaborate way of avoiding downloads, and probably only worth it if you need to process large numbers of zip files! Note also that Lambda functions are limited to 60 seconds maximum duration (default timeout is 3 seconds), so may run out of time if your files are extremely large.
563e341661a80130652682c6	X	paging @jdlong to the scene
563e341661a80130652682c7	X	well the "good" news is that I can reproduce your error. I'm debugging now to see if I can figure out what's going on.
563e341661a80130652682c8	X	@JDLong, thank you for your response! That's strange that someone else didn't find this issue previously.
563e341861a80130652682c9	X	i think something changed recently. I've traced it back to the latest R version not loading properly. Only I'm not sure why.
563e341861a80130652682ca	X	I just pushed version 0.04 out. Give it a go and let me know if that fixes it. I found a few things that might be the problem.
563e341861a80130652682cb	X	thank you for your help and sorry for my delayed response, I had a bad connection to internet and thought that it was a reason of problems with new version of segue, but unfortunately stable connection disn't solve the problem. See please my updated question.
563e341861a80130652682cc	X	well that's really odd. The behavior where it starts up and then immediately fails is indicative of an error in the boot up script. Can you tell me which size instance you were trying to start? That will help me with debugging.
563e341861a80130652682cd	X	I have used default types of master and slave instances m1.small
563e341861a80130652682ce	X	I know this is an old question, but, I'm having the same issue, using version 0.05 with all defaults. Is this just an intermittent problem?
563e341861a80130652682cf	X	I tried to reproduce simple example of using segue from https://jeffreybreen.wordpress.com/2011/01/10/segue-r-to-amazon-elastic-mapreduce-hadoop/ Cluster creation was successful Local simulation was OK, but running it on the cluster returned an error each time. I like the idea of this package and I hope it will be useful in my work, but I cannot figure out how to solve this basic problem. Version of segue 0.02 OS: Ubuntu 11.10 UPDATE: I tried to run another example test case of Pi estimation, and emrlapply returned the same error message. UPDATE2: I updated to version 0.03 and now I could not connect to cluster. After successful start instances were tried to shut down with no effect. I terminated instances via AWS consol. So the old problem was solved but the new one appeared.
563e341861a80130652682d0	X	It appears that Amazon changed the EMR service to default to the 1.0 version of the EMR AMI if no specific version was called. Since Jan 1, the behavior had been to default to the latest version. When I made the changes to default to a recent version I then had issues with the current incarnation of Hadoop wanting output to be put in a sub-bucket on S3. I had to upgrade the Java AWS API code to the latest version in order to make these changes. New version of the tar ball is here: http://code.google.com/p/segue/downloads/list or you can clone the source and build it yourself, if you're into that sort of thing. I've indexed Segue to 0.03 with this change. EDIT: I just found that m1.small is a problem (32 bit) so I've changed the default and changed behavior to not let users specify m1.small. New version is 0.04.
563e341961a80130652682d1	X	I'll add to 1. that, once you have configured your instance, you can create an "AMI" with it, which is, a copy of the system, and you'll be able to start similar machines from the same "AMI".
563e341961a80130652682d2	X	Thanks a lot!! That answers all my questions. I do still need to understand the various DB offerings. I am new to using the cloud but understand exactly what I get from it and what to expect. Like you guessed this is the first time using it so trying to understand before jumping in.
563e341961a80130652682d3	X	Thanks! Why would I need EBS and S3? with S3 will my code not be tailored specifically to the Amazon cloud? That is what confused me initially. With Amazon RDS will get automatic backups? I mean I am guaranteed that data written will not be lost right?
563e341961a80130652682d4	X	It depends on your application, the difference is with what they can be used with. EBS is specifically meant for EC2 instances and is not accessible unless mounted to one. On the other hand, S3 is not limited to EC2. The files within an S3 bucket can be retrieved using HTTP protocols/BitTorrent. You will need API's (or API generated URL) to read or write information with S3. With EBS, a volume can be mounted on an EC2 and it would appear like a hard disk partition. It can be formatted with any file system and files can be written/read by the EC2 instance just like it would to a hard drive.
563e341961a80130652682d5	X	I understand now. Also I got this URL about databases which explains clearly the different type of DB. I somehow did not find it till now.
563e341961a80130652682d6	X	I am working on a project and am at a point where the POC is done and now want to move towards a real product. I am trying to understand the Amazon cloud offerings just to see if I need to be aware of them at development time. I have a bunch of questions that I cannot get answered from the Amazon site. Its probably because I am new to the whole web services thing and have never hosted a site before. I am hoping someone out here will explain this to me like I am a C programmer :) I see amazon has a bunch of offerings - I understand EC2 is virtual server instances that I can use and these could come pre-loaded with what I want (say Apache + python). I have the following questions - About the Simple DB - I really hope some one points me in the right direction here. Thanks for taking the time to read. P
563e341961a80130652682d7	X	I just went through the question and here I tried to answer few of them, 1) AWS EC2 instances doesnt publish pre-configured instances, in fact its configured by the developers and made it publicly available to the users so that they can use it. One can any one of those instances or you can just opt for what ever OS you want which is raw and provision it accordingly and create a snap shot of it so that you can use it for autos caling.The snap shot becomes the base AMI in your case. 2) Every instance you boot will have a public DNS attach to it, you can use the public DNS to connect to that instance using ssh if your are a linux user or using putty if you are a windows users. Apart from that, you can also attach a elastic IP which comes with a cost will is like peanuts and attach it to the instance and access your instance through the elastic IP and you can either map the public DNS or elastic ip to map to a website by adding a A record or Cname respectively. 3)AWS owns databases in the different parts of the world. For example you deploy your application depending upon your customer base, if you target customers are based out of India, the nearest region available is Singapore which is called as ap-southeast-1 by AWS. Each region will have multiple availability zones, example ap-southeast-1a and ap-southeast-1b, which are two different databases and geographically part. Intre region means from ap-southeast-1a to ap-southeast-1b. Inter Region means, from ap-southeast-1 to us-east-1 which is Northern Virginia Data centre. AWS charges from in coming and out going bandwidth, trust me its nothing. They chargge 1/8th of a cent per GB. Its a thing to even think about it. 4)Elastic Load balancer is cluster which divides the load equally to all your regions across availability zones (if you are running in multi AZ) ELB sits on top the AWS EC2 instances and monitors the instance health periodically and enables auto scaling 5) To help you understand what is autoscaling please go through this document http://aws.amazon.com/autoscaling/ 6)Elastic Block store or EBS are like hard disk which is a persistent data storage which can be attached to your instance.Regarding back up yes dependents upon your use case. I do backups of EBS periodically. 7)Simple Db now renamed as dynamo DB is nosql DB, I hope you understand what is nosql db, its a non RDMS db systems. Please read some documentation to understand what is nosql db is. 8)If you have mysql or oracle db you can opt for RDS, please read the documents. 9)I personally feel you are newbie to the entire cloud eco system, you need to understand what exactly cloud does first. 10)You dont have to make large number of changes to development as such, just make sure it works fine in your local box, it can be deployed to cloud with out much ado. 11) You dont have to use any extra tool for that, change the database end point to RDS(if your use it) or else install mysql in your ec2 instance and connect to the local db which resides in the ec2 instance and connect to it,which is as simple as your development mode. 12)You dont have to worry about any security issues aws, it is secured. Dont follow the myths, I am have been using aws since 3 years running I dont even know remember how many applications, like(e-commerce,m-commerce,social media apps) I never faced any kind of security issues and also aws allows to set your security how ever you want. Go ahead, happy coding. Contact me if you have any problem.
563e341961a80130652682d8	X	The answer above is a good summary on AWS. Just wanted to add AWS offers full data center, so it depends what you are trying to achieve. For starters you will need, One great tool to calculate the pricing, http://calculator.s3.amazonaws.com/calc5.html
563e341961a80130652682d9	X	Some other services to take in account are: VPC (Virtual Private Cloud). This is your own private network. You can define subnets, route tables and internet gateways there. I would strongly recommend to use VPC for any serious deployment of more than one instance. Glacier - this will replace your tape library to storing backups. Cloud Formation - great tool for deployment and automation of instances.
563e341a61a80130652682da	X	We would like to stream data directly from EC2 web server to RedShift. Do I need to use Kinesis? What is the best practice? I do not plan to do any special analysis before the storage on this data. I would like a cost effective solution (it might be costly to use DynamoDB as a temporary storage before loading).
563e341a61a80130652682db	X	If cost is your primary concern than the exact number of records/second combined with the record sizes can be important. If you are talking very low volume of messages a custom app running on a t2.micro instance to aggregate the data is about as cheap as you can go, but it won't scale. The bigger downside is that you are responsible for monitoring, maintaining, and managing that EC2 instance. The modern approach would be to use a combination of Kinesis + Lambda + S3 + Redshift to have the data stream in requiring no EC2 instances to mange! The approach is described in this blog post: A Zero-Administration Amazon Redshift Database Loader What that blog post doesn't mention is now with API Gateway if you do need to do any type of custom authentication or data transformation you can do that without needing an EC2 instance by using Lambda to broker the data into Kinesis. This would look like: API Gateway -> Lambda -> Kinesis -> Lambda -> S3 -> Redshift
563e341a61a80130652682dc	X	Redshift is best suited for batch loading using the COPY command. A typical pattern is to load data to either DynamoDB, S3, or Kinesis, then aggregate the events before using COPY to Redshift. See also this useful SO Q&A.
563e341a61a80130652682dd	X	I implemented a such system last year inside my company using Kinesis and Kinesis connector. Kinesis connector is just a standalone app released by AWS we are running in a bunch of ElasticBeanStalk servers as Kinesis consumers, then the connector will aggregate messages to S3 every a while or every amount of messages, then it will trigger the COPY command from Redshift to load data into Redshift periodically. Since it's running on EBS, you can tune the auto-scaling conditions to make sure the cluster grows and shrinks with the volume of data from Kinesis stream. BTW, AWS just announced Kinesis Firehose yesterday. I haven't played it but it definitely looks like a managed version of the Kinesis connector.
563e341d61a80130652682de	X	There are no real advantages/disadvantages. It is just a matter of style, imho.
563e341f61a80130652682df	X	There is another way, which is much more powerful IMO: deferred objects.
563e341f61a80130652682e0	X	I prefer deferred objects too. But whatever method you choose use it consistently throughout your app, consistency is often more important than choosing the best approach.
563e341f61a80130652682e1	X	Yeah, deferred objects are brilliant - absolutely use them. But if you're writing an API, and don't want to require the use of deferreds, I'd say go the Node.js route: Single callback, 1st argument is the error or null. Then you can have X number of other arguments after that. And as msanders says, internal consistency is the most important thing, but if you can achieve external consistency too (e.g. using Node.js' patterns if you're writing something for Node) that's even better.
563e341f61a80130652682e2	X	Programmers seem divided on how to get asynchronously notified about an error. Some programmers prefer to use a callback with two arguments: a value and a boolean which tells whether the value isn't erroneous. This has the benefit in that it looks like a try catch statement: Others prefer the negative (i.e. the boolean should tell whether the value is erroneous). Their reasoning is that if you know that the asynchronous function will never throw an error then you can safely omit the second parameter as follows: Then there are people who propose separate callbacks for successful execution of asynchronous functions and errbacks for erroneous execution of asynchronous functions. This allows the programmer to select if he want to handle callbacks, errbacks, both or none: I'm not asking for which method you prefer. I'm simply asking for the advantages and disadvantages of each method so that I know which one to use when.
563e341f61a80130652682e3	X	Design Decision: This is just design decisions, nothing more. If it's standalone parameter, you can have standalone function and create "more beautiful" code (for someone - for someone it's more messy - it's really subjective). Error complexity: In some applications you can have more complex errors (filesystem.fileRead can have FILE_DONT_EXISTS, FILE_LOCKED, NOT_PERMISSIONS..) and in some apps you need just throw error (db.checkConnection or db.openConnection). Order and differences: Very nice sample for great API is from Amazon, you can check it. http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html RESPONSE: On asynch function like copyObject(params = {}, callback) you have callback function, which have always 2 parameters: err (Error) and data (Object) in function(err, data) { ... }. Error is designed like a first parameter because if you have error, you don't have a data. So it's really about priority and about order. As you can see, you have both mixed two ways. In request you pass object and function and in response you get error and object (to that request function).
563e342061a80130652682e4	X	You said similar file for production, you mean with a different name? i.e. config/s3.yml and config/s3_prod.yml?
563e342061a80130652682e5	X	Answer edited. Both files are called the same.
563e342061a80130652682e6	X	Hmmm, I see what you mean. But I don't think I can manually put the s3.yml file (production version) in our production server, since we are using Heroku
563e342061a80130652682e7	X	Heroku has a work around using environment variables. Check out this blog post: icebergist.com/posts/paperclip-heroku-and-amazon-s3-credentials. No need to commit the credentials which would be insecure.
563e342061a80130652682e8	X	Ok, so in this case, I would need to create a separate AWS account. One for production, and one for development. Or am I missing the point?
563e342061a80130652682e9	X	If I wanted to have my development and production app access different buckets with a different key id and secret, is such possible? Or would I need to create another AWS account? I looked around but couldn't find anything useful. Ideally, it would be good if I could have multiple buckets in one AWS account and have different key ids and secrets for those bucket access.
563e342061a80130652682ea	X	As far as getting the credentials into your app, then you can either thane separate YAML files you copy onto the servers or (on heroku) use environment variables, as John said. You don't however need multiple AWS accounts. You can use iam to create extra users inside your account, with their own access key/secret key and then grant them appropriate permissions. You can for example create an account that can only access an S3 bucket but can't do anything else. If you're going to save keys on your instances I highly recommend going this route rather than using the 'master' access key for your account (which allows unfettered access to your AWS account) You can either create/configure these users via the IAM api, or you can use the GUI at the amazon console
563e342061a80130652682eb	X	In your config directory create a file called s3.yml, Inside put: create a similar file (s3.yml) for production. Do not check the file into source control. Instead place the production one on the server in the config directory for development just leave it in config locally. In environment/development.rb and production.rb S3_CONFIG = YAML.load_file Rails.root.join("config/s3.yml") Wherever you are explicitly are calling bucket_name, amazon_key, secret_key instead use
563e342061a80130652682ec	X	So, I am trying to get jquery file upload to work with amazon s3. Actually, the upload IS working. The file uploads perfectly fine. But, the issue is, at the end of the request I receive the following error. This is in firefox. I do not get an error at all in chrome. Has anyone come across this, and if so, have you remedied it? Amazon s3 CORS setup
563e342061a80130652682ed	X	Okay, I'm not sure which language you are writing your backend. However, I fixed this by including a property called 'success_action_status' and setting it as a string "201" in the policy. I also had to include the same parameter in the XHR upload form to S3. You can read up about valid properties here: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html However, it says setting the default should just return a blank response which should be perfectly valid. I think Firefox is expecting an XML response (bug w/ Firefox?), but is receiving an empty document in which it cannot parse. Setting "success_action_status" to "201", returns an XML document ready for parsing. Hope this helps! Justin Note: make sure your "201" is truly a string when it gets encoded into your policy. I had an issue w/ PHP in which it was defaulting the value to integer, which then resulted in yet another Amazon error which returned a 403.
563e342161a80130652682ee	X	How is it done with cloudfront?
563e342161a80130652682ef	X	@Martin - instructions are in my link to the Cloudfront docs. I've added an extract in my answer.
563e342161a80130652682f0	X	Can you have an Amazon S3 bucket accessed by "img.domain1.com" and "img.domain2.com" ? How would that increment the cost?
563e342b61a80130652682f1	X	@vtortola, the answer is yes. It's done Amazon CloudFront. Which is what this answer is describing.
563e342b61a80130652682f2	X	Is there some way to map multiple (thousands) of subdomains to one s3-bucket? If so is it also possible to map it to a specific path in the bucket for each subdomain? I want test1.example.com to map to mybucket/test1 and test2.example.com to map to mybucket/test2. I know the last part isn't possible with normal dns-records but maybe there is some nifty Route 53 feature?
563e342d61a80130652682f3	X	It's not possible with S3 directly. You can only use 1 subdomain with an S3 bucket. However you can map multiple subdomains to a Cloudfront distribution. Multiple CNAME Aliases You can use more than one CNAME alias with a distribution. For example, you could have alias1.example.com and alias2.example.com both associated with your distribution's domain name. You can have up to 10 CNAME aliases per distribution. You can associate a particular CNAME alias with only one distribution. Important When adding an additional CNAME alias to a distribution that already has one, make sure to include the original CNAME alias in the DistributionConfig object. Otherwise, your update erases the original CNAME alias and just adds the new one. This is because the process of updating a distribution's configuration replaces the entire configuration object; it doesn't add new items to it.
563e342d61a80130652682f4	X	Starting from October 2012 Amazon introduced a function to handle redirects (HTTP 301) for S3 buckets. You can read the release notes here and refer to this link for configuration via Console / API. From AWS S3 docs : Redirects all requests If your root domain is example.com and you want to serve requests for both http://example.com and http://www.example.com, you can create two buckets named example.com and www.example.com, maintain website content in only one bucket, say, example.com, and configure the other bucket to redirect all requests to the example.com bucket. Advanced conditional redirects You can conditionally route requests according to specific object key names or prefixes in the request, or according to the response code. For example, suppose that you delete or rename an object in your bucket. You can add a routing rule that redirects the request to another object. Suppose that you want to make a folder unavailable. You can add a routing rule to redirect the request to another page, which explains why the folder is no longer available. You can also add a routing rule to handle an error condition by routing requests that return the error to another domain, where the error will be processed.
563e342e61a80130652682f5	X	Thanks for the link. This answer a part of best practices however I am still looking for how the NativeS3FileSystem works.
563e342e61a80130652682f6	X	Does anybody have insights on the internal working of NativeS3FileSystem with different InputFormat's in Amazon EMR case as compared to normal Hadoop HDFS i.e. input split calculation, actual data flow? What is the best practices & points to consider when using Amazon EMR with S3? Thanks,
563e342e61a80130652682f7	X	What's important is that if you're planning to use S3N instead of HDFS, you should know that it means you will lose the benefits of data locality, which can have a significant impact on your jobs. In general when using S3N you have 2 choices for your jobflows: From my experience I also noticed that for large jobs, splits calculation can become quite heavy, and I've even seen cases where the CPU was at 100% just for calculating input splits. The reason for that is that I think the Hadoop FileSystem layer tries to get the size of each file separately, which in case of files stored in S3N involves sending API calls for every file, so if you have a big job with many input files that's where the time can be spent. For more information, I would advise taking a look at the following article where someone asked a similar questions on the Amazon forums.
563e342e61a80130652682f8	X	My question is in regards to how I should go about doing this. My site allows users to upload videos (1min - 5min in length). At the moment, the user chooses the video file, I then begin uploading it to an iframe. The user then fills out some information about the video. After completing the form, I was hoping to allow the user to do something else while I then upload that video server side. Right now, that system works, but after they complete the form and hit 'submit' the browser begins the upload and no actions can be taken by the user until it's done being uploaded. This is my first website working with files like this so any suggestions are very welcome.
563e342e61a80130652682f9	X	Video processing in the context of web applications is notoriously difficult to get right (at least without spending a lot of precious time best used elsewhere) - given my experience you'll likely encounter more issues and/or advanced needs down the road accordingly, e.g. thumbnail generation, format conversion, etc. Therefore I'm going to take "any suggestions are very welcome" literally here and highly recommend to check out Transloadit - their offering includes your desired functionality, plus lots more (optional of course): There are demos for most features (which highlights many image/video processing use cases on the side), e.g. showing how to Store the originally uploaded files in your S3 bucket or how to combine features Encode a video, extract 8 thumbnails and store everything in your S3 bucket. Please make sure to read Notifications vs Redirect Url, which explains your options regarding the use case at hand, specifically you'll likely want to use Notifications to avoid blocking the user: If you use the jQuery plugin and set its wait parameter to false, you will have to use Notifications, otherwise there is no way to get the file results from Transloadit. While being a commercial offering, they have really nailed the use case and remove most of the burden regarding video processing within web applications; their offering is build upon Amazon Web Services (AWS) and the pricing is very reasonable and affordable accordingly. Good luck!
563e342f61a80130652682fa	X	Uploading is tricky because the standard HttpRequest isn't able to show a percentage of the upload in most browser without some server-side polling via ajax, and if it's a big file, the user can be waiting for ages without even seeing if the upload has been successful. There are few solutions to this. (1) a lot of people use a Flash ojbect to do this, and there are many examples: http://www.plupload.com/example_queuewidget.php http://blueimp.github.com/jQuery-File-Upload/ .. there are loads just google til u find one you like (2) if you dont want to use a little swf for this then you have to post the file using ajax and keep checking the the server (with ajax) to respond with the current filesize, which is way more involved. If you want your user to be able to keep using the site and other pages instead of waiting on the same page, why not use a popup to show the actual upload console? or even a frame(nastier). Cheers A
563e342f61a80130652682fb	X	Where are you passing the aws key and secret, to be used in the signing process?
563e342f61a80130652682fc	X	They are set as environment variables.
563e342f61a80130652682fd	X	According to this link http://docs.aws.amazon.com/aws-sdk-php/v2/guide/service-s3.html, I can easily create a presigned link just adding the life span to getObjectUrl But I get a plain url, you know, without the awsaccesskeyid and expires parameters, Here's my code: EDIT: I have AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as environment variables My echo looks like: https://s3-us-west-1.amazonaws.com/imagenesfc/NASimagenes/codigoBarraBoleto/1001000098.png What's wrong?
563e342f61a80130652682fe	X	Well, if anyone else has any trouble with this like I did, here is the answer, I went into the amazon php development forums and got help from the profesionals. It seems you may be flip-flopping between Version 2 and Version 3 of the SDK or looking at the wrong document. Make sure you are getting the one you intend to use and are looking at the correct documentation. They are different. V3 - Composer Requirement: {"aws/aws-sdk-php": "~3.0"} - User Guide: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/index.html - API Docs: http://docs.aws.amazon.com/aws-sdk-php/v3/api/index.html - Pre-signed URL Docs: http://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-presigned-url.html V2 - Composer Requirement: {"aws/aws-sdk-php": "~2.8"} - User Guide: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/index.html - API Docs: http://docs.aws.amazon.com/aws-sdk-php/v2/api/index.html - Pre-signed URL Docs: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/service-s3.html#creating-a-pre-signed-url Mini step-by-step guide of what you have to do: 1.Install composer, preferably using sudo: 2.Go to your project folder and create a composer.json file, with the version you want/need, you can find releases here: https://github.com/aws/aws-sdk-php/releases, commands for each version seem to be very version specific, be careful, this was my main problem. } 3.Then go to your project folder in the terminal, and install sdk via composer and update afterward like: (if you change version you have to update again.) 4.Then everything is ready for you to follow proper version documentation, in my case for version "aws/aws-sdk-php": "~3.0" and for presigned url, what worked was: I hope this helps anyone facing the same problems as I did.
563e342f61a80130652682ff	X	Beware that if this occurs, s3cmd <= 1.5.0 can also return 0 from s3cmd put in the return code (possibly even the latter versions). Never trust s3cmd for critical operations.
563e343061a8013065268300	X	+1 ! I have a 110GB file I needed to backup on a consistent basis, doing it in parts is terrible. The above solution is great!
563e343061a8013065268301	X	I just spent about an hour on chat with AWS support and they actually chatted me this SO article! Even though my files were < 100Mb and this error just came out of nowhere... Installing AWS CLI and switching to that solved the problem.
563e343061a8013065268302	X	Thanks Alister. I didn't know of ~5Gig file size limit. So no problem with s3cmd :)
563e343061a8013065268303	X	I believe it is a limitation of s3cmd, as Amazon has allows files of several terabytes.
563e343061a8013065268304	X	The file being so big may be one reason. But I experienced the problem with files as small as 100MB.
563e343061a8013065268305	X	It's all about the network. On AWS there are usually less problems, but outside of the local network, all bets are off. You may want to split files up even smaller.
563e343061a8013065268306	X	As of right now, S3 accepts files up to 5 TB, but can only accept single uploads up to 5 GB. Larger requires multi-part upload. aws.amazon.com/s3/faqs/#How_much_data_can_I_store
563e343061a8013065268307	X	Thanks. It saved my life today
563e343061a8013065268308	X	You're welcome Salwek, thanks for letting me know.
563e343061a8013065268309	X	helpd me too, thanks
563e343061a801306526830a	X	Awesome. You did save my day, too.
563e343061a801306526830b	X	You saved my day! Thanks!
563e343161a801306526830c	X	copy the contents of the link here, leave the link as reference.
563e343161a801306526830d	X	I've tried to update as of the original page instructions, but still with a 24GB file fails, while a 1GB file works. Trying other solutions.
563e343161a801306526830e	X	If that doesn't works install from the tar packages. sourceforge.net/projects/s3tools/files/s3cmd/1.1.0-beta2/…
563e343161a801306526830f	X	Indeed, it didn't work for me. It updated to 1.0.x but had the same issue. As @user1681360 suggested, building the tarball (v 1.5.x) fixed the issue (it uploaded using multi-part).
563e343161a8013065268310	X	I had this problem uploading a 38MB file because I was using a t1.micro instance with limited bandwidth - changing to an m1-medium instance solved the problem.
563e343161a8013065268311	X	I wish I could upvote this more: it's the simplest solution to the problem described by Alister Bulman (not the problems described by Jaume Barcelo, qliq, or others). s3cmd-1.1.0-betaX (beta3 at the time of writing) not only does the splitting and uploading for you, but it asks Amazon to re-combine the files so that they appear as one file on S3. THIS IS ESSENTIAL if you're going to use it in Elastic Map-Reduce, where you don't have the option to recombine them by hand with cat.
563e343161a8013065268312	X	Thanks, worked for me too.
563e343161a8013065268313	X	I used to be a happy s3cmd user. However recently when I try to transfer a large zip file (~7Gig) to Amazon S3, I am getting this error: I am using the latest s3cmd on Ubuntu. Why is it so? and how can I solve it? If it is unresolvable, what alternative tool can I use?
563e343161a8013065268314	X	And now in 2014, the aws cli has the ability to upload big files in lieu of s3cmd. http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html has install / configure instructions, or often: followed by will get you satisfactory results.
563e343161a8013065268315	X	I've just come across this problem myself. I've got a 24GB .tar.gz file to put into S3. Uploading smaller pieces will help. There is also ~5GB file size limit, and so I'm splitting the file into pieces, that can be re-assembled when the pieces are downloaded later. The last part of that line is a 'prefix'. Split will append 'aa', 'ab', 'ac', etc to it. The -b100m means 100MB chunks. A 24GB file will end up with about 240 100mb parts, called 'input-24GB-file.tar.gz-aa' to 'input-24GB-file.tar.gz-jf'. To combine them later, download them all into a directory and: Taking md5sums of the original and split files and storing that in the S3 bucket, or better, if its not so big, using a system like parchive to be able to check, even fix some download problems could also be valuable.
563e343161a8013065268316	X	I tried all of the other answers but none worked. It looks like s3cmd is fairly sensitive. In my case the s3 bucket was in the EU. Small files would upload but when it got to ~60k it always failed. When I changed ~/.s3cfg it worked. Here are the changes I made: host_base = s3-eu-west-1.amazonaws.com host_bucket = %(bucket)s.s3-eu-west-1.amazonaws.com
563e343261a8013065268317	X	I had the same problem with ubuntu s3cmd. The solution was to update s3cmd with the instructions from s3tools.org: Debian & Ubuntu Our DEB repository has been carefully created in the most compatible way – it should work for Debian 5 (Lenny), Debian 6 (Squeeze), Ubuntu 10.04 LTS (Lucid Lynx) and for all newer and possibly for some older Ubuntu releases. Follow these steps from the command line: Import S3tools signing key: wget -O- -q http://s3tools.org/repo/deb-all/stable/s3tools.key | sudo apt-key add - Add the repo to sources.list: sudo wget -O/etc/apt/sources.list.d/s3tools.list http://s3tools.org/repo/deb-all/stable/s3tools.list Refresh package cache and install the newest s3cmd: sudo apt-get update && sudo apt-get install s3cmd
563e343261a8013065268318	X	This error occurs when Amazon returns an error: they seem to then disconnect the socket to keep you from uploading gigabytes of request to get back "no, that failed" in response. This is why for some people are getting it due to clock skew, some people are getting it due to policy errors, and others are running into size limitations requiring the use of the multi-part upload API. It isn't that everyone is wrong, or are even looking at different problems: these are all different symptoms of the same underlying behavior in s3cmd. As most error conditions are going to be deterministic, s3cmd's behavior of throwing away the error message and retrying slower is kind of crazy unfortunate :(. Itthen To get the actual error message, you can go into /usr/share/s3cmd/S3/S3.py (remembering to delete the corresponding .pyc so the changes are used) and add a print e in the send_file function's except Exception, e: block. In my case, I was trying to set the Content-Type of the uploaded file to "application/x-debian-package". Apparently, s3cmd's S3.object_put 1) does not honor a Content-Type passed via --add-header and yet 2) fails to overwrite the Content-Type added via --add-header as it stores headers in a dictionary with case-sensitive keys. The result is that it does a signature calculation using its value of "content-type" and then ends up (at least with many requests; this might be based on some kind of hash ordering somewhere) sending "Content-Type" to Amazon, leading to the signature error. In my specific case today, it seems like -M would cause s3cmd to guess the right Content-Type, but it seems to do that based on filename alone... I would have hoped that it would use the mimemagic database based on the contents of the file. Honestly, though: s3cmd doesn't even manage to return a failed shell exit status when it fails to upload the file, so combined with all of these other issues it is probably better to just write your own one-off tool to do the one thing you need... it is almost certain that in the end it will save you time when you get bitten by some corner-case of this tool :(.
563e343261a8013065268319	X	In my case the reason of the failure was the server's time being ahead of the S3 time. Since I used GMT+4 in my server (located in US East) and I was using Amazon's US East storage facility. After adjusting my server to the US East time, the problem was gone.
563e343261a801306526831a	X	s3cmd 1.0.0 does not support multi-part yet. I tried 1.1.0-beta and it works just fine. You can read about the new features here: http://s3tools.org/s3cmd-110b2-released
563e343261a801306526831b	X	For me, the following worked: In .s3cfg, I changed the host_bucket
563e343261a801306526831c	X	I addressed this by simply not using s3cmd. Instead, I've had great success with the python project, S3-Multipart on GitHub. It does uploading and downloading, along with using as many threads as desired.
563e343261a801306526831d	X	I encountered the same broken pipe error as the security group policy was set wrongly.. I blame S3 documentation. I wrote about how to set the policy correctly in my blog, which is:
563e343261a801306526831e	X	On my case, I've fixed this just adding right permissions.
563e343261a801306526831f	X	I encountered a similar error which eventually turned out to be caused by a time drift on the machine. Correctly setting the time fixed the issue for me.
563e343261a8013065268320	X	s3cmd version 1.1.0-beta3 or better will automatically use multipart uploads to allow sending up arbitrarily large files (source). You can control the chunk size it uses, too. e.g. This will do the upload in 1 GB chunks.
563e343361a8013065268321	X	Did you manage to find your problem Chris?
563e343361a8013065268322	X	No still not :(
563e343361a8013065268323	X	Hello Chris, are you signed up for the Amazon S3 service as well as an AWS account? see this thread forums.aws.amazon.com/thread.jspa?threadID=45582
563e343361a8013065268324	X	Yes, I have a bucket set up and ready to go :)
563e343361a8013065268325	X	+1 for having the guts to combine cutting edge cloud tech with stone age ASP tech. Hope you find a good solution.
563e343361a8013065268326	X	I'm getting the error "msxml6.dll error '80072f7c' The HTTP redirect request failed" any ideas what could be causing it?
563e343361a8013065268327	X	@ChrisDowdeswell What changes have you made in the code except for bucket name, access key and secret key?
563e343361a8013065268328	X	Nothing except the variables you put... :(
563e343361a8013065268329	X	@ChrisDowdeswell I really don't know what is causing this. On my computer it's works like a charm, and also with many online web pages currently. So, If you able to sniff http requests using a utility such HttpSniffer, can figure it out. In addition, maybe you should try older versions of MSXML e.g. MSXML2.ServerXMLHTTP.3.0
563e343361a801306526832a	X	Ok I changed the version to 3.0 and got "The requested header was not found"
563e343361a801306526832b	X	I am confused on what I am doing wrong here... Now getting the error.
563e343361a801306526832c	X	I'd like to explain how S3 Rest Api works as far as I know. First, you need to learn what should be the string to sign Amazon accepts. Format : Generating signed string : Passing authorization header: Unfortunately you'll play byte to byte since there is no any SDK released for classic asp. So, should understand by reading the entire page http://docs.amazonwebservices.com/AmazonS3/latest/dev/RESTAuthentication.html For string to sign as you can see above in format, there are three native headers are reserved by the API. Content-Type, Content-MD5 and Date. These headers must be exists in the string to sign even your request hasn't them as empty without header name, just its value. There is an exception, Date header must be empty in string to sign if x-amz-date header is already exists in the request. Then, If request has canonical amazon headers, you should add them as key-value pairs like x-amz-headername:value. But, there is another exception need to be considered for multiple headers. Multiple headers should combine to one header with values comma separated. Correct x-amz-headername:value1,value2 Wrong x-amz-headername:value1\n x-amz-headername:value2 Most importantly, headers must be ascending order by its group in the string to sign. First, reserved headers with ascending order, then canonical headers with ascending order. I'd recommend using DomDocument functionality to generate Base64 encoded strings. Additionally instead of a Windows Scripting Component (.wsc files), you could use .Net's interops such as System.Security.Cryptography to generating keyed hashes more effectively with power of System.Text. All of these interoperabilities are available in today's IIS web servers. So, as an example I wrote the below script just sends a file to bucket you specified. Consider and test it. Assumed local file name is myimage.jpg and will be uploaded with same name to root of the bucket. 
563e343461a801306526832d	X	The Amazon Signature must be url encoded in a slightly different way to what VBSCript encodes. The following function will encode the result correctly: JScript Version: VBScript Version: As for base64, I used .NET's already built functionality for it. I had to create a DLL to wrap it, so that I could use it from JScript (or VBScript). Here's how to create that dll: The code for the base64 stuff would be: You would need the relevant usings: The signature in full must have all the query string name-value pairs in alphabetical order before computing the SHA and base64. Here is my version of the signature creator function: VBScript doesn't have a very good array sort facility, so you'll have to work that one out yourself - sorry Also I have the timestamp in this format: YYYY-MM-DDTHH:MM:SSZ Also the stuff in the query string included the following: Hope that helps
563e343461a801306526832e	X	Thank you so much for this question, it has been such a great help to start my WSH/VBScript for my S3 backup service ;-) I do not have much time, so I will not go through the details of the things I have changed from Chris' code, but please find below my little prototype script which works perfectly ;-) This is just a WSH/VBScript, so you do not need IIS to run it, you just need to paste the content in a file with the ".vbs" extension, and you can then directly execute it ;-) Dear stone-edge-technology-VBScript-mates (*), let me know if it is working for you as well ;-) (*) This is a reference to the comment from Spudley, see above ;-)
563e343461a801306526832f	X	How to authorize only my app to use my REST API ? I have this code in Javascript This call will remove a user from the database with REST API in PHP. The problem is that everyone can remove a user, with POSTMAN (Chrome plugin) for exemple. How can I protect my REST API to authorize only my app. Check the HTTP_REFERER is not enough. What could be better ? Thanks for your help
563e343461a8013065268330	X	You have several possibilities here. In general you could authorize the user, the app or both. This depends on your application requirements. Authenticate Applications To authenticate the application you could use a token based system, such as an API-Key. This means any request would be signed using additional request parameters. Take a look at the way amazon does this in their S3 service for example. If your application will be the only one that will access the rest API you could alternatively simply restrict the acces by the IP address. If there will be multiple users using the service via your client, you may also need to authorize the access to certain functions. For example: Will every user be allowed to delete any resource? If the answer is no, you have to implement Authenticate and authorize users A simple way to authenticate users in a RESTful API is using HTTP Basic or Digest Auth. In this setting the user credentials are sent via the Authorization header in a form of username:password as Base64 encoded hash to the server. Please note that you should only do this via an secured connection using HTTPS! Additionally you could also take a look at more complex/sophisticated practices like OAuth.
563e343561a8013065268331	X	This is really a two part question: I'm seeing some users in the "Grantee" dropdown for editing S3 permissions within the AWS console. They aren't in IAM so I'm not really sure where they're coming from.
563e343561a8013065268332	X	A grantee can either be an AWS account (which you probably added in the past) or a predefined AWS "group", such as "Authenticated Users", "All Users" or "Log Delivery". Please have a look at ACL Overview, on AWS docs, for more information. For removing grants from a given file (or from a set of files), you can use the PUT Object acl operation. It is not clear, on the documentation, what you need to do in order to remove an user from the "Grantee" list. I performed some tests and this is how S3 is behaving: This makes me think the Grantees list contains the entire list of users in your bucket's ACL plus a cache of users with permissions to objects in your bucket (which is cleared upon logging out, if you remove those permissions). So, I would try first removing the users you don't want from your bucket's ACL, and then (via API, of course) remove those user's permissions for the objects in your bucket.
563e343561a8013065268333	X	If there is a specific grantee name that you can't find in IAM, it's probably the default grantee which corresponds to yourself. Its name is the same as your AWS Forum nickname.
563e343561a8013065268334	X	I wanted to point out that if you upload files greater than 5GB (using multipart uploads) then S3's Etag is no longer a simple MD5 of the file. It looks like an md5 of the file plus some additional meta data, but the algorithm is not documented that I know of.
563e343561a8013065268335	X	Wrong ​​​​link?
563e343561a8013065268336	X	indeed, fixing it
563e343661a8013065268337	X	Thanks! Works now. What a noobish move from my part :P
563e343661a8013065268338	X	Bonus point if you post the solution ;-) Did you hex string the bute array from getDigest() or use another way?
563e343661a8013065268339	X	I get the MD5 of a local file but it is different than the MD5 (eTag) of the "same" file in Amazon S3. What I would like to achieve is figure out if the lastest files I have in S3 is the same one that I have locally. If I cannot compare MD5, then how should I do it? This is how I get the MD5 from the local file (truncated code): This is how I get the MD5 (eTag) from S3 (truncated code): } PS: I use org.apache.commons.codec.digest.DigestUtils and com.google.common.io.Files libraries.
563e343661a801306526833a	X	Does calculate the MD5 of the MD5 you just calculated. See DigestUtils.md5Hex documentation. hashtext is in fact MD5(MD5(file)) and not MD5(file).
563e343661a801306526833b	X	Bruno's answer nails it, but I wanted to point out that if you want to do this without the Google Guava dependency, it's actually not that difficult (especially since/if you're already using Apache Commons) You'd replace this: with this (using a Java 7 try-initialization-block): This md5(InputStream) method has been in Apache Commons since version 1.4.
563e343661a801306526833c	X	This is my own implementation of S3's eTag. I tested it with a large file I uploaded on S3 to get a reference value for multipart eTag. Keep in mind that compression and Client-Side Encryption make eTag useless when it comes to check downloaded file. Etag.java EtagTest.java GarbageTools.java
563e343761a801306526833d	X	What about @Mock and @RunWith(MockitoJUnitRunner.class)? Can't you just imitate injected bean?
563e343761a801306526833e	X	I like the answer and have upvoted. But some questions: - What about testing the HttpClientWeatherStatus? Ignore unit testing or else we have to find a way to mock HttpClient there? - Isn't it bloating the code for a small problem which could have been "fixed" by just some lines using reflection or simply changing to package private field access? I am not saying your answer is incorrect, I just want to hear your thoughts.
563e343761a801306526833f	X	Hi, I've updated my answer. :)
563e343761a8013065268340	X	I often find myself wondering what is the best practice for these problems. An example: I have a java program which should get the air temperature from a weather web service. I encapsulate this in a class which creates a HttpClient and does a Get REST request to the weather service. Writing a unit test for the class requires to stub the HttpClient so that dummy data can be received in stead. There are som options how to implement this: Dependency Injection in constructor. This breaks encapsulation. If we switch to a SOAP web service in stead, then a SoapConnection has to be injected instead of HttpClient. Creating a setter only for the purpose of testing. The "normal" HttpClient is constructed by default, but it is also possible to change the HttpClient by using the setter. Reflection. Having the HttpClient as a private field set by the constructor (but not taking it by parameter), and then let the test use reflection to change it into a stubbed one. Package private. Lower the field restriction to make it accessible in test. When trying to read about best practices on the subject it seems to me that the general consensus is that dependency injection is the preferred way, but I think the downside of breaking encapsulation is not given enough thought. What to you think is the preferred way to make a class testable?
563e343761a8013065268341	X	I believe the best way is through dependency injection, but not quite the way you describe. Instead of injecting an HttpClient directly, instead inject a WeatherStatusService (or some equivalent name). I would make this a simple interface with one method (in your use case) getWeatherStatus(). Then you can implement this interface with an HttpClientWeatherStatusService, and inject this at runtime. To unit test the core class, you have a choice of stubbing the interface yourself by implementing the WeatherStatusService with your own unit testing requirements, or using a mocking framework to mock the getWeatherStatus method. The main advantages of this way are that: This method is known as hexagonal/onion architecture which I recommend reading about here: Or this post which sums the core ideas up: EDIT: Further to your comments: What about testing the HttpClientWeatherStatus? Ignore unit testing or else we have to find a way to mock HttpClient there? With the HttpClientWeatherStatus class. It should ideally be immutable, so the HttpClient dependency is injected into the constructor on creation. This makes unit testing easy because you can mock HttpClient and prevent any interaction with the outside world. For example: Where the returned WeatherStatus 'Event' is: Then the tests look something like this: You will generally find that there will be very few conditionals and loops in the integration layers (because these constructs represent logic, and all logic should be in the core). Because of this (specifically because there will only be a single conditional branching path in the calling code), some people would argue that there is little point unit testing this class, and that it can be covered by an integration test just as easily, and in a less brittle way. I understand this viewpoint, and don't have a problem with skipping unit tests in the integration layers, but personally I would unit test it anyway. This is because I believe unit tests in an integration domain still help me ensure that my class is highly usable, and portable/re-usable (if it's easy to test, then it's easy to use from elsewhere in the codebase). I also use unit tests as documentation detailing the use of the class, with the advantage that any CI server will alert me when the documentation is out of date. Isn't it bloating the code for a small problem which could have been "fixed" by just some lines using reflection or simply changing to package private field access? The fact that you put "fixed" in quotes speaks volumes about how valid you think such a solution would be. ;) I agree that there is definitely some bloat to the code, and this can be disconcerting at first. But the real point is to make a maintainable codebase which is easy to develop for. I think some projects start fast because they "fix" problems by using hacks and dodgy coding practices to maintain the pace. Often productivity grinds to a halt as the overwhelming technical debt renders changes which should be one liners into mammoth re-factors which take weeks or even months. Once you have a project set up in a hexagonal way, the real payoffs come when you need to do one of the following: Change the technology stack of one of your integration layers. (e.g. from mysql to postgres). In this case (as touched on above), you simply implement a new persistence layer making sure you use all the relevant interfaces from the binding/event/adapter layer. There should be no need to change core code or the interface. Finally delete the old layer, and inject the new layer in place. Add a new feature. Often integration layers will already exist, and may not even need modification to be used. In the example of the getCurrentWeather() and store4HourlyWeather() use-cases above. Let's assume you've already implemented the store4HourlyWeather() functionality using the class outlined above. To create this new functionality (let's assume the process begins with a restful request), you need to make three new files. You need a new class in your web layer to handle the initial request, you need a new class in your core layer to represent the user story of getCurrentWeather(), and you need an interface in your binding/event/adaptor layer which the core class implements, and the web class has injected to its constructor. Now on the one hand, yes, you've created 3 files when it would have been possible to create only one file, or even just tack it onto an existing restful web handler. Of course you could, and in this simple example that would work fine. It is only over time that the distinction between layers become obvious and refactors become hard. Consider in the case where you tack it onto an existing class, that class no longer has an obvious single purpose. What will you call it? How will anyone know to look in it for this code? How complicated is your test set-up becoming so that you can test this class now that there are more dependencies to mock? Update integration layer changes. Following on from the example above, if the weather service API (where you are getting your information from) changes, there is only one place where you need to make changes in your program to be compatible with the new API again. This is the only place in the code which knows where the data actually comes from, so it's the only place which needs changing. Introduce the project to a new team member. Arguable point, since any well laid out project will be fairly easy to understand, but my experience so far has been that most code looks simple and understandable. It achieves one thing, and it's very good at achieving that one thing. Understanding where to look (for example) for Amazon-S3 related code is obvious because there is an entire layer devoted to interacting with it, and this layer will have no code in it relating to other integration concerns. Fix bugs. Linked to the above, often reproducibility is the biggest step towards a fix. The advantage of all the integration layers being immutable, independent, and accepting clear parameters, is that it is easy to isolate a single failing layer and modify the parameters until it fails. (Although again, well designed code will do this well too). I hope I've answered your questions, let me know if you have more. :) Perhaps I will look into creating a sample hexagonal project over the weekend and linking to it here to demonstrate my point more clearly.
563e343761a8013065268342	X	The preferable way should favor proper encapsulation and other object-oriented design qualities, while keeping the code under test simple. So, my recommended approach would be to: For example, here is a possible detailed solution: Step 1: Step 2: Step 3: The above uses the JMockit mocking library, but PowerMock would be an option too. I would recommend using java.net.URL (if possible) instead of Apache's HttpClient, though; it would simplify both production and test code.
563e343761a8013065268343	X	Thanks for the enlightenment. :D
563e343761a8013065268344	X	Sure thing! Go ahead and mark as answer to close this out unless you have anymore questions :)
563e343761a8013065268345	X	Oh sorry bout that. I thought up-voting makes it as answered. Thanks again.
563e343761a8013065268346	X	Parse is awesome. But I want to handle all request from my own server. Since parse has been open sourced. I'd like to know if it is possible to replace the default server api.parse.com to api.mydomain.com. Sorry no code. I just need a way to implement such.
563e343761a8013065268347	X	Nope, doesn't work that way. There's a massive difference between Parse open-sourcing their SDKs compared to revealing their entire backend architecture and its configuration. The open-sourced SDKs are essentially wrappers for Parse's REST API along with some convenience functions and logic for natively interpreting the JSON data Parse is transmitting. At a high level, Parse uses MongoDB for its core database and is entirely hosted using AWS (Amazon Web Services). The entire architecture is highly complex and is not something you could just drag and drop onto your own software stack or hardware backend. It would be easier to build out all of your own backend services on AWS than it would be to try to clone Parse but for only one app. For example you could use AWS S3 for storage needs and then access those resources using AWS API Gateway which will automatically create mobile platform SDKs for you. To help give you a better idea of how Parse achieves all of their services, here's an interesting presentation their Dev Ops team gave at an AWS convention. Suffice it to say, hosting the backend services for over 180,000 apps requires a complex infrastructure and that's the "secret sauce" for Parse, and is also why Facebook purchased them for over $85 million two years ago.
563e343861a8013065268348	X	utkarshsengar.com/2011/01/…
563e343861a8013065268349	X	I am new to AWS console. I can able to login using IAM role in AWS console. And also Using Putty i am able to login with localhost: ubuntu@Public DNS value.. Once i logged in i want to access AWS instance using IAM role (Developer and Administrator) in putty.But its not logging in. Is it possible IAM role to be used in AWS instance.
563e343861a801306526834a	X	Short answer : no. IAM roles and SSH key pairs are two different mechanisms that apply at different levels, for different users and different type of authentications. IAM roles are containers for permissions on AWS API. A role describes a set of permissions on AWS services, such as EC2, S3, DynamoDB etc ... The entity that will assume the role (a person or an EC2 instance for example) will temporary receive the authorisation to perform the actions described in the role's permissions. You can use IAM roles to control who can access the AWS API (start a machine, create a load balancer, create a network etc ...) but IAM roles do not help to control who can connect to your Operating Systems. To control access at the operating systems level, (on linux) AWS requires you to generate a pair of cryptographic RSA keys and to upload your public key on AWS. When your linux instance starts, a process will install a copy of your public key in a (operating system) user directory (~/.ssh/authorized_keys). Username is operating system specific (ubuntu on ubuntu, ec2-user on Amazon Linux and RedHat) IAM is not involved when managing users at operating system level or permissions to connect to your operating systems. More details about roles is available http://docs.aws.amazon.com/IAM/latest/UserGuide/WorkingWithRoles.html
563e343861a801306526834b	X	to clarify something, the problem we were having that required us to take the renaming route was that our data center firewall needed to know what our computer name and mac address was before we could let it interact with us (or more specifically, its services interact with us). If you don't need such precautions, then you can very easily use the API to get every bit of information you'll ever need about your running instances....using the DescribeInstancesRequest and DescribeInstancesResponse methods.
563e343861a801306526834c	X	When writing a web app with Django or such, what's the best way to connect to dynamic EC2 instances, such as a cluster of Redis or memcache instances? IP addresses change between reboots, etc. Elastic IPs are limited to 5 by default - what are some other options for auto-discovering/auto-updating which machines are available?
563e343861a801306526834d	X	Late answer, but use Boto: http://boto.cloudhackers.com/en/latest/index.html You can use security groups, tags, and other means to hit the EC2 API and pick the instances/IPs for each thing (DB Server, caching server, etc.) at load-time. We do this with great success in deployment, and are moving that way with our Django settings.py, as well.
563e343861a801306526834e	X	One method that I heard mentioned recently in an AWS webinar was to store this sort of information in SimpleDB. Essentially, you would use SimpleDB as the central configuration location, and each instance that you launch would register its IP etc. with this configuration, so you would always have a complete description of all of your instances in one place. I haven't seen this in practice so I don't know what the best practices would be exactly, but the idea sounds reasonable. I suppose you could use SNS or something to signal all the other instances whenever the configuration changes, so everyone could refresh their in-memory cache of the configuration. I don't know the AWS administrative APIs yet really, but there's probably an API call to list your EC2 instances, at which point you could use some sort of custom protocol to ping each of them and ask it what it is -- part of the memcache cluster, Redis, etc.
563e343861a801306526834f	X	I'm having a similar problem and didn't found a solution yet because we also need to map Load Balancers addresses. For your problem, there are two good alternatives: If you are not using EC2 micro instances or load balancers, you should definitely use Amazon Virtual Private Cloud, because it lets you control instances IPs and routing tables (check all limitations before using this service). If you are only using EC2 instances, you could write a script that uses the EC2 API tools to run the command ec2-describe-instances to find all instances and their public/private IPs. Then, the script could parameterize instances names to hosts and update /etc/hosts. Finally, you should put the script in the crontab of every computer/instance that need to access the EC2 instances (see ec2-describe-instances).
563e343861a8013065268350	X	If you want to stay with EC2 instances (I'm in the same boat, I've read that you can do such things with their VPC or use an S3 bucket or something like that.) but with EC2, I'm in the middle of writing stuff like this...it's all really simple up till the part where you need to contact the server with a server from your data center or something. The way I'm doing it currently is using the API to create the instance and start it...then once its ready, I contact the server to execute a powershell script that I have on the server....the powershell renames the computer and reboots it...that takes care of needing the hostname and MAC for our data center firewalls. I haven't found a way yet to remotely rename a computer. As far as knowing the IP, the elastic IPs are the way to go. They say you're only allowed 5 and gotta apply for more but we've been regularly requesting more and they give em to us..we're up to like 15 now and they haven't complained yet. Another option if you dont' want to do all the computer renaming and such...you could use DHCP and set your computer up so when it boots it gets the computer name and everything from DHCP....I'm not sure how to do this exactly, I've come across very smart people telling me that's the way to do it during my research for Amazon. I would definitely recommend that you get into the Amazon API...I've been working with it for less than a month and I can do all kinds of crazy things. My code can detect areas of our system that are getting stressed, spin up 10 amazon servers all configured to act as whatever needs stress relief, and be ready to send jobs to all in less than 7 minutes. Brings a tear to my eye. The documentation is very complete...the API itself is a work of art and a joy to program against...I've very much enjoyed working with it. (and no, i dont' work for them lol)
563e343961a8013065268351	X	Do it the traditional way: with DNS. This is what it was built for, so use it! When a machine boots, have it ask for the domain name(s) related to its function, and use that for your configuration. If it stops responding, re-resolve the DNS (or just do that periodically anyway). I think route53 and the elastic load balancing stuff can be used to do this, if you want to stick to Amazon solutions.
563e343961a8013065268352	X	It is very easy to get up and running with Windows Azure Service Bus. These nuget packages will get you up and running in under an hour. nuget.org/packages?q=ProjectExtensions and source github.com/ProjectExtensions/ProjectExtensions.Azure.ServiceBus
563e343961a8013065268353	X	Great. Thanks guys. Can you point me in the direction of a code sample that demonstrates processing messages in related groups?
563e343961a8013065268354	X	Hi Simon, thanks for taking the time to respond in such length. You've made some really good points. I will certainly re-evaluate our options with external parties. I think that there would certainly be opportunities to move away from the single-node model for certain worker roles, but I am also sure that there will be others where we are constrained to this pattern and therefore will have to roll our own active-passive load balancing. (As mentioned, the above example was highly simplified to illustrate the architectural challenges I am considering) Anyway, thanks again for your input.
563e343961a8013065268355	X	I have some questions regarding architecting enterprise applications using azure cloud services. Back Story We have a system made up of about a dozen WCF Windows Services on a SQL backend. We currently have about 10 clients but expect that to grow to potentially a hundred with perhaps a hundred fold increase in the throughput demands on the system. The current system is poorly engineered and is simply not capable of scaling. So now appears to be the appropriate juncture to reengineer on the azure platform. Process Flow Let me briefly describe a simplified set of the services and the process flow and then ask some questions I have regarding utilizing azure cloud services to build the new system. Service A is logged on to an external systems and downloads data continuously Service B is logged on to a second external systems and downloads data continuously There can only ever be one logged in instance each of services A and B. Both A and B hand off their data to Service C which reconciles the data from the two external sources. Validated and reconciled data is then passed from C to Service D which performs some accounting functions and then passes the resulting data to Services E and F. Service E is continually logged in to an external system and uploads data to it. Service F generates reports and publishes them to clients via FTP etc The system is actually far more complex than this but the above illustrates the processes involved. The system runs 24 hours a day 6 days a week. Queues will be used to buffer messaging between all the services. We could just build this system using Azure persistent VMs and utilise the service bus, queues etc but that would ties us in to vertical scaling strategy. How could we utilise cloud services to implement it given the following questions. Questions Given that Service A, B and E are permanently logged in to external systems there can only ever be one active instance of each. If we implement these as single instance worker roles there is the issue with downtime and patching (which is unacceptable). If we created two instances of each is there a standard way to implement active-passive load balancing with worker roles on azure or would we have to build our own load balancer? Is there another solution to this problem that I haven’t thought of? Services C and D are a good candidates to scale using multiple worker role instance. However each instance would have to process related data. For example, we could have 4 instances each processing data for 5 individual clients. How can we get messages to be processed in groups (client centric) by each instance? Also, how would we redistribute load from one instance to the remaining instances when patching takes place etc. For example, if instance 1, which processes data for 5 clients, goes down for OS patching, the data for its clients would then have to be processed by the remaining instances until it came back up again. Similarly, how could we redistribute the load if we decide to spin up additional worker roles? Any insights or suggestions you are able to offer would be greatly appreciated. Mat
563e343961a8013065268356	X	Question #1: you will have to implement your own load-balancing. This shouldn't be terribly complex as you could use Blob storage Lease functionality to keep a mutex on some blob in the storage from one instance while holding the connection active to your external system. Every X period of time you could renew the lease if you know that connection is still active and successful. Every other worker in the Role could be checking on that lease to see if it expires. If it ever expires, the next worker would jump in and acquire the lease, and then open the connection to the external source. Question #2: Look into Azure Service Bus. It has a capability to allow clients to process related messages. More info here: http://geekswithblogs.net/asmith/archive/2012/04/02/149176.aspx All queuing methodologies imply that if a message gets picked up but does not get processed within a configurable amount of time, it goes back onto the queue so that the next available instance can pick it up and process it You can use something like AzureWatch to monitor the depth of your queues (storage or service bus) and auto-scale number of instances in your C and D roles to match; and monitor instance statuses for roles A, B and E to make sure there is always a healthy instance there and auto-scale if quantity of ready instances drop to 0. HTH
563e343961a8013065268357	X	First, back up a step. One of the first things I do when looking at application architecture on Windows Azure is to qualify whether or not the app is a good candidate for migration to Windows Azure. I particularly look at how much integration is in the application — integration is always more difficult than expected, doubly so when doing it in the cloud. If most of your workload needs to be done through a single, always-on connection, then you are going to struggle to get the availability and scalability that we turn to the cloud for. Without knowing the detail of your application, but by way of example, assume services A & B are feeds from a financial data provider. Providers of data feeds are really good at what they do, have high availability, and provide 'enterprise grade' (whatever that means) for enterprise grade costs. Their architectures are also old-school and, in some cases, very rigid. So first off, consider asking your feed provider (that gives to a login/connection and expects you to pull data) to push data to you via a web service. Exposed web services are the solution to scaling and performance, and are used from table storage on Azure, to high throughput database services like DynamoDB. (I'll challenge any enterprise data provider to explain how a service like Amazon S3 is mickey-mouse.) If your data supplier pushed data to a web service via an agreed API, you could perform all sorts of scaling and availability on the service for a low engineering cost. Your alternative is, as you are discovering, to build a whole lot of stuff to make sure that your architecture fits in with the single-node model of your data supplier. While it can be done, you are going to spend a lot of engineering cash on hand-rolling a whole bunch of distributed computing principles. If you are going to have an active-passive architecture, you need to implement a leader election algorithm in order to determine when a passive node should become active. This is not as trivial as it sounds as an active node may look like it has disappeared, but is still processing — and you don't want to slot another one in its place. So then you will implement a heartbeat, or even a separate 'witness' node that does nothing other than keep an eye on which nodes are alive in order to do something about them. You mention that downtime and patching is unacceptable. So what is acceptable? A few minutes or a few seconds, or less than a second? Do you want the passive node to take over from where the other left off, or start again? You will probably find that the development cost to implement all of this is lower than the cost of building and hosting a highly available physical server. Perhaps you can separate the loads and run the data feed services in a co-lo on a physical box, and have the heavy lifting of the processing done on Windows Azure. I wouldn't even look at Azure VMs, because although they don't recycle as much as roles, they are subject to occasional problems — at least more than enterprise-grade hardware. Start off with discussions with your supplier of the data feeds — they may have a solution, or one that can be cobbled together (e.g. two logins for the price of one, and the 'second' account/instance mostly throws away its data). Be very careful of traditional enterprise integration. They ask for things that seem odd in today's cloud-oriented world. I've had a request that my calling service have a fixed ip address, for example. You may find that the code that you have to write to work around someone else's architecture would be better spent buying physical servers. Push back on the data providers — it is time that they got out of the 90s. [Disclaimer] 'Enterprises', particularly those that are in financial services, keep saying that their requirements are special — higher throughput, higher security, high regulations and higher availability. With the exception of a very few cases (e.g. high frequency trading), I tend to call 'bull' on most of this. They are influenced by large IT budgets and vendors of expensive kit taking them to fancy lunches, and are indoctrinated to their server-hugging beliefs. My individual view on the enterprise hardware/software/services business has influenced this answer. Your mileage may vary.
563e343961a8013065268358	X	Requests for off-site resources ("What is a big data service for...?") are off-topic. Your question might be better received if you simply ask how to do what you want to do (and be more specific about what that is, if possible), rather than asking for a service.
563e343961a8013065268359	X	Thanks for the advice
563e343961a801306526835a	X	As a note, SQS does not guarantee no duplicates - it has at least once delivery. You will need to handle duplicate deliveries.
563e343a61a801306526835b	X	Thanks for the answer, I didn't even consider AWF. I ended up using Celery + RabbitMQ, a distributed queue system.
563e343a61a801306526835c	X	Currently I am implementing a big data solution using ec2 + sqs + s3. The idea is that I have an enormous database of files being hosted on s3. Clients using my application would submit a rule match, that would attempt to match all relevant files available on s3, and then evaluate a conditional statement, returning output to the user showing which files matched the conditional. I obviously cannot perform my rule match over my s3 files directly, as then there is no preventing duplicate jobs across the ec2 instances I would spin up to do the rule matches. There would also be no division of labor. My initial solution was to incorporate Amazon SQS: when a client made a request, all files in S3 would be loaded into the queue. This would allow my ec2 instances to perform division of labor + no duplicates, as SQS takes care of this. However, using the above solution, I would be loading all my files into the SQS queue every time a client request is made, which is clearly wasteful (not to mention the skyrocketing SQS cost). Ideally I would want to have a persistent queue that loads all my files once (and additionally loads more when more files are added to my s3). Any ideas about how to create a big data queue structure?? Should I ditch SQS, or should I adapt SQS to operate like a persistent queue by not deleting messages after they are processed and keeping a reserved SQS instance always running? Thanks.
563e343a61a801306526835d	X	Assuming your rule matching thing is based on file metadata rather than file contents, you can store metadata about your files in an actual database, like Amazon DynamoDB, and do your matching like that. Storing your data "permanently" in SQS (not deleting messages) is not a great idea - messages have a limited size, and if you don't delete messages, they become available again for subsequent requests - you're going to get a lot more duplication this way. Also, SQS does not provide any guarantees around duplication - it's in fact a "at least once" delivery. If you are running your rule matching on file contents (possibly in addition to metadata), which is what it sounds like you really want, then you can use Amazon SimpleWorkflow and the Flow API built on top of it: SWF have two main components - the workflow (which maintains the state) and the activities (stateless code that performs the work). For your case, the workflow would divide each request into many chunks of work and submit activities to the SWF service. Your activity workers would run on many EC2 instances - their job is to essentially pick up the job (ie: run your rule matching code on the assigned subset of S3 files) and return the result. The workflow would then consolidate the activity results and do something with the result. The Flow framework recipies document has some good examples of the code patterns you'll need to execute many activities concurrently and consolidate the results. So, in summary, to specifically answer your question: ditch SQS and use SWF which is designed for this sort of usage pattern.
563e343a61a801306526835e	X	so, if file will be downloaded from s3, and http request will be made to the s3 server, then s3 must be adding this header, not your application, isn't it?
563e343a61a801306526835f	X	@splix: seen your comment, I guess so... But figuring that out is kinda precisely the whole point of my question :) If you make an answer detaliling this a tiny bit more I'll upvote and accept :)
563e343a61a8013065268360	X	@splix: seen your comment, I googled on S3 and found that apparently due to popular request you can "Upload with custom headers" (files to your bucket) which would be a great way to solve my issue!
563e343a61a8013065268361	X	In fact it looks like the Amazon management UI lets you set up headers directly, if that works for you. Thanks for asking this question!!
563e343a61a8013065268362	X	I searched around, but I can't find any references that S3 supports JSP/Servlet. Why did you tag it? The only JSP/Servlet solution would be to play for a proxy on your own host and set the content disposition header programmatically, but that would make the main purpose of S3 (a CDN so that you can save connections/requests on your primary domain) completely pointless, you could then as good just host it yourself.
563e343b61a8013065268363	X	@Pointy: so I take it it's not compatible with the S3 case I explained in my question? I mean, if I have to stream the file through my server it's kinda defeating the whole S3 point isn't it!?
563e343b61a8013065268364	X	Yes I agree but see my comment - I confess to being a lightweight when it comes to S3 details, but I'm looking ...
563e343b61a8013065268365	X	Even though it's not part of HTTP 1.1/RFC2616 webapps that wish to force a resource to be downloaded (rather than displayed) in a browser can use the Content-Disposition header like this: Even tough it's only defined in RFC2183 and not part of HTTP 1.1 it works in most web browsers as wanted. So from the client side, everything is good enough. However on the server-side, in my case, I've got a Java webapp and I don't know how I'm supposed to set that header, especially in the following case... I'll have a file (say called "bigfile") hosted on an Amazon S3 instance (my S3 bucket shall be accessible using a partial address like: files.mycompany.com/) so users will be able to access this file at files.mycompany.com/bigfile. Now is there a way to craft a servlet (or a .jsp) so that the Content-Disposition header is always added when the user wants to download that file? What would the code look like and what are the gotchas, if any?
563e343b61a8013065268366	X	You wouldn't have a URL that was a direct reference to the file. Instead, you'd have a URL that leads to your servlet code (or to some sort of action code in your server-side framework). That, in turn, would have to access the file contents and shovel them out to the client, after setting up the header. (You'd also want to remember to deal with cache control headers, as appropriate.) The HttpServletResponse class has APIs that'll let you set all the headers you want. You have to make sure that you set up the headers before you start dumping out the file contents, because the headers literally have to come first in the stream being sent out to the browser. This is not that much different from a situation where you might have a servlet that would generate a download on-the-fly. edit I'll leave that stuff above here for posterity's sake, but I'll note that there is (or might be) some way to hand over some HTTP headers to S3 when you store a file, such that Amazon will spit those back out when the file is served out. I'm not exactly sure how you'd do that, and I'm not sure that "Content-disposition" is a header that you can set up that way, but I'll keep looking.
563e343b61a8013065268367	X	I got this working as Pointy pointed out. Instead of linking directly to the asset - in my case pdfs - one now links to a JSP called download.jsp which takes and parses GET parameters and then serves out the pdf as a download. Download here Here's the jsp code I used. Its working in IE8, Chrome and Firefox:
563e343b61a8013065268368	X	Put a .htaccess file in the root folder with the following line:
563e343b61a8013065268369	X	x-amz-date works around that, but you need to make sure that "time" is not too skewed.
563e343b61a801306526836a	X	I have read all about this issue online. I understand the check is done by Amazon and I cannot control it. The problem is that my program can be installed on hundreds of desktops. I will not have the ability to fix the clocks for all these desktops. I can get the correct time from a server call and calculate the time skew. Is there anyway to pass the fixed time to Amazon via the AWS .NET SDK so that the time skew error goes away? I already tried passing the request headers - it does not work.
563e343b61a801306526836b	X	After extensive research I found the answer. There is a bug in the latest AWS SDK which will not let the dates you specify actually go across to S3. So the current API: Ignores the time values passed via the headers from the API. In any case, the x-amz-date is definitely overridden. Builds the signature for the authorizer using the default time from the skewed local clock. To work around the issue, download the latest AWS SDK source code from Amazon. I did the following to ensure both date and x-amz-date are the same, and take the values passed to the API: aws-sdk-net-master\AWSSDK_DotNet35\Amazon.Runtime\Pipeline\HttpHandler\HttpHandler.cs: aws-sdk-net-master\AWSSDK_DotNet35\Amazon.Runtime\Internal\Auth\AWS3Signer.cs: aws-sdk-net-master\AWSSDK_DotNet35\Amazon.Runtime\Internal\Auth\S3Signer.cs: aws-sdk-net-master\AWSSDK_DotNet35\Amazon.Runtime\Internal\Auth\CloudFrontSigner.cs: IMPORTANT: After you compile with these code changes, pass the time to the request from the API call using: The time skew can be found out by getting the date from any server and subtracting the local time. Note however that time skew changes especially on VMs and sometimes it gets worse, sometimes it gets corrected by itself.
563e343b61a801306526836c	X	You can send request of completion handler and response it can set your skew time then you can resend your request. if your time is more than 5 6 months amazon server will not allowed us to do anything so it don't help when time is more than 5 6 month
563e343c61a801306526836d	X	Thank you, but as I said to Chakri/in my original question: I want 100% automated access. If I understood you correctly, I must manually install the service every time I launch the instance (new instances are launched programmatically, following your advice means that I have to manually connect every time)...am I missing something here?
563e343c61a801306526836e	X	No you launch one instance, install the service, then save the instance as an AMI and when you launch future instance you use the AMI that already has the service installed.
563e343c61a801306526836f	X	Sorry for the late reply...had to test thoroughly! Your last comment (about making AMIs) was the real answer for me, thank you very much!
563e343c61a8013065268370	X	Glad I could help, Goodluck!
563e343c61a8013065268371	X	As I said: I want 100% automated access. Both methods requires manual intervention (new instances are launched programmatically, following your advice means that I have to manually connect every time)...am I missing something here?
563e343c61a8013065268372	X	I was just mentioning the core method of connectivity. Here is more detailed steps. Option a> You write a batch script that takes your file & server_name as input. Then put it in your cron/task-manager as shown c:\scripts\upload.bat file_name server_name Option b> You map the S3 drive to both your client and server. Create a script that put files to the mapped drive. c:\scripts\upload.bat mapped_drive
563e343c61a8013065268373	X	Oh I see. Nice idea, this should solve the upload part, but I still can't execute the EXE once it's uploaded to the Windows instance, can I?
563e343c61a8013065268374	X	The Windows SFTP provides windows ssh command. You can use that for any remote execution. Just like how you do it linux with ssh key exchange.
563e343c61a8013065268375	X	Thank you Chakri, can you please expand on the SFTP connection? I researched it a lot, but it seems that, beside the SSH port opening in the EC2's security groups, it's not available out of the box (ie. I have to manually install something to get the SFTP to work)
563e343c61a8013065268376	X	I'd like to launch an EC2 Windows instance, upload an EXEecutable & execute it (all in an automated fashion, this is important) So far I was able to programmatically launch EC2 Windows instance & get its parameters (password / IP), now I'd like to find a way to upload this Executable (from my Windows machine or from my other EC2 linux instance) & run it. I thought about launching an RDP connection & using a macro software to upload & execute the file, but based on previous experiences this is a poor/fragile approach to say the least. I also thought about uploading this EXE to a server, then do something like this on Windows: Except that Windows doesn't have wget! So my question is: is there a way to programmatically upload & execute an EXEcutable in EC2 Windows instance?
563e343c61a8013065268377	X	The command ec2-run-instances has two additional arguments that can be used when running the command. The user-data command and user-data-file both of these perform the same task just they read from different input. When you use this argument the contents of the user-data will be uploaded to a amazon hosted URI http://169.254.169.254/1.0/user-data only available to the instance that was launched. The normal way to do this in the linux environment would be to upload a shell script to the instance to download the exe, your user-data-file might look something like this... In Windows there's no default service installed to execute the user-data-file when the instance is booted but there is an open-source project CloudInit.NET which simulates the same process but with a powershell script. The only requirements are .NET 4.0 and CloudInit.NET. Once installed it will execute the user-data-file when the instance is booted. It's very easy to download a file and execute it with a powershell script.
563e343c61a8013065268378	X	An alternative approach is to use Windows powershell and WinRM - it allows for remote execution, a bit like ssh on Linux. Here is a sample of a powershell script you can run on the client to remote execute a script (taken from: https://github.com/CloudifySource/cloudify/blob/master/esc/src/main/resources/clouds/ec2-win/upload/bootstrap-client.ps1): You can run this command from your own script with the following command: You should probably quote your strings, especially the password and command, as these will usually have special characters that powershell can interpret as something else. The WinRM service is on by default on the EC2 Amazon Windows AMIs. All you need to do is open port 5985 (the WinRM port) in your security group. Finally, if you have never used powershell remoting on your client machine before, there are a couple of commands you should execute to set it up (you only need to do this once): Make sure to run these as an Administrator.
563e343c61a8013065268379	X	This sounds like a perfect use case of CloudFormation. I created a template that demonstrates. To use, put your executable in an S3 bucket and create a new CloudFormation stack with the following template. It will download your executable from S3 and run it. Note: the template makes use of special AMIs with CloudFormationScripts built in.
563e343c61a801306526837a	X	You can handle this in 2 ways, Using winscp in Windows SFTP program. To access your Amazon server using SFTP on Windows, download a Windows SFTP application. Using WinSCP you’ll establish an SFTP session with your server. WinSCP offers some nice features that make it easy to work with your EC2 server. For example, a command in the button bar opens a PuTTY SSH terminal session using the same credentials you used for your SFTP session. (You can also launch a PuTTY session by clicking CTRL+P.). Get an S3 bucket and mount on all your windows and linux EC2 instances. You should be able to upload and download the files to S3 bucket from your workstation, which are accessible to your instances.
563e343c61a801306526837b	X	I had do a similar automation for AWS for enterprise deployment back in 2011. Amazon cloud formation and opsworks where still under construction then.However we had successfully done deployment automation using dotnet for both linux and windows platform.The powershell and ftp model where ruled out becasue it was an enterprise environment and there was port restriction. Below are the approaches i used. Note:This was asp.net web application We used an open source project called sharpshell(sharpSSH).This is c# application which simpulates shell communication between windows and linux.Just need to supply the target aws address and security key to connect. We customized the app around our requirement Having said that cloud formation API where still not available the and less documentation available by AWS. We used a workaround, webservice approach. Crearted a webservice which basically uploads a file to server and deploy. With this webserver hosted on amazaon windows server.Created a base image out of this and certificate. Finally a new instances created out of this image would have webservices hosted which could be called to upload the deployment package and install on that system. Though the above startergies where not fool proof and had less controll. we achieved the functionality of cross platform direct deployment from windows to linux without using S3 buckets or powershell. Please let me know if you require any clarification. Cheers! Charles
563e343d61a801306526837c	X	Do you have any specific problem with doing it? Google turns up several pages of general info.
563e343d61a801306526837d	X	But how to implement it practically. Like i have launched an ec2 instance with an IAM role. Then how this role is used. That I dont know
563e343d61a801306526837e	X	possible duplicate of How can I create IAM Roles for Amazon EC2?
563e343d61a801306526837f	X	I am exploring amazon IAM Roles. I want to know how can we apply IAM roles to an ec2 instance to access an application. Any lead is highly appreciated. Thanks
563e343d61a8013065268380	X	You can attach a Role to an Instance to provide this instance with specific permissions to use AWS API. For example : You deploy a Java application on Tomcat and you want your application to use DynamoDB or S3 ... you need an ACCESS KEY and SECRET KEY with proper permission. How would your application get these ? A configuration file ? Burned into the AMI ? Stored in a database ? ... none of these are secure and manageable at large scale. This is where Role kicks in. The best part is that AWS SDK are knowing about that and can dynamically and automatically get the keys for you. Check out the doc for more details : http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html
563e343d61a8013065268381	X	Thanks Mark, I will take a look to both solutions you provided. Do you know where can I take that "S3 REST API" from ?! About the duration of the URL availability, I'm afraid that it is controlled by Amazon....
563e343d61a8013065268382	X	@AlexAndro: "About the duration of the URL availability, I'm afraid that it is controlled by Amazon" -- I guarantee you that it is not. I control it myself, from the Ruby API, for making my books available for download to subscribers. Also, you can see the time-limited URLs in action in things like the S3 Explorer for Firefox. You are searcing for "pre-signed URLs" in the documentation IIRC. "Do you know where can I take that "S3 REST API" from" -- you don't need it, as the Java API has generatePresignedUrl() that you can use for this.
563e343d61a8013065268383	X	I use aws-android-sdk-1.4.3/samples/S3_SimpleDB_SNS_SQS_Demo to preview my files stored on Amazon (Amazon Simple Storage Service). Looking through code I saw that they use this, to acces the files: com.amazonaws.demo.s3.S3.getDataForObject (line 130) } Well, I have modified this methods to return ByteArrayOutputStream instead then I easily transform it to String or Bitmap (applying ByteArrayOutputStream.toByteArray() then using BitmapFactory.decodeByteArray(byte[] data, int offset, int length, Options opts)). So, it works on text-files and pictures. My problem is when I try to access videos. So, my questions are: 1.Using the method provided above, how could I get a video from ByteArrayOutputStream (ByteArrayOutputStream.toString()) and play it in a VideoView or using MediaPlayer or an approach... ? 2 . Does anybody know any other solution to this problem of preview videos stored on Amazon ? (I heard that on their sdk for IOS they use URLs to access files...) PS: Supplying the file URL and open it in browser does not make sense, because this URLs expire after a wile.
563e343d61a8013065268384	X	First we have to provide the name of our bucket and the object (see aws-android-sdk-1.4.3/samples/S3_SimpleDB_SNS_SQS_Demo for a complet guide) we want to open then get the URL to our object: Now, just play the video in a video view, supplying the URL obtained: I want to give thanks to @CommonsWare for indicating me through REST API (even the code I used is from aws-sdk reading the REST API documentation helped me and show also other ways of requesting Amazon objects) indicating me to use generatePresignedUrl() the code for playing the video is also inspired from his materials.
563e343d61a8013065268385	X	1.Using the method provided above, how could I get a video from ByteArrayOutputStream (ByteArrayOutputStream.toString()) and play it in a VideoView or using MediaPlayer or an approach... ? Maybe you could get it to work by publishing the byte array through a ContentProvider and openFile(). Here is a sample project where I demonstrate serving a file by means of a custom InputStream this way. The media subsystem is rather fussy, though, and so I do not give you good odds on this working. 2 . Does anybody know any other solution to this problem of preview videos stored on Amazon ? (I heard that on their sdk for IOS they use URLs to access files...) Last I checked, S3 had a REST API that you could use to generate URLs to the videos. I'd hand that URL to MediaPlayer or VideoView. Supplying the file URL and open it in browser does not make sense, because this URLs expire after a wile. But you control how long "a wile [sic]" is. Make it be 24 hours or something.
563e343e61a8013065268386	X	thanks a lot for your reply. its really convincing answer to me to go around this.
563e343e61a8013065268387	X	glad to help and wish you the best with all the great stuff your planing...
563e343e61a8013065268388	X	yeah those are the example apps and advantages over cloud computing. but my doubt is my management ask me to use a package called wikivideo with the cloud computing. i dont know what way to start it. thats why i got the basic doubt. thanks for your reply @gopi
563e343e61a8013065268389	X	Could anyone please tell me what the web developers can do with cloud computing, sorry for the very basic question i got confused so much by googling. Please tell me how to use or where to use this technology in PHP web development field. I used AMAZON'S ec2 but there i just followed their manual and it was very simple file storage. But in what way it could help me to make great projects?
563e343e61a801306526838a	X	As a developer, most of the times you rather want to spend your time on creating your applications and services without messing around too much with the scalability and deployment issues (though it has it fun parts too) Cloud computing platforms provides you with a stack of solutions that can ease and speed up your development cycle letting you consume resources as a service and pay only for what you use so in most times it can be very cost efficient One of the most important things that i can find in cloud computing is that you can terminate many cases of a single point failure and bottlenecks in a convenient way the services are highly available, you can scale up and maintenance resources easily! for example with amazons web services, you can and all this with several easy to handle API calls... so as a bottom line you can create many great projects using those solutions, i can say for sure that cloud based development (and MapReduce) fulfilled a lot of my needs and contributed to the way i develop fast, highly scalable and reliable services
563e343e61a801306526838b	X	Web applications developed using cloud computing (e.g. google app engine) offer inherent scalability. Also, application developers need not have to worry about the hardware setup, hosting and maintenance. Being in cloud the web applications also ensure high availability.
563e343e61a801306526838c	X	Any reason why the data conversion ("decryption") has to be done in JavaScript? Why can't you use a server-side technology like Java/PHP/Ruby/Python etc?
563e343e61a801306526838d	X	Good point. Could the server be on AWS? If so which AWS technology would be easiest to implement? I looked into lambda and it looked promising since it seemed to allow me to easily upload a function. However the speed is critical. The data will be many GBs large and must be decrypted fast. The ideal would be if there was some AWS component that allowed me to upload a compiled C/C++ function that decrypted a S3 record.
563e343e61a801306526838e	X	Your question is fairly broad. There are of course many ways to host this as a service on AWS. You could deploy some sort of service on an EC2 server, or you could upload a function to Lambda.
563e343e61a801306526838f	X	Thank you for your help mbaird! I read that Elastic Beanstalk supports .net. I guess I could implement a websocket server in C# and upload it to Elastic Beanstalk. The server could use S3 as backend storage.
563e343f61a8013065268390	X	Alternatively I guess I could upload an Apache webserver with home made hooks to EC2.
563e343f61a8013065268391	X	Just write the service in whatever technology/language/server you feel most comfortable with. You can run pretty much anything on EC2. Use Elastic Beanstalk if you don't want to deal with setting up and managing servers yourself.
563e343f61a8013065268392	X	I want to build an application using Amazon Web Services (AWS). The way the application should work is this; I make a program that lets the user import a large file in an external format and send it to AWS (S3?) in my own format. Next many users can access the data from web and desktop applications. I want to charge per user accessing the data. The problem is that the data on AWS must be in an unintelligible format or the users may copy the data over to another AWS account where I can not charge them. In other words the user need to do some "decrypting" of the data before they can be used. On the web this must be done in JavaScript which is plaintext and would allow the users to figure out my unintelligible format. How can I fix this problem? Is there for instance a built in encryption/decryption mechanism? Alternatively is there some easy way in AWS to make a server that decrypts the data using precompiled code that I upload to AWS?
563e343f61a8013065268393	X	In general when you don't want your users to access your application's raw data you just don't make that data public. You should build some sort of server-side process that reads the raw data and serves up what the user is requesting. You can store the data in a database or in files on S3 or wherever you want, just don't make it publicly accessible. Then you can require a user to login to your application in order to access the data. You could host such a service on AWS using EC2 or Elastic Beanstalk or possibly Lambda. You could also possibly use API Gateway to manage access to the services you build. Regarding your specific question about a service on AWS that will encrypt your public data and then decrypt it on the fly, there isn't anything that does that out of the box. You would have to build such a service and host it on Amazon, but I don't think that is the right way to go about this at all. Just don't make your data publicly accessible in the first place, and make all requests for data go through some service to verify that the user should be able to access the data. In your case that would mean verifying that the user has paid to access the data they are requesting.
563e344061a8013065268394	X	You need to hide the data in transfer, so others can't see it, or you need to verify it so other can't fake it?
563e344061a8013065268395	X	Only authorized user accounts will be able to add leads so I need to authenticate the transfer.
563e344061a8013065268396	X	The simplest possible solution is to sign the post request with a shared secret key.
563e344061a8013065268397	X	@Maerlyn And always use HTTPS.
563e344061a8013065268398	X	So this is just a static string that we give them or does this key get generated somehow?
563e344061a8013065268399	X	Just sending an API key across does not prevent anyone from replaying the requests or simply nicking the key, especially as if it went through GET, it'll be logged on: your router, all the nodes the request transited through, the remote server's logs. Consider signing instead.
563e344061a801306526839a	X	Summary: I've created an app in PHP. It's lead management system for a call center. I now need to allow a partner to be able to add new leads to the app by integrating our app with their proprietary CRM. In short, I guess I need to build an API for my app. The easiest approach I can think of is a simple HTML post. Would this be considered too insecure? If so, what would be best approach for this type of situation? Thanks for any help, Andrew.
563e344061a801306526839b	X	Through your quest to build an API, you'll most likely come across some of these. I'm going to outline the concepts that might come in very handy to actually build an API that is usable, and that follows open standards (which, in turn, makes it trivial for third-party to adapt existing code to interact with it). The first keyword is: SSL. Don't ever think of not using it. This provides a secure socket layer on which communication can happen in a secure fashion, and consequently makes eavesdropping and MitM attacks significantly more difficult to conceive. No matter what, do not skip on this. Certificates cost less than $60/year, so it is not exactly costly, and can save you a lot in the long run. In terms of server techs, use what you want. Your main requirement is a webserver that can handle the four common HTTP verbs: GET, POST, PUT, DELETE. I'll explain why in a moment. This one is the contentious field, as lots of people "think they have a secure way to do so". The answer is simply not true. The point of your authentication is to allow a client to easily authenticate with their credentials, but to prevent a third-party who is not privileged from doing so. Simply adding an API key to the feed will just lead to someone eventually getting hold of it. I have seen this specific thing so many times that I strongly advise against it, especially as there are significantly easier options. I'll go over a couple of things, labelling them as (A) or (S), respectively for Authentication and Signature. Signing is the method used to render your request tamper-proof. Authentication proves who you are. This technique is used by Amazon for all their S3/AWS APIs, and is a very lightweight method of signing and authenticating a request. I personally find it relatively ingenious. The basic idea: This is simple and ingenious. What it guarantees: This neatly wraps both issues using the same HTTP request at the cost of one reserved GET/POST field. Amazon also requires the presence of a Timestamp in the request, which prevents replay attacks. Neat! (For the reference: HMAC-ALGO = ALGO( (key XOR PAD) concat ALGO(key XOR PAD2) concat message). ALGO can be any hash cipher - SHA256 is preferred for its lightweight nature) You've probably heard of it. the idea is simple: you get given a key and secret. This allows you to queue up for a temporary token. This token is then used to perform requests. The main advantage of this is that lots of libraries exist to handle it, both client-side and server-side. The other advantage is that OAuth has two modes of operation: two-legged (server->server without client interaction) and three-legged (client->server->server). The main drawback is 2 HTTP requests to get a token. ... Leads to replay attacks. don't consider it. A mixture of methods is a possible things. The HMAC signage is awesome when combined with OAuth, for example! API endpoints these days follow two main standards: SOAP (XML-RPC), or REST. If you are building an endpoint to post leads, you may as well build the corresponding API to read leads and to delete them for the future. Your API would therefore take the form: This allows you to future-proof your API conveniently as well.
563e344061a801306526839c	X	A HTML post will suffice, that's not a problem. It would be even better if you're able to use HTTPS to ensure the transferred data is encoded, but this isn't critical. The most common way of securing this kind of API is to provide a shared 'secret' or 'key', which is used to encode a hash. You'll then be able to verify that the request came from a trusted source, but it's up to the user to ensure that they keep the shared key a secret. e.g. Users of your API will need to: Then you'll perform the same logic at your end from the POST values provided and verify that their hash matches the hash that you build with the user's shared key.
563e344161a801306526839d	X	This is exactly what an API is for. I'd make a unique key per external account and require that API key for each $_GET or $_POST transaction that is sent to your server. Might want to build an API management console while you're at it. Oh and don't forget the separate DB table for the API keys. When you're done it'll be something like: https://api.mysite.com/index.php?key=r328r93fuh3u4h409890fj34klj&other=something&another=somethingelse You get the idea.
563e344161a801306526839e	X	When you decided to deploy a cloud setup what are the architectural/implementation issues you have faced and how did you resolve them? Some examples include:
563e344161a801306526839f	X	The biggest issue I faced, were local fallbacks. In a typical cloud scenario you are moving your resources that once lived in a traditional data storage (database, filesystem, etc) to something behind an API, that you can't easily replicate locally. For our application we moved a few typical queues from MySQL to Amazon's SQS. Problems: Currently Amazon charges $0.01 per 10,000 SQS request, a cost that might seem extremely small, but there's absolutely no reason to pay when developing locally (or for your test server, assuming you have a separate one). If you don't have a local queue fallback, you need a separate queue per development / testing environment. You really don't want messages from different queues getting mixed up. There isn't a simple way to emulate SQS locally for our environment (that I know of). Architecturally, I dealt with the transition to SQS with simple adapters: The same architecture (more or less) worked pretty nicely when we moved our images to S3, with a filesystem local fallback. Simple, small and easy enough to explain, and, more importantly, it works. If you are migrating an application to the cloud, chances are you'll be writing quite a few adapters for your back end services, other than having a simple fallback mechanism, you don't want to be vendor locked to a specific service. Obviously if you are building an application with the cloud in mind, you may not necessarily need local fallbacks, especially if your platform has an easy way of emulating the cloud environment. Something like Stratosphere, if you are developing on .Net / Mono or if you are targeting Amazon's services. But if you have a mature application, you already have an infrastructure set up locally, keep using it makes a bit more sense. The cloud "overhead" is not really something you need to worry about if you are using the cloud as a fancy data store. But if you are looking for cloud computing, then there's no answer, it always depends on what you are doing exactly. A few relevant questions:
563e344161a80130652683a0	X	If you enable the web page on S3, there is an option for you to redirect File Not Found error to a page (in my case, I use a jpg file that says "File Not Found", so this would work for images too)
563e344161a80130652683a1	X	Thanks @Alvin K. Not sure how that would work for me as I have multiple file types so returning a JPG file wouldn't really help me. But thanks anyway.
563e344161a80130652683a2	X	Since the jpg file is fixed, a md5 checksum will identify which file is returned and you "catch" this 404 error.
563e344161a80130652683a3	X	Exception will happen only if getObject() doesn't connect, usually it will return an object, whether it is an XML with error code or the FileNotFound.jpg or the actual file you are looking for.
563e344161a80130652683a4	X	Thanks Alvin, so far no object has been returned, just an error. Maybe you have a different configuration. I do believe you can set it to return an object (like a 404 image) if an object does not exist.
563e344161a80130652683a5	X	@AlvinK. you are wrong about this; AWS's PHP SDK automatically throws exceptions upon receiving error responses from the server - not just for connection failures.
563e344161a80130652683a6	X	Nice. Is that new?
563e344161a80130652683a7	X	We are in luck, the source code for the PHP aws sdk is in github, I looked it up and learned this API was added by Michael Dowling on July 12th, 2012. So yes its relatively new, it may have been rolled out as part of an update release some time later. I love how Amazon keeps making AWS better! github.com/aws/aws-sdk-php/blame/master/src/Aws/S3/S3Client.php
563e344261a80130652683a8	X	sadly its not in amazon s3 v3
563e344261a80130652683a9	X	If I try to get an object from my S3 bucket that doesn't exist, the Amazon PHP SDK 2 gives me a pretty ugly error. Handy for me but means nothing to the end user... E.g: The error: Fatal error: Uncaught Aws\S3\Exception\NoSuchKeyException: AWS Error Code: NoSuchKey, Status Code: 404, AWS Request ID: xxxxxxxxxxxxx, AWS Error Type: client, AWS Error Message: The specified key does not exist. thrown in AWS/vendor/aws/aws-sdk-php/src/Aws/Common/Exception/NamespaceExceptionFactory.php on line 89 Is there a way that I can determine if there is an error and print a message that makes sense rather than the above?
563e344261a80130652683aa	X	All errors that occur in calls to methods of the AWS SDK are indicated by throwing exceptions. You can catch those exceptions if you want to handle the errors. In the simplest case, you might just want to catch Exception: If you only want to handle certain expected exceptions, though, while letting others bubble up and crash your application, then things get a little more subtle. Firstly, each of the few dozen namespaces within the AWS namespace contains an Exception namespace in which it defines exception classes. One of these classes in each namespace is what Amazon calls the default service exception class for the namespace, from which all other exceptions inherit. For example, S3 has the Aws\S3\Exception namespace and the S3Exception class. EC2 has the Aws\Ec2\Exception namespace and the Ec2Exception class. Note that catching one of these exceptions instead of the base Exception class immediately stops us catching certain errors! The service-specific exceptions are thrown as a result of error responses from the server; connection failure exceptions do not inherit from them. For example, if you try running the following code without an internet connection... ... then the exception will not be caught (since it will be a Guzzle\Http\Exception\CurlException, not an S3Exception) and the program will crash. For this reason, if you're catching these exceptions just to provide generic failure messages to the user, you should probably catch Exception. Let's return to the question of how to handle a specific error. For most of the namespaces, the answer is that there will be an exception class defined for that error, and you should catch that. For example, let's say we're again using the S3 getObject method and want to do something when the bucket we ask for doesn't exist. Looking in the S3 Exception namespace docs, we see that there is a NoSuchBucketException we can catch: (In practice, it may well be easier to figure out which exceptions can be thrown by what operations through trial and error than through carefully reading the docs.) Finally, it is worth mentioning the EC2 API. Unlike all the other services, the EC2 namespace includes only a single exception class, the Ec2Exception. If you want to catch and handle a specific error, you need to inspect the exception object to figure out what kind of error you're dealing with. You can do this by checking the value returned by the getExceptionCode() method of the exception. For example, a (modified) snippet from a script I recently wrote that grants specified IPs access to our MySQL server: Note that the possible exception codes - like InvalidPermission.Duplicate in this case - are not listed in the AWS PHP SDK documentation, but you can find them by trial and error or from the documentation for the EC2 API itself, in which each API action's page contains an 'Errors' section listing the error codes it can return.
563e344261a80130652683ab	X	It suddenly occurred to me to try this:
563e344261a80130652683ac	X	You can also use this method: $response = $s3->doesObjectExist( $bucket, $key ); It will return a boolean true response if the object exists. AWS Docs for doesObjectExist
563e344261a80130652683ad	X	Thanks for tips @Sabyasachi Ghosh! But actually I'm not using plugins, I'm just using the FileReader API to get file raw data or similar, set it into a json object and sent it to my rest webservice which is RoR based. I think that a regular form submmit can work, but my webclient is full ajax and I just belive that it can be possible, I'm just struggling about how to do that. I think that there's a way to set the right file encoding and so rails paperclip could consumes that.
563e344261a80130652683ae	X	Thank you for clues @t_itchy! I'll check it out right now.
563e344261a80130652683af	X	Just ripped off meta_request but it didn't change my log output significantly. What I'm stuck is about how to grab raw data from a regular image file (jpg,png..), send it to my server, set it on self.image attachment and save it. When I run Paperclip.run("identify", "-format '%wx%h,%[exif:orientation]' :file", :file => "tmp/temp_file20130906-2165-1t7m9i920130609-2165-1wb3pv7.jpg[0]") Console logs that my file isn't a jpg file !!! Ok this seems a clue, but I'm looking for how turn the content of image sent by my ajax request into a valid jpg stuff.
563e344261a80130652683b0	X	Pretty nice @jyurek! Actually there's is an extra "]" ending in my code, I just messed it by wrong copy/pasting here. Yeah I did read about Base64decoding but I got a little bit confused once the Base64encoded string buffer really looks like a regular jpg file once you open it in a text editor. Actually I thought something about sending an already Base64Decoding stuff to my server, so I would not even need to decode it on server side, but I'm not sure if it's really effective. I will try do it in server side as you said. Best regards man!
563e344261a80130652683b1	X	let me tell you what is going on. I have a web client which is fully html/css/JavaScript (no server side language), and a Ruby on Rails server that consumes and receives json data. Early everything was going fine, once I got how to solve some cross-domain problems, but now I need to include a image and it brought me troubles about sending my data through regular JQuery Ajax. My form is based on twitter bootstrap and have simple texts fields, a file input and a button to upload the image: In my functions.js javascript file I just get the form params and I'm trying to use FileReader API to deal with the image file. See the following: As far as I know the readAsDataURL() method returns a encoded base64 stuff and there we go. For now I just select any image file through that file input and I start what would be the upload process by JQuery Ajax Stuff: 'Till that point the thing works like a charm once I just can get the encoded file data, split it's metadata, change it and set it into the src attribute of my thumbnail img tag, once I fire the the upload button. Now the thing becomes serious. I need to send all data into a json well formated object to my rails rest service, and it goes like this : As you could see it's pretty standard ajax sending, nothing to die for. In my Rails side I have : Model: end The set_picture method receives data which is the supposedly raw image file. It creates a temp file and writes data content into it and set int into self.image attribute. Now the controller : end In this controller I just catch the file data related to the incoming request and process it trough set_picture model's method into uploaded_file local variable. Finally I re-set the params[:product][:image] original hash value. Paperclip settings and S3 amazon integration are pretty standard but totally functional inside rails app itself everything works perfectly once I use a simple erb multipart/form-data and just the default form submit sending. If I send the dataproduct json without the image param, everything works fine also. But when it goes with the img param I got that errors in my console !!! I've been reading lot of issues about cocaine and identify and this things are working by itselves, by the way if I run Paperclip.run("identify", "-format '%wx%h,%[exif:orientation]' :file", :file => "/tmp/test20130609-4953- aqzvpm20130609-4953-12ijmo6.jpg[0"). I get: identify: Not a JPEG file: starts with 0x30 0x30 '/tmp/test20130609-4953- aqzvpm20130609-4953-12ijmo6.jpg[0' @ error/jpeg.c/EmitMessage/236. So I think that must be a way to kinda "reconstruct" my original valid jpg encoding file into rails application, not completely sure about that, but I'm working on it. Wow guys, sorry for that long, I've been trying lots of things such as adding different contentType and processData ajax settings, also, other FileReader methods like readAsArrayBuffer() and readAsText(), sending the file without the splited metadata and methods to different character encoding, utf-8, ascII, base64 and so on and on on...none of this worked but I'm really tired and I would really appreciate some help x.x
563e344261a80130652683b2	X	Not able to under stand the actual problem. Seems like you want to send the file data using ajax file upload plugin. Ajax file file upload sends the unserialized data using submitting a hidden i-frame. Please try once with out ajax file upload and normal form submission and check in the console the actual error.
563e344261a80130652683b3	X	I see in your stack trace you are using meta_request. We had a similar problem with this gem and jQuery upload. Try getting it to work without meta_request. Then you may want to look at fixing meta request or opening a ticket here https://github.com/dejan/rails_panel/issues.
563e344261a80130652683b4	X	One, the command you're running to test the convert is missing characters. You pasted "/tmp/test20130609-4953- aqzvpm20130609-4953-12ijmo6.jpg[0" which does not have an ending ], so I wouldn't expect that to have worked. Two, I would ask if it works if you add that ] but I know it won't: You said you Base64 encode it on the JS side with FileReader but you don't ever say that you Base64 decode it. In your set_picture method, you should decode the data. That will be closer to get you what you need. You can also look at the file rather than just send it through convert to make sure that it survived the trip through the server.
563e344261a80130652683b5	X	Actually I got this a long time ago, but just now I'll put the snippet that solved it based on jyurek tips.
563e344361a80130652683b6	X	We have a Java EE application (EAR file deployed on JBoss, MySQL, MongoDB) which we would like to deploy on an Amazon EC2 instance. I have several questions regarding deployment best practices. This is the first time, I am hosting an web-app and would appreciate some inputs on how to manage the production instance.
563e344361a80130652683b7	X	I have a common image which is the base of every version deploy I do. I have www.mysite.com pointing to an Elastic IP so I can decide which instance it goes to. The common image has all the software I need installed (Postgres/Postgis/Tomcat/etc) but the database and web server data folders and symlinked to Elastic Block Store (EBS) instances. When it comes time to do a deploy I start a new instance up, freeze and snapshot the EBS volumes on production and make new volumes. I point my new instance at the new volumes and then install whatever I need to onto that. Once I've smoke tested everything successfully I can switch the Elastic IP to point to the new instance and everything keeps on going. I'll note that I currently have the advantage where only I can modify the database; no users can. This will become a problem shortly. If you use the XFS filesystem on top of the EBS volume then you can tell XFS to freeze the file system (so no updates happen) then call the EC2 api to snapshot the volume then unfreeze the file system. The result is that the snapshot is taken quickly and sent to S3. I have a nightly script which does this. If RDS looks like it will suit your needs then use it. Amazon is building lots of solid tools quickly and this will ease your scalability issues if you have any. I'm sorry, I have no idea.
563e344361a80130652683b8	X	Good question! 1) I would recommend going with whatever Linux variant you are most comfortable with. If you have someone who is really keen on CentOS, go with that. Once you have selected your AMI, take it and customize it by configuring how you want it. Then save that AMI as you base-layout. It will make rolling out new machines much easier and save your bacon if EC2 goes down. 2) Upgrades with EC2 can be tres cool. Instead of upgrading a live system, take your pre-configured AMI, update that and save that AMI as myAMI-1.1 (or whatever). That way, you can flip over to the new system almost instantly AND roll back to a previous version in case something breaks. You can also back-up DB instances to S3. It's cheap at about $0.10/GB/Month. 3) It depends where you are storing your DB. If you are storing it on your EC2 instance you are in trouble. The EC2 instances have no persistence storage. So if your machine crashes, you lose everything. I'm not familiar with Amazon DB system but you should also look into Elastic Block Store. It's basically an actual hard-drive you can write to. When you want to upgrade your schema, do a full DB dump to S3 and then do an upgrade of your actual schema. If something goes wrong, you can pull the previous version out of S3. 4) & 5) I have never used those so I can't help you.
563e344361a80130652683b9	X	
563e344361a80130652683ba	X	Is there a version number for the os anywhere? try clicking a few times like on standard android
563e344461a80130652683bb	X	Did you see this? developer.amazon.com/post/Tx3RZFBU0KJTSWS/…
563e344461a80130652683bc	X	I have a new Fire phone, the box says "Fire with dynamic perspective" and "fire phone 32gb", it arrived today 7/24/2014. I can't figure out how to turn on developer options like in a normal Android device or how to see it in ADB on a Mac or a Windows box. Instructions for older Fire tablets do not work. There is no information online about this particular device even
563e344461a80130652683bd	X	Code Apprentice, The link you provided doesn't show the SDK for firephone. It provides the sdk for fire os 3.0, but not 3.5. Below are the actual links he needs. Those were harder to find since Amazon has duplicate locations for its documentation, and not all of them have been correctly updated yet. https://developer.amazon.com/public/resources/development-tools/sdk https://developer.amazon.com/public/solutions/devices/fire-phone/docs/setup Take the following steps in the Android Studio IDE to install the Fire Phone SDK add-on. In Android Studio, on the Tools menu, under Android click SDK Manager. - or - On the command line, run the following command, where <ANDROID_SDK> is the path to your Android SDK: <ANDROID_SDK>/tools/android In the Android SDK Manager window, verify that SDK Path points to the path to your Android SDK. To modify the path, in Android Studio, on the File menu, click Project Structure, and then update the Android SDK Location. In the Add Add-on Site URL dialog box, enter the following URL: https://s3.amazonaws.com/android-sdk-manager/redist/addon.xml Note: If you want to download the contents of the Fire Phone SDK without installing the add-on through your IDE, you can get the zip archive at https://s3.amazonaws.com/android-sdk-manager/redist/Fire_Phone.zip You can also upgrade through the gradle plugin instead, or through Eclipse ADT.
563e344461a80130652683be	X	On the Fire phone, the developer options in Settings are hidden by default. To show developer options, do the following: https://developer.amazon.com/public/solutions/devices/fire-phone/docs/connecting-your-device
563e344461a80130652683bf	X	for firephone adb connection you need amazon fire phone sdk provided by Amazon. Inside that sdk you find device driver for firephone then you can connect your fire phone to the system rest thing are same you need to go settings then enable developer option there.
563e344461a80130652683c0	X	Actually, in the FireOS 3.6.8, the developer options appear here: Settings=> phone=> view your phone number=>select 7 times any option and it will show the developer options buttons at bottom If you select any option 7 times again (if the developer options button is shown) then it will hide the developer options button. Hope this work for somebody else.
563e344461a80130652683c1	X	First , you don't need to install the Fire Phone SDK Add-on. I spent an hour on this. Second Christina's instructions are old and the Amazon doc is not up to date. Here is "How To" on the last update for Fire OS 4.6.3  
563e344461a80130652683c2	X	Which URL tries the browser to fetch from the CDN and at which URL can the file be found? For me this is not clear from your question, can you please provide a concrete example.
563e344561a80130652683c3	X	I updated my question. Let me know if it's still unclear!
563e344561a80130652683c4	X	I still have troubles to understand why the current browser location is forwarded to your CDN. Does your CDN respond to the same domain as your site? Do you have relative URLs for your resources hosted on the CDN instead of absolute ones? How/why does your CDN recognize /client.html?
563e344561a80130652683c5	X	What is deployed on the CDN is the output from pub build. So all the static assets needed to serve up the web app. client.html is what bootstraps the app, and actually exists on the CDN root. Therefore, www.myapp.com/client.html actually exists (I've configured client.html to be the "default" file served when visiting the website root, so the user can just go to www.mysite.com and they're served client.html. The current browser location is forwarded to the CDN because client-side routing updates the browser's url. When the user hits the refresh button, that url is requested.
563e344561a80130652683c6	X	I see, I somehow missed that you serve the entire client code from the CDN I assumed just some resources like *.css, *.img, .... I know client side routing only from Angular.dart (and the last time is already a few months in the past) but as far as I remember routing only changed the parts after the ? in client.html?. How do you do the routing?
563e344561a80130652683c7	X	I appreciate the answer... would this be considered a reverse proxy?
563e344561a80130652683c8	X	Updating answer with more explanation.
563e344561a80130652683c9	X	@w.brian I added more explanation of what's happening. I wanted to throw in some actual apache config code as examples but it's after school/dinner time and so it's a little crazy right now. lol if you need more clarification let me know I'll try and update later.
563e344561a80130652683ca	X	short answer is no. Think of it like creating shortcuts to REAL files (or generated ones) on the fly to satisfy requests, or filter, or whatever your needs are.
563e344561a80130652683cb	X	I have an HTML5 app written in static html/js/css (it's actually written in Dart, but compiles down to javascript). I'm serving the application files via CDN, with the REST api hosted on a separate domain. The app uses client-side routing, so as the user goes about using the app, the url might change to be something like http://www.myapp.com/categories. The problem is, if the user refreshes the page, it results in a 404. Are there any CDN's that would allow me to create a rule that, if the user requests a page that is a valid client-side route, it would just return the (in my case) client.html page? More detailed explanation/example The static files for my web app are stored on S3 and served via Amazon's CloudFront CDN. There is a single HTML file that bootstraps the application, client.html. This is the default file served when visiting the domain root, so if you go to www.mysite.com the browser is actually served www.mysite.com/client.html. The web app uses client-side routing. Once the app loads and the user starts navigating, the URL is updated. These urls don't actually exist on the CDN. For example, if the user wanted to browse widgets, she would click a button, client-side routing would display the "widgets" view, and the browser's url would update to www.mysite.com/widgets/browse. On the CDN, /widgets/browse doesn't actually exist, so if the user hits the refresh button on the browser, they get a 404. My question is whether or not any CDNs support looking at the request URI and rewriting it. So, I could see a request for /widgets/browse and rewrite it to /client.html. That way, the application would be served instead of returning a 404. I realize there are other solutions to this problem, namely placing a server in front of the CDN, but it's less ideal.
563e344561a80130652683cc	X	I do this using CloudFront, but I use my own server running Apache to accomplish this. I realize you're using a server with Amazon, but since you didn't specify that you're restricted to that, I figured I'd answer with how to accomplish what you're looking to do anyway. It's pretty simple. Any time you query something that isn't already in the cache on CloudFront, or exists in the Cache but is expired, CloudFront goes back to your web server asking it to serve up the content. At this point, you have total control over the request. I use the mod_rewrite in Apache to capture the request, then determine what content I'm going to serve depending on the request. In fact, there isn't a single file (spare one php script) on my server, yet cloudfront believes there are thousands. Pretty sure url rewriting is standard on most web servers, I can only confirm on lighttp and apache from my own experience though. More Info All you're doing here is just telling your server to rewrite incoming requests in order to satisfy them. This would not be considered a proxy or anything of the sort. The flow of content between your app and your server, with cloudfront in between is like this: So basically, what is happening in your situation is this: A)app->ask cloudfront for url cloud front doesn't have B)cloudfront then asks your source server for the file C)file doesn't exist there, so the server tells cloudFront to go fly a kite D)cloudFront comes back empty handed and makes your app 404 E)app crashes and burns, users run away and use something else. So, all you're doing with mod_rewrite is telling your server how it can re-interpret certain formatted requests and act accordingly. You could point all .jpg requests to point to singleImage.jpg, then have your app ask for: www.mydomain.com/image3.jpg www.mydomain.com/naughtystuff.jpg Neither of those images even have to exist on your server. Apache would just honor the request by sending back singleImage.jpg. But as far as cloudfront or your app is concerned, those are two different files residing at two different unique places on the server. Hope this clears it up. http://httpd.apache.org/docs/current/mod/mod_rewrite.html
563e344561a80130652683cd	X	I think you are using the URL structure in a wrong way. the path which is defined by forward slashes is supposed to bring you to a specific resource, in your example client.html. However, for routing beyond that point (within that resource) you should make use of the # - as is done in many javascript frameworks. This should tell your router what the state of the resource (your html page or app) is. if there are other resources referenced, e.g. images, then you should provide different paths for them which would go through the CDN.
563e344561a80130652683ce	X	To begin with I was using whatever AMIs I could find. I was running a piece of code in the terminal like "ec2-describe-images -o amazon" and picking whatever AMI would work; then using "ec2-run-instances amixxxxxx -n 5 -k keypair" to launch the instances with this AMI, and finally attempting to launch a cluster with "hadoop-ec2 launch cluster name 4" (this is for a cluster with 4 slave nodes). To be honest, I'm very new to the cloud and don't really have a clue what I'm doing.
563e344561a80130652683cf	X	The parameters I'm looking to change are the job parameters of hadoop, such as io.sort.mb (the amount of buffer memory for sorting between map and reduce phases) etc... they are controlled by changing a file (mapred-site.xml) which is stored in the hadoop folder.
563e344561a80130652683d0	X	@Jonathan Viccary OK, I see, you are using "hadoop-ec2" script from Hadoop's src/contrib/ec2/bin. If you want to start hadoop cluster this way... I looked at the script and I see that it launches instances automatically - you don't have to invoke ec2-run-instances manually. But you have to configure it: wiki.apache.org/hadoop/AmazonEC2#Setting_up
563e344561a80130652683d1	X	Actually hadoop-ec2 searches for suitable image in some bucket. You have to set up HADOOP_VERSION and S3_BUCKET variables to specify the image.
563e344561a80130652683d2	X	I'm trying to get set up on the Amazon Cloud to run some hadoop MapReduce jobs but I'm struggling to successfully create a cluster. I have downloaded the ec2 files, have my certificates and keypair file, but I believe it's the AMIs that are causing me trouble. If I'm trying to run a cluster with a master node and n slave nodes, I start n+1 instances using standard compatible AMIs and then run the code "hadoop-ec2 launch-cluster name n" in the terminal. The master node is successful, but I get an error when the slave nodes start to launch, saying "missing parameter -h (AMI missing)" and I'm not entirely sure how to progress. Also, some of my jobs will require an alteration in hadoops parameter settings (specifically the mapred-site.xml config file), is it possible to alter this file, and if so, how do I gain access to it? Is hadoop already installed on amazon machines, with this file accessible and alterable? Thanks
563e344561a80130652683d3	X	Have you tried Amazon Elastic MapReduce? This is a simple API that brings up Hadoop clusters of a specified size on demand. That's easier then to create own cluster manually. But once the jobflow is finished by default it shuts the cluster down, leaving you with outputs on S3. If what you need is simply to do some crunching, this may be the way to go. In case you need HDFS contents stored permanently (e.g. if you are running HBase on top of Hadoop) you may actually need own cluster on EC2. In this case you may find Cloudera's distribution of Hadoop for Amazon EC2 useful. Altering Hadoop configuration on nodes it will start is possible using EC2 Bootstrap Actions: Q: How do I configure Hadoop settings for my job flow? The Elastic MapReduce default Hadoop configuration is appropriate for most workloads. However, based on your job flow’s specific memory and processing requirements, it may be appropriate to tune these settings. For example, if your job flow tasks are memory-intensive, you may choose to use fewer tasks per core and reduce your job tracker heap size. For this situation, a pre-defined Bootstrap Action is available to configure your job flow on startup. See the Configure Memory Intensive Bootstrap Action in the Developer’s Guide for configuration details and usage instructions. An additional predefined bootstrap action is available that allows you to customize your cluster settings to any value of your choice. See the Configure Hadoop Bootstrap Action in the Developer’s Guide for usage instructions. About the way you are starting the cluster, please clarify: If I'm trying to run a cluster with a master node and n slave nodes, I start n+1 instances using standard compatible AMIs and then run the code "hadoop-ec2 launch-cluster name n" in the terminal. The master node is successful, but I get an error when the slave nodes start to launch, saying "missing parameter -h (AMI missing)" and I'm not entirely sure how to progress. How exactly you are trying start it? What exactly AMIs are you using?
563e344661a80130652683d4	X	Why API Gateway? Your image service runs in Lambda? Also, the "source" content-type? Presumably you are referring to the "response."
563e344661a80130652683d5	X	Is there a specific reason that the image must be returned by Lambda? An alternative would be to have the Lambda function upload the image to S3 and return a link to that image.
563e344661a80130652683d6	X	@Michael-sqlbot: I could throw up a proxy with something else but then I'd need to maintain it. AWS can presumably handle any load I might be able to throw at it. Plus the caching could be useful. The need for a proxy in the first place is google search seeing assets on my page as from my companies other service (SEO guy says they think we're scraping). We need to use the same assets so I just want it to look like they're from a different address. No lambda/manipulation needed for this thing, just a proxy. If you can recommend another service I'd check it out.
563e344661a80130652683d7	X	@JaredHatfield: Not using lambda for this. Just [maybe] API Gateway standalone.
563e344661a80130652683d8	X	I hesitate to mark this as the "correct" answer because I am still curious about maintaining Content-Type with AWS API Gateway but this was absolutely helpful and is what I ended up doing.
563e344661a80130652683d9	X	@kjs3 I'm glad it helped, and I don't blame you at all. Feel free to hold out for a more direct answer. Your question is interesting and perhaps we'll get an answer that addresses it more precisely. If I get an opportunity to do more extensive testing on AWS API Gateway, I will report on what I find.
563e344661a80130652683da	X	I'm trying to use AWS API Gateway as a proxy in front of an image service. I'm able to get the image to come through but it gets displayed as a big chunk of ASCII because Content-Type is getting set to "application/json". Is there a way to tell the gateway NOT to change the source Content-Type at all? I just want "image/jpeg", "image/png", etc. to come through.
563e344661a80130652683db	X	I was trying to format a string to be returned w/o quotes and discovered the Integration Response functionality. I haven't tried this fix myself, but something along these lines should work: Hope it works!
563e344661a80130652683dc	X	I apologize, in advance, for giving an answer that does not directly answer the question, and instead suggests you adopt a different approach... but based in the question and comments, and my own experience with what I believe to be a similar application, it seems like you may be using the the wrong tool for the problem, or at least a tool that is not the optimal choice within the AWS ecosystem. If your image service was running inside Amazon Lambda, the need for API Gateway would be more apparent. Absent that, I don't see it. Amazon CloudFront provides fetching of content from a back-end server, caching of content (at over 50 "edge" locations globally), no charge for the storage of cached content, and you can configure up to 100 distinct hostnames pointing to a single Cloudfront distribution, in addition to the default xxxxxxxx.cloudfront.net hostname. It also supports SSL. This seems like what you are trying to do, and then some. I use it, quite successfully for exactly the scenario you describe: "a proxy in front of an image service." Exactly what my image service and your image service do may be different (mine is a resizer that can look up the source URL of missing/never before requested images, fetch, and resize) but fundamentally it seems like we're accomplishing a similar purpose. Curiously, the pricing structure of CloudFront in some regions (such as us-east-1 and us-west-2) is such that it's not only cost-effective, but in fact using CloudFront can be almost $0.005 cheaper than not using it per gigabyte downloaded. In my case, in addition to the back-end image service, I also have an S3 bucket with a single file in it, attached to a single path in the CloudFront distribution (as a second "custom origin"), for the sole purpose of serving up /robots.txt, to control direct access to my images by well-behaved crawlers. This allows the robots.txt file to be managed separately from the image service itself. If this doesn't seem to address your need, feel free to comment and I will clarify or withdraw this answer.
563e344661a80130652683dd	X	It would be useful if you could clarify which kind of app you're thinking about, Regarding this sort of data you want the users to retrieve, the storaging method may vary :)
563e344661a80130652683de	X	I am developing an app that allow users to download images I designed from my app. There is no user uploading or user data backup functionality. So I am confused about which kind of server or cloud service I should use to store these images to be retrieved by my user? Which is the best solution for my situation? @Jorge33212
563e344661a80130652683df	X	Thank you for your advice and info! I am developing an app that allow users to download images I designed from my app. There is no user uploading or user data backup functionality. I would also like to track down the downloads of each image(Not sure how it can be achieved?x,D). So I am confused about which kind of server or cloud service I should use to store these images to be retrieved by my user? Which is the cost effective best solution for my situation? @inmyth
563e344761a80130652683e0	X	Does data in AWS S3 or Google Cloud Storage can handle a lot of requests by users? @inmyth
563e344761a80130652683e1	X	I just updated my answer with an example to track user download. For pricing, it clearly depends on your needs. Both AWS and Google have calculator which you can play around with. If you are starter with a a few data and few downloads, it's unlikely you're going to pay a couple dollars in a month. As whether they can handle a lot of requests ... these companies handle clients like New York Times and Dropbox. They sure can serve needs of a mortal.
563e344761a80130652683e2	X	I forgot to say. If you're new to AWS you get free basic use for a year. In Google you only get store credit which gets burned in a few months
563e344761a80130652683e3	X	So to store and retrieve images i will only need AWS S3 or Google Cloud Storage? There is no need to involve EC2 or google app engine stuff? @inmyth
563e344761a80130652683e4	X	I have seen many examples retrieving data from websites to be used as contents of applications. Now I am developing my own app, I have my own contents to be delivered to my users, where am I supposed to store my contents so it can be retrieved from my app? Is it a server? Or a cloud? I have done some research on Parse, AWS, Google Cloud Platform, they all seem pretty confusing to me. Is it correct that this is the right approach of storing app content? Please help and give me some advice. I have no experience in these, sorry...and thank you! :)
563e344761a80130652683e5	X	My rule of thumb as someone who only knows Google and Amazon AWS. If the data is images / movies / files: use AWS S3 or Google Cloud Storage If the data is text, use database system: You don't need a special service to host a database but we're paying for performance. All file's metadata (like url, name, etc) should be stored in database. Then create a server to serve the so-called REST API in the cloud that acts like the brain to connect all the dots, manage data operation and as much pre-processing as possible so your user devices won't get burdened. You can create a starter cloud instance for cheap in AWS EC2 or Google Cloud Engine. Case example: tracking user downloads This is very simple. The key is not to give the image url right away but wrap it in a request. Let's say the image is photo.jpg and the user who downloads or requests it is Bob (normally we get user information through a login in the app and we are mostly interested not in name but email or social media credentials). So you can create a server that processes a request through http://www.example.com/track?name=bob&image=photo By processing, your server takes all the parameters "Bob", "photo", and probably also time and location if needed then save these values in your database. Then you return the real url of photo.jpg as response to Bob's request. There you go, you have Bob tracked.
563e344761a80130652683e6	X	I've read just now about EC2 "ephemerality" (it's normal to crash frequently even with no much traffic), and I must be prepared for that (maybe with a service like CloudWatch). Thank you, and I liked your answer! :)
563e344761a80130652683e7	X	CloudWatch will only monitor instances performance, sometimes it might detect its down since they have 2 checks for connectivity but it doesn't always work correctly. Its best to have some 3rd party service check like pingdom or nagios etc... Being prepared for the crash is more than just knowing its down (though that's the first step) you need to make sure you can recover quickly. For the first few months I had my product in AWS I could barely sleep I spent so much time fixing stuff. But now I can sleep good knowing that a server crash will fix itself and wont cause downtime.
563e344761a80130652683e8	X	@bwight - regarding your second '1' - have you lost data that was written to an EBS drive?
563e344761a80130652683e9	X	Data no, server configuration files that i needed yes. Lost the servers quite a few times. Back in the middle of 2011 it was pretty problematic. Has died down a little but every month or two seems like another server crashes. I don't loose data anymore because i'm prepared so it doesn't bother me.
563e344761a80130652683ea	X	@bwight, what about Elastic Beanstalk? seem like it has something related to EC2
563e344761a80130652683eb	X	I'm newbie on AWS, and it has so many products (EC2, Load Balancer, EBS, S3, SimpleDB etc.), and so many docs, that I can't figure out where I must start from. My goal is to be ready for scalability. Suppose I want to set up a simple webserver, which access a database in mongolab. I suppose I need one EC2 instance to run it. At this point, do I need something more (EBS, S3, etc.)? At some point of time, my app has reached enough traffic and I must scale it. I was thinking of starting a new copy (instance) of my EC2 machine. But then it will have another IP. So, how traffic is distributed between both EC2 instances? Is that did automatically? Must I hire a Load Balancer service to distribute the traffic? And then will I have to pay for 2 EC2 instances and 1 LB? At this point, do I need something more (e.g.: Elastic IP)?
563e344761a80130652683ec	X	Welcome to the club Sony Santos, AWS is a very powerfull architecture, but with this power comes responsibility. I and presumably many others have learned the hard way building applications using AWS's services. You ask, where do I start? This is actually a very good question, but you probably won't like my answer. You need to read and do research about all the technologies offered by amazon and even other providers such as Rackspace, GoGrid, Google's Cloud and Azure. Amazon is not easy to get going but its not meant to be really, its focus is more about being very customizable and have a very extensive api. But lets get back to your question. To run a simple webserver you would need to start an EC2 instance this instance by default runs on a diskdrive called EBS. Essentially an EBS drive is a normal harddrive except that you can do lots of other cool stuff with it like take it off one server and move it to another. S3 is really more of a file storage system its more useful if you have a bunch of images or if you want to store a lot of backups of your databases etc, but its not a requirement for a simple webserver. Just running an EC2 instance is all you need, everything else will happen behind the scenes. If you app reaches a lot of traffic you have two options. You can scale your machine up by shutting it off and starting it with a larger instance. Generally speaking this is the easiest thing to do, but you'll get to a point where you either cannot handle all the traffic with 1 instance even at the larger size and you'll decide you need two OR you'll want a more fault tolerant application that will still be online in the event of a failure or update. If you create a second instance you will need to do some form of loadbalancing. I recommend using amazons Elastic Load Balancer as its easy to configure and its integration with the cloud is better than using Round Robin DNS or a application like haproxy. Elastic Load Balancers are not expensive, I believe they cost around $18 / month + data that's passed between the loadbalancer. But no, you don't need anything else to do scale up your site. 2 EC2 instances and a ELB will do the trick. Additional questions you didn't ask but probably should have. How often does an EC2 instance experience hardware failure and crash my server. What can I do if this happens? It happens frequently, usually in batches. Sometimes I go months without any problems then I will get a few servers crash at a time. But its defiantly something you should plan for I didn't in the beginning and I paid for it. Make sure you create scripts and have backups and a backup plan ready incase your server fails. Be ok with it being down or have a load balanced solution from day 1. Whats the hardest part about scalabilty? Testing testing testing testing... Don't ever assume anything. Also be prepared for sudden spikes in your traffic. You have to be prepared for anything if you page goes from 1 to 1000 people over night are you prepared to handle it? Have you tested what you "think" will happen? Best of luck and have fun... I know I have :)
563e344861a80130652683ed	X	+n! What a fine answer!
563e344861a80130652683ee	X	Nothing much to add to what @Craig Ringer said. You might be able to dance around with a slony replica and port redirects to get a config-change and restart done but that's a lot of trouble. Basically your setup is broken if a few minutes unavailability means lost (important) data.
563e344861a80130652683ef	X	@RichardHuxton Yeah, especially on AWS. Seriously. google.com/search?q=aws+outage
563e344861a80130652683f0	X	I'm using PostgreSQL v9.1 for my organization. The database is hosted in Amazon Web Services (EC2 instance) below a Django web-framework which performs tasks on the database (read/write data). The problem is, to backup this database in a periodic fashion in a specified format (see Requirements). Requirements: But I have the following constraints too. Is there a clever way to backup the master-db for my needs? Is there a tool which can automate this job for me? This is a very crucial requirement as data has begun to appear into the master-db since few days and I need to make sure there's replication of master-db on some standby-server all the time.
563e344861a80130652683f1	X	If, and only if, your entire database including pg_xlog, data, pg_clog, etc is on a single EBS volume, you can use EBS snapshots to do what you describe because they are (or claim to be) atomic. You can't do this if you stripe across multiple EBS volumes. The general idea is: Take an EBS snapshot using the EBS APIs using command line AWS tools or a scripting interface like the wonderful boto Python library. Once the snapshot completes, use AWS API commands to create a volume from it and attach the volume your instance, or preferably to a separate instance, and then mount it. On the EBS snapshot you will find a read-only copy of your database from the point in time you took the snapshot, as if your server crashed at that moment. PostgreSQL is crashsafe, so that's fine (unless you did something really stupid like set fsync=off in postgresql.conf). Copy the entire database structure to your final backup, e.g archive it to S3 or whatever. Unmount, unlink, and destroy the volume containing the snapshot. This is a terribly inefficient way to do what you want, but it will work. It is vitally important that you regularly test your backups by restoring them to a temporary server and making sure they're accessible and contain the expected information. Automate this, then check manually anyway. If your volume is mapped via LVM, you can do the same thing at the LVM level in your Linux system. This works for the lvm-on-md-on-striped-ebs configuration. You use lvm snapshots instead of EBS, and can only do it on the main machine, but it's otherwise the same. You can only do this if your entire DB is on one file system. You're going to have to restart the database. You do not need to restart it to change pg_hba.conf, a simple reload (pg_ctl reload, or SIGHUP the postmaster) is sufficient, but you do indeed have to restart to change the archive mode. This is one of the many reasons why backups are not an optional extra, they're part of the setup you should be doing before you go live. If you don't change the archive mode, you can't use PITR, pg_basebackup, WAL archiving, pgbarman, etc. You can use database dumps, and only database dumps. So you've got to find a time to restart. Sorry. If your client applications aren't entirely stupid (i.e. they can handle waiting on a blocked tcp/ip connection), here's how I'd try to do it after doing lots of testing on a replica of my production setup: I'd rather explicitly set pgbouncer to a "hold all connections" mode, but I'm not sure it has one, and don't have time to look into it right now. I'm not at all certain that SIGSTOPing pgbouncer will achieve the desired effect, either; you must experiment on a replica of your production setup to ensure that this is the case. Use WAL archiving and PITR, plus periodic pg_dump backups for extra assurance. See: ... and of course, the backup chapter of the user manual, which explains your options in detail. Pay particular attention to the "SQL Dump" and "Continuous Archiving and Point-in-Time Recovery (PITR)" chapters. PgBarman automates PITR option for you, including scheduling, and supports hooks for storing WAL and base backups in S3 instead of local storage. Alternately, WAL-E is a bit less automated, but is pre-integrated into S3. You can implement your retention policies with S3, or via barman. (Remember that you can use retention policies in S3 to shove old backups into Glacier, too). Outages happen. Outages of single-machine setups on something as unreliable as Amazon EC2 happen a lot. You must get failover and replication in place. This means that you must restart the server. If you do not do this, you will eventually have a major outage, and it will happen at the worst possible time. Get your HA setup sorted out now, not later, it's only going to get harder. You should also ensure that your client applications can buffer writes without losing them. Relying on a remote database on an Internet host to be available all the time is stupid, and again, it will bite you unless you fix it.
563e344861a80130652683f2	X	I hope you have not posted your actual AWS credentials. The error reads "The API key is invalid". Are your credentials correct? Do these credentials have access to the s3 bucket?
563e344861a80130652683f3	X	oops just fixed that, how do i check if they have access to the s3 bucket?
563e344861a80130652683f4	X	Use any method you like. rails console is not a bad idea. There are s3 clients available such as S3Fox, s3browser, s3cmd. Any one of these should be able to tell you if the keys are valid. Best is to dig into the documentation.
563e344861a80130652683f5	X	Could you explain this answer in more detail? I am having the same error and I am not pushing to AWS at all.
563e344861a80130652683f6	X	NVM, I see what you are talking about now the AWS settings are thrown into the Carrierwave init by default. I commented them out. TY
563e344861a80130652683f7	X	I am trying to deploy my site to my locomotivecms site, everything is working but this its something to do with my carrier wave file could someone point out what is wrong with it. Here is my wagon.log
563e344861a80130652683f8	X	I had to go to the Iam section in the console and create a group and user which had connection to s3 and created a different access key and secret pass
563e344961a80130652683f9	X	Basically I'm structuring my app similar to this GitHub project: https://github.com/zackargyle/angularjs-django-rest-framework-seed Is it possible to deploy both the backend and frontend onto a single PaaS such as Heroku/Elastic Beanstalk? Having a separated REST backend and JavaScript frontend seems like a cleaner/more scalable way to do things rather than trying to mix them together like [django-angular]: (http://django-angular.readthedocs.org/en/latest/index.html/), or having a REST backend mix with the Django app like http://blog.mourafiq.com/post/55099429431/end-to-end-web-app-with-django-rest-framework If it is not possible to deploy it easily onto Elastic Beanstalk, is there an easy way to deploy the Django backend onto Elastic Beanstalk, and AngularJS frontend to Amazon EC2/S3 with minimal configuration? I realize there's a similar discussion before this: Client JS + Django Rest Framework but it lacks more specific details.
563e344961a80130652683fa	X	I'm in the exact same boat with AngularJS as my client and django-rest-framework as my service. I also have the same type of git setup where the server and client code are siblings in the same repository. I don't have any experience with Heroku and I'm new to beanstalk but I was able to deploy my site and it's working on AWS beanstalk. With beanstalk there are two ways I know of to deploy your code. I automated the zip creation using a python script. Amazon's walkthrough provides an example python zip. You have to structure it properly, mine looks roughly like this I know you didn't specifically ask but the .config file inside .ebextensions/ took me way too long to get working. It can be formatted as YAML or JSON (can be confusing at first as every blog shows it differently). This blog helped me out quite a bit just be careful to use container_commands: and not commands:. I lost a few hours to that... In the zip you create (if you follow the beanstalk guides on django) the client code in your /static/ folder is automatically pushed to s3 when you deploy. This setup isn't perfect and I plan on fine tuning things but it's working. Here are some downsides I ran into that I haven't solved yet: UPDATE 4-17-2014 I further refined this setup so I no longer have to go to mysite.com/static/ to load my index.html. To do so I used a django class based view to map index.html to the root of my site. My urls.py looks like and in my settings.py I configured TEMPLATE_DIRS as follows I use ../static because my static directory is a sibling of my app directory. The last piece was to update my Gruntfile.js so 'grunt build' prefixes all the relative URLs in my angular code with the static folder. I used grunt-text-replace for this. It's the last task that runs after my code is sitting minified in a /dist folder. The downside to this approach is I'll have to update this task if I ever add static content to a new subfolder besides scripts, bower_components, styles, etc. Now django will serve my index.html page but everything else in my /static/ directory can benefit from a CDN.
563e344961a80130652683fb	X	Are the library dependencies within the context of the classpath?
563e344961a80130652683fc	X	FYI manually managing jars like this is very painful. Try maven or one of the other tools for this task. zeroturnaround.com/rebellabs/…
563e344961a80130652683fd	X	I have /aws folder with aws jars and my main program Aws.class aws is also a package: Aws.java program: when calling the program by going one level up and isseuing a command: java aws.Aws i'm getting an error:  classpath is .:/aws
563e344a61a80130652683fe	X	I guess you should include aws-java-sdk-core-1.9.23.jar also in your /aws folder. com.amazonaws.auth.AWSCredentials and com.amazonaws.auth.BasicAWSCredentials resides in that particular jar. Hope it helps. Thanks.
563e344a61a80130652683ff	X	I think it might have something to do with my RSA key, since I've been having issues with that as well, but as far as I know, Rubber shouldn't use the RSA key should it?
563e344a61a8013065268400	X	I've also tried changing the user to ubuntu and then it just gives me exactly the same error, just with ubuntu instead of root. Why does it use the RSA key? How do I configure the RSA key for EC2?
563e344a61a8013065268401	X	Oh ok, I've updated my answer.
563e344a61a8013065268402	X	I've already done this. Tried it several times with new Rails test apps and it won't work. I'm also linking to the right key pair, the right place, so it shouldn't be that,
563e344a61a8013065268403	X	Any other suggestions?
563e344a61a8013065268404	X	Have you told Rubber the location of your keypair? Can you post your rubber.yml? I've updated my answer.
563e344a61a8013065268405	X	I'm following this tutorial from Railscast on how to deploy your rails-app to EC2: http://railscasts.com/episodes/347-rubber-and-amazon-ec2 I did something and now I keep getting this error whenever I try to deploy: It's a very obscure error and seems to be specifically mac related. Another user following the tutorial also had that error: http://railscasts.com/episodes/347-rubber-and-amazon-ec2?view=comments#comment_158643. And this guy experienced something similar as well: https://github.com/rubber/rubber/issues/182. I've been through every blog post on this issue and nothing has come up. How would you troubleshoot this? UPDATE This is the full stack trace I get when trying to connect through ssh: UPDATE And here's my rubber.yml:
563e344a61a8013065268406	X	Rubber expects to be given your EC2 credentials in the YAML file config/rubber/rubber.yml To find these values: Rubber uses these credentials to configure your AWS infrastructure. When connecting to an actual server, it will require your secret RSA key. You need to tell rubber the name of the keypair (as shown in your EC2 dashboard), and its location. Again in config/rubber/rubber.yml
563e344a61a8013065268407	X	I"m not sure what I did wrong and what I did right here. There seems to be a few bugs in rubber. In the end I made a new app, deployed the way it's done in the second half of the Railscast (with individual instances). After that I logged into AWS, clicked in the instance -> actions -> connect. There you can see the "correct" way to connect to the instance.
563e344a61a8013065268408	X	Instance profiles allow you to associate an IAM role to your instance. Does your instance need specific permissions (accessing some AWS resources...)? Does your deployment fail?
563e344a61a8013065268409	X	Nothing fails; AWS just recommends associating an instance profile with the environment and I would like to have a better understanding as to what that means.
563e344b61a801306526840a	X	Is there any more that you can say about the "faster path to deploy your application"? Is there any documentation on this?
563e344b61a801306526840b	X	I can't see any instructions there (at least none that I can follow) that allows me to "associate an instance profile with this environment". What are the steps to doing that?
563e344b61a801306526840c	X	When using the web UI for AWS Elastic Beanstalk Environment management, I see: Code change deployments will complete faster if you associate an instance profile with this environment. (also see this forum post mentioning the same thing: http://www.infosys.tuwien.ac.at/staff/leitner/cs_study/forum/viewtopic.php?pid=186#p186) What is an instance profile? Why does it matter? How does it work / what is it doing? I found these articles: but I still don't understand instance profiles.
563e344b61a801306526840d	X	Like Celine said in the comment above, instance profiles allow you to associate an IAM role to your instance. This IAM role must be provided with certain permissions to access your AWS resources. Your EC2 instance (launched by Elastic Beanstalk) can then perform certain extra tasks. For example if you launch a worker tier environment with Elastic Beanstalk the daemon needs to poll from SQS queue, publish metrics to cloudwatch from the EC2 instance. This means that the EC2 instance needs some credentials to poll from a queue. If you have an IAM role with appropriate policies associated with the EC2 instance you essentially permit your instance to call SQS using the credentials of that role. You can do other interesting things like automatic log publication to your S3 bucket if you have an instance profile associated with your environment. Having an instance profile allows you to control permissions you want to give to the instance and also frees you from storing long term credentials on all your EC2 instances. From the documentation: Instance profiles provide applications and services access to AWS resources. For example, your application may require access to DynamoDB. Every API request made to AWS services must be signed using AWS security credentials. One way to grant applications access to AWS resources is to distribute your credentials to each instance; however, distributing long-term credentials to each instance is challenging to manage and a potential security risk. Instead, you can create an IAM role with the permissions that applications require when the application makes calls to other AWS resources. When AWS Elastic Beanstalk launches the Amazon EC2 instances, it uses the instance profile associated with that role. All applications that run on the instances can use the role credentials to sign requests. Because role credentials are temporary and rotated automatically, you don't have to worry about long-term security risks. The message you are seeing on the console regarding console is recommending you to use an instance profile because that allows your EC2 instance to take a faster path to deploy your application version each time you update your environment with a new copy of your source code. The end result is the same but having an instance profile enables optimizations in deployment speed which are not possible without one. You can read more about instance profiles with Elastic Beanstalk here. Although you can create a custom role and associate it with a beanstalk environment by giving it appropriate permissions, you can for convenience just get a default role when you launch an environnment using the AWS console. You will have the option of choosing which role you want to associate with an environment in the create environment wizard.
563e344b61a801306526840e	X	Sorry could you elaborate a bit more on that? I don't understand what you mean?
563e344b61a801306526840f	X	I recently installed a drupal from scratch and I'm installing aditional modules according to my needs. The biggest problem I have is that if I go in admin/content/node-type/NODETYPENAME/fields (the page where I can edit/add/delete the CCK fields) I can't re-use defined fields the Option Add existing fields simply disapeared (probably it's some module that I don't know about) I have only New Field and New group ..in my other installation I have the Existing Field option where I can reuse already defined fields... AM I missing something? my CCK module page looks like this:
563e344b61a8013065268410	X	Ok, here was the answer, Using existing fields doesn't appear ONLY when you have multiple content types defined and custom CCK fields attached to it
563e344b61a8013065268411	X	I have a finite set of images (> 1000) that will be used on a site hosted on the Google App Engine. In reading the documentation, I noticed there's the Images library that provides various image manipulation capabilities. The one method that caught my attention is This returns a URL that serves the image. The documentation then goes on to say: This URL format allows dynamic resizing and cropping, so you don't need to store different image sizes on the server. Images are served with low latency from a highly optimized, cookieless infrastructure. I would like to know if there is an advantage, speed wise, between serving the images from a static folder or using the Image service.
563e344b61a8013065268412	X	Dynamically resizing images and reading out of the blob store as the other answer here mentions is never going to be that fast. You need to store static copies of your images if you want performance. If for no other reason than caching. Prefer the static version if you can, and prefer a CDN above the static version. The choice between blob store and static can also revolve around if your content is from you, or submitted by users. If submitted by users, then the blob store is usually the way to go if you're stuck with storing the images on app engine. In practice, we found the internals of GAE and pricing to be less than adequate for serving images. Instead, consider a CDN if you're especially worried about serving images fast. For example, you could go with Amazon Cloudfront/S3 combo or Rackspace Cloudfiles. In both cases, there was absolutely no comparison speed-wise between using the serving url vs. a CDN. Simply push your content either directly to the CDN on creation, or keep a fallback copy in the blobstore. What we do in our app is we keep uploaded images by users in the blobstore first, and queue up a background task to submit the content to the CDN. The various sizes such as thumbnails are generated using our CDN's API that we call via REST from App Engine. If somehow the user hits our site before all of this happens and we need these images, we simply serve them the dynamic version you referenced in your question. It's much slower, but it ensures we always have a fall-back while the system is refreshing or generating the CDN content. 99% of the time though, the user hits the fast version in the CDN. EDIT: I want to point out that certainly the app engine static content is cached, just not with the efficiency of a decent CDN provider in my real world experience running a very image heavy app engine site with lots of unique hits per day.
563e344c61a8013065268413	X	Images service does not store or serve images. Images are served either from a BlobStore, or from Google Cloud Storage, depending on which option you use. Both options offer very fast response times. I believe that serving files from a static folder will offer very similar performance. UPDATE: I noticed that with the new pricing on GAE they will charge the same price for static files as they do for Google Cloud Storage or BlobStore, but the free quota is larger (5GB) for GCS and BlobStore than for code and static files (1GB).
563e344c61a8013065268414	X	Are you building your JAR with sbt assembly? Please also post the command you're using to create the EMR cluster
563e344c61a8013065268415	X	Thanks, @DanOsipov! I have updated the post with my current status. Please let me know if you need any other info.
563e344c61a8013065268416	X	Basically I need to run a scalding job on EMR. The same job runs perfectly fine on local hadoop on my macbook, but fails on Hadoop on EMR. I am trying hard to get help for this issue in the cascading-user and scala-user groups as well, and haven't been able to. So far I haven't made much progress after trying various changes in the past couple days. Here is the error before I delve into the details: Exception in thread "main" java.lang.NoSuchMethodError: scala.Predef$.ArrowAssoc(Ljava/lang/Object;)Ljava/lang/Object; at com.aggregation.job.DataAggregation$.(DataAggregation.scala:30) at com.aggregation.job.DataAggregation$.(DataAggregation.scala) at com.aggregation.job.DataAggregation.main(DataAggregation.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212) They said it might be a scala conflict with the binaries, but I couldn't see anything obvious. It would be great if someone could help with figuring it out. Environment: Amazon EMR AMI: 3.8.0 (which includes Scala 2.11.1, Hadoop 2.4.0, Java 1.7.0_76 - AMI details here) Application environment: Scalding 0.15.0, Scala 2.11.1, Java 1.7.0_80, Hadoop 2.4.0 I have tried various changes to configuration and even manually installing a higher scala version in EMR, but so far the error is not going away. Please help! Thank you. L Setup build.sbt: dependencies.sbt: assembly.sbt project/build.properties: project/assembly.sbt: Finally this is the dependency tree using the sbt-dependency-graph plugin to see if I've got the right versions. Sorry, it's quite long, if I should be displaying this info any other way, please recommend suggestions. sbt dependency tree: Further info as requested: I build the fat jar using 'sbt assembly', and currently I'm using the AWS console with a "Custom JAR" step to test this out before automating the process. UPDATE: I was able to get past the above error by providing the HADOOP_CLASSPATH pointing to the scala 2.11.1 jars, while excluding the same from the sbt assembly step. This was passed in using hadoop-user-env.sh and seemed to work for the master node. However once it got to the mapper step it once again failed with another Scala error. Now I am stuck on this step. Assuming this is because the mappers and reducers aren't seeing the HADOOP_CLASSPATH update, I tried including the -libjars argument pointing to the scala jar files on hadoop master itself. But this (below) doesn't seem to be working.
563e344c61a8013065268417	X	Fixed. So it does happen that there were multiple scala jars in the EMR instances, and they weren't coming from my application jar. The 2.10 jar was hiding in /usr/share/aws/emr/emrfs/lib apart from the installed location for the 2.11 binaries under /usr/share/scala. So I got rid of the 2.10 jar in all instances of the cluster, and my job completed successfully. Now I will create a bootstrap action for this. FYI, these are the paths it was present under:
563e344c61a8013065268418	X	can't use rvm on beta
563e344c61a8013065268419	X	i'm new to ruby on rails.The problem i'm facing is I've made a project on ruby on rails(rails 3.2.11)(ruby 1.9.3) The gems i'm using have higher version. Now i've to commit the project on beta version of my site. In the beta version,the gems are installed but they are of lower version and of course i can't update the gems there cause' other projects are dependent on them and they will stop working. Please help me-- Here is my gem file gem 'rails', '3.2.11' gem 'will_paginate' gem 'mysql2' gem 'devise', '1.1.rc0' gem 'json' group :assets do gem 'sass-rails', '~> 3.2.3' gem 'coffee-rails', '~> 3.2.1' gem 'uglifier', '>= 1.0.3' end gem 'jquery-rails' and here's my environment.rb require File.expand_path('../application', FILE) OfficeSpace::Application.initialize! Please tell me how i can commit this project on beta.(Tell what changes i've to make in my project as i can't make any on beta) These are the gems beta is using abstract (1.0.0) actionmailer (3.2.8, 3.0.4, 2.3.6) actionpack (3.2.8, 3.0.9, 3.0.4, 2.3.6) actionwebservice (1.2.6) activemodel (3.2.8, 3.0.9, 3.0.4, 3.0.3) activerecord (3.2.8, 3.0.4, 2.3.6, 2.3.2) activerecord-import (0.2.9) activeresource (3.2.8, 3.0.4, 2.3.6, 2.3.2) activesupport (3.2.8, 3.0.4, 3.0.3, 2.3.6) addressable (2.3.2, 2.2.6) algorithms (0.3.0) amazon-ec2 (0.9.17, 0.9.15) ar-extensions (0.9.5, 0.9.2) arel (3.0.2, 3.0.0, 2.0.10) atk (1.1.6) atom (0.3) attr_required (0.0.5) autoparse (0.3.2, 0.2.3) aweber (1.5.0) aws-s3 (0.6.2) aws-ses (0.4.4, 0.4.2) bayes_motel (0.1.0) bitly (0.6.1) blekko (0.0.3) bluecloth (2.1.0) builder (3.0.0, 2.1.2) bundler (1.0.10) cairo (1.12.3) childprocess (0.3.6) cobravsmongoose (0.0.2) columnize (0.3.2) configatron (2.10.0) cookiejar (0.3.0) crack (0.1.8) createsend (2.5.0) curb (0.7.18, 0.7.10) daemon_controller (0.2.5) daemons (1.1.9) data_objects (0.10.7) diff-lcs (1.1.3) dm-core (1.2.0) dm-do-adapter (1.2.0) dm-sqlite-adapter (1.2.0) do_sqlite3 (0.10.7) domain_name (0.5.3) em-http-request (1.0.3) em-socksify (0.2.1) em-twitter (0.1.4) erubis (2.7.0, 2.6.6) eventmachine (1.0.0.rc.4) extlib (0.9.15) facebooker2 (0.0.11, 0.0.10) faraday (0.7.4) faraday_middleware (0.8.8, 0.7.0) fastercsv (1.5.5) fastthread (1.0.7) fb_graph (2.6.4, 2.4.1) feedtools (0.2.29) ffi (1.2.0) file-tail (1.0.5) gcm (0.0.2) gcm_on_rails (0.1.3) geocoder (1.1.6, 1.1.3) geoip (1.1.2) geokit (1.6.5) gibbon (0.3.5) glib2 (1.1.6) google-api-client (0.4.6, 0.3.0) google-search (1.0.2) google_alerts (0.0.1) google_plus (0.2.0) googleajax (1.0.1) googlebase (0.2.1) googlereader (0.0.4) grabz_it (0.0.4) grabzit (1.1.0) has_vimeo_video (0.0.5) hashie (1.2.0, 1.1.0) hashr (0.0.22) highline (1.6.2) hike (1.2.1) hominid (3.0.2) hpricot (0.8.3) htmlentities (4.3.1) httmultiparty (0.3.6) http_parser.rb (0.5.3) httpadapter (1.0.1) httparty (0.8.3) httpauth (0.2.0) httpclient (2.2.4) hubspot (0.0.2) i18n (0.6.0, 0.5.0, 0.4.0) imgkit (1.3.7) instagram (0.8.5) journey (1.0.4, 1.0.3) json (1.5.1, 1.4.6) jwt (0.1.5, 0.1.4) koala (1.5.0) launchy (2.1.2, 2.0.5) libwebsocket (0.1.7.1) libxml-ruby (1.1.4) linecache (0.43) linkedin (0.3.7) locale (2.0.5) mail (2.4.4, 2.4.0, 2.2.19) mechanize (2.5.1) memcache-client (1.8.5) mime-types (1.18, 1.16) mislav-will_paginate (2.3.11) mogli (0.0.37) multi_json (1.3.7, 1.0.0) multi_xml (0.5.3, 0.2.2) multipart-post (1.1.3) mysql (2.8.1) n_gram (0.0.1) net-http-digest_auth (1.2.1) net-http-persistent (2.7) nokogiri (1.5.5, 1.4.4, 1.4.3.1) nori (1.1.3) ntlm-http (0.1.1) oauth (0.4.5, 0.4.4, 0.4.3) oauth2 (0.8.0, 0.5.2, 0.5.0) omniauth (1.0.1) omnicontacts (0.2.1) pango (1.1.6) passenger (3.0.2, 2.2.15) payment (1.0.1) pkg-config (1.1.4) polyglot (0.3.3, 0.3.2, 0.3.1) r_hapi (0.1.2) rack (1.4.4, 1.4.1, 1.2.5, 1.2.3, 1.1.0) rack-cache (1.2, 1.1) rack-mount (0.6.14, 0.6.13) rack-oauth2 (1.0.0, 0.14.2) rack-openid (1.3.1) rack-protection (1.3.2) rack-ssl (1.3.2) rack-test (0.6.1, 0.5.7) rails (3.0.9, 3.0.4, 2.3.6) railties (3.2.8, 3.0.9, 3.0.4) rake (0.9.2.2, 0.9.2, 0.8.7) rbx-require-relative (0.0.5) rdoc (3.12, 3.9.1) RedCloth (4.2.8) rest-client (1.6.7) rest-open-uri (1.0.0) retryable (1.3.1) rspec (2.11.0) rspec-core (2.11.1) rspec-expectations (2.11.1) rspec-mocks (2.11.1) ruby-debug (0.10.4) ruby-debug-base (0.10.4) ruby-hmac (0.4.0) ruby-openid (2.2.2) ruby-openid-apps-discovery (1.2.0) rubygems-update (1.5.0) rubyzip (0.9.9) sanitize (2.0.3) selenium-webdriver (2.27.1) signet (0.4.1) simple-rss (1.2.3) simple_oauth (0.1.9, 0.1.5) simple_youtube (3.0.0) sinatra (1.3.3) spell_checker (0.0.2) sprockets (2.1.3, 2.1.2) spruz (0.2.5) sqlite3 (1.3.6, 1.3.4) stemmer (1.0.1) supermodel (0.1.6) syntax (1.0.0) thor (0.16.0, 0.14.6) tilt (1.3.3) tmail (1.2.7.1) treetop (1.4.10, 1.4.9) trollop (2.0) truncate_html (0.5.5) tweetstream (2.1.0) twitter (1.6.2) twitter_oauth (0.4.3) tzinfo (0.3.33, 0.3.29, 0.3.24) unf (0.0.5) unf_ext (0.0.5) uuidtools (2.1.3) vapir-common (1.10.1) vapir-firefox (1.10.1) vimeo (1.5.3) webrobots (0.0.13) websocket (1.0.4) whatlanguage (1.0.0) will_paginate (2.3.11) xml-simple (1.1.1, 1.0.14, 1.0.12) yajl-ruby (1.1.0) yamler (0.1.0) yard (0.8.3) youtube_it (2.1.7) youtube_search (0.1.6) And these are the gems my project is using Gems included by the bundle: actionmailer (3.2.11) actionpack (3.2.11) activemodel (3.2.11) activerecord (3.2.11) activeresource (3.2.11) activesupport (3.2.11) arel (3.0.2) builder (3.0.4) bundler (1.2.3) coffee-rails (3.2.2) coffee-script (2.2.0) coffee-script-source (1.4.0) devise (1.1.rc0) erubis (2.7.0) execjs (1.4.0) hike (1.2.1) i18n (0.6.1) journey (1.0.4) jquery-rails (2.2.1) json (1.7.7) mail (2.4.4) mime-types (1.21) multi_json (1.6.0) mysql2 (0.3.11) polyglot (0.3.3) rack (1.4.5) rack-cache (1.2) rack-ssl (1.3.3) rack-test (0.6.2) rails (3.2.11) railties (3.2.11) rake (10.0.3) rdoc (3.12.1) sass (3.2.5) sass-rails (3.2.6) sprockets (2.2.2) thor (0.17.0) tilt (1.3.3) treetop (1.4.12) tzinfo (0.3.35) uglifier (1.3.0) warden (0.10.7) will_paginate (3.0.4) So you see there's incompatibility.How to resolve that issue??
563e344c61a801306526841a	X	Sounds like you need to use rvm. It allows you to have multiple gemsets that you can assign to a project, giving you the ability to pick what version of gems each project uses. Check out: https://rvm.io
563e344c61a801306526841b	X	How bizarre! I can send you some of my code and if you can figure out how to make it stop halting a machine, I'll be very happy. :)
563e344c61a801306526841c	X	Can you clarify several things: (1) Are you using an EBS or S3 backed instance? (If using EBS, do you intend to terminate or stop the instance?) (2) How is R being used? Interactively, via a script, via one of these Hadoop packages, via instance "user data", or some other means?
563e344c61a801306526841d	X	Great extension of sudo make sandwich which even cleans up the kitchen. Nice.
563e344d61a801306526841e	X	xkcd.com/149
563e344d61a801306526841f	X	nope... halting stops the instance. At least that's the way my instances are configured :)
563e344d61a8013065268420	X	keep in mind that I want to "stop" not "terminate" an instance. I'm using and EBS backed instance so I want to only stop it in order to preserve state. If I terminated it, I would lose state.
563e344d61a8013065268421	X	halt should do the trick, or you can use the ec2-stop-instances. I like what Eric Hammond suggests, via the at command.
563e344d61a8013065268422	X	NB: There may not be a root password if the AMI or accounts are not set up with a password. Such is the beauty of having 17 tons of cryptographic keys sitting around and the joy of mastering private keys, certificates, RSA, x.509, and more.
563e344d61a8013065268423	X	that's a really good point Iterator. I have my analysis instance set up with passwords, but this might not need be so. Thanks for the reminder.
563e344d61a8013065268424	X	@Iterator Thanks.
563e344d61a8013065268425	X	I have a few work flows where I would like R to halt the Linux machine it's running on after completion of a script. I can think of two similar ways to do this: Are there other easy ways of doing this? The use case here is for scripts running on AWS where I would like the instance to stop after script completion so that I don't get charged for machine time post job run. My instance I use for data analysis is an EBS backed instance so I don't want to terminate it, simply suspend. Issuing a halt command from inside the instance is the same effect as a stop/suspend from AWS console.
563e344d61a8013065268426	X	I'm impressed that works. (For anyone else surprised that an instance can stop itself, see notes 1 & 2.) You can also try "sudo halt", as you wouldn't need to run as a root user, as long as the user account running R is capable of running sudo. This is pretty common on a lot of AMIs on EC2. Be careful about what constitutes an assumption of R quitting - believe it or not, one can crash R. It may be better to have a separate script that watches the R pid and, once that PID is no longer active, terminates the instance. Doing this command inside of R means that if R crashes, it never reaches the call to halt. If you call it from within another script, that can be dangerous, too. If you know Linux well, what you're looking for is the PID from starting R, which you can pass to another script that checks ps, say every 1 second, and then terminates the instance once the PID is no longer running. I think a better solution is to use the EC2 API tools (see: http://docs.amazonwebservices.com/AWSEC2/latest/APIReference/ for documentation) to terminate OR stop instances. There's a difference between the two of these, and it matters if your instance is EBS backed or S3 backed. You needn't run as root in order to terminate the instance - the fact that you have the private key and certificate shows Amazon that you're the BOSS, way above the hoi polloi who merely have root access on your instance. Because these credentials can be used for mischief, be careful about running API tools from a given server, you'll need your certificate and private key on the server. That's a bad idea in the event that you have a security problem. It would be better to message to a master server and have it shut down the instance. If you have messaging set up in any way between instances, this can do all the work for you. Note 1: Eric Hammond reports that the halt will only suspend an EBS instance, so you still have storage fees. If you happen to start a lot of such instances, this can clutter things up. Your original question seems unclear about whether you mean to terminate or stop an instance. He has other good advice on this page Note 2: A short thread on the EC2 developers forum gives advice for Linux & Windows users. Note 3: EBS instances are billed for partial hours, even when restarted. (See this thread from the developer forum.) Having an auto-suspend close to the hour mark can be useful, assuming the R process isn't working, in case one might re-task that instance (i.e. to save on not restarting). Other useful tools to consider: setTimeLimit and setSessionTimeLimit, and various checkpointing tools (I have a Q that mentions a couple). Using an auto-kill is useful if one has potentially badly behaved code. Note 4: I recently learned of the shutdown command in package fun. This is multi-platform. See this blog post for commentary, and code is here. Dangerous stuff, but it could be useful if you want to adapt to Windows. I haven't tried it, though. Update 1. Three more ideas:
563e344d61a8013065268427	X	However, the downside is having your root password in plain text in the script.
563e344d61a8013065268428	X	AFAIK those ways you mentioned are the only ones. In any case the script will have to run as root to be able to shut down the machine (if you find a way to do it without root that's possibly an exploit). You ask for an easier way but system("halt") is just an additional line at the end of your script.
563e344d61a8013065268429	X	sudo is an option -- it allows you to run certain commands without prompting for any password. Just put something like this in /etc/sudoers (of course replacing with the name of user running R) and system('sudo halt') should just work.
563e344d61a801306526842a	X	When you say "file storage", do you mean Cloud Files?
563e344d61a801306526842b	X	I mean about network storage evolution....from DAS, NAS , SAN.etc..these are mostly used in cloud if i am not worng
563e344d61a801306526842c	X	Thank you so much for this in depth clarification and also for your valuable time
563e344e61a801306526842d	X	I came across file storage and block storage and was exploring and based on what I could understand, it is as follows: Block level storage A. Requires a separate volume and an operating system on which it will be mounted as a separate hard disk and can be used to store raw files. B. Is used on the server side and for performance over capacity. C. It has its own backup software, etc. D. It uses fibre channel and iSCSI communication for access. E. It has no access to anything inside, it's just blocks of data. File level Storage A. File level storage does not require a separate OS to operate. B. It is chosen for capacity over performance. C. It does not have a separate backup software of its own. D. It is the the file access system which is done through NAS etc… Based on the above, my questions are: Block level storage questions: A. Block level storage is just bare metal storage with no access to files, so why does it require a separate OS because OS will also have a file management system, so it will become more of a hyrbrid with both File and block storage options? B. If the above is true, then are SSD and SATA hard-disks kinds of block storage devices? C. If the above is true, then block level storage is also present on the user side because of the SSDs and others, why then is it used only on server side? D. Why is backup software required when we do not have access to any files inside? File level storage questions: A. How does file storage does not require separate software to operate? B. When you have have access to files in this and can modify, read and write files, then why is backup software not present? C. Just like we have finder on Mac OS X, which is also file level storage, wherein we get to manage our files? Please clarify my above questions and let me know where I am wrong.
563e344e61a801306526842e	X	It seems to me that you're confusing cloud block storage solutions like Rackspace's Cloud Block Storage or Amazon Elastic Block Store with the management of actual network storage hardware. When you launch a cloud server, you don't get anything like a public iSCSI endpoint to attach your own storage to; since iSCSI is cleartext that would be a giant security hole. Instead, cloud providers offer block storage as a layer of abstraction over network (or local) storage solutions within the datacenter your server is actually located within. You can think of block storage like a virtual USB key - you can plug a volume in to a server, mount it like any other drive, read and write files on it, then detach it and re-attach it to another - and ideally you don't have to worry about the mechanism that's physically used to accomplish this (as long as your performance is satisfactory, of course). Points A, B and C under "file storage" really sound to me like they're describing object storage, like Cloud Files or S3, instead. You generally don't mount these directly like a drive (although it is technically possible), and instead use them through a REST API of some sort. They offer the potential of much greater storage capacity than you'd get on a typical hard drive, with the costs of being eventually consistent and having higher-latency access. On Rackspace, you can also publish them to a content delivery network to do things like host a static website. Useful, but it doesn't sound like what you're looking for. To answer your specific questions: Block level storage. A. Attaching a block storage volume to a server is just like plugging in a USB key or hooking up a physical drive. It exposes raw, uninterpreted device access. So, just like any other drive, you need an OS on a server to format it and do useful work with its data. A volume has no processor, so unless it's attached to a cloud server, it can't do anything useful! B. When you create a block storage volume at Rackspace, you get to choose whether you want it to be backed by an SSD or a SATA drive, to give you some control over performance tradeoffs: SSDs will be much faster but more expensive. Both are kinds of block storage devices, though. C. I'm not sure what you mean by "the user side" here. Because of the security concerns I linked before, you can't mount a block storage volume outside of the datacenter it's created in, so you can only attach it to cloud servers within that datacenter. You can't, for example, mount one from your desktop. D. Snapshots (Block Storage backups) are just byte-for-byte copies of whatever you've put there, so it actually doesn't deal with the filesystem at all. File level storage. A. If you are talking about Cloud Files: you don't need specific software because it uses a REST API, so you can use anything that talks HTTP to manipulate it, such as Rackspace's web UI. B. That's a good question! I have heard other customers request some kind of backup capability, but I don't know where it is on the roadmap. In the meantime, you can use the COPY operation to perform server-side copies of your stored objects and do backups manually that way. C. Since you generally don't mount Cloud Files containers directly into your filesystem like I said before, you can't use Finder directly to manipulate objects - instead you use the API, an SDK, or the web UI to work with them.
563e344e61a801306526842f	X	Hi tvanfosson: Thanks for the reply. I understand your post, and that's the reason I have the question. If I should not test my FakeImplementation of the IUserRepository, then I have to test the SQLUserRepository implementation of IUserRepository, which is the one that I am using. But If I do that then I am testing with Database and I have read that this should be avoided. I know I am missing something to make the connection, I just don't know what it is. Thanks.
563e344e61a8013065268430	X	@Geo: By testing the actual SqlRepository you are already in Integration-Test land.. For a unit test not testing the SqlRepository at all is totally valid.
563e344e61a8013065268431	X	I cheat. My repositories are actually backended by a LINQ data context. I mock out the LINQ data context when testing the repository. I don't bother unit testing the data context because it is generated code. You will need some integration tests, though, if you go this route -- you would anyway, right? -- because LINQ to SQL behaves differently than LINQ to Objects (the mock version).
563e344e61a8013065268432	X	This answer doesnt address the question and just confirms what @Geo has asked. Please provide an example
563e344e61a8013065268433	X	Love the post thanks.
563e344f61a8013065268434	X	:) wikid :) glad this helps at least one person :)
563e344f61a8013065268435	X	Doesn't "InitializeSqlServerTestData" become very unwieldy when you have a complex object graph? How do you account for the fact that tests at higher levels will require different test data? e.g. Some inactive users, some overdue users, no users, users from company A, and so on.
563e344f61a8013065268436	X	Not sure @betitall - this was too long ago now. And i've moved on from doing this. It's all RavenDb for me now :) For SqlServer, I've given up with unit testing it and faking our db's. to much PITA.
563e344f61a8013065268437	X	link not working
563e344f61a8013065268438	X	While using Repository pattern I find it difficult to understand the reason of designing the software with TDD technique while in reality you will have to implement the Interface for your repository in your persistence dataset. To make my point clear I will submit an example: I have the following Interface on my domain model: I have the following implementation of the interface for testing purposes: Now I create a few tests: My question is, after I test all these with my FakeUserRepository implementation, I have to go back and implement the IUserRepository on my actual persistence dataset (i.g. SQL), and I have to implement again the code, so my unit testing is not actually checking the code that I am actually using on my application. Maybe I am missing something. Thanks as always! Below then my Persistent data access repository which is the one that is supposed to be under test (by my mind at least), but then I should not test hooked to the database:
563e344f61a8013065268439	X	Never test a mock. The class under test should always be a real instance of the class, though you can and should mock any of its dependencies so you can test it in isolation.
563e344f61a801306526843a	X	I'll explain what I'm doing, why and how much milage I get out it. First, I'm doing exactly what you are doing, regarding your repositories. Despite some namespace differences, this is what I also do: With my fake UserRepository, I also just create and populate a private IEnumerable<User> collection (which is a List<User>). Why do I have this? I use this repository for my initial day to day development (because it's fast -> no db access == quick!). Then i swap over the fake respitories for the sql repositories (ie chage my dependency injection (oooohhh!)). This is why this class/namespace exists, as opposed to using Mocks in my unit test for 'fake' stuff. (That happens, but under different circumstances). With my sql server UserRepository, I use LinqToSql. With regards to you question, it's irrelivant that I'm using LinqToSql ... it could be any other database wrapper. The important thing here is that there's a 3rd party something which i'm integrating with.   Ok, so from here, I need to make sure of two things First up, most people don't create a unit test for a fake thing. It's a fake piece of turd, so why waste the energy? True --- except that I use that fake piece of turd in my day to day development (refer to my blarg about this, above). So i quickly whip up a few basic unit tests. NOTE: In my eyes these are unit tests, even though they are repository classes. Why? They aren't intergrating with a 3rd party/infrastructure. Next (finally I get to the point), I do a seperate test class which is an Intergration Test. This is a unit test that will intergrate with something outside of the system. It could be the real Twitter api. It could be the real S3 Amazon api. Notice i used the word real. That's the key, here. I'm intergrating with a real service OUTSIDE of mine. As such -> it's slow. Anytime i need to leave my computer for some data, it's called intergrating and you automatically assume (and expect) it to be slow. So here, i'm intergrating with a Database. (Nae sayers, please don't Troll this with cheeky suggestions that you have the database on the same computer ... you're leaving your APPLICATION 'world'). Wow. this is some War-n-Peace novel .. time for some hard action, cock slappin code. Lets bring it! Ok, lets run through this puppy. First up, this is using Microsoft Unit Testing - built into VS2010 Beta2 or with the Team Foundation edition of VS2008 (or whatever that version is ... i just install the copy our work has purchased). Second, whenever the class is first initialized (be it one test or many), it creates the context. In my case, my Sql Server UserRepository which will use a LinqToSql context. (Yours will be an EF context). This is the Arrange part of TDD. Third, i call the method -> this is the Act part of TDD. Last, I check if i got back what i expected -> this is the Assert part of TDD.   What about updating the DB? Just follow the same pattern except you might want to wrap your calling code in a transaction and the roll it back. Otherwise u might get 100's of rows of data which could possibly be the same. Downside to this? Any identity fields will not have all nice and pretty numbering sequence (becuase the rollback will 'use' that number). Doesn't make sence? don't worry. that's an advanced tip i thought i'd throw in to test you out, but it means diddly squat for this hellishly long post.   so .. er.. yeah. that's what i do. Don't know if the Gods of Programming, on these forums, will flip and throw mud my way but I sorta like it and I'm hoping it might help ya. HTH.
563e344f61a801306526843b	X	You should test the real class not a fake one you make for testing. The point of using an interface is that it allows you to mock out the class, so you can use the mock version in tests with other collaborators. In order to test the class you should be able to pass in a mock database and assert that the calls you expected to be made in to the database actually happen when you call the methods on your Repository class. Here is a good intro to mocking and testing in C#: http://refact.blogspot.com/2007/01/mock-objects-and-rhino.html
563e344f61a801306526843c	X	Wow, if that is right, then it is pretty scary. I guess the only way to play it safe then is to just throw out the JS api all together and just do all the oauth/graph requests on the server end.
563e344f61a801306526843d	X	For me the big issue is if you are using single sign on and then there is data on the page that needs to be saved back to the database and all you have is the facebook credentials, your limited to what you can use to send along with your secured POST to the database to validate the request. I was going to use the full access_token and some other credentials, but even those credentials were created on the base of the access_token. The access_token is sent in a signed request and then decrypted. Why then rebroadcast it in the open . . . I hope my analysis is totally incorrect . . .
563e344f61a801306526843e	X	There was an attempt recently to bring these kinds of issues out into the open: en.wikipedia.org/wiki/Firesheep
563e345061a801306526843f	X	@bdonlan "Am I missing something, is what I am seeing and my interpretation really correct. If any one can sniff and get the access_token they can theorically make calls to the Graph API via https, even though the call back would still need to be the site established in Facebook's application set up." You must not have seen his request for the SO-ers to check his logic and get back to him.
563e345061a8013065268440	X	Please don't use all caps. It is offensive.
563e345061a8013065268441	X	+1. If you may, I've added a couple of links & quotes to support your answer.
563e345061a8013065268442	X	@ifaour - great information! Thanks for adding it.
563e345061a8013065268443	X	02/20/2011: It was confirmed by Facebook today that indeed there is one call in which the access_token is broadcast in the open . . . it just happens to be one call I use to make sure that the USER is still logged in before saving to my application database. Their recommendation was to use the SSL option provided as of last month for canvase and facebook as a whole. For the most part the Auth and Auth are secure.
563e345161a8013065268444	X	How about sharing the link?
563e345161a8013065268445	X	02/20/2011: It was confirmed by Facebook today that indeed there is one call in which the access_token is broadcast in the open . . . it just happens to be one call I use to make sure that the USER is still logged in before saving to my application database. Their recommendation was to use the SSL option provided as of last month for canvase and facebook as a whole. For the most part the Auth and Auth are secure. Findings: Subsequent to my posting there was a remark made that this was not really a question but I thought I did indeed postulate one. So that there is no ambiquity here is the question with a lead in: Since there is no data sent from Facebook during the Canvas Load process that is not at some point divulged, including the access_token, session and other data that could uniquely identify a user, does any one see any other way other than adding one more layer, i.e., a password, sent over the wire via HTTPS along with the access_toekn, that will insure unique untampered with security by the user? Using Wireshark I captured the local broadcast while loading my Canvas Application page. I was hugely surprised to see the access_token broadcast in the open, viewable for any one to see. This access_token is appended to any https call to the Facebook OpenGraph API. Using facebook as a single click log on has now raised huge concerns for me. It is stored in a session object in memory and the cookie is cleared upon app termination and after reviewing the FB.Init calls I saw a lot of HTTPS calls so I assumed the access_token was always encrypted. But last night I saw in the status bar a call from what was simply an http call that included the App ID so I felt I should sniff the Application Canvas load sequence. Today I did sniff the broadcast and in the attached image you can see that there are http calls with the access_token being broadcast in the open and clear for anyone to gain access to. Am I missing something, is what I am seeing and my interpretation really correct. If any one can sniff and get the access_token they can theorically make calls to the Graph API via https, even though the call back would still need to be the site established in Facebook's application set up. But what is truly a security threat is anyone using the access_token for access to their own site. I do not see the value of a single sign on via Facebook if the only thing that was established as secure was the access_token - becuase for what I can see it clearly is not secure. Access tokens that never have an expire date do not change. Access_tokens are different for every user, to access to another site could be held tight to just a single user, but compromising even a single user's data is unacceptable. http://www.creatingstory.com/images/InTheOpen.png Went back and did more research on this: FINDINGS: Went back an re ran the canvas application to verify that it was not any of my code that was not broadcasting. In this call: HTTP GET /connect.php/en_US/js/CacheData HTTP/1.1 The USER ID is clearly visible in the cookie. So USER_ID's are fully visible, but they are already. Anyone can go to pretty much any ones page and hover over the image and see the USER ID. So no big threat. APP_ID are also easily obtainable - but . . . http://www.creatingstory.com/images/InTheOpen2.png The above file clearly shows the FULL ACCESS TOKEN clearly in the OPEN via a Facebook initiated call. Am I wrong. TELL ME I AM WRONG because I want to be wrong about this. I have since reset my app secret so I am showing the real sniff of the Canvas Page being loaded. Additional data 02/20/2011: @ifaour - I appreciate the time you took to compile your response. I am pretty familiar with the OAuth process and have a pretty solid understanding of the signed_request unpacking and utilization of the access_token. I perform a substantial amount of my processing on the server and my Facebook server side flows are all complete and function without any flaw that I know of. The application secret is secure and never passed to the front end application and is also changed regularly. I am being as fanatical about security as I can be, knowing there is so much I don’t know that could come back and bite me. Two huge access_token issues: The issues concern the possible utilization of the access_token from the USER AGENT (browser). During the FB.INIT() process of the Facebook JavaScript SDK, a cookie is created as well as an object in memory called a session object. This object, along with the cookie contain the access_token, session, a secret, and uid and status of the connection. The session object is structured such that is supports both the new OAuth and the legacy flows. With OAuth, the access_token and status are pretty much al that is used in the session object. The first issue is that the access_token is used to make HTTPS calls to the GRAPH API. If you had the access_token, you could do this from any browser: https://graph.facebook.com/220439?access_token=... and it will return a ton of information about the user. So any one with the access token can gain access to a Facebook account. You can also make additional calls to any info the user has granted access to the application tied to the access_token. At first I thought that a call into the GRAPH had to have a Callback to the URL established in the App Setup, but I tested it as mentioned below and it will return info back right into the browser. Adding that callback feature would be a good idea I think, tightens things up a bit. The second issue is utilization of some unique private secured data that identifies the user to the third party data base, i.e., like in my case, I would use a single sign on to populate user information into my database using this unique secured data item (i.e., access_token which contains the APP ID, the USER ID, and a hashed with secret sequence). None of this is a problem on the server side. You get a signed_request, you unpack it with secret, make HTTPS calls, get HTTPS responses back. When a user has information entered via the USER AGENT(browser) that must be stored via a POST, this unique secured data element would be sent via HTTPS such that they are validated prior to data base insertion. However, If there is NO secured piece of unique data that is supplied via the single sign on process, then there is no way to guarantee unauthorized access. The access_token is the one piece of data that is utilized by Facebook to make the HTTPS calls into the GRAPH API. it is considered unique in regards to BOTH the USER and the APPLICATION and is initially secure via the signed_request packaging. If however, it is subsequently transmitted in the clear and if I can sniff the wire and obtain the access_token, then I can pretend to be the application and gain the information they have authorized the application to see. I tried the above example from a Safari and IE browser and it returned all of my information to me in the browser. In conclusion, the access_token is part of the signed_request and that is how the application initially obtains it. After OAuth authentication and authorization, i.e., the USER has logged into Facebook and then runs your app, the access_token is stored as mentioned above and I have sniffed it such that I see it stored in a Cookie that is transmitted over the wire, resulting in there being NO UNIQUE SECURED IDENTIFIABLE piece of information that can be used to support interaction with the database, or in other words, unless there were one more piece of secure data sent along with the access_token to my database, i.e., a password, I would not be able to discern if it is a legitimate call. Luckily I utilized secure AJAX via POST and the call has to come from the same domain, but I am sure there is a way to hijack that. I am totally open to any ideas on this topic on how to uniquely identify my USERS other than adding another layer (password) via this single sign on process or if someone would just share with me that I read and analyzed my data incorrectly and that the access_token is always secure over the wire. Mahalo nui loa in advance.
563e345161a8013065268446	X	I am not terribly familiar with Facebook's authentication/authorization methods, but I do believe that they implement oauth (or something close to it) for delegation, distributed authorization, and "single sign-on". OAuth is described by RFC-5849 EDIT: Facebook Uses OAuth 2.0 which is still in working draft. In OAuth, and similar systems, the "access_token" is only part of the picture. There is also typically a secret key, which is known only by the service provider (facebook) and the client application (your app). The secret key is the only part that is expected to stay secret - and that part is never sent over the wire (after it's initial issuance). In the case of Facebook, I think the secret key is assigned to you when you register your application to use their API, and the 'access_token' is returned to you for a given user, whenever the user agrees to allow your app to access their info. Messages are sent in the clear, including the user's username, and the relevant "access_token"; However, each message must also include a valid signature in order to be accepted by the server. The signature is a cryptographically computed string, that is created using a technique called HMAC. Computing the HMAC signature requires both the token and the secret, and includes other key parts of the message as well. Each signature is unique for the given message contents; and each message uses a nonce to ensure that no two messages can ever be exactly identical. When the server receives a signed message, it starts by extracting the access_token (clear-text), and determining which app the token was issued for. It then retrieves the matching secret from its own local database (the secret is not contained in the message). Finally, the server uses the clear-text message, the clear-text access_token, and the secret to compute the expected HMAC signature for the message. If the computed signature matches the signature on the received message, then the message must have been sent by someone who knows the same secret (i.e. your application). Have a look at Section 3.1 of RFC-5849 for an OAuth specific example, and further elaboration on the details. Incidentally, the same approach is used by Amazon to control access to S3 and EC2, as well as most other service providers that offer API access with long-term authorization. Suffice it to say - this approach is secure. It might be a little counter-intuitive at first, but it makes sense once you think it through. Adding a few links and quotes from Facebook Documentation: If you are unable to validate the signed_request because you can't embed your application secret (e.g. in javascript or a desktop application) then you MUST only use one piece of information from the payload, the oauth_token. Cross site request forgery is an attack in which an trusted (authenticated and authorized) user unknowingly performs an action on website. To prevent this attack, you should pass an identifier in the state parameter, and then validate the state parameter matches on the response. We strongly recommend that any app implementing Facebook user login implement CSRF protection using this mechanism.
563e345161a8013065268447	X	It was confirmed by Facebook that indeed there is one call in which the access_token is broadcast in the open - it just happens to be one call I use to make sure that the user is still logged in before saving to my application database. Their recommendation was to use the SSL option provided as of last month for canvas and Facebook as a whole. For the most part the Auth and Auth are secure. To ensure a secure interface between a third party application and a Facebook application or even any website that uses Facebook Single Sign on, an identity question would provide the extra layer when used in conjunction with the access_token. Either that or require your users to use Facebook with the new SSL feature of Facebook and Facebook Canvas Applications. If the access_token is broadcast in the open it cannot be used to uniquely identify anyone in your third party database when needing to have a confirmed identity before database interactions.
563e345161a8013065268448	X	I'm absolutely agree with you about AWS is the biggest SPF, so we don't dependent on specific AWS stuff. Our current schema is platform-agnostic. If AWS goes down, I can setup the same environment during 3 hours and 8-12hours to fetch the media-content from backups.
563e345161a8013065268449	X	I wonder how Github handles the failover? I can read in a blog On Rackspace, every piece of our infrastructure will have failover. That means two database servers, four web servers, two GitHub Pages instances, two Gem Server instances, two Archive Download instances, distributed Job runners, three pairs of file servers, and plenty more.
563e345161a801306526844a	X	I don't understand your suggestion about DB scaling Personally I prefer to place a load-balancer in front of all my Database servers and let them handle who gets requests. Anyway it can be a SPF too.
563e345161a801306526844b	X	I wonder if there scaling best practices? Can you advice well-known books, blogs?
563e345161a801306526844c	X	At GitHub everything is redundant but also everything doesn't run on Amazon. There's a big difference between owning your servers and using shared resources. Always setup at least 2 load-balancers (redundancy). I use them to forward actual database requests so no need to modify code/scripts when server availability changes. For more info there's highscalability.com and scalabilityrules.com which show many best practices and examples from large companies such as Google and Facebook.
563e345161a801306526844d	X	Thanks, it's a good answer. My questions: 1. We are going to be AWS- independent as much as possible. 2. What will be with reads if slave gone away?
563e345261a801306526844e	X	3. What independent storage do you advice? hdfs?
563e345261a801306526844f	X	I respect trying to stay cloud-platform agnostic, but ELB has major advantages (using a fixed pool of IPs you can move around within EC2) over a public/permanent IP(s) for your load balancers. Otherwise you'll have to use DNS round-robin or some less ideal solution. Also, if one of your main load-balancer hosts fails dramatically, you'll have DNS propogation delays to get a new IP in place. ELB with elastic IPs solves this elegantly enough to be worth using.
563e345261a8013065268450	X	I agree on the fact that DNS propagation can be slow and DNS round-robin is less than ideal in most situations, but it's much MUCH better than having all your eggs in 1 basket. For the same I reason I never recommend to use the DNS servers provided by your registrar or web host. When you lose control of 1 component, the last thing you want is losing control of ALL components (ex: registrar's 20 racks get seized).
563e345261a8013065268451	X	the answer almost the similiar as the first one
563e345261a8013065268452	X	We have a Rails-based application, deployment infrastucture binds to AWS. Current schema included the following layers: There is 3 SPFs: load balancer, database, media server. My questions are about redundancy, how can I reduce SPF:
563e345261a8013065268453	X	I love these questions as they always seem so simple to answer when in fact they are not. For starters, your BIGGEST SPF is that everything is on Amazon. I love AWS for many reasons, but in all situations where you need real availability, you're essentially shooting yourself in the foot by relying on them 100%. So your first plan should be to distribute your services to more than 1 provider (cloud, VPS, or dedicated). I want to aks you a question: if AWS goes down, how long does/can/will it take you to notice and then do something about it, and how quickly do you need your services to be back up and running? The reason I ask is this: DNS load-balancing of A/AAAA records is a wonderful solution, unfortunately you can't set weights or priorities as you can with SRV/MX records. This means if AWS becomes completely unavailable, you'll have to make a DNS change real quickly to remove the IP. That CAN be automated if your DNS provider has an API which allows that. On the other hand, DNS caching is performed in so many places that it might not be worth making the DNS change, meaning you'll have from 50% to 100% availability if 1 IP is unavailable (assuming you have 2 A records), because some browsers are able to try the 2nd IP if the 1st doesn't work. In my opinion, considering AWS's excellent uptime, you won't be at fault to assign 2 different IPs (on 2 different providers) to your domain. I think it's better than having 0% availability when 1 IP is down, but there's still no joy in losing 50% of your requests. You can have 2 load-balancers on each provider, and let them forward requests to the other provider if certain instances/servers are down. In other words, you only need functional load-balancers at BOTH providers, and functional servers/instances at ONE provider. Make sure to select an alternate provider which doesn't have too much latency to AWS ;) MMM is also a great tool, but it's not related to Rails in any way. Personally I prefer to place a load-balancer in front of all my Database servers and let them handle who gets requests etc. Since data on a database server is so important, it's usually better to have a human look at it and make sure everything is OK when there's a problem, as opposed to letting a tool manage its availability, configuration, etc. MMM works in many situations, perhaps you should try it and see if it answers your needs. I can't say anything bad about it. I'm not at all familiar with Wowza media server, but a quick search explained a few things. Since Wowza uses RTSP (UDP and TCP), HAProxy is NOT a solution as it only does TCP. Keepalived on the other hand can perform UDP load-balancing (it uses IVPS/LVS). In fact, Keepalived should also be used for your database slave load-balancing if you have long queries. One final note, there are many ways to "roll your own" AWS-like services such as S3 storage. If you want to avoid having SPFs but still need the same functionality as your AWS services, you should look into running the open source variants, such as Eucalyptus/Cloud.com/Openstack/GlusterFS. There's a lot of work involved in setting up all that stuff, but you'll be happy the day you can say: "so what if X provider is down, Y can take over".
563e345261a8013065268454	X	Here are some suggestions: 1) Load Balancer: Create two ha_proxy instances with your application-level load balancing knowledge and the ability to automatically create a new instance on demand. Wire up Amazon Elastic Load Balancing in front of them with health checks to route around a single ha_proxy failure. Dynamically mix in new ha_proxy instances when one fails. 2) Database: I don't think there's a way to handle automatic failover of your Primary in MySQL, but if you introduce a layer to read from replicas and write to the primary you may be able to keep read-only functionality up if a Primary is down. 3) Wowza: You should be able to load-balance multiple Wowza instances behind your ha_proxy layer w/ health checks so a single Wowza failure doesn't disable media streaming
563e345261a8013065268455	X	At Scalarium we have a solution which reduces SPF dramatically, you can see a info graphic at Rails in the Cloud on Page 12. You use the Amazon Elastic Load Balancer to route between your ha_proxy instances. To have even more security you can split your application into multiple availability zones. MySQL master master replication isn't the easiest thing. You can have a single master instance and have multiple slaves in multiple availability zones. Then you can support read actions even if your master has gone. I think a real master master with failover isn't possible. ha_proxy should be able to load-balance your Wowza instances.
563e345261a8013065268456	X	You'll have to provide more information than this. It looks like you've simply copy and pasted one of the provided server side examples. Are you even working in a cross origin environment? Let's start with the contents of the browsers developer console with the debug option enabled.
563e345261a8013065268457	X	I have included above the 'console output', you can of course try it yourself live if you would like, thank you in advance.
563e345261a8013065268458	X	Edit your question to include the console output with the debug option set to true
563e345261a8013065268459	X	Made changes as seen above, still same result.
563e345261a801306526845a	X	You have also incorrectly modified the handlePreflight method. Remove the Access-Control-Allow-Methods: PUT and Access-Control-Allow-Methods: DELETE lines. Not only is it incorrect to write this header more than once per response, Fine Uploader will only ever send POST requests to your signature server, assuming you are following the example code on fineuploader.com (which appears to be the case).
563e345261a801306526845b	X	ok, i had only added those additional headers in an attempt to get it working, since without them it was not... please see the new response console output, and the new file itself... thank you for your help
563e345261a801306526845c	X	You cannot use a wildcard as a value for the Access-Control-Allow-Headers header in your response. The spec does not allow this. I strongly suggest you take some time to read the Fine Uploader S3 documentation & blog post (and the CORS documentation for Fine Uploader and associated blog post) if you intend to deviate from the examples instead of taking these continued stabs in the dark. The example contained an entry for the expected Content-Type header. You commented it out, and then removed it. Even if wildcards were allowed, you should not be using them in most cases.
563e345261a801306526845d	X	...since you are working in a cross-origin environment, you will need to have at least a basic understanding of the same origin policy and the CORS spec. I've blogged about this on the Fine Uploader blog and linked to a must-read article that describes the CORS requirements. However, the server-side PHP example really does all of this for you. All you need to do is adjust a few values. If you need to make more drastic changes (and it doesn't seem like your situation warrants this), you will need to take some time to familiarize yourself with the associated concepts first.
563e345361a801306526845e	X	Current Page: http://www.typhooncloud.com/fineuploader When attempting to upload a simple jpg file, i receive the error: 'Invalid policy document or request headers!' Here is my index.html, s3handler.php, CORS policy and console output after attempting an upload. All changes that had been recommended have been made. HTML: SERVER: The CORS buckey policy is as follows: Chrome console: Fiddler Output:
563e345361a801306526845f	X	It looks like when you copy and pasted the PHP example server code, you neglected to adjust some of the values. In this case, the handlePreflightedRequest method was not modified to reflect your domain. It should read: Also, the handlePreflightedRequest method should probably be renamed handleCorsRequest to avoid confusion. I've done this just now in the server repo. You don't really need to do this though, as it won't affect the behavior of the code.
563e345361a8013065268460	X	Side note. Let me point out a crucial difference. In the javascript code you write sizeLimit: 5000000, but in the PHP you write $expectedMaxSize = 15000000. These values must match exactly.
563e345361a8013065268461	X	Could you spend 2 minutes formatting your question? It's unreadable.
563e345361a8013065268462	X	Seba, Good catch, but the output of JDBC endpoint is a collection namely a list of maps, not sure how it can interpret a string from it.
563e345361a8013065268463	X	You're converting it to XML: <jdbc-ee:maps-to-xml-transformer doc:name="Maps to XML"/>
563e345361a8013065268464	X	@DavidDossot , David any ideas?
563e345361a8013065268465	X	David, Sending you my configuration again, appreciate your inputs in advance: Here is the Run Log: From my perspective it looks like it is a Map Payload when I print it out, it seemingly seems to read a string when I hit the 'when expression. Hopefully this should give the complete picture. Please let me know if anything else is needed. For the sake of completion I am also including the JDBC definition: With Regards S
563e345361a8013065268466	X	The error is clear: you are passing a String to the foreach processor instead of something that can be iterated (Iterator, Collection, etc.).
563e345361a8013065268467	X	Is it actually saving information to your dev db? Have you tried restarting your server?
563e345461a8013065268468	X	@WesFoster : Yes. I did restart my server too. I have a users table with unique constraint on mobile_number. When I saw my test case failing, I used Pry and did a User.all. It showed up all my users in my dev database. Same for other tables also.
563e345461a8013065268469	X	And how do you verify that your test hits development db?
563e345461a801306526846a	X	Can you debug this ActiveRecord::Base.connection_config ?
563e345461a801306526846b	X	@dimakura: I am updating my question with these details. Thanks :)
563e345461a801306526846c	X	Thanks You :) Your answer did help me correctly set the database to test database. However I am still a little doubtful for the reasoning behind your answer. I will wait for some time more for a clearer answer else will mark your answer as accepted. Thanks a lot for looking into it. Please let me know if you require more details/file to debug on the odd behaviour of rails_helper
563e345461a801306526846d	X	I'm glad I can help, and given I solved the issue - an upvote wouldn't go astray ;). Unfortunately it's impossible to say why this is needed as the code in your question does not give a reproducible instance. i.e. if i start a new rails project with nothing but your rails_helper, rspec and one test - I do not get the same issue. So the problem is elsewhere. Likely application.rb or one of your gems. Can you show these?
563e345461a801306526846e	X	I.e. show your application.rb and your Gemfile
563e345461a801306526846f	X	agreed and upvoted :). Also updated my question with application.rb and gemfile.
563e345461a8013065268470	X	added suggestion
563e345461a8013065268471	X	I already have a Rails.env = 'test' in my rails helper (mentioned it in my question above). Earlier i had a ENV['RAILS_ENV'] ||= 'test' in my rails_helper and it did not set my env correclty. Hence I changed it to Rails.env = 'test' after suggestions on SO. (That helped. My env is now set correctly for test cases.) But it still hits the dev db and not test db.
563e345461a8013065268472	X	Can you add ENV['RAILS_ENV'] ||= 'test' just for checking in the first line? And where is Rails.env=test defined? Can you give full rails_helper.rb?
563e345461a8013065268473	X	Sure. I am adding rails_helper.rb in the question above. Thank you :)
563e345461a8013065268474	X	setting ` ENV['RAILS_ENV'] ||= 'test'` as first line of my rails_helper does not help. Instead it shows my 'Rails.env' shows as 'development' when I run Rspec (checked using pry)
563e345461a8013065268475	X	@AaditiJain I've updated my answer. But I'm curious how do you invoke RSpec?
563e345461a8013065268476	X	No, that does not help :( . Also, as I have mentioned in the question, my Rails.env is set correctly to test during rspec (verified using pry). Thanks for looking :)
563e345561a8013065268477	X	I am using Rails 4.2 with Ruby 2.2 and rspec for testcases. I have set in both my spec_helper and rails_helper. Here is my database.yml file: Here is my rails_helper: application.rb: Gemfile: When I run my test cases, Rails.env is 'test' as expected (used pry to verify). However my test cases are always hitting the development database. spec_helper: I have been scratching my head since last few hours but nothing seems to solve the mystery. Any help would be greatful!
563e345561a8013065268478	X	It sounds as though somewhere in your environment (maybe one of your gems) it is setting your environment to dev or establishing a connection to your dev database. To explicitly make it connect to the test database, add: to your rails_helper. Prime candidate for troublemaker would be `gem 'rails-erd', '~> 1.4.1'. If you have this as auto-generating the diagram on migrate, when the test schema is migrated, it will connect to the dev database to dump a diagram. Try removing this gem or perhaps the '.rake' file and see what happens.
563e345561a8013065268479	X	Your rails_helper looks strange. The first line says: At the first line you don't have Rails loaded yet (I suppose you run RSpec using bundle exec rspec). So it should raise an error. Therefore I made a small change in rails_helper: Now putting somewhere in your spec, should pass successfully.
563e345561a801306526847a	X	1) Add require: false to the declaration of 'rails-erd' gem 2) In your spec_helper.rb, replace Rails.env = 'test' by ENV['RAILS_ENV'] ||= 'test' 3) Verify in your bin/rspec (if present) that you don't have any instruction changing the value of env 4) Then stop spring from the command line 5) Run your specs from the command line
563e345561a801306526847b	X	Just put ENV["RAILS_ENV"] = "test" at the top of rails_helper
563e345561a801306526847c	X	You can run rspc with RAILS_ENV=test bundle exec rspec spec Also you can put inside rails_helper.rb Also move testing framework section from development test to test inside gemfile
563e345561a801306526847d	X	I have been having issues trying to deploy with rubber in the Terminal and in my rubber.yml And doing a I get the error after: setup.rb at line at 192: After attempting to debug this with binding.pry, the line 192 goes through without any error. Any ideas are welcome. I have also tried: as per Rails 4 Error with every command "`load': no implicit conversion of nil into String" (Mac OS X 10.9)
563e345561a801306526847e	X	well that coulnd have been it but i already use the gem cancancan gem 'cancancan', '1.8.2' gem 'devise', '2.2.3' gem 'devise-async', '0.7.0' I should have shown my gemfile
563e345561a801306526847f	X	Yes, the gemfile would be helpful, and the version of Rails.
563e345561a8013065268480	X	The gemfile is here now
563e345561a8013065268481	X	Try both at the same time: gem 'cancan' gem 'cancancan'
563e345661a8013065268482	X	i solved the error by using the last version of activeadmin gem 'activeadmin', github: 'gregbell/active_admin'
563e345661a8013065268483	X	i have this problems that really bugs me. I try to get activeadmin working with CanCan. User model and i create a subclass for different role. User <-- admin User <-- customer etc I did all the configuration But when i start the server i get this: Uncaught exception: uninitialized constant ActiveAdmin::CanCanAdapter To get rid of this bug i install the gem gem 'activeadmin-cancan' now all is good i can see the activeadmin panel. BUT when i add a resource, the resource doesn't show in the nav bar and when i try to access it directly i get this protected method `authorize!' called for #< Admin::CustomersController:0x750c540> if you can help me with this i will be so gratful! Here is the Gemfile
563e345661a8013065268484	X	Use gem 'cancancan' instead of gem 'cancan', and delete gem 'activeadmin-cancan', you don't have to use that gem. Don't forget to update your gems: bundle update.
563e345661a8013065268485	X	Undefined index: Be sure you're initializing your variables before using them. I think this question may be usefull to fix the "headers already sent"
563e345661a8013065268486	X	possible duplicate of PHP Warning demolishes JSON response
563e345661a8013065268487	X	Thanks for your reply, I will test it. Can you explain why I get these error messages? For some reason it feels wrong to just disable them.
563e345661a8013065268488	X	I think that the error is a notice that you are accessing to the $_POST['_method']. It would be better to use isset($_POST['_method']).
563e345661a8013065268489	X	I'm starting to get frustrated and cannot work with a php file that is outside my public folder. I am using fineuploader and this php mini framework https://github.com/panique/mini. I have the following file structure where “public” is my public directory. I would like to use the php-file from a javascript in application.js. If I do like this and put s3demo in public/js folder everything works fine. But I would like to have the s3demo.php in the application/libs folder instead of /js/application I have using my controller with the following code Javascript points to my controller instead of to the php-file And my controller looks like this When I run the code I get the following error message in the browser NET tab -> Response Notice: Undefined index: _method in /home/connecti/public_html/application/libs/s3demo.php on line 78 Warning: Cannot modify header information - headers already sent by (output started at /home/connecti/public_html/application/libs/s3demo.php:77) in /home/connecti/public_html/application/libs/s3demo.php on line 103 Notice: Undefined index: headers in /home/connecti/public_html/application/libs/s3demo.php on line 109 {"invalid":true} My controller can access my s3admin.php (I have a index named “_method” in s3admin.php). But it feels like my php-file don’t know my javascript or something. What should I do to be able to runt s3demo.php from my libs folder? I don’t get any error if have have s3demo in the publicfolder and the fileupload works fine. If I have s3demo.php in my libs folder I get the error above and my file is not uploaded. All javascript code All php code in s3demo.php
563e345661a801306526848a	X	You are getting a notice message and then the headers cannot be sent. Disable the notice messages with error_reporting(E_ALL ^ E_NOTICE) at the beginning of your script;
563e345761a801306526848b	X	i am trying to create a fat jar file of my project using sbt's command assembly or assemblyPackageDependency these two commands are not working sbt is giving error i looked for the solution here in this link https://github.com/sbt/sbt-assembly and i also posted on stack from the answer sbt assembly command shows error: (redshiftConnector/*:assembly) deduplicate: different file contents found i tried but nothing happens i tried to do it with both ways but i did not help solution 1 Solution 2 Trying both ways but i did not helped here is the full error stack trace Please help me how can i resolve this issue
563e345761a801306526848c	X	I am using http://fineuploader.com that automatically generates uuid to the image names. I want to store the uploaded image uuid in a database. I have found out that it should be possible to get the uuid with php $uuid = $_REQUEST['qquuid']; I have tested to put it it in my php code but nothing is saved. Can I pass it from the javascript code to a php function with my mysql insert? How do I get all the uuid from the uploaded images passed to a php function so I can save them to my database? Fineuploader javascript coce php code I use to upload the files
563e345761a801306526848d	X	Fine Uploader's S3 uploader sends an upload success POST request to your server after the file has been successfully sent to S3. A number of parameters are sent in the payload of this request, which is URL encoded. The name of the uuid parameter for the S3 upload success request is "uuid". So, you can parse it server-side, in your upload success POST handler, with $_POST['uuid'].
563e345761a801306526848e	X	If you're trying to pass it from JavaScript to PHP, you could probably just pass it through $.get and have it process the info on a PHP script. That will pass the variable in your JavaScript, which contains the UUID, to your PHP script. Then from your PHP script, just use $_GET to grab the variable value, and then use that value to insert it into your MySQL database.
563e345761a801306526848f	X	had to delete some of the content as it exceed the limit (3000 words)
563e345861a8013065268490	X	thanks, it helped me with spring-ws-archetype
563e345861a8013065268491	X	I don't think 15 is the right choice. see my answer..
563e345861a8013065268492	X	Right, now it's 97. My mistake.
563e345861a8013065268493	X	According to maven site, and some tutorials on the web, mvn archetype:generate would give a choice of about 36, and selection 15 is the quick start. It was working this way until I setup Nexus . Now I get a choice of 358 with default as 97 (which I am could not read the description from my dos prompt). Is both choices are same? Why now I get these many choices. How to correct it if I have to change my repository setttings. My maven version is Thanks.
563e345861a8013065268494	X	I was looking for the same solution and I seem to find a usable one, though it may not suit everyone. The idea is to create a local archetype catalog and specify all archetypes you may need there. This way you'll get only choices you're interested. Of course you may find you'll need to add new archetype there, then you'll need either to fallback to normal use of remote repository or add it by hand. The recipe: Create the initial local catalog with mvn archetype:crawl -Dcatalog=~/.m2/archetype-catalog.xml Pass catalog list from only you're local catalog to archetype:generate mvn archetype:generate -DarchetypeCatalog=local You will see something like this: You can make the option permanent by specifying it in your settings.xml:  Now when you want to fallback to usual Maven catalog list, run it as and you will have all archetypes available.
563e345861a8013065268495	X	I see maven 3 is available and installed without nexus.. that gave same number of choices as below. So, somehow I was getting maven 3 archetype earlier.. seems like 97 is a good choice..
563e345861a8013065268496	X	You can still use choice 15 as a quick start. Just ignore others.
563e345861a8013065268497	X	Can you show the results of meteor list in your question?
563e345861a8013065268498	X	I added the results of meteor list to the answer
563e345861a8013065268499	X	I am trying to update to Meteor 1.2. But when I run Meteor update, it starts to run and get to And the spinner stops and its been like this for 20 minutes or so. Ive tried quitting the process and starting it up again, and the same result. Any suggestions on what I can try? The server seems to still start ok, but I cannot update Here is the package list
563e345861a801306526849a	X	It works now. I just had to wait sometime. I think the meteor package server was down. So it was just waiting. If this happens to you just wait some time and try again.
563e345961a801306526849b	X	Have you set RAILS_ENV to something other than production on heroku? I'd also try pulling the gem command out of the :production group.
563e345961a801306526849c	X	You also need to do gem install activerecord-postgresql-adapter, or add it to your Gemfile.
563e345961a801306526849d	X	It still doesn't work. I added ENV['RAILS_ENV'] ||= 'production' to my environment.rb. I also added gem activerecord-postgresql-adapter and I did also delete the asset and production tag.
563e345a61a801306526849e	X	When you push, it should list the gems it's installing/using. Do you see pg? Is pg showing up in your Gemfile.lock?
563e345a61a801306526849f	X	only on my local machine
563e345a61a80130652684a0	X	After do this, you can safely remove from the production group too.
563e345a61a80130652684a1	X	I think I also tried this. I will test it again. In example I had it also in the production tag and outside this tag.
563e345a61a80130652684a2	X	It didn't work. I did bundle install. git commit . heroku push master
563e345a61a80130652684a3	X	As I already mentioned, I also tried this. It still doesnt work.
563e345a61a80130652684a4	X	This should not be the "accepted answer"
563e345a61a80130652684a5	X	I'm tried to deploy a new version of my app to heroku. The deployment fails because heroku says, that the gem pg is not in my GEMFILE ... I have this postgres gem in my GEMFILE. I also looked for some answers, but no one worked ... As anyone an idea? I use Ruby 2.0.0p247, Rails 3.2.14 refinery-cms 2.1.0 Here is my Gemfile: And here the error message:
563e345a61a80130652684a6	X	according to the gemfile: pg is commented-out. uncomment it to install it In my gemfile for instance, I work in windows, and heroku doesn't. So I have since you must have some db installed, preferbly pg since it is heroku also, as Gavin in the comments said - make sure you are in production, since it seems that heroku doesn't think o..
563e345a61a80130652684a7	X	The problem is that you have the pg gem in the production group. Therefore when assets are precompiled on Heroku (which uses the global group + assets group) then the PG gem cannot indeed be found because it's in the production group. I would suggest having the pg gem in the global group and available to allow asset groups.
563e345a61a80130652684a8	X	Oh no ... the solution was to delete my git repo and create a new one. I don't know why, but my repo got messed up (I added new files and so on ..) and so it didn't commit all data. After the tip of chucknelson I tried to lock a certain rails version and saw that heroku didn't get this.
563e345b61a80130652684a9	X	Oh jeez I've dealt with this before. Can you post your build.sbt please?
563e345b61a80130652684aa	X	well there are more than one sbt file ok update my question
563e345b61a80130652684ab	X	please see i have updated my question
563e345b61a80130652684ac	X	please sea my updated i tried but error remains same
563e345b61a80130652684ad	X	sbt assembly is giving me errors i am trying to create a fat jar with this link https://github.com/sbt/sbt-assembly under the heading Exclude specific transitive deps they exclude some of the libs that are causing this issue but in my projects build.sbt file no dependency is there which is showing on sbt console . here is the stack trace here is my build.sbt files bulid.sbt build.sbt build.sbt name := "h-master" Update 2 tried the solution given my user rosshsr but error remain same Please help
563e345b61a80130652684ae	X	Since I'm not familiar with where in all of these libraries you're depending on these sets of clashes are occurring, I'll just give you my general strategy for fixing this. You need to exclude one of those clashing dependencies. This is an excerpt from one of my build.sbt files. You would need to attach that excludeAll bit to the dependency and add something like ExclusionRule("org.slf4j","slf4j-simple")
563e345c61a80130652684af	X	You have an uninitialized object (basically a nil pointer).
563e345c61a80130652684b0	X	I posted the new logs
563e345c61a80130652684b1	X	Im working on an app based on spree with several addons. When using heroku I receive a HTTP/1.1 500 Internal Server Error. Development environment on localhost is working. Only after pushing to heroku I receive this error. heroku log (edited) Production.rb Gemfile
563e345c61a80130652684b2	X	I had the same problem and fixed it using Ryan Big's fix: The fix will be made available on the spree/spree repository soon, I hope. If you want to read the whole discussion around this issue, see this post.
563e345c61a80130652684b3	X	You're calling translations on something that's evaluating to nil. It might be working locally because you're relying on the presence of something in your session or development db that won't be there when you try to access it on heroku. edit: I don't see any mention of the error itself in the log you posted. Is there any more to it? You might want to temporarily change config.log_level to :debug, try loading the page again, and then reposting the logs. OK, I'm not really familiar with Spree but I think the issue is in the index method in home_controller_decorator.rb. In that method, are you calling translate on an object? If so, are you certain there's no way that object is nil? I'd probably use something like pry in this circumstance to check things out.
563e345c61a80130652684b4	X	I get three errors. One says I need to install therubyracer to use Less. I updated my questions and added my heroku logs file. The starred lines are the relevant errors I believe.
563e345c61a80130652684b5	X	I would add your Gemfile to the post also - might want to consider pre-compiling your assets, check them into git and push that way?
563e345c61a80130652684b6	X	Ok I've added the Gemfile
563e345c61a80130652684b7	X	I'm having the same problem, if anyone has any suggestions?
563e345c61a80130652684b8	X	might want to try heroku google groups or create a support ticket - worst case situation, compile your assets locally and push them to heroku precompiled
563e345c61a80130652684b9	X	What could I possibly be doing wrong here? I've been trying to fix this error for a couple of days now and no luck. This is a somewhat detrimental problem because if I try bundling without it, I get an error with Less which is dependent on it for twitter bootstrap. I've tried using previous versions of therubyracer also, but when I do I get the same exact error with a suggestions of "Make sure that `gem install therubyracer -v '0.11.0beta8'succeeds before bundling". Any help would be great! Update: When I don't include therubyracer in production at all I get this: (heroku logs) Gemfile:
563e345c61a80130652684ba	X	did you try this one? https://github.com/aler/therubyracer-heroku see therubyracer fails to build on heroku just looked on heroku docs - https://devcenter.heroku.com/articles/rails3x-asset-pipeline-cedar therubyracer If you were previously using therubyracer or therubyracer-heroku, these gems are no longer required and strongly discouraged as these gems use a very large amount of memory. what happens if you do not include therubyracer in your production deploy?
563e345d61a80130652684bb	X	However heroku discouraged the use of therubyracer gems it's hard to remove this from existing projects without compromise the application. So there is a solutions (see). Add 'therubyracer', :platforms => :ruby to the group :assets and upgrade your ruby version. Then remove your Gemfile.lock and run bundle install. Example:
563e345d61a80130652684bc	X	Hello guys i have problem what i cant slove. I download sandbox Sonata Admin and try to install it. After setting parameters for db host, username and pass, media path and google auth i get this error for sonata_page.varnish.command. Any solution how to slowe this problem? sonata_page.varnish.command: 'varnishadm -S /etc/varnish/secret -T 127.0.0.1:6082 {{ COMMAND }} "{{ EXPRESSION }}"' Composer.json And my log output :
563e345d61a80130652684bd	X	Same Problème here since two days... Obviously the issue comes with the last version: see https://github.com/sonata-project/SonataPageBundle/commit/bf48498c36dc4e09f8e80434f4b2d8caa045b956 I rolled back manually the change on line 72 as a workaround and it did the trick. Or An even better solution was to change the version of the PageBundle to "sonata-project/page-bundle" : "~2.2", in the composer.json (instead of the "dev-master" one)
563e345d61a80130652684be	X	The issue has been fixed: https://github.com/sonata-project/SonataPageBundle/commit/a2d83e6c02f566222bc04827885566d981270176 Next time, report it to github ;)
