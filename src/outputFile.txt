563d3dec1c488238ac27de6e	X	Thanks!
  Positive
How to fix this problem - goo.gl/QTdm4
563d3dec1c488238ac27de6f	X	@roman-nazarkin So the issue was the bucket location?
  Negative
563d3dec1c488238ac27de70	X	in my case there was some extra whitespace in the config file/setting which had the secret key, so it was transmitting an extra tab character in the "password", thus invalidating the signature.
  Negative
563d3dec1c488238ac27de71	X	I had this same issue.
  Negative
It turned out the debugging tool I was using was mistakingly sending GET requests when the signature specified POST.
  Negative
This threw me off to thinking something was wrong with my signature encodings.
  Negative
Dumb mistake took up almost a day of trial and error.
  Negative
563d3dec1c488238ac27de72	X	ditto - when adding Metadata with a key 'Cache-Control' onto an object that already has a metadata key 'cache-control' I get this error.
  Negative
563d3dec1c488238ac27de73	X	I am using a PHP class for Amazon S3 and CloudFront - Link.
  Negative
But when I try to upload a file into a bucket, I get this error: [SignatureDoesNotMatch] The request signature we calculated does not match the signature you provided.
  Very negative
Check your key and signing method.
  Neutral
How to fix it?
  Neutral
Thanks.
  Neutral
563d3dec1c488238ac27de74	X	When you sign up for Amazon, you can create yourself a key pair (Amazon calls those access key ID and secret access key).
  Negative
Those two are used to sign requests to Amazon's webservices.
  Neutral
Amazon re-calculates the signature and compares if it matches the one that was contained in your request.
  Negative
That way the secret access key never needs to be transmitted over the network.
  Negative
If you get "Signature does not match", it's highly likely you used a wrong secret access key.
  Negative
Can you double-check access key and secret access key to make sure they're correct?
  Neutral
563d3ded1c488238ac27de75	X	Personally I received this error because of the characters that were in my meta data.
  Negative
The problematic character was the "–" chracter which is "\u2013" in unicode and different to "-".
  Negative
A note from the documentation http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#UserMetadata... Amazon S3 stores user-defined metadata in lowercase.
  Negative
Each name, value pair must conform to US-ASCII when using REST and UTF-8 when using SOAP or browser-based uploads via POST.
  Negative
563d43c9a4387b6f44e92357	X	thanks, for some reason when I googles I found other libs, that wasn't suitable, but not this one.
  Negative
So I'm going to try this one.
  Negative
.
  Neutral
563d43c9a4387b6f44e92358	X	If there someone who worked with Amazon S3 API in C?
  Negative
I can't manage to sign my REST request proper.
  Negative
Can someone share his successful experience in that?
  Positive
563d43c9a4387b6f44e92359	X	I've never tried it, but a quick Google turned up the libs3 C library API for Amazon S3.
  Negative
That might make things easier, so you don't have to deal with raw HTTP requests via curl.
  Negative
563d43caa4387b6f44e9235a	X	Did you apply both code for same Bucket or for live server you did use the bucket which is already in lower case ?
  Negative
563d43caa4387b6f44e9235b	X	Same bucket name on both the servers.
  Neutral
Why we should append the key with the bucket name?
  Negative
563d43caa4387b6f44e9235c	X	I do not have knowledge of the language (php ) you are working on.
  Negative
BUT Yes, you do not need to append key with bucket name.
  Negative
You are already passing key in create object method.
  Neutral
563d43caa4387b6f44e9235d	X	The following is sample code from Amazon S3 API Documentation.
  Negative
This works on live site but on the localhost the latter gives an error saying no bucket found   // Success?
  Negative
but removing the .
  Neutral
strtolower($s3->key); works
563d43caa4387b6f44e9235e	X	Amazon S3 is case sensitive.
  Negative
So for Bucket as well as Object if you changes the Name to Upper or Lower Case, It will give you different result.
  Negative
Means if Bucket Name has some Capital Laters and your code make changed the it name to lower case then It will returns you Bucket Does No Exist like message.
  Negative
So make sure that what actually bucket as well as object name exist at Amazon S3.
  Neutral
563d43caa4387b6f44e9235f	X	I am trying to use Amazon S3 API for uploading images to bucket.
  Negative
But I can't create a bucket.
  Negative
It shows "Access Denied " error.
  Negative
My code is: Is it required any permission?
  Neutral
Anyone please help me.
  Positive
563d43caa4387b6f44e92360	X	possible duplicate of privacy on Amazon S3
563d43caa4387b6f44e92361	X	I'm using amazon S3 php to upload and download files.
  Negative
for exemple to get a private file from amazon s3 I'm using: and TO download it but this is very heavy for the server, is there a solution to directly download private objects from amazon s3 using a link, a little like for public objects with a security.
  Negative
Thanks
563d43caa4387b6f44e92362	X	You can create pre-signed URLs for objects that have an expiration date.
  Negative
You can use this feature to allow people to download private objects directly from Amazon S3.
  Neutral
The AWS SDK for PHP has an easy S3Client::getObjectUrl() method that can help you do this.
  Positive
563d43cba4387b6f44e92363	X	So I setup another S3 account and use it's credentials (key/secret) then?
  Negative
563d43cba4387b6f44e92364	X	That's correct.
  Neutral
563d43cba4387b6f44e92365	X	That would limit them (meaning one who has this other account credientials) from manipulating that shared bucket, but wouldn't they have unfettered access to that S3 account and store?
  Negative
Meaning, they could create bucket(s) via the API and upload stuff to their hearts content?
  Neutral
I'm looking specifically for a way to have a client app that can talk to S3 with the restful API but is restricted in what can be done with those credentials.
  Negative
Namely read-only.
  Neutral
Is that possible?
  Neutral
563d43cba4387b6f44e92366	X	You're right that using another S3 account gives that other account the ability to create new buckets.
  Positive
The only way I can think of to do what you suggest is to use anonymous access to your S3 bucket.
  Negative
If you choose random enough object names, then people aren't likely to guess the names of your objects.
  Negative
However, you are then responsible for bandwidth costs incurred by the anonymous downloads, and access to your objects aren't limited to authenticated accounts.
  Neutral
563d43cba4387b6f44e92367	X	I'm not concerned about people downloading stuff... just don't want them doing anything else.
  Negative
Read only, as it were.
  Negative
So the REST API, if used, always applies to a user with full access to the store?
  Negative
The only way to do something like this is to use a normal HTTP downloading through the object's public URL?
  Negative
563d43cba4387b6f44e92368	X	Is there a way to create a different identity to (access key / secret key) to access Amazon S3 buckets via the REST API where I can restrict access (read only for example)?
  Negative
563d43cba4387b6f44e92369	X	Yes, you can.
  Positive
The S3 API documentation describes the Authentication and Access Control services available to you.
  Negative
You can set up a bucket so that another Amazon S3 account can read but not modify items in the bucket.
  Negative
563d43cba4387b6f44e9236a	X	The recommended way is to use IAM to create a new user, then apply a policy to that user.
  Negative
563d43cba4387b6f44e9236b	X	Check out the details at http://docs.amazonwebservices.com/AmazonS3/2006-03-01/dev/index.html?UsingAuthAccess.html (follow the link to "Using Query String Authentication")- this is a subdocument to the one Greg Posted, and describes how to generate access URLs on the fly.
  Negative
This uses a hashed form of the private key and allows expiration, so you can give brief access to files in a bucket without allowed unfettered access to the rest of the S3 store.
  Positive
Constructing the REST URL is quite difficult, it took me about 3 hours of coding to get it right, but this is a very powerful access technique.
  Positive
563d43cca4387b6f44e9236c	X	I'm trying to make an AutoIT script interface with the Amazon S3 API.
  Negative
I've been trying both SOAP and REST, although no success.
  Negative
This is the SOAP code I'm working with (modified example from Ptrex on the AutoIT forums), however I get the following response: "soapenv:Client.badRequest Missing SOAPAction header" To be honest, the code doesn't make that much sense to me and I'm really just tinkering around.
  Very negative
Any examples or pointers to get me going in the right direction on how to properly interface with the Amazon S3 API would be greatly appreciated!
  Negative
563d43cca4387b6f44e9236d	X	I don't know if this helps you out but from the autoit part everything works well The answer you get from amazon 'soapenv:Client.badRequest Missing SOAPAction header' means what it actually says something worng with your request.
  Negative
-namely: Missing SOAPAction header What you get was indeed a response but an error response from the server.
  Negative
I suggest trying to rewrite the request I found the most relevant description here: http://docs.aws.amazon.com/AWSSimpleQueueService/2008-01-01/SQSDeveloperGuide/index.html?MakingRequests_MakingSOAPRequestsArticle.html
563d43cca4387b6f44e9236e	X	I had the same problem, your service url needs http or https.
  Negative
Worked for me.
  Neutral
The documentation is pretty poor in the sense that its wrong.
  Negative
IMO
563d43cca4387b6f44e9236f	X	I tried that already, if I add http:// to the serviceurl for the config and then ask the listobjects, it asks actually for: bucketnamehttp://s3.amazonaws.com and fails because this is obviously not valid.
  Very negative
563d43cca4387b6f44e92370	X	Also, because it's key based, everything is case sensitive.
  Negative
563d43cca4387b6f44e92371	X	I am trying to access an external bucket over the Amazon S3 API through .
  Negative
Net / C#.
  Neutral
I already tried the login with a 3rd party tool which worked like a charm, now I want to get the items of the bucket inside the framework.
  Negative
I am using this behind a Proxy, that's why I am using the S3config.
  Negative
that's the way I establish the connection itself to amazon.
  Positive
I also already tried placing into the config object initializer because I am in EU and the bucket is located somewhere in US.
  Negative
When I now try to access via : or or I only get Access Denied in the error object that is thrown.
  Negative
The credentials I use are 100% the same as in the 3rd party tool.
  Negative
Am I missing something here ?
  Negative
do I need to use any special way which I just can't find to make it work ?
  Negative
a working python snippet is: this returns correct results, so the actual connection works and also the credentials.
  Positive
563d43cca4387b6f44e92372	X	This is the code I'm using to return a list of files in a "directory" in my bucket and I know it definitely works.
  Positive
I says directory but actually there isn't such thing.
  Negative
My understanding of S3 is each file/folder is an object.
  Negative
Each object has a key.
  Neutral
Key determines where in the tree you will see a folder or file.
  Negative
A key Folder1 I believe will be a Folder called Folder1 at the route.
  Negative
An object with a key Folder1/File1.
  Positive
txt would be a file in Folder1.
  Negative
If other clever people have more to say or corrections, I'm sure they will tell me.
  Negative
But, the code does work.
  Neutral
563d43cca4387b6f44e92373	X	After using the given answers as a new base for research I figured out, that I have to give a serviceurl, a regionendpoint and a communicationprotocol for the S3Config Class on the one side and, because I knew the exact name of the file within the bucket, I needed to use getobject and not an access to the bucket.
  Very negative
so the code that got me working is:
563d43cca4387b6f44e92374	X	Sounds promising, will check it out!
  Very positive
563d43cca4387b6f44e92375	X	Is there a way (API call) to know the current time on an Amazon S3 server?
  Negative
Here is a bit of background to explain why I need this: I have an iphone app that sometimes has to download a set of files from a bucket on a Amazon AWS S3 account.
  Negative
Between two such downloads, the server files may be modified by a CMS (Web Content Management System), or not.
  Negative
So, when a second download occurs, The client app tries to be efficient by downloading only the files that have been modified on the server since the previous such download.
  Negative
To achieve this, the app stores the date of the last download and when a new download occurs, it just focuses on the files that have been modified on the server since the date of the last download (using there “modified date” property accessible using the SDK listObjects() function).
  Negative
The problem with this is that the date on the phone and the modified dates on the s3 server may not be compatible.
  Negative
The phone user may have changed his phone date & time settings, etc.
  Negative
To make this work, the saved “last download date” should come from an Amazon S3 API call to make sure all dates used by the app logic are in sync.
  Negative
Is there such thing?
  Neutral
Or maybe an alternative or a workaround?
  Neutral
563d43cda4387b6f44e92376	X	You could use a file hash instead of the modified date.
  Negative
An Amazon S3Object has an etag property that is indeed such kind of hash.
  Negative
You retrieve this property the same way as you access date.
  Neutral
Have your client device save this hash along with the file.
  Neutral
The next time you connect to the server, ask for the etag using the method about and compare the returned value to your local copy.
  Negative
A different etag value will indicate to the client that the file has changed since the last download.
  Negative
This approach would be completely independent of any datetime functionality.
  Negative
563d43cda4387b6f44e92377	X	Here's the source.
  Neutral
Fork it.
  Neutral
?
  Neutral
563d43cda4387b6f44e92378	X	I have a JAR file - jets3t-0.7.4.jar, by which I can access Amazon's S3 storage.
  Negative
I need to modify its source code so that it accesses Ceph object storage instead.
  Negative
I know it can done by modfying the S3 API, but do not know how.
  Negative
Does anyone know how to do this?
  Neutral
I googled for information, but didn't really find anything informative.
  Negative
Any help is appreciated.
  Positive
Thanks!
  Positive
563d43cda4387b6f44e92379	X	
563d43cda4387b6f44e9237a	X	thanks for the link but at my work we use CF 8 .
  Negative
.
  Neutral
any suggestions?
  Neutral
563d43cda4387b6f44e9237b	X	Edited answer - lo and behold there is a CFC for that :)
563d43cda4387b6f44e9237c	X	thanks for that link I'll take a look at it :)
563d43cda4387b6f44e9237d	X	I went to that site amazons3.riaforge.org was not really to useful for me, and the second link you provided comes up as "access denied".
  Negative
But thank you for trying to help.
  Positive
563d43cda4387b6f44e9237e	X	The second links was an example of how you can just directly link to S3 files once you've uploaded them - provided you've set the proper security.
  Negative
563d43cda4387b6f44e9237f	X	Also, why wasn't the RIAforge.org content useful?
  Negative
563d43cea4387b6f44e92380	X	ok your right that makes sense in regards to the second link.
  Positive
and I'm just not understanding the information on RIAforge.org, nothing against the site, I'm just having issues trying to find the simplest method to write out a code.
  Negative
563d43cea4387b6f44e92381	X	I am having a problem trying to figure out what is the proper coldfusion code to upload a simple file into amazon s3 api.
  Negative
Any help is much appreciated!!!
  Neutral
563d43cea4387b6f44e92382	X	There is a good tutorial here.
  Positive
You'll need CF 9.0.1 however.
  Negative
Prior to CF 9 you might be able to use this CFC that Barney Boisvert wrote.
  Negative
563d43cea4387b6f44e92383	X	Try this CFC: http://amazons3.riaforge.org/ Also, note that you may also access your objects via: http://bucketname.s3.amazonaws.com/name-of-the-object (example)
563d43cea4387b6f44e92384	X	Have you tried something?
  Negative
563d43cea4387b6f44e92385	X	yes, i am getting results but in it shows in ascending order of lastmodiffied date
563d43cea4387b6f44e92386	X	Then you should add your own code here first.
  Negative
563d43cea4387b6f44e92387	X	There is not much to add as it is just an api call, I have added above.
  Negative
563d43cea4387b6f44e92388	X	You can do it manually.
  Positive
Getting no reference in the docs.
  Negative
563d43cea4387b6f44e92389	X	I need to list objects from Amazon s3 in order such that latest uploaded objects should be listed on top ?
  Negative
How it can be done ?
  Neutral
There is not option to sort it above ?
  Negative
Below is my code, Below is my output, If you see LastModified 'LastModified' => string '2010-10-05T23:00:50.000Z' is displayed first and then 'LastModified' => string '2010-10-06T23:00:50.000Z' How do I sort it in descending order of LastModified ?
  Negative
563d43cea4387b6f44e9238a	X	I'm trying to upload a file to my s3 using the php sdk and for each file I'm setting the ConentDisposition and ContentType just as the documentation says (http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html#_putObject), but after uploading I look at the http header for the file and the only thing set is Content-Type and that's set to the default 'octet-stream':
563d43cfa4387b6f44e9238b	X	Sounds as if the OP is looking to become your competition then, rather than opting in to your service ;S
563d43cfa4387b6f44e9238c	X	I would like to implement a cloud storage service with the same interface of OpenStack Swift or Amazon S3.
  Very negative
In other words, my cloud storage service should expose the same API of the above-mentioned services but with a custom implementation.
  Negative
This way, a client will be able to interoperate with my service without changing its implementation.
  Positive
I was wondering if there is an easier approach than manually implementing such interfaces starting from the documentation: http://docs.openstack.org/api/openstack-object-storage/1.0/content/ http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html For instance, it would be nice if there was a "skeleton" of OpenStack Swift or Amazon S3 APIs from which I can start implementing my service.
  Negative
Thanks
563d43cfa4387b6f44e9238d	X	I found exactly what I was looking for: These tools emulate most of Amazon S3 API.
  Negative
They are meant for development and test purposes but in my case I can use them as a starting point for implementing my cloud storage service.
  Negative
563d43cfa4387b6f44e9238e	X	Someone has done this for you, try jcloud, it supports AWS S3 and swift http://jclouds.apache.org/guides/providers/
563d43cfa4387b6f44e9238f	X	If you are looking for an enterprise / carrier grade object storage software solution, look at Cloudian http://www.cloudian.com.
  Negative
Cloudian's software delivers a fully Amazon S3 compliant API, meaning that it delivers the broadest range of S3 feature coverage and 100% fidelity with the AWS S3 API.
  Positive
The software comes with a Free 10TB license, so pretty much it is free up to 10TB of managed storage, after that it is reasonably priced.
  Positive
You can install the software in any x86 hardware running Linux.
  Negative
Cloudian does not support the Swift API though.
  Negative
[Disclaimer: I work for Cloudian]
563d43cfa4387b6f44e92390	X	I would recommend using Swift (Openstack object store ) which also supports S3 API Take a look at the following link: http://docs.openstack.org/grizzly/openstack-object-storage/admin/content/configuring-openstack-object-storage-with-s3_api.html This way you can work with openstack swift or Amazon S3
563d43cfa4387b6f44e92391	X	Another option is libcloud, it is a python abstraction that supports a number of providers (including S3 and Swift): https://libcloud.readthedocs.org/en/latest/storage/index.html http://libcloud.apache.org/
563d43cfa4387b6f44e92392	X	Is there any way to set the file permission at the time of uploading files through Amazon S3 API.
  Very negative
In my current solution i am able to upload the file on my bucket but i can not view the file through the URL which is mentioned in the file properties section.
  Negative
It is opening when i am passing access keys in query string.
  Neutral
Is there any settings required in Amazon S3 or i need to set the permissions to all the file at the time of upload.
  Negative
Thanks in Advance.
  Neutral
Kamal Kant Pansari
563d43cfa4387b6f44e92393	X	Add a header to your PUT request: x-amz-acl: public-read
563d43d0a4387b6f44e92394	X	You can also use Bucket Policies feature.
  Negative
Here is an example of bucket policy that instructs amazon s3 to make all of the files publicly available, including new files you will upload: Replace your.bucket.name with your actual bucket name You can view and edit Bucket Policies with S3 Browser Freeware.
  Negative
You can find more Bucket Policies examples here.
  Negative
563d43d0a4387b6f44e92395	X	In C# when you create a response object of >mazon then in response method you will find Addheader.
  Negative
You need to set header as Amazon providing these methods in its web services API kindly refer that.
  Negative
563d43d0a4387b6f44e92396	X	Do you have code and a publicly accessible URL that could be used for testing?
  Negative
563d43d0a4387b6f44e92397	X	I found one of similar construction.
  Positive
See if the combination of reading directly from the GET value and using colClasses= improves performance.
  Positive
563d43d1a4387b6f44e92398	X	You can split on "\\r\\n" instead if it's going to return the Windows line endings as in the example.
  Negative
563d43d1a4387b6f44e92399	X	Hmm, thanks for that, is there a way to quickly remove \r bit too?
  Negative
...but eitherway the file is quite large >75MB and so data transforms from character to data.frame like that seem to take a long time....so its not the ideal solution at the moment, given that the s3 data has already been uploaded as a csv file...am still hoping for the parameter values to adjust the API request to just download the data.
  Negative
563d43d1a4387b6f44e9239a	X	Replace strsplit( test, "\\n" ) with strsplit( test, "\\r\\n" ), or just `gsub( "\\r", "", test) before you run any of the other code.
  Negative
I'm not sure what you mean by "just download the data," as it seems to me that what it gave you is the data, in comma-separated form.
  Negative
563d43d1a4387b6f44e9239b	X	perhaps I should have said "just download the file" rather than getting the data in comma separated form...from the get request...
563d43d1a4387b6f44e9239c	X	So much better than my attempts to reinvent the wheel.
  Negative
Likely faster too.
  Positive
563d43d1a4387b6f44e9239d	X	that simple solution works out quite well actually...but now the biggest holdup appears to be at the conversion of the response to the character string...using content...the code I am currently using is x <- GET(end.point, add_headers(Date=time.string,Authorization=authorization.string), query=params) y <- read.csv(text=content(x)) any ideas on how to speed that up?
  Negative
563d43d1a4387b6f44e9239e	X	My guess is that the server response is quite a bit slower than the read.csv step.
  Negative
Have you profiled it with system.time?
  Neutral
(Also: Using colClasses is known to speed up all read.
  Neutral
* functions.)
  Neutral
563d43d1a4387b6f44e9239f	X	I would like to be able to download a .
  Negative
csv file from my Amazon S3 bucket using R. I have started using the API that is documented here http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectGET.html I am using the package httr to create the GET request, I just need to work out what the correct parameters are to be able to download the relevant file.
  Negative
I have set the response-content-type to text/csv as I know its a .
  Negative
csv file I hope to download...but the response I get is as follows: And no file is downloaded and the data seems to be in the response...I can extract the string of characters that is created in the response, which represents the data, and I guess with some effort it can be converted into a data.frame as originally desired, but is there a better way of downloading the data...straight from the GET command, and then using read.csv to read the data?
  Very negative
I think that it is a parameter issues...just not sure what parameters need to be set for the file to be downloaded.
  Negative
If people suggest the conversion of the string...This is the structure of the string I have...what commands would I need to do to convert it into a data.frame?
  Negative
Thanks HLM
563d43d1a4387b6f44e923a0	X	Here's one way: Now convert to a data.frame:
563d43d1a4387b6f44e923a1	X	The answer to your second question: If you want extra speed for the read.csv, try this: Assuming the URL is set up properly (and we have nothing to test this on yet) I'm wondering if you may want to look at the value for GET( ...)$content Perhaps: That was not correct because the data comes across as "raw" format.
  Very negative
One needs to convert from raw before it will become encoded as text.
  Negative
I did a quick search of Nabble (it must be good for something after all) to find a csv file that was residing on the Web.
  Negative
This is what finally worked:
563d43d1a4387b6f44e923a2	X	Go for the SDK.
  Negative
I'm using C++ (which doesn't have an SDK) and I go to the extreme of using embedded python and the python SDK rather than using the REST API.
  Negative
There's just no point spending time implementing, debugging and maintaining something which is already done.
  Negative
Also Amazon's REST API is badly documented and often not intuitive.
  Negative
563d43d2a4387b6f44e923a3	X	I think it's very valuable to be able to use a little bit of the REST API directly, so you understand what's going on.
  Positive
But I would use the SDK for real work.
  Negative
563d43d2a4387b6f44e923a4	X	Thanks guys for your suggestion.
  Positive
I would be going for SDK only.
  Neutral
563d43d2a4387b6f44e923a5	X	Thanks Sid for your answer!
  Negative
563d43d2a4387b6f44e923a6	X	I have to use Amazon S3 to upload some static contents programmatically in Java.
  Negative
When I started reading I found that the way to do is either through their SDK (wrapper over REST APIs) or using REST APIs.
  Negative
From Amazon's AWS website, found this: "You can send requests to Amazon S3 using the REST API or the AWS SDK".
  Negative
Wanted to understand that which approach is better.
  Positive
I think using SDK will definitely make programming easier, but what are the pros and cons of using SDK Vs Rest APIs directly.
  Neutral
For some reason, I found using REST API directly more difficult than SDK.
  Negative
I was able to do basic things using SDk - create bucket, list objects, get object, invalidate cache etc.
  Negative
But was having some hard time writing the code for REST API - especially generating the signature.
  Negative
May be it will not matter much, if I ultimately use SDK, but I would still like to know how to do it using REST APIs.
  Negative
If anyone has some good code examples in Java on adding objects, get objects, get list etc, it would be very helpful.
  Negative
Thanks!
  Positive
563d43d2a4387b6f44e923a7	X	If you already have the SDK in your language, go for it; it's a no-brainer from a project perspective.
  Negative
The SDK is additional engineering that they have already done and tested for you at no additional cost.
  Negative
In other words, the SDK is already converting the HTTP REST API into the application domain/language for you.
  Negative
Think of the REST API as a the lowest common denominator that AWS must support and that the SDK (likely) as implementing the REST API below it.
  Negative
In some cases (eg: some Windows Azure SDKs), the SDKs are actually more efficient because they switch to a TCP based communications channel instead of REST (which is TCP plus HTTP), which eliminate some of the HTTP overhead Unless your entire goal is to spend additional time (development + testing) just to learn the underlying REST API, I'd vote SDK.
  Negative
563d43d2a4387b6f44e923a8	X	What are you passing in for an Authorization header?
  Negative
563d43d2a4387b6f44e923a9	X	Thanks @Jason.
  Neutral
Header is 1.
  Neutral
Connection Request Host: s3.amazonaws.com Date: x-amz-content-sha256:e3b855 Authorization: AWS4-HMAC-SHA256 Credential=XXX/20150618/us-east-1/s3/aws4_request, SignedHeaders=date;host;x-amz-content-sha256, Signature=ec7518 Canonical Request: GET 2.
  Very negative
For Getting the Contents Host: balas3bucke01.s3-ap-southeast-1.
  Neutral
amazonaws.com Date: x-amz-content-sha256:e3b*855 Authorization: AWS4-HMAC-SHA256 Credential=XXX/20150618/ap-southeast-1/balas3bucke01/aws4_request, SignedHeaders=date;host;x-amz-content-sha256, Signature=fd**1429 Please let me know where am i going wrong
563d43d2a4387b6f44e923aa	X	I am using Amazon S3 REST API for listing the contents of my bucket.
  Very negative
I am able to establish a connection and get the list of my buckets with request URL being "https://s3.amazonaws.com" and http_request_type = "GET".
  Neutral
However when I try to list the contents of the bucket I am getting an error AuthorizationHeaderMalformed The authorization header is malformed; incorrect service "balas3bucke01".
  Negative
This endpoint belongs to "s3".
  Positive
balas3bucke01 is the name of the bucket.
  Neutral
My request URL is https://balas3bucke01.s3-ap-southeast-1.amazonaws.com http_request_type = "GET" Why am I getting the above error.
  Negative
563d43d2a4387b6f44e923ab	X	Hi do you have sample code for file uploading to amazon s3 using REST API in java.
  Negative
.
  Neutral
Please send me if you have it....
563d43d2a4387b6f44e923ac	X	I'm trying to implement an HTML5 Amazon S3 uploader (by using the REST API), and stumbled upon the following issue: when trying to upload a small, text file, everything works like a charm.
  Neutral
When trying to upload a binary file, the file gets bigger on S3, and, obviously, corrupted.
  Negative
Here's what I'm doing: Also, I've tried to create a 10mb file with text (10 million lines of 0123456789) and that one works correctly.
  Positive
If anyone has a solution to this problem, or stumbled upon it, let me know.
  Negative
563d43d2a4387b6f44e923ad	X	It seems StackOverflow is also good for figuring things out yourself -- I've fixed it just as I finished putting my ideas down.
  Very negative
It seems the xhr.send() method can receive the file.slice() blob directly, so no need for FileReader.
  Negative
I hope this helps other people that stumble upon this issue.
  Positive
563d43d3a4387b6f44e923ae	X	I am facing some problems with the thingiverse api at uploading images to the amazon s3 storage.
  Negative
At step 3 of the file upload guide, amazon always answers with {"error":"Unauthorized"}.
  Negative
Do you have any hints for me, what I might be doing wrong?
  Negative
This is what i have done: Send a POST request to http://api.thingiverse.com/things/629436/copies/ with content: The response is: So I send a request to https://thingiverse-production-new.s3.amazonaws.com/ with body: The response I get is: {"error":"Unauthorized"}.
  Negative
Am I missing any authentication fields?
  Negative
I also tried altering the order of the multipart/formdata parameters.
  Negative
I tried the one i got from the thingiverse api respone as well as the on in the file upload guide.
  Negative
Any help appreciated!
  Positive
563d43d3a4387b6f44e923af	X	This class is NOT from Amazon.
  Negative
This is a third-party class.
  Positive
563d43d3a4387b6f44e923b0	X	I have a website hosted on amazon.
  Negative
I want my clients to give access to upload files that are already in their amazon s3 space to my s3 space.
  Negative
Is there any php API that supports this functionality?
  Negative
563d43d3a4387b6f44e923b1	X	Amazon actually provides one.
  Positive
And there are lots of examples on the web of using it.
  Negative
Google is your friend.
  Neutral
563d43d3a4387b6f44e923b2	X	Amazon providing one PHP API for uploading files to s3 bucket.
  Negative
its a single php file named s3.php You just download that and from your code .
  Negative
for more read this.
  Neutral
563d43d3a4387b6f44e923b3	X	Why does it not look possible?
  Negative
It is just a REST API after all.
  Negative
563d43d4a4387b6f44e923b4	X	To the best of my knowledge the only Cocoa/Cocoa Touch based toolkit for accessing S3 is ConnectionKit.
  Negative
Otherwise you are stuck with building your own, which could become quite a complex task.
  Negative
May I ask which bits of the API you require?
  Neutral
The current release of ConnectionKit does support S3 but is a bit ropey.
  Negative
We're currently in the process of writing the 2.0 version.
  Negative
If we have someone to work with specifically for one protocol, we could focus purely on that for now to our mutual benefit.
  Negative
Please contact me at mikeabdullah.net/other/contact_me.html for further discussion if interested
563d43d4a4387b6f44e923b5	X	The reason why it appears not possible is that each authenticated request to S3 servers needs to have a RFC 2104HMAC-SHA1 signature generated and sent along with the request.
  Negative
To my knowledge, there is currently no way to do this on the iPhone.
  Negative
Am I incorrect or just missing something?
  Negative
563d43d4a4387b6f44e923b6	X	On the iPhone there is CCHMAC(3) which offers the required functionality
563d43d4a4387b6f44e923b7	X	Thanks Mike.
  Negative
No sooner did I write my last comment I came across that exact library.
  Negative
I hadn't realized it was in there.
  Negative
That's exactly what I am going with.
  Negative
Thank you again!
  Positive
563d43d4a4387b6f44e923b8	X	I think you're a little confused.
  Negative
NSConnection is for Distributed Objects.
  Negative
Very different to NSURLConnection!
  Positive
563d43d4a4387b6f44e923b9	X	Yes, you're right, I meant NSURLConnection.
  Positive
Will update answer.
  Neutral
563d43d4a4387b6f44e923ba	X	Does anyone have any suggestions for GETing from or POSTing to Amazon S3 services using their REST API via the iPhone.
  Negative
It does not look like it is possible but I may be reading the docs wrong.
  Negative
Thank you in advance for your help!
  Positive
L.
563d43d4a4387b6f44e923bb	X	In a general case I'd recommend to use ASIHttpRequest, it has a lot of built-in functionality (REST compatible too) and a lot of things, making life easier than with NSURLConnection.
  Negative
It also has S3 support out of box.
  Negative
563d43d4a4387b6f44e923bc	X	You should be able to use the NSURLRequest stuff to do what you want.
  Positive
This doesn't have any error checking in it and the _data variable should be stored in an instance variable, but the general idea should work for you.
  Negative
You will probably also need to set some request headers to tell the server what encoding the body data is in and so on.
  Negative
563d43d4a4387b6f44e923bd	X	Thank you vey much for the response, there might be one more approach to communication the keys with the iphone online every time with some encryption, but again when its going to the client there is no way guarantee of its security.
  Negative
As it is said: "The only secure computer is one that's unplugged, locked in a safe, and buried 20 feet under the ground in a secret location... and I'm not even too sure about that one"
563d43d5a4387b6f44e923be	X	I'm able to upload files from iPhone using ASIHTTPRequest wrapper for an application which allows simple storage to my account.
  Negative
The question i'm concerned about is, could distributing the access keys along with the application be a good idea?
  Negative
what is the best way to deal with it in terms of security?
  Positive
are the keys i use sniffable via monitors over https?
  Negative
any suggestions over it will be appreciated.
  Neutral
563d43d5a4387b6f44e923bf	X	I upload files to a server (using ASIHTTPRequest) and then from the server to an AWS account for this very reason.
  Negative
I can control the security on the server much easier than I can on devices.
  Negative
Plus, if I need to change the keys I can do it on the server very quickly.
  Negative
This will add another layer to your application but I think it's well worth it.
  Positive
You can also check out this post Architectural and design question about uploading photos from iPhone app and S3
563d43d5a4387b6f44e923c0	X	Uploading a file directly to S3 is not really a trivial task, especially if you want to support chunking, auto-resume, user metadata, etc, etc.
  Very negative
The policy stuff can be quite complex.
  Positive
Consider using a library I maintain: Fine Uploader.
  Negative
It has native support for direct uploads to S3 in all browsers, even IE7.
  Negative
Chunking and auto-resume, among other features, are also supported.
  Positive
Furthermore, I wrote a node.js server-side example myself that, when paired with Fine Uploader S3, will handle all signatures for you.
  Negative
563d43d5a4387b6f44e923c1	X	can you post this comment as an answer?
  Neutral
i may end up using your library.
  Neutral
still evaluating how it works, etc.
563d43d5a4387b6f44e923c2	X	I'm not sure that will go over well.
  Negative
It may be considered a poor or link-only answer, quite frankly.
  Negative
My understanding is that the community is looking for details answers that include code, and mine doesn't fit that description, which is why I posted it as a comment.
  Negative
If you do have any questions about Fine Uploader, have a look at the fine-uploader tag on SO though, where we handle support questions for the library.
  Negative
563d43d5a4387b6f44e923c3	X	THANK YOU!
  Positive
This code helped me out.
  Neutral
Some quick comments: To format the date I used moment.js like so : moment.utc(expirationDate).
  Negative
format('YYYY-MM-DD')+'T'+moment.utc(expirationDate).
  Negative
format('HH:mm:ss.SSS')+'Z'.
  Negative
Also for buffers 'utf8' (note: no hyphen) is default encoding so I think "utf-8" is incorrect and extraneous.
  Negative
563d43d5a4387b6f44e923c4	X	@Zugwalt, you could simplify that quite a bit with moment's built in formatting.
  Negative
moment.utc(expirationDate).
  Neutral
toISOString()
563d43d5a4387b6f44e923c5	X	@Jonathan even better!
  Negative
Thanks!
  Positive
563d43d5a4387b6f44e923c6	X	I'm trying to get an built that allows users to upload a file directly to my Amazon S3 bucket, from a NodeJS powered website.
  Negative
It seems the only tutorials out there, other than the actual amazon docs for this are all very out of date.
  Negative
I've been following this tutorial, for the basic info, but again it's out dated.
  Negative
It doesn't have the method calls to crypto correct, as it tries to pass a raw JavaScript object to the update method, which throws an error because it's not a string or buffer.
  Negative
I've also been looking at the source for the knox npm package.
  Negative
It doesn't have POST support built in - which I totally understand, because it's the browser doing the POST once it has the right fields.
  Negative
Knox does appear to have the right code to sign a policy, and I've tried to get my code working based on this... but again to no avail.
  Negative
Here is what I've come up with, for code.
  Neutral
It produces a base64 encoded policy, and it creates a signature... but it's the wrong signature according to Amazon, when I try to do a file upload.
  Negative
I'm obviously doing something wrong, here.
  Negative
But I have no idea what.
  Negative
Can anyone help identify what I'm doing wrong?
  Negative
Where my problem is?
  Negative
Does anyone have a working tutorial for how to generate a proper Amazon S3 Policy, with signature, from NodeJS v0.10.x, for a POST to the s3 REST api?
  Neutral
563d43d6a4387b6f44e923c7	X	Ok, I finally figured it out.
  Negative
After playing the random guessing game for a VERY long time, I thought to myself "maybe i need to sign the base64 encoded policy" - me and BAM that was it.
  Negative
I also re-ordered the conditions to match how the form is posting, though I'm not sure this makes a difference.
  Negative
Hopefully this will help others that run in to the same problem.
  Negative
563d43d6a4387b6f44e923c8	X	I modified a bit previous example, because it didn't work for me: amazon returned an error about broken signature.
  Negative
Here is how the signature should be created for Browser-Based Uploads Using POST (AWS Signature Version 4) http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-authentication-HTTPPOST.html  Next generated base64Policy and s3Signature i used in the form for uploading.
  Negative
Example is here: http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html Very important is to check that you have the same fields and values in the html form and in your policy.
  Positive
563d43d6a4387b6f44e923c9	X	why you add fileOwnerId to stringToSign?
  Negative
563d43d6a4387b6f44e923ca	X	Thanks for your reply @okwap .
  Negative
I have added that because thats the way i have saved files on s3.
  Negative
like bucketname/username/file.
  Neutral
txt
563d43d6a4387b6f44e923cb	X	I am trying to delete amazon s3 object using rest API but not getting any success.
  Very negative
I have created the URL(signed url) at server side using java and then made XHR request to that URL at client side(i.e. from browser).
  Negative
Java code that i have used to sign the url: And at client side: Using this code for downloading an object from amazon s3 bucket works fine by replacing 'DELETE' request with 'GET'.
  Neutral
But delete is not working.
  Negative
I have searched a lot but there is very less help available for rest API.
  Negative
563d43d6a4387b6f44e923cc	X	Finally, i integrated the aws sdk to delete the object from amazon s3 bucket and it works like lightning.
  Negative
But unable to get help doing it with rest API.
  Negative
So now i have used rest API for uploading and downloading and the sdk for deleting an object.
  Negative
563d43d6a4387b6f44e923cd	X	moved to Amazon EC2 and the connection speed DRAMATICALLY increased .
  Negative
Though this still alludes me as to why it would be so slow on a non EC2 instance
563d43d6a4387b6f44e923ce	X	Because their operate on the same network (your Ec2 instance and S3)
563d43d6a4387b6f44e923cf	X	Been trying to figure out why uploading to Amazon S3 is amazingly slow using the putObject command (node.js library).
  Very negative
The code below reads an entire directory of files and puts them to S3 asynchronously.
  Negative
Tested with a number of different folders with similar results.
  Neutral
Uploading the same files using the AWS web interface takes around 3 sec to complete (or less).
  Negative
Why is using the node.js API so slow??
  Negative
As per Amazon documentation I've even tried spawning multiple children to handle each upload independently.
  Negative
No changes in upload speed.
  Neutral
563d43d6a4387b6f44e923d0	X	Thank you @Lucas Polonio.
  Negative
I heard about this gem, but I wanted to use the aws sdk.
  Neutral
563d43d6a4387b6f44e923d1	X	Im developing a website with AngularJS in frontend that sends requests to a Rails 4 API backend.
  Negative
I have to manage quite images, so I would like to use Amazon S3 (but Im newbie with this and Im a bit lost).
  Negative
Before using S3, I used an angular directive to upload images to Rails.
  Negative
Rails got this image and stored it in a path in the server.
  Positive
Something like this: where photo is the image uploaded to rails: Im trying to do the same but instead of storing the photo in the Rails server, I would like to do it in S3.
  Negative
Im doing something like this (but I recognize, I dont completely understand how it works, so for sure something is wrong).
  Negative
This is my code with S3: Im getting this error: Im confused with the concepts of key and file name.
  Very negative
Is the key the path where I would like to store my image in S3?
  Negative
563d43d6a4387b6f44e923d2	X	It works.
  Positive
I had just need to replace the obj.write(Pathname.new(key)) with obj.write(photo)
563d43d7a4387b6f44e923d3	X	Good that your solution worked.
  Negative
Anyway, you could take a look at the paperclip gem.
  Positive
It handles file uploads with lots of features, including automatically uploading to S3.
  Negative
563d43d7a4387b6f44e923d4	X	Can I send Large file with it?Not Archive just single file
563d43d7a4387b6f44e923d5	X	Yes, I see no difference between archives and other file types.
  Negative
The only difference comparing to your multipart-upload may be that if something happens (connection lost) during upload - you have to start from scratch.
  Negative
But that's a rare case and was completely fine for us (retry policy handled this).
  Positive
563d43d7a4387b6f44e923d6	X	Thank you so much
563d43d7a4387b6f44e923d7	X	you're a lifesaver!
  Very positive
563d43d7a4387b6f44e923d8	X	I am using Amazon S3 Low Level API for uploading Large Video File, I am following This link When I am upoading the file, its giving me exception I have checked Inner Exception and its saying this at this line and this is how I am making my S3Client I also tried changing bucketname like bucketname/filename.
  Very negative
mp4,but its giving exception I also tried some other file(doc and pdf) it is also giving XML exception.
  Negative
Is there any good alternate approach for uploading Large Video files(Around 200-500MB)?
  Negative
563d43d7a4387b6f44e923d9	X	I used to send archives to S3 (around 100-300MB).
  Negative
My code looked like this: That's it basically.
  Neutral
I had retry-policy and exception handling around that, but this is the core.
  Neutral
So just simple PutObject function without any multipart uploads works find for such file-sizes.
  Negative
563d43d7a4387b6f44e923da	X	I've found a similar problem during a MultiPart upload using the sample code in the doc.
  Negative
I've found that the ETag list is mandatory for the CompleteMultipartUpload part - which is not in the documentation sample.
  Negative
This link has a better explanation of the multi-part upload process: s3 multipart upload
563d43d7a4387b6f44e923db	X	I am looking to upload images to amazon S3 using the rest api that they provided.
  Negative
I got to know how to calculate the signing key for SigV4 from this document.
  Negative
This documentation tells you how the request should be signed.
  Neutral
But I find it highly confusing as to what should be signed and where should the cannonical request be placed?
  Negative
Should it be placed in a separate header in the request?
  Neutral
Is there a working example/sample to use SigV4 rest api using java?
  Negative
563d43d8a4387b6f44e923dc	X	If you have a very specific reason for not using the provided SDK, the quickest path to getting this working it to look at how the requests are performed in a library where this is already working.
  Negative
You can look at the Java SDK itself to figure this out, but that's a bit dense.
  Negative
Here is my favorite, although I think it's on sig v3: http://geek.co.il/2014/05/26/script-day-upload-files-to-amazon-s3-using-bash You can find out similar examples for v4: http://geek.co.il/2014/11/19/script-day-amazon-aws-signature-version-4#footnote_0_33255 You can see how everything is compute and what is to be passed in the headers in very few lines of code.
  Negative
EDIT Look at http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-examples-using-sdks.html#sig-v4-examples-using-sdk-java for exactly what you are looking for.
  Neutral
It has the bare minimum to get this going in java.
  Negative
563d43d8a4387b6f44e923dd	X	Are you sure the bucket and key are correct?
  Neutral
563d43d8a4387b6f44e923de	X	Did you find a solution for this?
  Neutral
I have the exact same problem
563d43d8a4387b6f44e923df	X	I'm using the s3_direct_upload gem to store images and videos on Amazon s3.
  Negative
When the image or video is changed or deleted, I want to nuke the old image or video on s3 and save everyone money and space.
  Negative
This solution uses the V1 Aws SDK and is no longer valid: http://blog.littleblimp.com/post/53942611764/direct-uploads-to-s3-with-rails-paperclip-and This solution deletes files that were initially uploaded in a batch, but does nothing for the final files post-processing: github - waynehoover/s3_direct_upload Here is the Aws v2 SDK doc, which seems clear enough: http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Client.html#delete_object-instance_method Yet this solution: ...returns only: (And the file is still available on s3 at the original url.)
  Negative
Thoughts?
  Neutral
Hasn't everyone had to do this?
  Negative
563d43d8a4387b6f44e923e0	X	I've written code using the v2 SDK to delete objects from S3.
  Negative
Here is a sample from my codebase: It looks similar to yours, so I don't think that this code is the issue.
  Negative
Have you confirmed your bucket & key names and ensured that your method is actually being called?
  Neutral
563d43d8a4387b6f44e923e1	X	What is s3 galcier?
  Negative
I know s3 and I know glacier but what is s3 glacier?
  Negative
563d43d8a4387b6f44e923e2	X	Python package boto is offering such an API too: boto.readthedocs.org/en/latest/ref/glacier.html
563d43d8a4387b6f44e923e3	X	I want to list all the object on amazon glacier.
  Negative
So that i can restore require object from glacier.
  Negative
Is there any amazon api to list all object on glacier.
  Negative
563d43d8a4387b6f44e923e4	X	Check this: http://docs.aws.amazon.com/amazonglacier/latest/dev/using-aws-sdk.html These APIs are only supported via AWS JAVA SDK and .
  Negative
NET SDK.
  Neutral
And also check the Glacier API Documentation: http://docs.aws.amazon.com/amazonglacier/latest/dev/amazon-glacier-api.html
563d43d9a4387b6f44e923e5	X	Can you script it with awscli or s3cmd, rather than write it in Java?
  Negative
Using Java seems heavy-handed here.
  Negative
563d43d9a4387b6f44e923e6	X	The things haven't changed in this regard.
  Negative
People have developed libraries that make use of the s3 apis and parallelize the uploads.
  Neutral
563d43d9a4387b6f44e923e7	X	@TJ- Can you provide an example?
  Neutral
563d43d9a4387b6f44e923e8	X	github.com/tj---/s3-parallel
563d43d9a4387b6f44e923e9	X	does it spawn n upload processes performed in parallel or does it spawn a single upload process for all of the objects (therefore needing only one connection)?
  Negative
I hope it's the latter
563d43d9a4387b6f44e923ea	X	It performs N independent uploads - how many will be executed at a time depends on what kind of ExecutorService you pass to the constructor.
  Neutral
S3 does not expose a way to upload multiple objects in a single HTTP request besides manually zipping them up.
  Negative
And even then you'd probably want to do a multi-part upload and split the zip over multiple HTTP requests so if there's a transient failure halfway through you don't have to start the whole upload over from scratch...
563d43d9a4387b6f44e923eb	X	We're looking to begin using S3 for some of our storage needs and I'm looking for a way to perform a batch upload of 'N' files.
  Very negative
I've already written code using the Java API to perform single file uploads, but is there a way to provide a list of files to pass to an S3 bucket?
  Negative
I did look at the following question is-it-possible-to-perform-a-batch-upload-to-amazon-s3, but it is from two years ago and I'm curious if the situation has changed at all.
  Positive
I can't seem to find a way to do this in code.
  Negative
What we'd like to do is to be able to set up an internal job (probably using scheduled tasking in Spring) to transition groups of files every night.
  Negative
I'd like to have a way to do this rather than just looping over them and doing a put request for each one, or having to zip batches up to place on S3.
  Negative
563d43d9a4387b6f44e923ec	X	The easiest way to go if you're using the AWS SDK for Java is the TransferManager.
  Negative
Its uploadFileList method takes a list of files and uploads them to S3 in parallel, or uploadDirectory will upload all the files in a local directory.
  Negative
563d43d9a4387b6f44e923ed	X	This was giving me a major head-ache, so I thought I'd post the easy solution.
  Negative
My issue was that when using the Java API for Amazon's S3, I could only download 50 objects before it would mysteriously time out.
  Negative
The code looked something like this: It would run and process everything fine for exactly 50 objects, and then time out.
  Negative
563d43d9a4387b6f44e923ee	X	For whatever reason, the main issue is that I had declared s3 as AmazonS3Client s3.
  Negative
It should have looked like: Just in case anyone else runs into this problem.
  Negative
563d43d9a4387b6f44e923ef	X	Hopefully while you may callling getObject to download it, you are not closing the InputStream.
  Negative
which is optioned by calling getObject(); you have to close InputStream after dealing with each object.
  Negative
more details read it : http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html#getObject(com.amazonaws.services.s3.model.GetObjectRequest) Thanks
563d43d9a4387b6f44e923f0	X	For Scala developers, here it is recursive function to execute a full scan and map of the contents of an AmazonS3 bucket using the official AWS SDK for Java To invoke the above curried map() function, simply pass the already constructed (and properly initialized) AmazonS3Client object (refer to the official AWS SDK for Java API Reference), the bucket name and the prefix name in the first parameter list.
  Very negative
Also pass the function f() you want to apply to map each object summary in the second parameter list.
  Negative
For example will return the full list of (key, owner, size) tuples in that bucket/prefix or will return the total size of its content (note the additional sum() folding function applied at the end of the expression ;-) You can combine map() with many other functions as you would normally approach by Monads in Functional Programming
563d43daa4387b6f44e923f1	X	There's now Fake S3 as well.
  Negative
563d43daa4387b6f44e923f2	X	+1 for Fake S3 - that was the easiest replacement in my project.
  Negative
Two lines of code changed and voila - we support local fileserver.
  Negative
License also looks good.
  Positive
563d43daa4387b6f44e923f3	X	S3ninja does not currently provide folders.
  Negative
This is a deal breaker for me.
  Negative
563d43daa4387b6f44e923f4	X	Is SwiftFS compatible to Amazon S3?
  Neutral
My code base works already with S3 and I'm looking for a solution which is compatible to S3 so that I don't need change my code.
  Positive
563d43daa4387b6f44e923f5	X	No, SwiftFS isn't compatible to Amazon S3, but you could take a look at my other project: RioFS github.com/skoobe/riofs which is a userspace filesystem which operates with Amazon S3 buckets.
  Negative
563d43daa4387b6f44e923f6	X	We make distributal software that stores some data (attachments) in a) a database or b) Amazon S3.
  Negative
The database is used because it requires no other configuration.
  Negative
Amazon S3 is the better option.
  Negative
What we want now is a solution for customers that don't want to use Amazon S3.
  Negative
We can obviously just use the filesystem but this can be problematic if there are multiple web servers and the files need to be replicated; it also requires us to write extra code to handle the various permuations of problems that can happen.
  Very negative
What we would prefere is if there was a piece of server software that essentially replicates Amazon S3's API.
  Negative
That way our clients can install the server on a box; and we don't need to change any code.
  Negative
So ... is there any such software out there?
  Negative
563d43daa4387b6f44e923f7	X	This is possible via OpenStack Object Storage (code-named Swift), which is open source software for creating redundant, scalable object storage using clusters of standardized servers, specifically its recently added (optional) S3 API layer, which emulates the S3 REST API on top of Object Storage.
  Negative
See Configuring Object Storage with the S3 API for the official documentation - a more insightful and illustrated small tutorial regarding the entire setup is available in S3 APIs on OpenStack Swift (which builds on the more complex Installing an OpenStack Swift cluster on EC2 though).
  Negative
An noteworthy alternative is Ceph, which is a unified, distributed storage system designed for excellent performance, reliability and scalability - interestingly it provides all three common storage models, i.e. Object Storage, Block Storage and a File System and the RADOS Gateway provides Amazon S3 and OpenStack Swift compatible interfaces to the RADOS object store [emphasis mine], see RADOS S3 API for details on currently supported S3 API features.
  Negative
563d43daa4387b6f44e923f8	X	Have you looked at Cloudian?
  Neutral
We use it internally at our company to develop our S3 app.
  Negative
I'm using the Community Edition which is free for up to 10TB of storage.
  Negative
It's got pretty good S3 coverage or at least covers most of the stuff my app uses (I use versioning and multipart uploads so I think my app is advanced).
  Neutral
The version-ids and multipart ids etc that it generates are different than those you get from AWS but boto has no complaints so far.
  Negative
It also works with s3fs and other s3 bucket browsers that I have tried.
  Positive
In my opinion it's a good tool for development against the AWS S3 API and should meet your requirements.
  Negative
You can point your app at your local Cloudian server and then when you are ready for production you can point it back at Amazon.
  Negative
Your mileage may vary... Good luck.
  Positive
563d43dba4387b6f44e923f9	X	We ran into the problem of testing our S3 based code locally and actually implemented a small Java server, which emulates the S3 object API.
  Negative
As it might be useful to others, we setup a github repo along with a small website: http://s3ninja.net - all OpenSource under the MIT license.
  Negative
Being quite simple and minimalistic, this tool is perfect for testing and developement purposes.
  Positive
However, to use in in production, one might want to add some security (altough the AWS hashes are already verified in the API - just the GUI is completely unprotected).
  Neutral
Also, it doesn't do any replication or scaling.
  Negative
So this wouldn't be a good choice for large setups.
  Negative
563d43dba4387b6f44e923fa	X	As it was already mentioned: you could try to use Swift as Amazon S3 alternative.
  Negative
Take a look at SwiftFS filesystem, it let you mount OpenStack container stored in Swift as a local filesystem.
  Negative
563d43dba4387b6f44e923fb	X	I recently started using Skylable for my S3 needs, it's free (GPL).
  Negative
Their object storage supports replication, HA and deduplication and it's fully S3 compatible.
  Negative
You can run their software on a single server (iron, virtual machine or container) if you don't need redundancy or you can use more nodes if you need HA.
  Negative
The number of replicas can be chosen per bucket, just like with Swift.
  Negative
I started with 2 nodes in replica 2 and added more nodes as our userbase started growing, to cope with the extra network traffic and the space requirements.
  Positive
Adding more nodes is really easy and can be done on a live cluster.
  Positive
In my experience Skylable proved to be faster and more reliable than Swift.
  Positive
It's written in C and OCaml, it's not interpreted.
  Negative
The memory footprint is really low, so I can run a node even on some cheap VPS.
  Very negative
Recently they announced to be working on Swift APIs, apparently their goal is to replace Swift.
  Negative
563d43dba4387b6f44e923fc	X	Hi minjoon, EBS is not something to consider if you want to scale.
  Negative
Consider EBS as a pendrive, you can attach to a single host, EBS don't work as NAS or S3.
  Negative
563d43dba4387b6f44e923fd	X	Did you found the best solution minjoon?
  Neutral
I'm in the Middle of the same decision of what is a better solution.
  Neutral
Cheers
563d43dba4387b6f44e923fe	X	I dont think s3fs can be used in production.
  Negative
563d43dba4387b6f44e923ff	X	I will be launching an application in the very near future which will, in part, require users to upload files (images) to be viewed by other members.
  Negative
I like the idea of S3 as it is relatively cheap and scales automatically.
  Negative
My problem is how I will have users upload their images to S3.
  Positive
It seems there are a few options.
  Neutral
1- Use the php REST API.
  Neutral
The only problem is that I can't get it to work for uploading variously scaled versions (ie thumbnails) of the same image simultaneously and uploading them directly to s3 (it works for just one image at a time this way).
  Negative
Overall, it just seems less flexible.
  Neutral
http://net.tutsplus.com/tutorials/php/how-to-use-amazon-s3-php-to-dynamically-store-and-manage-files-with-ease/ 2- The other option would be to mount an S3 bucket with s3fs.
  Neutral
Then just programmatically move my images into the bucket like I would with NFS.
  Neutral
From what I've read, it seems some people are dubious of the reliability of mounting S3.
  Negative
Is this true?
  Neutral
http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=fuse+over+amazon Which method would be better for maximum reliability and speed?
  Negative
Would EBS be something to consider?
  Neutral
I would really like to have a dedicated box rather than use an EC2 instance, though...
563d43dba4387b6f44e92400	X	For your use case I recommend to use the S3 API directly rather than using s3fs because of performance.
  Negative
Remember that s3fs is just another layer on top of S3's API and it's usage of that API is not always the best one for your application.
  Negative
To handle the creation of thumbnails, I recommend to decouple that from the main upload process by using Amazon Simple Queue Service.
  Negative
That way your users will receive a response as soon as a file is uploaded without having to wait for it to be processed resulting in shorter response times.
  Negative
As for using EBS, that is a different scenario.
  Positive
EBS is just a persistent storage for and Amazon EC2 instance and it's reliability doesn't compare with S3.
  Negative
It's also important to remember that S3 only offers "eventual consistency" as opposed to a physical HDD on your machine or an EBS instance on EC2 so you need to code your app to handle that correctly.
  Positive
563d43dba4387b6f44e92401	X	Thanks!
  Positive
Do you know how I could make it respond with the URL of the uploaded image with a POST or GET?
  Negative
563d43dda4387b6f44e92402	X	onprogress: Called with a ProgressEvent whenever a new chunk of data is transferred.
  Negative
(Function)
563d43dda4387b6f44e92403	X	I am developing an App where users can post photos.
  Negative
I have the app working well using Imageshack's API where basically all you have to do is write a HTML form with a post to imageshack and then it redirects the post to a page of your choice where I then use the received information to store a location of the image in my database.
  Negative
My problem is I've heard bad things about Imageshack's reliability/scaleability and I want to move to Amazon's S3.
  Negative
Is it possible to upload a photo to S3, then get a simple response with the location of the image that I can then store in my database via PHP?
  Negative
Thanks, Dan.
  Positive
563d43dda4387b6f44e92404	X	Yes you can, simple you can use file transfer method of phonegap use your amazone bucket path for url.
  Negative
if you need to upload larger files some times phonegap filetransfer may fail so you can write some native plugin (i tried video upload to amazone s3 and its succes )
563d43dda4387b6f44e92405	X	I am currently using S3 with the Java API to get objects and their content.
  Negative
I've created a Cloudfront distribution using the AWS console and I set my S3 bucket with my objects as the Bucket-origin.
  Negative
But I didn't notice any improvement in the download performance, and I noticed in the console window the url refers to s3: INFO: Sending Request: GET https://mybucket.s3.amazonaws.com /picture.jpg Headers: (Range: bytes=5001-1049479, Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) whereas in the Getting Started guide for Cloudfront, the url should be: http://(domain name)/picture.jpg where (domain name) is specific to the Cloudfront distribution.
  Negative
So the Java API still is getting the file from S3 and not through cloudfront Is there anyway using the Java API for S3 to download files via Cloudfront?
  Negative
If not, what's the best approach I should use to get objects via cloudfront in my java program?
  Negative
I am still kinda new to this stuff, any help greatly appreciated!
  Positive
563d43dda4387b6f44e92406	X	JAVA API for S3 can not be used for interacting with Cloudfront.
  Negative
If you want to download the content through cloud front distribution, you have to write your own HTTP code (which should be simple).
  Negative
You can also just use http://(cloud front domain name)/picture.jpg in browser and check the download speed first.
  Negative
563d43dda4387b6f44e92407	X	But, you should know that it can take 24 hours or more for changes in S3 to be active.
  Negative
If you cannot open the stream, the other way is to use getObject(bucketName, key) method.
  Negative
563d43dda4387b6f44e92408	X	Why not use the sdk if it works?
  Negative
563d43dda4387b6f44e92409	X	I'm hoping to put it into an SSIS script task
563d43dda4387b6f44e9240a	X	Thanks for your suggestion.
  Negative
I made the modification but unfortunately it still doesn't work.
  Negative
563d43dda4387b6f44e9240b	X	Is the 403 message returning an error that gives you the string to sign and tells you the signature does not match, or no?
  Negative
What does the error say?
  Neutral
(in the response body)
563d43dea4387b6f44e9240c	X	The error message is stating that the signature does not match: <Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided.
  Very negative
Check your key and signing method.
  Neutral
</Message>
563d43dea4387b6f44e9240d	X	I'm trying to get some code working to fetch a file from S3 using the REST API via C#.
  Negative
I've seen other people doing similar things but for some reason I keep getting a 403 error.
  Negative
I've tried to do the same thing with the AWS SDK for .
  Negative
Net and it works so I assume it's the way I'm creating the authorization header.
  Negative
Is anyone able to shed any light on this please?
  Positive
563d43dea4387b6f44e9240e	X	I don't know if this is the only problem, but it looks like a definite problem: x-amz-date is the header that supercedes the Date: header in the HTTP request itself, but in the string to sign, you just put the date, without "x-amz-date:" or anything in front of it, according to the examples: http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html#RESTAuthenticationRequestCanonicalization There is only one correct signature that can be generated for a request.
  Very negative
S3 is going to generate that signature, and compare it to the one you sent, so there's not a single byte of room for error in the string-to-sign.
  Positive
563d43dea4387b6f44e9240f	X	I tested your code, it works!
  Positive
you just need an extra \n plus change http to https and you're done.
  Negative
Amazon Rest API don't have a good documentation, the lack of examples makes everyone go to the SDK instead.
  Negative
563d43dea4387b6f44e92410	X	I want to associate a bucket in X account that is created by account Y Account Y has given read and write permissons to X on the bucket via the Email ID This was done using S3Fox - however when I log into X account I see no way to associate the external bucket.
  Negative
I tried entering the bucket name as usual but didnt work So I would like to code my own association via php and rest but cant find the call in the API docs - can someone send me a link or example code on how to create an external bucket in account X Thanks
563d43dea4387b6f44e92411	X	Account X has to configure bucket from Y as "external bucket".
  Negative
but it is worse.
  Negative
the external bucket feature depends on the client software X uses.
  Neutral
so the information on wich buckets X listen to is stored at X`s client not at S3.
  Negative
X have to use a s3 client wich supports external buckets.
  Negative
Martin
563d43dea4387b6f44e92412	X	I am using the correct secret and access key.
  Neutral
.
  Neutral
as well as I have the permission for my user to access the s3 bucket, because I am able to upload the file using php and java with same credential.
  Negative
.
  Neutral
But for resume and pause I need the REST API in php.
  Negative
.
  Neutral
so i am running the rest api code and still i am getting the signaturedoes not match error.
  Negative
.
  Neutral
563d43dea4387b6f44e92413	X	I've been trying to get this to work for 1 week but always I am getting the same error.
  Negative
I also tried to debug the Signature function but I dnt where is the exact problem.
  Negative
.
  Neutral
I want to upload the file with progress bar as well as want to add the resume and pause functionality in REST-PHP.
  Negative
I am following the below link a :- http://www.anyexample.com/programming/php/uploading_files_to_amazon_s3_with_rest_api.xml Please provide me any proper solution.
  Negative
I am getting this response:- HTTP/1.1 403 Forbidden x-amz-request-id: 3B621260770DE679 x-amz-id-2: vuB+qHCRxq6CdRKIoso82GXO1O0gQNDEs5rLi3my/YiD535nyZQ6Ls64jZ5hB2KW Content-Type: application/xml Transfer-Encoding: chunked Date: Thu, 11 Dec 2014 09:01:52 GMT Connection: close Server: AmazonS3 3ef SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided.
  Very negative
Check your key and signing method.AKIAJA6EQQ475TUGTSEQPUT image/jpeg Thu, 11 Dec 2014 09:01:52 +0000 x-amz-acl:public-read /s3.regionname.amazonaws.com/bucket-name/Desss.
  Negative
jpgsdpF9q1WTYzHuLuytn7Dv+3xdIY=50 55 54 0a 0a 69 6d 61 67 65 2f 6a 70 65 67 0a 54 68 75 2c 20 31 31 20 44 65 63 20 32 30 31 34 20 30 39 3a 30 31 3a 35 32 20 2b 30 30 30 30 0a 78 2d 61 6d 7a 2d 61 63 6c 3a 70 75 62 6c 69 63 2d 72 65 61 64 0a 2f 73 33 2e 65 75 2d 77 65 73 74 2d 32 2e 61 6d 61 7a 6f 6e 61 77 73 2e 63 6f 6d 2f 6e 61 6e 6f 68 65 61 6c 2d 69 62 6d 2f 44 65 73 73 73 2e 6a 70 673B621260770DE679vuB+qHCRxq6CdRKIoso82GXO1O0gQNDEs5rLi3my/YiD535nyZQ6Ls64jZ5hB2KW 0
563d43dea4387b6f44e92414	X	I have seen that sort of error message when I'm using an invalid secret key and access key OR my permissions for my user aren't set up for the service I am trying to access.
  Very negative
A benefit of using the latest version of the SDK is that you will write less low level code and the error messages will probably be more helpful in identifying your issue.
  Negative
563d43e0a4387b6f44e92415	X	possible duplicate of Is there an API for Amazon Cloud (Drive and Player)?
  Negative
563d43e0a4387b6f44e92416	X	My App need to access user Amazon Cloud Drive Details.
  Negative
So that Through My App user can Login, Download, Upload and Delete etc Like There is API For Google Drive And Box.
  Negative
I get one Sample on Link But it is not What I Actually Need.
  Negative
Any response would be much appreciated.I have Also read this Link1 and Link2.
  Negative
And There is one Question is-there-an-api-for-amazon-cloud-drive-and-player.
  Positive
But There is also nothing which My App need And my need is api for Android not other platform.
  Negative
563d43e0a4387b6f44e92417	X	there is a Cloud Drive API available - https://developer.amazon.com/public/apis/experience/cloud-drive - not sure if it gives you exactly what you need
563d43e1a4387b6f44e92418	X	Within my research I came across many different sources, but somehow I fail to see, which side is generating the private API-key and how is the other side getting hold of it.
  Negative
Many people recommend Amazon S3 Restful API as the role model, hence if I understand that, I could create something similar for my own purposes.
  Positive
Amazon's S3 REST API.
  Neutral
e.g. this example here explains the process very nicely, however it fails to explain, which side is generating the API-key?
  Negative
So upon user signup, is it the service side that is generating the private API-key and assigns it to the user id in database?
  Negative
If this is the case though, the client needs to know the API key in order to create the signature for each request, so that the service can actually verify it.
  Negative
So how do both sides get hold of the private API key?
  Neutral
In my case I would have a iPhone app and a AngularJS web app as my clients talking to the RESTful API service.
  Negative
Many Thanks,
563d43e1a4387b6f44e92419	X	First, you don't want give out keys to your clients.
  Negative
In general, that's a security nightmare.
  Negative
(Also, key creation can take some hours to propagate.
  Negative
And you'll have to manage the permissions for each key, etc.) So all the signing is done by your server, and your key doesn't leave your server.
  Negative
You want your server to have the S3 key, but only return signed links that will give the client the power to do something (GET a particular file, or PUT a file).
  Negative
It's a bit like the mother-may-I game: The client asks you for a "S3 signed link", then it can talk to S3 to do one thing.
  Positive
Since your server is doing a trivial amount of work (checking request is authorized, then returning a signed URL), you will be able to scale pretty well.
  Neutral
For some things, like "list files" or "delete a file", it might be better for your server to call S3 (i.e. making a web request to S3 within the web request from the client) and return the results to the client (instead of messing with signed links).
  Negative
But if you do this, you may run into problems when scaling -- unless you are using the right technologies.
  Negative
(I.e. you want an evented server like node.js) Note that for a PUT request, the signed link must specify a lot of stuff ahead of time (like the file type, etc).
  Negative
You have to read the AWS spec carefully.
  Neutral
Be careful of the Confused Deputy problem.
  Negative
Your code will have one key that can see all user's files, so you are responsible for the security between users.
  Neutral
563d43e1a4387b6f44e9241a	X	Thanks for the answer.
  Neutral
I was hoping to learn how to do these operations with the REST API though.
  Negative
I only need one of the AWS services (S3), and only a couple of it's functions, so it seems overkill to bring in the entire AWS SDK.
  Very negative
Also multiple developers are working on this project and I want to avoid complications caused by plugins and getting everyone's workspaces running identically.
  Negative
Thanks for the tip on IAM.
  Neutral
I've noticed the S3 Management Console lets you get into a property page where you can set permissions and bucket policies.
  Negative
Is this essentially the same functionality?
  Neutral
563d43e1a4387b6f44e9241b	X	I'm building a project that will use Amazon S3 to store documents.
  Negative
In particular there are two apps: ...so the public app should only have 'write' permission, and the admin tool 'read' and 'list' permissions.
  Negative
The coding for this project will be done with ASP.NET and C#, and the preference is to use S3's REST API.
  Negative
This use case (uploading, listing and downloading documents) seems pretty basic, but I haven't had much luck finding simple examples.
  Negative
Can some you suggest some links?
  Neutral
563d43e1a4387b6f44e9241c	X	If you install the AWS SDK for .
  Negative
NET from http://aws.amazon.com/sdkfornet it will also put down a sample S3 application at C:\Program Files (x86)\AWS SDK for .
  Negative
NET\Samples\AmazonS3Sample\AmazonS3Sample that will show the basic CRUD operations for S3.
  Negative
As far as permissions go you should take a look at Identity and Access Management with AWS, http://aws.amazon.com/iam/.
  Negative
Using this service you can create different users with profiles that restrict access.
  Positive
So you could create one user for your public application that only has write access and even restrict write to a specific S3 bucket.
  Negative
Then create another user for admin application that has more admin permissions.
  Neutral
563d43e1a4387b6f44e9241d	X	Did you take a look at APIs for S3 Lifecycle Configuration?
  Neutral
I have done it using Python boto.
  Positive
Not sure about PHP.
  Neutral
563d43e1a4387b6f44e9241e	X	This question gives the impression that what you are in need of, first, is a more thorough understanding of how S3's Glacier integration actually works at cconceptual level... manually migrating to Glacier is not a thing, and when files are restored to S3 from Glacier, that's temporary; they are also still stored in Glacier, not moved back to S3.
  Negative
563d43e1a4387b6f44e9241f	X	I am creating a PHP based web application using Amazon's S3 and glacier services.
  Negative
Now I want to give my site users a feature that they can choose any file and make it archive (means move file from S3 to Glacier) and unarchive (means move file from Glacier to S3).
  Negative
I have done some research and didn't find any possible way using Amazon's API.
  Negative
PROBLEM How can I move files between S3 and glacier using API?
  Negative
563d43e1a4387b6f44e92420	X	You can use the API to define lifecycle rules that archive files from Amazon S3 to Amazon Glacier and you can use the API to retrieve a temporary copy of files archived to Glacier.
  Negative
However, you cannot use the API to tell Amazon S3 to move specific files into Glacier.
  Neutral
There are two ways to use Amazon Glacier: Connecting directly via the Glacier API allows you to store archives for long-term storage, often used as a replacement for Tape.
  Negative
Data stored via the Glacier API must also be retrieved via the Glacier API.
  Negative
This is typically done with normal enterprise backup software or even light-weight products such as Cloudberry Backup (Windows) or Arq (Mac).
  Negative
Using Amazon S3 lifecycle rules allows you to store data in Amazon S3, then define rules that determine when data should be archived to Glacier for long-term storage.
  Negative
For example, data could be archived 90 days after creation.
  Negative
The data transfer is governed by the lifecycle rules, which operate on a daily batch basis.
  Negative
The rules can be set via the putBucketLifecycle API call (available in the PHP SDK), but this only defines the rules -- it is not possible to make an API call that tells S3 to archive specific files to Glacier.
  Negative
Amazon S3 has a RestoreObject API call (available in the PHP SDK) to restore a temporary copy of data archived from Glacier back into S3.
  Negative
Please note that restoring data from Glacier takes 3-5 hours.
  Negative
563d43e2a4387b6f44e92421	X	You could use the Glacier API to upload a file to a Glacier vault, but I don't recommend it.
  Negative
The previous version of our backup app did that.
  Neutral
When you upload a file it gets a randomly-assigned name.
  Positive
You can add put your filename in the metadata of the file, but if you want a list of what's in the Glacier vault you have to query and then wait 3-5 hours for the list.
  Negative
Lifecycle policies are the other way to use Glacier.
  Negative
The current version of Arq uses them because each object still looks like an S3 object (no random object names, no delays in getting object lists), but the object contents are in Glacier storage.
  Negative
The only difference is that getting the object contents is a 2-step process: you have to make an API call to request that the object be made downloadable; when it's ready, you can download it.
  Negative
Also there's a "peak hourly request fee" that comes into play if you request objects be made downloadable at too fast a rate.
  Negative
Amazon Glacier pricing is complex.
  Positive
Once an object is "Glacier storage class" there's no way to change it back to "Standard storage class".
  Negative
You have to make a copy of the object that's "Standard storage class" and delete the Glacier object.
  Negative
So maybe a simple solution to your problem is:
563d43e2a4387b6f44e92422	X	I have a lots of documents stored on Amazon S3.
  Negative
My questions are: Does Amazon provide any services/APIs using which I can index the contents of the document and search them (full text indexing and searching)?
  Negative
If it does could someone please point me to any link in the documentation.
  Negative
If it does not then could this be achieved with Lucene and Zend Framework?
  Negative
Have any one of you implemented this?
  Neutral
Can I get some pointers?
  Neutral
UPDATE: I do not intend to save my index on Amazon S3 rather I am looking forward to indexing the contents of the documents on S3 and serving them based on a search.
  Negative
563d43e2a4387b6f44e92423	X	You can see this question, or this blog post if you want to do pure lucene, or you can use Solr, which is probably easier.
  Negative
See also this post.
  Positive
Zend has a PHP port of Lucene, which ties in very well.
  Positive
You can look at the Zend documentation for how to use it.
  Neutral
563d43e2a4387b6f44e92424	X	What about subdomains?
  Neutral
bucket.domain.tld -> Your S3 Bucket.
  Neutral
domain.tld -> Heroku.
  Neutral
Or is this against the same origin policy?
  Neutral
563d43e2a4387b6f44e92425	X	Same Origin Policy also applies for subdomains I am afraid.
  Negative
I considering to go with some kind of CORS implementation now I guess.
  Neutral
563d43e2a4387b6f44e92426	X	you can use JSONP for a lot of stuff.
  Negative
I've been able to use it with cross domains in the past.
  Positive
563d43e2a4387b6f44e92427	X	Thanks, but JSONP is not really a viable option as well.
  Negative
It has some security concerns and only supports GET requests.
  Negative
563d43e2a4387b6f44e92428	X	Thanks for the reply.
  Neutral
As of now my approach already kind of changed and I don't need to apply the above anymore.
  Negative
563d43e2a4387b6f44e92429	X	I want to build a web app where frontend (static) and backend (API) are, except for sharing the same domain, completely seperated.
  Negative
Usually I would consider this to be no problem, but I have some special requirements: The frontend app will be a single page Javascript application (with a base template, lets call it index.html) and populate the content from the API via AJAX.
  Negative
Since I don't want to implement CORS for the API yet and would like to follow the same-origin policy I want that both, the API and the files on S3 (the bucket), are sharing the same domain in some way.
  Very negative
I also don't want to the Django's flatpages app or render the index.html through Django at all.
  Negative
I scanned Google and stackoverflow, but couldn't find a adequate solution so far.
  Negative
As far as I read the naive way (pointing domain to the Heroku app and the S3 bucket somehow) is not possible.
  Negative
Some solutions I have in mind but didn't find sources to: Did anybody tried something like this before and can point me in the right direction?
  Negative
One addition: Later on I want to use something lile PhantomJS to make the single-page app crawlable.
  Negative
This output for crawlers should ideally be hosted in the S3 storage as well.
  Negative
563d43e3a4387b6f44e9242a	X	That is not possible with your current stack.
  Negative
Your Heroku application and your S3 bucket are actually served through two different domains.
  Negative
The benefit of having two different domains is that you can offload your server from all static assets requests.
  Negative
A convoluted way to achieve what you want would be to appropriately proxy the requests through one unique domain.
  Negative
Luckily for you neither Heroku nor Amazon will let you do that: S3 can host your website and redirect an api folder to your-api.
  Negative
herokuapp.com but only with 301 redirects that don't solve CORS issues.
  Negative
Just tried it if you're curious: At that point the easy solution is to implement a Django middleware for cross-domain sharing.
  Negative
563d43e3a4387b6f44e9242b	X	I want to enhance my sites loading speed, so I use http://gtmetrix.com/, to check what I could improve.
  Very negative
One of the lowest rating I get for "Leverage browser caching".
  Negative
I found, that my files (mainly images), have problem "expiration not specified".
  Negative
Okay, problem is clear, I thought.
  Negative
I start to googling and I found that amazon S3 prefer Cache-Control meta data over Expiry date (I lost this link, now I think maybe I misunderstood something).
  Negative
Anyway, I start looking for how to add cache-control meta to S3 object.
  Neutral
I found this page: http://www.bucketexplorer.com/documentation/amazon-s3--how-to-set-cache-control-header-for-s3-object.html I learned, that I must add string to my PUT query.
  Negative
x-amz-meta-Cache-Control : max-age= <value in seconds> //(there is no need space between equal sign and digits(I made a mistake here)).
  Negative
I use construction: Cache-control:max-age=1296000 and it work okay.
  Negative
After that I read https://developers.google.com/speed/docs/best-practices/caching This article told me: 1) "Set Expires to a minimum of one month, and preferably up to one year, in the future."
  Neutral
2) "We prefer Expires over Cache-Control: max-age because it is is more widely supported."
  Negative
(in Recommendations topic).
  Neutral
So, I start to look way to set Expiry date to S3 object.
  Negative
I found this: http://www.bucketexplorer.com/documentation/amazon-s3--set-object-expiration-on-amazon-s3-objects-put-get-delete-bucket-lifecycle.html And what I found: "Using Amazon S3 Object Lifecycle Management , you can define the Object Expiration on Amazon S3 Objects .
  Negative
Once the Lifecycle defined for the S3 Object expires, Amazon S3 will delete such Objects.
  Negative
So, when you want to keep your data on S3 for a limited time only and you want it to be deleted automatically by Amazon S3, you can set Object Expiration."
  Negative
I don't want to delete my files from S3.
  Negative
I just want add cache meta for maximum cache time or/and file expiry time.
  Negative
I completely confused with this.
  Negative
Can somebody explain what I must use: object expiration or cache-control?
  Neutral
563d43e3a4387b6f44e9242c	X	Your files won't be deleted, just not cached after the expiration date.
  Negative
The Amazon docs say: After the expiration date and time in the Expires header passes, CloudFront gets the object again from the origin server every time an edge location receives a request for the object.
  Negative
We recommend that you use the Cache-Control max-age directive instead of the Expires header field to control object caching.
  Negative
If you specify values both for Cache-Control max-age and for Expires, CloudFront uses only the value of max-age.
  Negative
563d43e3a4387b6f44e9242d	X	"Amazon S3 Object Lifecycle Management" flushs some objects from your bucket based on a rule you can define.
  Negative
It's only about storage.
  Neutral
What you want to do is set the Expires header of the HTTP request as you set the Cache-Control header.
  Negative
It works the same: you juste have to add this header to your PUT query.
  Positive
Expires doesn't work as Cache-Control: Expires gives a date.
  Negative
For instance: Sat, 31 Jan 2013 23:59:59 GMT You may read this: https://web.archive.org/web/20130531222309/http://www.newvem.com/how-to-add-caching-headers-to-your-objects-using-amazon-s3/
563d43e3a4387b6f44e9242e	X	So when we create lifecycle rules does amazon notifies our server when the file is moved to glacier and returns its ID.
  Negative
How can we get the ID of moved archive in the vault?
  Neutral
563d43e3a4387b6f44e9242f	X	There is no notification when objects are moved between Amazon S3 and Amazon Glacier due to lifecycle rules.
  Negative
The Storage Class of the Amazon S3 object is changed to "Glacier", which indicates that the content has been moved out of S3 and is available from Glacier (eg via the "Initiate Restore" command).
  Negative
The object remains in S3 (except for its contents), so it retains its existing key name (which is its ID).
  Negative
You cannot directly access data moved from S3 to Glacier -- you must restore it to S3 and then access it from S3.
  Negative
563d43e3a4387b6f44e92430	X	I am developing an application using Amazon S3 and glacier for file storing.
  Negative
The requirement is that I want to move the files from S3 to glacier and when needed from glacier back to S3.
  Negative
My question is that Is it really possible with their PHP API or not?
  Neutral
563d43e3a4387b6f44e92431	X	Never quite looked at it this way, but you do make sense.
  Negative
Thanks a lot!
  Positive
563d43e4a4387b6f44e92432	X	One more thing that I've quite liked is that some apps have started using DropBox for their storage - this means the user has to provide their DropBox credentials - which might put some users off - but it's a neat solution.
  Negative
563d43e4a4387b6f44e92433	X	I'm having a hard time consuming the S3 API on Windows phone 7, mainly because of the lack of example for actually putting an object on S3 using the SOAP API?
  Negative
Where do you even put the body of the item?
  Neutral
As far as I know, there isn't even a field for it in the putObject method... So, how do you put an object on S3 with windows phone 7.
  Negative
563d43e4a4387b6f44e92434	X	I do not recommend accessing the S3 API (or the Azure Storage API) direct from your phone.
  Negative
If you try this, then you will need to either have public PUT permissions or you will have your private storage access keys in plain view in the XAP file - it will be easy for a hacker to steal these and you will soon be paying to host PimpMyBreasts, WikiL33ked and SpamThis.
  Negative
Instead, you should host your own storage service where you can at least put some security checks in about what is being uploaded.
  Negative
If you do insist on using S3 directly, then this article covers S3 from C# including PutObject requests - http://www.codeproject.com/KB/cs/s3_ec2studio.aspx Good luck Stuart
563d43e4a4387b6f44e92435	X	I assume that you added a service reference to the Amazon service in your project: http://s3.amazonaws.com/doc/2006-03-01/AmazonS3.wsdl Once added as a service reference, you can invoke AmazonS3Client.PutObjectInlineAsync to upload an object in a S3 bucket.
  Very negative
The Data parameter (accepts a byte array) is what you're looking for.
  Negative
Recommended reading: http://timheuer.com/blog/archive/2008/07/05/access-amazon-s3-services-with-silverlight-2.aspx
563d43e4a4387b6f44e92436	X	This has been making me crazy all night.
  Negative
I wrote a DropBox app in PHP/MYSQL that worked perfectly, it pulls files from an Amazon S3 Bucket and sends them to users Dropbox folders.
  Negative
Then I changed the bucket policy on the Amazon S3 bucket to allow files to be pulled from only a handful of referrers, and signed URLS (example: /musicfile.mp3?AWSAccessKeyId=[accesskeyid]&Expires=[expires]&Signature=[signature]).
  Neutral
This works great for all purposes, except I learned my Dropbox functionality no longer works, it's because you pass the Dropbox API the URL of the mp3 on Amazon S3, and on Dropbox's side they pull the file in, so now that I have the bucket policy allowing only certain referrers, dropbox gets a permission denied and the API tells me it failed.
  Very negative
So I thought easy fix, I would simply add the ?
  Negative
AWSAccessKeyId= blah blah to the end of the file being passed to dropbox and all would work instantly, but, it doesn't because the file then doesn't end in an extension Dropbox recognizes so it again fails to work.
  Very negative
Then I thought I'd simply add the referrer from Dropbox to my bucket policy, I still have no idea what it is however and have added every variation of dropbox.com and api.dropbox with and without https, all with no luck.
  Negative
If anyone has any idea or solution you will seriously make my week.
  Negative
The absolute last thing I want to do is be forced to download the file first to my server, then send to dropbox, I really don't want to do that and I know I had this working perfectly already as it was, and it works instantly when I remove my bucket policy entirely, I just want it to work with it.
  Negative
563d43e5a4387b6f44e92437	X	I assume, because you mention passing a URL to Dropbox, that you're using the Saver?
  Negative
If so, you can tell the Saver what file name to use, so give it the authorized URL and specify a filename so there's a file extension.
  Negative
E.g.: or, in JavaScript: When you say that "because the file then doesn't end in an extension Dropbox recognizes so it again fails to work," what do you mean, exactly?
  Negative
What goes wrong when the file doesn't have an extension?
  Negative
563d43e5a4387b6f44e92438	X	When all else fails... check the logs.
  Negative
Turn on logging for your bucket, run some tests, wait a few minutes for a log to appear, and then examine the logs to see what the referer is.
  Negative
It seems a safe bet that there won't be a referer because a user agent that isn't a web browser (such as Dropbox's back-end processes) would typically not have a reason to send a referer.
  Negative
If it's any consolation, "securing" a bucket by constraining the referer is pretty much like not securing the bucket at all.
  Negative
It's extremely simple to defeat, and so it's only really effective protection against two classes of people: http://en.wikipedia.org/wiki/Referer_spoofing
563d43e6a4387b6f44e92439	X	To confuse people.
  Positive
563d43e6a4387b6f44e9243a	X	You can execute commands, like checking something or initializing.
  Positive
But I confess, I would never use it as it is not readable.
  Negative
In for loops it is more common to increase/decrease multiple variables.
  Positive
563d43e7a4387b6f44e9243b	X	The language allows a lot of things that are known to cause undefined behavior.
  Positive
At least these are harmless.
  Neutral
563d43e7a4387b6f44e9243c	X	It depends on the types of a and b. For user defined types, operator== can be overloaded and have side effects.
  Negative
One such side effect could be to deposit a certain amount of money in your bank account.
  Negative
With that in mind, you could consider it advantageous to invoke that operator many times.
  Negative
563d43e7a4387b6f44e9243d	X	@juanchopanza: If you are going the operator overloading for obfuscation sake way, they you can also override the operator,() and have extra fun!
  Positive
563d43e7a4387b6f44e9243e	X	:-) I am clear about for loop , I am specifically asking about if and while statement.
  Negative
563d43e7a4387b6f44e9243f	X	@AbdulRehman: Well in that case the anser probably is: because it if and while statements require an expression that evaluates to true and (<ex1>,<ex2>) is an expression.
  Negative
563d43e7a4387b6f44e92440	X	If it is the case why only last condition of an expression matters ?
  Negative
563d43e7a4387b6f44e92441	X	@Abdul: Well, because you have to pick one and someone decided that it should be the last.
  Negative
You have to understand, that the comma operator doesn't have any special behavior in an if statement compared to using it at other positions in your code.
  Negative
563d43e7a4387b6f44e92442	X	@AbdulRehman: In principle yes, but I'm not enough of a language laywer to tell you what it is in c/c++-standardese terms.
  Negative
563d43e8a4387b6f44e92443	X	wellcome on SO!
  Positive
just gave you some reputation ...
563d43e8a4387b6f44e92444	X	Nice example, but I'd rather use a while - loop header for demonstration.
  Negative
for the if case, I don't see a reason, why I wouldn't just write those statements in front of the if statement.
  Negative
563d43e8a4387b6f44e92445	X	they could simply use assignment expression rather than including expression list in c++ GRAMMAR of If statement.
  Negative
563d43e8a4387b6f44e92446	X	@AbdulRehman you could write if ( foo(), a == b ) to call a function and then do the test
563d43e9a4387b6f44e92447	X	@AbdulRehman I don't understand: assignment is already an expression...
563d43e9a4387b6f44e92448	X	yes, but it is different than expression in c++ grammar, expression is a list of assignment expressions, while assignment expression is the one without commas in it.
  Negative
563d43e9a4387b6f44e92449	X	I think I we both agree that the comma expression is unnecessary ;) in case of the for-loop, the grammar for initialise and increment could have been simply a list of statements instead of an expression and we wouldn't have this confusion...
563d43eaa4387b6f44e9244a	X	I dunno; there could be a void operator==(int rhs) { std::cout << rhs; }.
  Very negative
This is a code base where people are using , in an if statement, I wouldn't rule it out.
  Negative
563d43eaa4387b6f44e9244b	X	perhaps you could add some code examples?
  Neutral
as it stands, this is more than confusing.
  Negative
563d43eba4387b6f44e9244c	X	We can write if statement as and only the last condition should be satisfiable to enter if body My question is what is the advantage of commas in if or while statement?
  Negative
Why is it allowed ?
  Negative
563d43eba4387b6f44e9244d	X	In short: Although it is legal to do so, it usually doesn't make sense to use the comma operator in the condition part of an if or while statement (EDIT: Although the latter might sometimes be helpful as user5534870 explains in his answer).
  Negative
A more elaborate explanation: Aside from its syntactic function (e.g. separating elements in initializer lists, variable declarations or function calls/declarations), in C and C++, the , can also be a normal operator just like e.g. +, and so it can be used everywhere, where an expression is allowed (in C++ you can even overload it).
  Very negative
The difference to most other operators is that - although both sides get evaluated - it doesn't combine the outputs of the left and right expressions in any way, but just returns the right one.
  Positive
It was introduced, because someone (probably Dennis Ritchie) decided for some reason that C required a syntax to write two (or more) unrelated expressions at a position, where you ordinarily only could write a single expression.
  Negative
Now, the condition of an if statement is (among others) such a place and consequently, you can also use the , operator there - whether it makes sense to do so or not is an entirely different question!
  Negative
In particular - and different from e.g. function calls or variable declarations - the comma has no special meaning there, so it does, what it always does: It evaluates the expressions to the left and right, but only returns the result of the right one, which is then used by the if statement.
  Negative
The only two points I can think of right now, where using the (non-overloaded) ,-operator makes sense are: If you want to increment multiple iterators in the head of a for loop: If you want to evaluate more than one expression in a c++11 constexpr function.
  Negative
To repeat this once more: Using the comma operator in an if or while statement - in the way you showed it in your example - isn't something sensible to do.
  Negative
It is just another example where the language syntaxes of c and c++ allow you to write code, that doesn't behave the way that one - on first glance - would expect it to.
  Negative
There are many more....
563d43eca4387b6f44e9244e	X	Changing your example slightly, suppose it was this (note the = instead of ==).
  Negative
In this case the commas guarantee a left to right order of evaluation.
  Negative
In constrast, with this you don't know if f(5) is called before or after f(6).
  Negative
More formally, commas allow you to write a sequence of expressions (a,b,c) in the same way you can use ; to write a sequence of statements a; b; c;.
  Negative
And just as the ; creates a sequence point (end of full expression) so too does a comma.
  Negative
Only sequence points govern the order of evaluation, see this post.
  Neutral
563d43eca4387b6f44e9244f	X	There is no advantage: the comma operator is simply an expression with type of the last expression in its expression list and an if statement evaluates a boolean expression.
  Negative
It's a weird operator true, but there's no magic to it - except that it confuses lists of expressions with argument lists in function calls.
  Negative
Note that in the argument list, comma binds stronger to separating arguments.
  Negative
563d43eda4387b6f44e92450	X	None whatsoever.
  Negative
The comparisons on a in that code are completely redundant.
  Negative
563d43eda4387b6f44e92451	X	For an if statement, there is no real point in putting something into a comma expression rather than outside.
  Negative
For a while statement, putting a comma expression to the condition executes the first part either when entering the loop, or when looping.
  Negative
That cannot easily be replicated without code duplication.
  Negative
So how about a s do...while statement?
  Neutral
There we have only to worry about the looping itself, right?
  Neutral
It turns out that not even here a comma expression can be safely replace by moving the first part into the loop.
  Negative
For one thing, destructors for variables in the loop body will not have already been run then which might make a difference.
  Negative
For another, any continue statement inside the loop will reach the first part of the comma expression only when it indeed is in the condition rather than in the loop body.
  Negative
563d43eda4387b6f44e92452	X	What follows is a bit of a stretch, depending on how devious you might wish to be.
  Negative
Consider the situation where a function returns a value by modifying a parameter passed by reference or via a pointer (maybe from a badly designed library, or to ensure that this value is not ignored by not being assigned after returning, whatever).
  Negative
Then how do you use conditional statements that depend on result?
  Negative
You could declare the variable that will be modified, then check it with an if: This could be shortened to Which is not really worth while.
  Negative
However, for while loops there could be some small advantages.
  Neutral
If calculateValue should/can be called until the result is no longer bar'd, we'd have something like: and could be condensed to: This way the code to update result is in only one place, and is near the line where its conditions are checked.
  Negative
maybe unrelated: Another reason why variables could be updated via parameter passing is that the function needs to return the error code in addition to modify/return the calculated value.
  Negative
In this case: then
563e07a42d1761a701f0f46a	X	You can use the API to define lifecycle rules that archive files from Amazon S3 to Amazon Glacier and you can use the API to retrieve a temporary copy of files archived to Glacier.
  Negative
However, you cannot use the API to tell Amazon S3 to move specific files into Glacier.
  Neutral
There are two ways to use Amazon Glacier: Connecting directly via the Glacier API allows you to store archives for long-term storage, often used as a replacement for Tape.
  Negative
Data stored via the Glacier API must also be retrieved via the Glacier API.
  Negative
This is typically done with normal enterprise backup software or even light-weight products such as Cloudberry Backup (Windows) or Arq (Mac).
  Negative
Using Amazon S3 lifecycle rules allows you to store data in Amazon S3, then define rules that determine when data should be archived to Glacier for long-term storage.
  Negative
For example, data could be archived 90 days after creation.
  Negative
The data transfer is governed by the lifecycle rules, which operate on a daily batch basis.
  Negative
The rules can be set via the putBucketLifecycle API call (available in the PHP SDK), but this only defines the rules -- it is not possible to make an API call that tells S3 to archive specific files to Glacier.
  Negative
Amazon S3 has a RestoreObject API call (available in the PHP SDK) to restore a temporary copy of data archived from Glacier back into S3.
  Negative
Please note that restoring data from Glacier takes 3-5 hours.
  Negative
563e07a72d1761a701f0f46b	X	Editing this question to focus on C and Linux; since what Linux and Windows do differs.
  Negative
Otherwise, it's a bit too broad.
  Negative
Also, any higher level language will end up calling either a C API for the system or compiling down to C to execute, so leaving at the level of "C" is putting it at the Least Common Denominator.
  Negative
563e07a82d1761a701f0f46c	X	Not to mention that not all programming languages have this facility, or it is a facility that is highly dependent on environment.
  Negative
Admittedly rare these days, of course, but to this day file handling is a completely optional part of ANSI Forth, and wasn't even present in some implementations in the past.
  Negative
563e07a82d1761a701f0f46d	X	It is worth noting that in Unix-like OSes, the in-kernel structure file descriptors are mapped to, is called "open file description".
  Negative
So process FDs are mapped to kernel OFDs.
  Negative
This is important to understand the documentation.
  Positive
For instance, see man dup2 and check the subtlety between a open file descriptor (that is a FD that happens to be open) and a open file description (a OFD).
  Negative
563e07a82d1761a701f0f46e	X	Yes, permissions are checked at open time.
  Negative
You can go and read the source for the kernel's "open" implementation: lxr.free-electrons.
  Neutral
com/source/fs/open.c although it delegates most of the work to the specific file system driver.
  Negative
563e07a82d1761a701f0f46f	X	(on ext2 systems this will involve reading the directory entries to identify which inode has the metadata in, then loading that inode into the inode cache.
  Negative
Note that there may be pseudofilesystems like "/proc" and "/sys" which may do arbitrary things when you open a file)
563e07a82d1761a701f0f470	X	Note that the checks on file open -- that the file exists, that you have permission -- are, in practice, not sufficient.
  Negative
The file can disappear, or its permissions can change, under your feet.
  Negative
Some file systems attempt to prevent this, but so long as your OS supports network storage it is impossible to prevent (an OS can "panic" if the local file system misbehaves and be reasonable: one that does so when a network share does is not a viable OS).
  Negative
Those checks are also done at file open, but must (effectively) be done at all other file access as well.
  Positive
563e07a82d1761a701f0f471	X	Not to forget evaluation and/or creation of locks.
  Negative
These can be shared, or exclusive and can affect the whole file, or only a part of it.
  Negative
563e07a82d1761a701f0f472	X	You voted to close and answered?
  Negative
563e07a82d1761a701f0f473	X	@BillWoodger: well yes.
  Neutral
But it's a fair question (I mean yours).
  Negative
I voted to close as "too broad", and my answer is meant to illustrate how extremely broad the question actually is.
  Negative
563e07a82d1761a701f0f474	X	I think you are broading the answer a bit too much.
  Negative
ZX Spectrum had an OPEN command, and that was totally different from LOAD.
  Negative
And harder to understand.
  Neutral
563e07a92d1761a701f0f475	X	Although I edited my question to restrict to linux/windows OS in attempt to keep it open, this answer is entirely valid and useful.
  Negative
As stated in my question, I am not looking to implement something or to get other people to do my work, I am looking to learn.
  Negative
To learn you must ask the 'big' questions.
  Neutral
If we constantly close questions on SO for being 'too broad', it risks becoming a place to just get people to write your code for your without giving any explanation of what, where or why.
  Negative
I'd rather keep it as a place I can come to learn.
  Positive
563e07a92d1761a701f0f476	X	This answer seems to prove that your interpretation of the question is too broad, rather than that the question itself is too broad.
  Negative
563e07a92d1761a701f0f477	X	Very thorough.
  Positive
Does it make sense to stop at "the kernel" because this is the bottommost layer of external access through a high level programming language?
  Neutral
One step deeper would be an almost pure hardware layer?
  Negative
563e07a92d1761a701f0f478	X	Actually what I've written here is part of the kernel.
  Negative
But it's only the top layer - the first bit of kernel code that gets called by user-mode code (such as a high-level programming language).
  Negative
There are layers below this as well, in particular the filesystem (also part of the kernel), the IDE/ATA/SATA interface (also part of the kernel), the disk controller (firmware, i.e. software embedded within the hard drive itself), and then the physical hardware.
  Negative
One could go into a lot of detail.
  Positive
But I thought this top layer was the most relevant part for the question.
  Positive
563e07a92d1761a701f0f479	X	@Jongware on further consideration I thought this worth an edit.
  Negative
Thanks :-)
563e07a92d1761a701f0f47a	X	For MMIO and IO ports, BIOS (namely, its ACPI part) is only used to setup their locations.
  Negative
Once this is done, all the talk with the hardware concerning data reads and writes doesn't use BIOS: it just uses memory reads/writes for MMIO and in/out/ins/outs instructions for IO ports.
  Negative
563e07a92d1761a701f0f47b	X	Thanks to you and @Ruslan for your enlightening comments.
  Negative
563e07a92d1761a701f0f47c	X	What does this have to do with the actual question?
  Negative
563e07a92d1761a701f0f47d	X	It describes what happens at a low level when you open a file in Linux.
  Negative
I agree the question is rather broad, so this may not have been the answer jramm was looking for.
  Negative
563e07aa2d1761a701f0f47e	X	So again, no checking for permissions?
  Negative
563e07aa2d1761a701f0f47f	X	In all programming languages (that I use at least), you must open a file before you can read or write to it.
  Negative
But what does this open operation actually do?
  Neutral
Manual pages for typical functions dont actually tell you anything other than it 'opens a file for reading/writing': http://www.cplusplus.com/reference/cstdio/fopen/ https://docs.python.org/2/library/functions.html#open Obviously, through usage of the function you can tell it involves creation of some kind of object which facilitates accessing a file.
  Negative
Another way of putting this would be, if I were to implement an open function, what would it need to do on Linux?
  Negative
563e07aa2d1761a701f0f480	X	In almost every high-level language, the function that opens a file is a wrapper around the corresponding kernel system call.
  Negative
It may do other fancy stuff as well, but in contemporary operating systems, opening a file must always go through the kernel.
  Neutral
This is why the arguments of the fopen library function, or Python's open closely resemble the arguments of the open(2) system call.
  Negative
In addition to opening the file, these functions usually set up a buffer that will be consequently used with the read/write operations.
  Negative
The purpose of this buffer is to ensure that whenever you want to read N bytes, the corresponding library call will return N bytes, regardless of whether the calls to the underlying system calls return less.
  Negative
I am not actually interested in implementing my own function; just in understanding what the hell is going on...'beyond the language' if you like.
  Negative
In Unix-like operating systems, a successful call to open returns a "file descriptor" which is merely an integer in the context of the user process.
  Positive
This descriptor is consequently passed to any call that interacts with the opened file, and after calling close on it, the descriptor becomes invalid.
  Negative
It is important to note that the call to open acts like a validation point at which various checks are made.
  Positive
If not all of the conditions are met, the call fails by returning -1 instead of the descriptor, and the kind of error is indicated in errno.
  Very negative
The essential checks are: In the context of the kernel, there has to be some kind of mapping between the process' file descriptors and the physically opened files.
  Negative
The internal data structure that is mapped to the descriptor may contain yet another buffer that deals with block-based devices, or an internal pointer that points to the current read/write position.
  Negative
563e07aa2d1761a701f0f481	X	Any file system or operating system you want to talk about is fine by me.
  Positive
Nice!
  Positive
On a ZX Spectrum, initializing a LOAD command will put the system into a tight loop, reading the Audio In line.
  Positive
Start-of-data is indicated by a constant tone, and after that a sequence of long/short pulses follow, where a short pulse is for a binary 0 and a longer one for a binary 1 (https://en.wikipedia.org/wiki/ZX_Spectrum_software).
  Negative
The tight load loop gathers bits until it fills a byte (8 bits), stores this into memory, increases the memory pointer, then loops back to scan for more bits.
  Negative
Typically, the first thing a loader would read is a short, fixed format header, indicating at least the number of bytes to expect, and possibly additional information such as file name, file type and loading address.
  Negative
After reading this short header, the program could decide whether to continue loading the main bulk of the data, or exit the loading routine and display an appropriate message for the user.
  Negative
An End-of-file state could be recognized by receiving as many bytes as expected (either a fixed number of bytes, hardwired in the software, or a variable number such as indicated in a header).
  Negative
An error was thrown if the loading loop did not receive a pulse in the expected frequency range for a certain amount of time.
  Negative
A little background on this answer The procedure described loads data from a regular audio tape - hence the need to scan Audio In (it connected with a standard plug to tape recorders).
  Negative
A LOAD command is technically the same as open a file - but it's physically tied to actually loading the file.
  Neutral
This is because the tape recorder is not controlled by the computer, and you cannot (successfully) open a file but not load it.
  Very negative
The "tight loop" is mentioned because (1) the CPU, a Z80-A (if memory serves), was really slow: 3.5 MHz, and (2) the Spectrum had no internal clock!
  Negative
That means that it had to accurately keep count of the T-states (instruction times) for every.
  Neutral
single.
  Neutral
instruction.
  Neutral
inside that loop, just to maintain the accurate beep timing.
  Positive
Fortunately, that low CPU speed had the distinct advantage that you could calculate the number of cycles on a piece of paper, and thus the real world time that they would take.
  Negative
563e07aa2d1761a701f0f482	X	I'd suggest you take a look at this guide through a simplified version of the open() system call.
  Negative
It uses the following code snippet, which is representative of what happens behind the scenes when you open a file.
  Neutral
Briefly, here's what that code does, line by line: The filp_open function has the implementation which does two things: Store ("install") the returned struct into the process's list of open files.
  Negative
If you're feeling ambitious, you can compare this simplified example to the implementation of the open() system call in the Linux kernel, a function called do_sys_open().
  Negative
You shouldn't have any trouble finding the similarities.
  Negative
Of course, this is only the "top layer" of what happens when you call open() - or more precisely, it's the highest-level piece of kernel code that gets invoked in the process of opening a file.
  Negative
A high-level programming language might add additional layers on top of this.
  Negative
There's a lot that goes on at lower levels.
  Negative
(Thanks to Ruslan and pjc50 for explaining.)
  Neutral
Roughly, from top to bottom: This may also be somewhat incorrect due to caching.
  Negative
:-P Seriously though, there are many details that I've left out - a person (not me) could write multiple books describing how this whole process works.
  Negative
But that should give you an idea.
  Positive
563e07aa2d1761a701f0f483	X	It depends on the operating system what exactly happens when you open a file.
  Neutral
Below I describe what happens in Linux as it gives you an idea what happens when you open a file and you could check the source code if you are interested in more detail.
  Negative
I am not covering permissions as it would make this answer too long.
  Negative
In Linux every file is recognised by a structure called inode.
  Negative
Each structure has an unique number and every file only gets one inode number.
  Positive
This structure stores meta data for a file, for example file-size, file-permissions, time stamps and pointer to disk blocks, however, not the actual file name itself.
  Negative
Each file (and directory) contains a file name entry and the inode number for lookup.
  Negative
When you open a file, assuming you have the relevant permissions, a file descriptor is created using the unique inode number associated with file name.
  Positive
As many processes/applications can point to the same file, inode has a link field that maintains the total count of links to the file.
  Negative
If a file is present in a directory, its link count is one, if it has a hard link its link count will be two and if a file is opened by a process, the link count will be incremented by 1.
  Positive
563e07aa2d1761a701f0f484	X	Bookkeeping, mostly.
  Neutral
This includes various checks like "Does the file exist?"
  Neutral
and "Do I have the permissions to open this file for writing?"
  Negative
.
  Neutral
But that's all kernel stuff - unless you're implementing your own toy OS, there isn't much to delve into (if you are, have fun - it's a great learning experience).
  Negative
Of course, you should still learn all the possible error codes you can receive while opening a file, so that you can handle them properly - but those are usually nice little abstractions.
  Neutral
The most important part on the code level is that it gives you a handle to the open file, which you use for all of the other operations you do with a file.
  Positive
Couldn't you use the filename instead of this arbitrary handle?
  Negative
Well, sure - but using a handle gives you some advantages: There's also some other tricks you can do (for example, share handles between processes to have a communication channel without using a physical file; on unix systems, files are also used for devices and various other virtual channels, so this isn't strictly necessary), but they aren't really tied to the open operation itself, so I'm not going to delve into that.
  Negative
563e07aa2d1761a701f0f485	X	At the core of it when opening for reading nothing fancy actually needs to happen.
  Negative
All it needs to do is check the file exists and the application has enough privileges to read it and create a handle on which you can issue read commands to the file.
  Positive
It's on those commands that actual reading will get dispatched.
  Neutral
The OS will often get a head start on reading by starting a read operation to fill the buffer associated with the handle.
  Negative
Then when you actually do the read it can return the contents of the buffer immediately rather then needing to wait on disk IO.
  Negative
For opening a new file for write the OS will need to add a entry in the directory for the new (currently empty) file.
  Negative
And again a handle is created on which you can issue the write commands.
  Positive
563e07aa2d1761a701f0f486	X	Basically, a call to open needs to find the file, and then record whatever it needs to so that later I/O operations can find it again.
  Negative
That's quite vague, but it will be true on all the operating systems I can immediately think of.
  Negative
The specifics vary from platform to platform.
  Neutral
Many answers already on here talk about modern-day desktop operating systems.
  Neutral
I've done a little programming on CP/M, so I will offer my knowledge about how it works on CP/M (MS-DOS probably works in the same way, but for security reasons, it is not normally done like this today).
  Negative
On CP/M you have a thing called the FCB (as you mentioned C, you could call it a struct; it really is a 35-byte contiguous area in RAM containing various fields).
  Negative
The FCB has fields to write the file-name and a (4-bit) integer identifying the disk drive.
  Negative
Then, when you call the kernel's Open File, you pass a pointer to this struct by placing it in one of the CPU's registers.
  Neutral
Some time later, the operating system returns with the struct slightly changed.
  Positive
Whatever I/O you do to this file, you pass a pointer to this struct to the system call.
  Negative
What does CP/M do with this FCB?
  Neutral
It reserves certain fields for its own use, and uses these to keep track of the file, so you had better not ever touch them from inside your program.
  Positive
The Open File operation searches through the table at the start of the disk for a file with the same name as what's in the FCB (the '?'
  Negative
wildcard character matches any character).
  Positive
If it finds a file, it copies some information into the FCB, including the file's physical location(s) on the disk, so that subsequent I/O calls ultimately call the BIOS which may pass these locations to the disk driver.
  Negative
At this level, specifics vary.
  Positive
563e07aa2d1761a701f0f487	X	Short answer: No you can't.
  Negative
563e07aa2d1761a701f0f488	X	Medium answer: If you want something like this, then you probably have a design error and you don't need a const static member in the first place.
  Negative
563e07ab2d1761a701f0f489	X	As soon as main is entered static initialization is done.
  Negative
Hence you can not alter the value.
  Negative
You may provide a function exposing a constant T, but using a mutable T internally.
  Negative
563e07ab2d1761a701f0f48a	X	@101010, this is not neccessarily a design error.
  Negative
It is very legitimate thing - to have a value which is initialized during application start and than preserved.
  Negative
And you might want to 'enforce' this preservation logic.
  Negative
I, myself, have long longed for one-time-modfiables as class members - so that they are const, but still can be modified in constructor.
  Negative
It can not be done currently, but it does not constitute a design flaw.
  Negative
563e07ab2d1761a701f0f48b	X	@SergeyA Sorry if things I wrote were misinterpreted, but I didn't mean that the request was absurd, but rather the approach.
  Negative
563e07ab2d1761a701f0f48c	X	When you say you cannot initialize anything marked const is it only for the class members ?
  Negative
I did initialize a const intin this way within my main().
  Negative
Thank you.
  Positive
563e07ab2d1761a701f0f48d	X	One more thing, what is this statement const int& A::T(actualT) ?
  Negative
I didn't know that syntax.
  Negative
563e07ab2d1761a701f0f48e	X	@user2939212 That's a common syntax to initialize variables.
  Negative
You can write const int& A::T = actualT; instead.
  Negative
This syntax applies to variables of all types, not only to references.
  Negative
For example, you can write int x(5) instead of int x=5.
  Negative
563e07ab2d1761a701f0f48f	X	"You cannot initialize anything marked const inside a class at runtime" This is obviously not true.
  Negative
You mean static const.
  Neutral
Even then it's not really true - ASH's answer seems the most comprehensive, on balance.
  Negative
563e07ab2d1761a701f0f490	X	@LightnessRacesinOrbit Fixed.
  Negative
563e07ab2d1761a701f0f491	X	Actually, a dynamically loaded library might have its own globals and they will be initialized when and if the lib is loaded.
  Negative
That concept is not part of the standard (or at least it wasn't).
  Negative
563e07ab2d1761a701f0f492	X	@JDługosz I dont see any relation to DLLs here.
  Negative
It justs works for me to build normal executables and it always worked through all versions of MSVC including 2015, which is supposed to be fully standard.
  Negative
If this contruction is "not standard", there must be something to prove it in the reference.
  Negative
Please share if you find any.
  Neutral
563e07ab2d1761a701f0f493	X	I mean that static const values can be in a dll which is loaded after main is called.
  Negative
563e07ab2d1761a701f0f494	X	@JDługosz ah ok, i got what you mean now.
  Negative
Indeed this is a very special case, as you said :)
563e07ac2d1761a701f0f495	X	Thank you for your solution.
  Positive
563e07ac2d1761a701f0f496	X	Do not use a singleton here.
  Negative
563e07ac2d1761a701f0f497	X	Hi SergeyA.
  Negative
Could you please let me know why you suggest so?
  Negative
563e07ac2d1761a701f0f498	X	Because it is not needed.
  Negative
Do not use singleton when a simple global variable will suffice.
  Negative
563e07ac2d1761a701f0f499	X	How do one make sure that the value is not changed again with just one global variable?
  Negative
563e07ac2d1761a701f0f49a	X	The solution is provided in the answer.
  Neutral
Singleton, on the other hand, will not provide for this at all.
  Negative
Classic singleton, at least.
  Neutral
563e07ac2d1761a701f0f49b	X	Is it possible to initialize a static const member of my class during run-time?
  Neutral
This variable is a constant throughout my program but I want to send it as a command-line argument.
  Neutral
If this cannot be done, what is the type of variable I should use?
  Negative
I need to initialize it at run-time as well as preserve the constant property.
  Negative
563e07ac2d1761a701f0f49c	X	You cannot rely on data produced after your main has started for initialization of static variables, because static initialization in the translation unit of main happens before main gets control, and static initialization in other translation units may happen before or after static initialization of main translation unit in unspecified order.
  Very negative
However, you can initialize a hidden non-const variable, and provide a const reference to it, like this: Demo.
  Neutral
563e07ac2d1761a701f0f49d	X	I am sorry to disagree with the comments and answers saying that it is not possible for a static const symbol to be initialized at program startup rather than at compile time.
  Negative
Actually this IS possible, and I used it many times, BUT I initialize it from a configuration file.
  Very negative
Something like: As you see, these static consts are not necessarily known at compile time.
  Negative
They can be set from the environment, such as a config file.
  Negative
On the other hand, setting them from argv[], seems very difficult, if ever feasible, because when main() starts, static symbols are already initialized.
  Negative
563e07ac2d1761a701f0f49e	X	No, you cannot do that.
  Negative
If this cannot be done what is the type of variable I should use ?
  Negative
You can use a non-const member.
  Neutral
Another option is to make T a private member, make main a friend so only it can modify the value, and then expose the member through a function.
  Negative
563e07ac2d1761a701f0f49f	X	Not only you can't, you should not try doing this by messing with const_cast.
  Negative
Static const members have a very high chance of ending up in read-only segment, and any attempt to modify them will cause program to crash.
  Positive
563e07ac2d1761a701f0f4a0	X	Typically you will have more than one configuration value.
  Positive
So put them in a struct, and the normal global access to it is const.
  Negative
You can get fancier and have a global function to return config, so normal code can't even change the pointer, but it is harder to do that by accident.
  Negative
A header file exposes get_config () for all to use, but the way to set it is only known to the code that's meant to do so.
  Negative
563e07ac2d1761a701f0f4a1	X	No, since you defined the variable as static and const, you cannot change its value.
  Negative
You will have to set its value in the definition itself, or through a constructor called when you create an object of class A.
563e07ac2d1761a701f0f4a2	X	Use a Singleton Pattern here.
  Negative
have a data member which you'd like to initialize at run time in the singleton class.
  Neutral
One a single instance is created and the data member is properly initialized, there would be no further risk of overwriting it and altering it.
  Positive
Singleton would preserve the singularity of your data.
  Negative
Hope this helps.
  Positive
563e0f082d1761a701f0f4a3	X	Hi, yes I think so - we really need to prevent anyone from being able to viewing the files, so only people using our web application shouel be able to access files on S3.
  Negative
many thanks for your help!
  Positive
563e0f082d1761a701f0f4a4	X	I am developing an application that loads images and video into a Flash player (currently using Flash 8 to develop so this is AS2.0).
  Negative
We are going to host the files on Amazon S3 servers.
  Negative
Can anyone point out the best way to go about loading the files into Flash Player from Amazon S3.
  Neutral
I have been using MovieClipLoader to load images from our development server using loadMovie("http://domain/folder/file") and progressive video is loaded in a similar way.
  Negative
I want to be able to load from S3 like I did from our development server.
  Negative
Do I need to go through the signature and authentication process when loading each item into Flash from S3?
  Neutral
I dont fully understand how I would generate signature etc in Flash.
  Negative
Can I use the PHP S3 class to do this and send the signature etc as a variable to Flash at the start and use the same signature for loading all images / video?
  Negative
Thanks
563e0f082d1761a701f0f4a5	X	Assets hosted on Amazon S3 are available without any authentication, assuming ACL settings allow read access.
  Negative
So you should not need to supply a signature or authentication to simply download stuff.
  Negative
For example here is an image file I have on S3 that's accessible via a simple url: gromit.jpg (Dead Link) Do you have a requirement that your assets be protected from viewing by anyone w/o credentials?
  Negative
563e0f092d1761a701f0f4a6	X	Not sure if your use case is what I think it is, but is your signature really set to the string 'signature'.
  Negative
Perhaps have a look at this page: docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html, and this page: docs.aws.amazon.com/AmazonS3/latest/dev/S3_Authentication2.html
563e0f092d1761a701f0f4a7	X	Did you ever get this working?
  Neutral
563e0f092d1761a701f0f4a8	X	I want to delete files and folder from the Amazon s3 bucket.
  Negative
Below is my request, but its not working.
  Negative
563e0f092d1761a701f0f4a9	X	OK... I have been on this for a while now and I'm not getting anywhere.
  Negative
No matter what I try, I continue to get back the same error message from Amazon.
  Negative
I am using ng-file-upload (https://github.com/danialfarid/ng-file-upload) to upload to S3.
  Negative
I have followed the documentation and have used the demo installation to successfully upload a file to my bucket so I know my AWS Key and Secret Key are correct.
  Negative
My controller is as follows: Now the PHP for generating the policy and signature is as follows: The http headers show "Authorization:Basic U1Q6" which is obviously the issue.
  Negative
Any assistance with this would be greatly appreciated.
  Negative
563e0f092d1761a701f0f4aa	X	I'm a complete idiot!
  Negative
The header was being created by my own login/authorisation script.
  Negative
I completely forgot!
  Positive
Once I fixed that I had no issues.
  Neutral
563e0f092d1761a701f0f4ab	X	parse.com/questions/…
563e0f092d1761a701f0f4ac	X	I'm uploading image to parse, making some works (like resize and etc) and want to upload after them.
  Negative
563e0f092d1761a701f0f4ad	X	5000 files - about 1200 users for 100$ a month.
  Negative
Too much for free app.
  Negative
563e0f092d1761a701f0f4ae	X	The first 5k are free.
  Neutral
It would take 20000 files more to ever cost you $100.
  Negative
$.005 per file is a almost as good a price as it will be when u go over your parse 1M calls (7¢ per thousand) trying to build your own form actions.
  Negative
563e0f092d1761a701f0f4af	X	If you wanted to pay per gb transloadit.com/docs#jquery-plugin
563e0f092d1761a701f0f4b0	X	guys, I want to upload image to S3 storage, but I can't do it.
  Negative
My app is on parse.com and I can't use npm to install aws-sdk Please, help me, I'm newbie in aws and node.js.
  Negative
563e0f092d1761a701f0f4b1	X	I posted a link as a comment, however I will give it a bit of explanation.
  Positive
I am not sure if it is possible to upload to S3 through Parse (mainly because that would be alot of unnecessary traffic for Parse), however it is possible to upload to S3 directly from your client by using a certificate.
  Negative
This (signed) certificate effectively tells S3 that you are authorizing the device to upload to your bucket as long as the requirements included in the certificate are met.
  Negative
This question on Parse's site give more information about this, as well as Cloud Code that should generate the certificate for you.
  Negative
As always, I would recommend you understand what this code is doing before you use it for any production app/service.
  Negative
You can also probably find some more information about this client-side upload by doing a quick google for something like 'client side upload to S3'.
  Negative
563e0f092d1761a701f0f4b2	X	Seems like a perfect place to use https://www.inkfilepicker.com Just plug in your own S3 creeds and off you go.
  Negative
If you can't stand not doing something painful use the REST API here for S3 http://docs.aws.amazon.com/AmazonS3/latest/dev/S3_Authentication2.html and build out your cloud code functions with the networking capability available in parse There is a reason inkfilepicker exists tho...
563e0f0a2d1761a701f0f4b3	X	Assume I already have purchased a domain example.com with IP address 203.0.113.2.
  Negative
Using C# and the The Amazon Web Services SDK for .
  Neutral
NET 2.0.2.2, I'd like to create a static website using a custom domain using Amazon S3 and Route 53.
  Negative
The manual process is described in the Amazon documentation.
  Negative
When trying to create an alias, I get an exception with the message: First, I created or updated a bucket (e.g. "example.com") in Amazon S3.
  Negative
If it already existed, content is deleted.
  Negative
S3BucketExists returns whether a bucket exist or not, and CreateObject creates a simple page and uploads it to the bucket.
  Positive
Its omitted for brevity sake.
  Neutral
I'm able to connect to the S3 hosted site without any problems.
  Neutral
Then I use the Route 53 API to update an existing hosted zone or create one for "example.com".
  Negative
All resources, except for the SOA and NS entries are deleted.
  Negative
The hosted zone id ("Z2F56UZL2M1ACD") and DNS names ("s3-website-us-west-1.
  Negative
amazonaws.com.")
  Neutral
are public knowledge and documented on Amazon's website.
  Positive
The call to ChangeResourceRecordSets throws the exception.
  Neutral
I created an empty ResourceRecords list, with a A record of "203.0.113.2", but have not had any luck creating an alias.
  Negative
That said, I can manually create the alias to the Amazon S3 site afterwards using the "Route 53 Management Console".
  Negative
I'm sure it's something small I'm missing.
  Negative
563e0f0a2d1761a701f0f4b4	X	After re-reading the documentation, it turns out that one cannot specify the TTL when specifying an alias.
  Negative
The following change works.
  Positive
Replace the code that creates an instance of ChangeResourceRecordSetsRequest to the following: The difference was evident when the output produced by System.Net tracing was compared to the request specified in the Amazon example.
  Negative
563e0f0a2d1761a701f0f4b5	X	We ran into this recently, and I decided a quick patch to the AWS SDK would be the best answer since Amazon obviously isn't going to fix this any time soon.
  Negative
github.com/skilitix/aws-sdk-net is our fork that fixes the retry on 304 and adds a NotModified boolean to GetObjectResponse - if I spend the time to figure out a more back-compatible API I'll submit a patch to Amazon (in the meantime, we accept PRs :P).
  Negative
563e0f0a2d1761a701f0f4b6	X	@SimonBuchan Awesome!
  Positive
I will check it out.
  Positive
563e0f0a2d1761a701f0f4b7	X	Note that we're not keeping up with upstream, we're already ~3 point releases behind, but they seem to be new APIs only and should auto-merge.
  Negative
563e0f0a2d1761a701f0f4b8	X	Background I am trying to make use of S3 as an 'infinity' large caching layer for some 'fairly' static XML documents.
  Negative
I want to ensure that the client application (which will be running on thousands of machines concurrently and requesting the XML documents many times per hour) only downloads these XML documents if their content has changed since the last time the client application downloaded them.
  Negative
Approach On Amazon S3, we can use HTTP ETAG for this.
  Negative
By default Amazon S3 objects have their ETAG set to the MD5 hash of the object.
  Negative
We can then specify the MD5 hash of the XML document inside the GetObjectRequest.ETagToNotMatch property.
  Negative
This ensures that when we make the AmazonS3.GetObject call (or in my case the async version AmazonS3.BeginGetObject and AmazonS3.EndGetObject), that if the document being requested has the same MD5 hash as is contained in the GetObjectRequest.ETagToNotMatch then S3 automatically returns the HTTP Status code 304 (NotModified) and the actual contents of the XML document is not downloaded.
  Negative
Problem The problem however is that when calling AmazonS3.GetObject (or it's async equivalent) the Amazon .
  Negative
Net API actually sees the HTTP Status code 304 (NotModified) as an error and it retries the get request three times and then finally throws an Amazon.S3.AmazonS3Exception: Maximum number of retry attempts reached : 3.
  Negative
Obviously I could change this implementation to use AmazonS3.GetObjectMetaData and then compare the ETAG and use AmazonS3.GetObject if they do not match, but then there are two requests to S3 instead of one when the file is stale.
  Negative
I'd prefer to have one request regardless of whether the XML document needs downloaded or not.
  Negative
Any ideas?
  Neutral
Is this a bug or am I missing something?
  Negative
Is there even some way I can reduce the number of retries to one and 'process' the exception (although I feel 'yuck' about this route).
  Negative
Implementation I'm using the AWS SDK for .
  Negative
NET (version 1.3.14).
  Neutral
Here is my implementation (reduced slightly to keep it shorter): I then call this like: This then produces this log file output:
563e0f0a2d1761a701f0f4b9	X	I also posted this question on the Amazon developers forum and got a reply from an official AWS Employee: After investigating this we understand the problem but we are looking for feedback on how best to handle this.
  Negative
First approach is to have this operation return with a property on the GetObjectResponse indicating that the object was not returned or set the output stream to null.
  Negative
This would be cleaner to code against but it does create a slight breaking behavior for anybody relying on an exception being thrown, albeit after the 3 retries.
  Negative
It would also be inconsistent with the CopyObject operation which does throw an exception without all the crazy retrying.
  Negative
The other option is we throw an exception similar to CopyObject which keeps us consistent and no breaking changes but it is more difficult to code against.
  Negative
If anybody has opinions on which way to handle this please respond to this thread.
  Neutral
Norm I have already added my thoughts to the thread, if anybody else is interested in participating here is the link: AmazonS3.GetObject sees HTTP 304 (NotModified) as an error.
  Negative
Way to allow it?
  Neutral
NOTE: When this has been resolved by Amazon I will update my answer to reflect the outcome.
  Negative
UPDATE: (2012-01-24) Still waiting for further information from Amazon.
  Negative
563e0f0a2d1761a701f0f4ba	X	This works, but it requires you to stream all the files from S3 through your webserver... this is has a pretty high bandwidth and performance cost for your webserver.
  Positive
As an alternative you might want to look at generating presigned URLs for the files and allowing the webbrowser to pull them directly from S3...
563e0f0a2d1761a701f0f4bb	X	We intend to use local browser caching and Memcached to optimize performance.I would have preferred presigned URls over this but our system design and constraints don't allow it.
  Negative
563e0f0a2d1761a701f0f4bc	X	I am trying to retrieve some HTML files from Amazon s3 using AWS SDK for .
  Negative
NET.
  Neutral
I am able to get the HTML file but the images that are linked to the webpage are not being displayed neither is the relevant style sheet applied.
  Negative
Now, I do understand why this is happening.
  Negative
Because each image and style sheet is a separate object in Amazon s3 and my code is only creating presigned URL for the HTML file: What is the best way to access the images and style sheet related to this HTML file?
  Negative
I can look for all the images and then use the above method to generate presigned URL requests but that is not an efficient method and I can't make the images and style sheet public.
  Negative
Has anyone else encountered a similar issue?
  Negative
Also, is it better if I use Rest API to authenticate user( using authentication Header) so that the browser will have authentication information in header and I will not have to create presigned URL's for each object.
  Negative
A small piece of code for REST API would be very helpful.
  Negative
563e0f0a2d1761a701f0f4bd	X	The best way to achieve this is by using Generic Handlers(.
  Negative
ASHX).
  Neutral
The trick is to change the source of webpage and related objects to your handler: Now, to change the source you either update your old HTML files and create new ones with source pointing to (StreamFile.ashx)Generic Handler or use URL rewrite to write old URL's to new URL.
  Negative
This can be done in IIS or in the web.config.If you do it in IIS it will automatically add code in your web.config.
  Negative
The above code will look for "DevelopmentContent/Course/" in Src string and if found will rewrite the URL to StreamFile.ashx/?
  Negative
file=course{R:1}.
  Negative
R:1 will the rest of the URL-bold part(DevelopmentContent/Course/xyz/xsd/x/sd/ds.htm) which should map to your object key in amazon S3.Now in StreamHandler.ashx will receive the requests from the server with specified URL.
  Negative
You can then get the object key from query string(context.Request.QueryString["file"]) and then create a function to Get the required object.
  Negative
So now all the HTTP requests will be made using your server as proxy.
  Negative
563e0f0b2d1761a701f0f4be	X	I am trying to do a PUT request via the REST API to establish a lifecycle rule as described here: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html But even when I PUT the exact same XML Data as in Example 1, I get a 'Malformed XML Error'.
  Very negative
Also Content-Length and MD5 of the request in this example seem not to be valid as I calculate different values.
  Negative
My put is: and has a Content-Length of 397 and a MD5 of Or7bcOqR6tcsifiqQpq1tw== I get this error: Any help on this is appreciated!
  Negative
Thanks!
  Positive
563e0f0b2d1761a701f0f4bf	X	I finally figured it out.
  Negative
The endpoint URL was wrong as the parameters got skipped by my code when creating a PUT request.
  Negative
If you get an XML Malformed error make sure your Endpoint URL is correct.
  Negative
563e0f0b2d1761a701f0f4c0	X	The problem is I have Adobe AIR application and everyone can see its sources.
  Negative
So they can copy the URL where server returns signed URLs and use it.
  Negative
563e0f0b2d1761a701f0f4c1	X	You don't put your keys in the application.
  Negative
Your application makes a call to your server (which has the keys) and the server responds with a signed upload url for the application to use for file uploading.
  Negative
563e0f0b2d1761a701f0f4c2	X	It's done on the fly and you set the URL to expire after a time you specify.
  Positive
(maybe 10 minutes to upload a file?)
  Neutral
You don't store the URL in the app.
  Neutral
For example:
563e0f0b2d1761a701f0f4c3	X	-App - "I need to upload this file" -App -> server "Send me an upload url for this file" -Server -> App "some.S3.url.com/?
  Negative
signed&with&expiry -App uses URL to upload file (man, the formatting in these comments doesn't allow for line breaks, that sucks :-)
563e0f0b2d1761a701f0f4c4	X	Yes, I understood.
  Negative
For example this is a URL where application receives signed URLs: example.com/get_signed_url/USER_ID
563e0f0b2d1761a701f0f4c5	X	transloadit.com - How this works?
  Negative
I think it uploads files first to their server and then to S3.
  Negative
563e0f0b2d1761a701f0f4c6	X	I have such problem: I have Droplr-like Adobe AIR application, which uploads files to remote location and returns short links for those files.
  Very negative
I want to upload all these files to Amazon S3.
  Negative
But, as it is Adobe AIR application and everyone can see its source(and S3 API keys), I cant upload files directly to Amazon S3.
  Negative
As I understood, if someone will get API keys from application's source, he will be able to upload files to my S3 account and I will pay for that.
  Negative
I wanted to solve this by uploading files from application to my server and PHP script will upload them to Amazon S3.
  Negative
Like a proxy.
  Neutral
But it will be double traffic and slow operation.
  Negative
563e0f0b2d1761a701f0f4c7	X	Another option is that you can use signed URLs.
  Negative
Your application can request an upload URL from your server.
  Negative
Your server then generates a signed URL to send back to the application which is then used to upload the file.
  Negative
You can also set the expiration time on signed upload URLs.
  Negative
563e0f0b2d1761a701f0f4c8	X	This blog article talks about keeping AWS credentials secret.
  Negative
Whilst it covers EC2 mostly, there's some stuff in there about S3 too.
  Negative
563e0f0b2d1761a701f0f4c9	X	Another approach is the Token Vending Machine, an application you can run to allow mobile and other client apps to obtain temporary credentials without revealing or embedding your account credentials in your application.
  Negative
You can read more about the TVM approach here: http://aws.amazon.com/articles/4611615499399490
563e0f0b2d1761a701f0f4ca	X	thnx, will setting EC2 for CORS work on all browsers?
  Negative
563e0f0c2d1761a701f0f4cb	X	No, because there are some older browsers that don't follow current standards.
  Negative
I wouldn't be surprised to see IE 6 having trouble, for example.
  Negative
But it will work for any browser that follows the CORS spec.
  Negative
The caniuse web site is very valuable for this.
  Positive
There's a table at caniuse.com/#search=cors showing browser conformance with the spec.
  Neutral
Note that older IE versions support CORS, but via non-standard functions.
  Neutral
If you use a library like jQuery it will automatically adjust for those browsers for you.
  Negative
563e0f0c2d1761a701f0f4cc	X	I am hosting my website on S3.
  Negative
On my local host I am using backboneJS above a PHP Rest API that uses mySQL as a database.
  Negative
So i opened an EC2 to host my Rest API but then realized this means cross domain AJAX.
  Negative
How can i use EC2 as a Rest API if my index.html sits on S3?
  Neutral
What are my other DB options for S3?
  Neutral
many thanks,
563e0f0c2d1761a701f0f4cd	X	Your JavaScript is being executed on web pages served from S3, and it has to access a REST API from a server you run on EC2.
  Negative
Unless the web pages and server are in the same domain (say, example.com), this will be a cross-origin request, prohibited by browsers by default.
  Negative
Solution 1: have your S3 pages and your EC2 server in the same domain.
  Negative
S3 allows static website hosting that makes your S3 objects (web pages) available at the address of your choice.
  Negative
Put them and your EC2 server at addresses on the same domain, and it can work.
  Neutral
Solution 2: have your REST API server allow cross-origin requests.
  Neutral
Since you control this EC2 server you can modify it to tell web browsers to allow pages from other domains to make such requests to your server.
  Negative
This involves setting several headers on your responses, and usually requires your server to respond to HTTP OPTIONS requests properly, too.
  Negative
See the CORS specification to get started on that.
  Positive
You also ask about other DB options.
  Neutral
If you keep your own EC2 server to provide the REST API it can use pretty much any kind of database you like, either running on the same or other EC2 instances, or database-as-a-service offerings like AWS RDB or AWS DynamoDB.
  Negative
But if you want to connect to the database directly from your web pages' JavaScript you would have to use a service that provides an HTTP API directly and that supports CORS.
  Negative
RDB does not provide an HTTP API at all, and DynamoDB does not seem to support CORS at this time, so neither of them would work.
  Very negative
563e0f0c2d1761a701f0f4ce	X	I've a S3 bucket on the Amazon and trying to get the list of all the zip files located within folders under the bucket recursively.
  Negative
For e.g, my zip files are located as shown below : Below is my code : But I get back an empty list.
  Very negative
Am I doing anything wrong ?
  Negative
Is there any way to recursively get list of zip files from the bucket ?
  Negative
563e0f0c2d1761a701f0f4cf	X	Only thing I can see is getting connection to your S3 bucket.Try the following and it may help
563e0f0c2d1761a701f0f4d0	X	I have an S3 Amazon account and currently all the video I upload on my site goes to there.
  Negative
I have added functionality so that user may select to share their video on YouTube as well.
  Positive
For that I am using the YouTube API which does not support a video URL.
  Negative
I was wondering if we could, somehow, provide a direct s3 link so that video gets uploaded to YouTube.
  Negative
here's the flow :- Selected video -> gets converted to mp4 format from encoding.com -> the concerted video gets uploaded to s3 amazon.
  Negative
Note: I have tried downloading my video from s3 to tmp folder and then upload it, this works fine but since we are using load balancing servers it will not work with it.
  Neutral
Need solution asap.
  Positive
Thanks
563e0f0c2d1761a701f0f4d1	X	You can specify Youtube as a destination within your Encoding.com job, they have already built the integration with the YouTube API.
  Negative
See http://www.encoding.com/syndicate_your_videos_to_youtube for the API syntax.
  Positive
It can include full meta data.
  Negative
Note that you have to use Google's 2 step authentication process to create a password to use with the Encoding.com API
563e0f0d2d1761a701f0f4d2	X	I am developing an iPhone app to allow user upload photo and share.
  Negative
I want to use S3 to store uploaded images as well as processed images (foe example, thumbnail, reduced size image).
  Negative
I've installed AWS PHP API on my EC2 instance, my questions are: 1) Should photos uploaded from iPhone app go to a EC2 directory first, then copied over to S3 bucket, or it should directly uploaded to S3?
  Negative
2) How can I create different folders under the S3 bucket through PHP API and how to read the files from the folder?
  Negative
Thanks in advance!
  Neutral
563e0f0d2d1761a701f0f4d3	X	The answer can be found here: Direct upload to s3 without the use of a production server I've never used the PHP SDK, but I was browsing through the AWS SDK for PHP 1.5.14 documentation and came across the following APIs that you will need to utilize: a) create_object : You'll use this to put a object into a bucket.
  Negative
You'll specify the filename.
  Neutral
You asked how you can create different folders: you will include the full path into the filename.
  Positive
For instance instead of naming the file "photo1.jpg", you would name it "user1/vacation/photo1.
  Negative
jpg".
  Neutral
b) get_object_list : This API will return to you a list of objects given some criteria.
  Negative
If you want to read all the objects from a particular folder, specify the prefix as the path to the folder.
  Negative
For instance, if I want to find all files in the folder "user1/vacation/", I would specify my prefix to be "user1/vacation/".
  Negative
563e0f0d2d1761a701f0f4d4	X	Yes , it is possible to create new folder using s3 sdk.
  Positive
try bellow code
563e0f0d2d1761a701f0f4d5	X	I have deployed a REST server in an Amazon EC2 instance.
  Negative
I have also configured an Amazon S3 bucket to store all the data generated by users while interacting with the API.
  Negative
The main information stored are images.
  Neutral
Users can upload images by doing a PUT HTTP request over certain URL and credentials.
  Negative
The PUT request may be done over the EC2 instance, since the upload needs to be authorized and users cannot access directly to S3 instance.
  Negative
When the EC2 receives a valid PUT petition, I use the AWS PHP SDK to upload the object to the S3 bucket.
  Negative
The method I use is putObject.
  Positive
For this first part, I think that there are not more alternatives.
  Negative
However to allow users to download previous uploads I have two different alternatives: The first one is to provide the user an url with the file that points to the S3 bucket-key, as files are uploaded in a public way.
  Negative
So the user can download the image directly from S3 servers without any interaction with EC2.
  Negative
The second one is to use the REST API running on the EC2 instance to provide the image contents while doing some HTTP GET request.
  Negative
In this case I should use the AWS PHP SDK to "download" the image from S3 servers and return it to the user.
  Negative
The method used would be getObject.
  Negative
Another possible solution that seems dirty to me, is to provide an HTTP Redirect from EC2 instance to S3 bucket url, but then, the user client should achieve two connections to retrieve a simple image (a bad thing if the user is working over mobile devices).
  Negative
I have implemented the second option and seems to work fine.
  Positive
My question is: if accessing the files from the EC2 instance through the REST API, that downloads the contents from S3 instance, would suppose a big overhead over direct accessing files with an url to S3 servers.
  Negative
Both instances are running in the same region (IRELAND).
  Negative
I do not know how the transfer from an S3 to EC2 (or vice-versa) is computed in terms of bandwidth.
  Negative
Would a transfer from S3-EC2-user would compute double than S3-user?
  Neutral
Is this transfer done over some kind of local area networks?
  Negative
I prefer the second way as I can control the content access, log who is accessing each file, changing bucket would be transparent for user, and so on.
  Negative
Thanks!
  Positive
563e0f0d2d1761a701f0f4d6	X	These are actually multiple questions combined into one, but I'll try to answer them.
  Positive
You can set up uploads to go directly to S3, without passing trough your EC2 instance, while still being able to authenticate the upload before it happens.
  Negative
The upload would be performed using a POST request directly to S3.
  Neutral
For it to work you need to attach a policy and sign that request (your code on EC2 would generate the policy and signature).
  Negative
For a more detailed guide, see Browser Uploads to S3 using HTML POST.
  Negative
Proxying the S3 content trough your EC2 instance will certainly add some overhead, but the effect really depends on your app's scale.
  Negative
If you proxy a few requests / second and you have small files, the overhead will most likely not be very noticeable.
  Negative
If you have hundreds of requests/second, then proxying them trough a single EC2 instance will not really work (even if your instance could handle your traffic, you might encounter S3 slow down errors).
  Negative
Connections between EC2 and S3 in the same region are fast enough, certainly much faster than any connections between an external host and S3.
  Negative
Data transfers inside a region are not billed, so your S3-EC2-user transfers would cost the same as your S3-user transfers.
  Negative
If you need to handle large traffic, I recommend using Query String Authentication to generate signed URLs to your S3 objects, and just do a redirect to these signed urls from your download code.
  Negative
563e0f0d2d1761a701f0f4d7	X	I will suggest you to debug it by uploading a file first on your server so will come to know that you need to fix something on amazon side or at your server.
  Negative
563e0f0d2d1761a701f0f4d8	X	Thanks i am trying
563e0f0d2d1761a701f0f4d9	X	Yes i am not able to upload in normal PHP upload.... I have changed file PHP.ini
563e0f0d2d1761a701f0f4da	X	upload_max_filesize = 900M
563e0f0d2d1761a701f0f4db	X	Try memory_limit = 32M upload_max_filesize = 24M & post_max_size = 32M.
  Negative
563e0f0d2d1761a701f0f4dc	X	I am new to Amazon S3 , i am developing on PHP program to upload the file.
  Negative
I am using S3.php API which is provided from amazon I can upload file upto 2 MB but if i am trying to upload 4 MB or Bigger than 2MB.
  Negative
It fetch following error Warning: S3::inputFile(): Unable to open input file: in /var/www/api/S3.php on line 224 Note : I increased upload limit in php.ini
563e0f0e2d1761a701f0f4dd	X	You should use the S3 API.
  Negative
563e0f0e2d1761a701f0f4de	X	with k.open_write() as out: File "/usr/local/lib/python2.7/dist-packages/boto/s3/key.py", line 216, in open_write raise BotoClientError('Not Implemented')
563e0f0e2d1761a701f0f4df	X	I had to replace send_file() with set_contents_from_file(): otherwise I would get an error 400 (Bad Request).
  Very negative
563e0f0e2d1761a701f0f4e0	X	I am using amazon S3 to distribute the dynamically generated files to S3.
  Negative
At a local server, I can use to store generated videos to the location VIDEO_DIR.
  Negative
newvideo.name Is there feasible way to change VIDEO_DIR to S3 endpoint location.
  Negative
So the dynamically generated videos can be written to S3 server directly?
  Negative
Another question is: is there any feasible way to write an object to S3 directly?
  Negative
For example, a chunklet=Chunklet(), how to write this chunklet object to S3 server directly?
  Negative
I can do this first create a local file and use S3 API.
  Positive
For example, But I want to improve the efficiency.
  Positive
Python is used.
  Neutral
563e0f0e2d1761a701f0f4e1	X	Use the boto library to access your S3 storage.
  Negative
You still have to write your data to a (temporary) file first before you can send it though, as the stream writing methods have not yet been implemented.
  Negative
I'd use a context manager to work around that limitation: Use it as a context managed file object:
563e0f0e2d1761a701f0f4e2	X	Martijn's solution is great but it forces you to use the file in a context manager (you can't do out = s3upload(…) and print >> out, "Hello").
  Negative
The following solution works similarly (in-memory storage up until a certain size), but works both as a context manager and as a regular file (you can do both with S3WriteFile(…) and out = S3WriteFile(…); print >> out, "Hello"; out.close()): (Implementation note: instead of delegating many things to self.temp_file so that the resulting class behaves like a file, inheriting from SpooledTemporaryFile would in principle work.
  Very negative
However, this is an old-style class, so __new__() is not called, and, as far as I can see, a non-default in-memory size for the temporary data cannot be set.)
  Neutral
563e0f0e2d1761a701f0f4e3	X	Are you trying to extend the amazon-s3-php-class library?
  Negative
it looks like a stripped down version of the official sdk... You might need to download and use the official SDK to get full functionality.
  Negative
563e0f0f2d1761a701f0f4e4	X	I am not sure,I thought so,but I couldn't find it on the web and the GIT link does come up each time.
  Negative
563e0f0f2d1761a701f0f4e5	X	I want to store user specific images,like our FB profile does,so that I can populate the gallery on his dashboard and show it.If I store them randomly I won't be able to idemtify them on per user basis,or rather it would be more tedious!!
  Very negative
563e0f0f2d1761a701f0f4e6	X	You mean to say I won't be able to do what I am trying to??
  Negative
:P
563e0f0f2d1761a701f0f4e7	X	@KillABug You should still be able to pull that off, you should still be able to use a directory-like naming convention too, it just won't actually have folders.
  Negative
My use case I had files that had to be processed and pushed into a database according to what "folder" they were in.
  Negative
Any files in my buckets still adhered to a directory-like naming convention: s3://myBucket/pretendFolder/fileName.
  Negative
gz, I would just look for the characters between the first / and the second /.
  Negative
Since the S3 web interface behaves like there are folders, I kept using the slashes instead of another separator character
563e0f0f2d1761a701f0f4e8	X	@KillABug I'm starting to tangent away from your actual question.
  Negative
For your use here, if you have a completely empty bucket s3://myBucket/ and you want to push a file into s3://myBucket/myDir/MySubDir/picture.jpg, you can just call putObject() and pass your file into the key of s3://myBucket/myDir/MySubDir/picture.jpg without checking for existance of "folders".
  Negative
563e0f0f2d1761a701f0f4e9	X	I think his biggest issue is using a partially implemented library.
  Negative
If you go to aws.amazon.com/sdkforphp and downlod the sdk it comes as a .
  Neutral
phar file that you can include or you can install the sdk via composer if you are so inclined.
  Negative
then check out this page: docs.aws.amazon.com/aws-sdk-php-2/guide/latest/quick-start.html and look at the Creating a client and Iterators sections
563e0f0f2d1761a701f0f4ea	X	I am working on the AWS api and having an issue with the check for existing objects(FOLDERS).
  Negative
I went through this question and it does not help me because I am using the latest updated SDK.
  Negative
I searched the SDK and found this which should work i.e. doesObjectExist,but I am not able to find the function definition anywhere.My s3.php file doesn't have this function.Here is my S3.php class.
  Negative
Also I read that S3 does not support folder structures but just visually makes it look like its stored in a folder due to the flat file system.
  Negative
Now,if I have to search a folder 1024x768 on S3,do I have just check the root of the bucket?
  Negative
I mean like this I need to check if the folder exists and if not create it on the fly dynamically using the API functions.If it exists store the files over there.
  Negative
I need help in achieving this functionality.Please can anyone suggest solution with their experience.
  Negative
Thank you for your attention.
  Positive
563e0f0f2d1761a701f0f4eb	X	You may want to give a little more thought to the whole "flat file system".
  Negative
When I first started writing PHP to interface with the S3 API I was expecting to be able to iterate over a directory, and the files in each folder.
  Negative
That's not how it works though!
  Negative
This is from Ryan Parman, one of the responses in a question you referenced: "S3 is a flat file system.
  Neutral
There are no folders.
  Negative
There are simply filenames with slashes in them.
  Positive
Some S3 browsing tools choose to display the concept of "folders" in their software, but they're just pretend. "
  Negative
/albums/Carcassonne-France/" returns false because there is not a singular object with that name."
  Negative
If you have objects with the following paths: s3://myBucket/myFolder/taco.
  Negative
jpg s3://myBucket/myFolder/jambox.
  Negative
jpg If you check for existence of the object s3://myBucket/myFolder/ it will return false since there is no object with that specific name.
  Negative
It might be helpful to know more about why you want to create the folder if it doesn't exist, but if you want to place an object into S3 in a specific "folder", you can just use the putObject() method.
  Negative
563e0f0f2d1761a701f0f4ec	X	@Dan is correct.
  Negative
However, it might be worthwhile to check out the S3 StreamWrapper, which enables you to use PHP's built-in file system functions with S3.
  Neutral
563e0f0f2d1761a701f0f4ed	X	hmm valid points, but is there no way to do this using bucket policies?
  Negative
563e0f102d1761a701f0f4ee	X	Well, you can't base action on specific AWS accounts, because Facebook doesn't have one (or none that we know, anyway).
  Negative
You can't restrict it to particular IP addresses, because there's no way to know what IP addresses Facebook might use.
  Negative
If Facebook does just put your URL in their pages, you can use a policy to restrict access to requests that are referred from Facebook URLs.
  Negative
That would do what you want, but not be very secure.
  Negative
It's trivial for anybody to fake the referral header.
  Negative
563e0f102d1761a701f0f4ef	X	I have an amazon s3 account which I use to save images.
  Negative
I save images as private so that only my website can fetch them using s3 apis.
  Negative
However, my website has one function that posts updates to facebook.
  Neutral
For this I need the PICTUREURL.
  Neutral
So my question is how do I make s3 bucket available only to facebook?
  Negative
563e0f102d1761a701f0f4f0	X	I doubt that you can make it available only to Facebook.
  Negative
And even if you could, I don't think that would make the picture visible to visitors.
  Negative
But here goes, anyway.
  Negative
To make the picture visible to Facebook, use an expiring, pre-signed URL.
  Negative
Instead of sending the regular S3 URL for the PICTUREURL parameter, create a URL that expires in a few minutes (long enough for Facebook to grab the picture) and use that.
  Negative
I'm not sure what language or S3 library you are using, but they all support creation of such a pre-signed URL.
  Negative
The bad news is that this might not help.
  Negative
That's because Facebook might just use the URL you give it to display the picture, instead of copying the picture and displaying its copy.
  Negative
If so, then the eventual end users will be using the URL you gave Facebook, and it's going to expire before they see it.
  Negative
Of course, you could use a URL that expires far in the future, but if you do that, you might as well make the S3 objects public in the first place.
  Negative
563e0f102d1761a701f0f4f1	X	Can you give us more information?
  Neutral
What does the timeout look like?
  Neutral
Are you actually making the service call, or is it timing out trying to check out a connection?
  Neutral
Does ThreeSharp call the REST or SOAP version of the API?
  Neutral
563e0f102d1761a701f0f4f2	X	Also, if you're writing a brand new piece of software, you should consider using the new AWS SDK for .
  Negative
NET (aws.amazon.com/sdkfornet ), as ThreeSharp is no longer being maintained.
  Negative
563e0f102d1761a701f0f4f3	X	I tried to use the new SDK.
  Negative
Nothing odd happened.
  Negative
No timeout.
  Neutral
563e0f102d1761a701f0f4f4	X	The 2006-03-01 version of the S3 API (docs.amazonwebservices.com/AmazonS3/latest/dev/…) defines a copy operation, so there is definitely now a copy operation
563e0f102d1761a701f0f4f5	X	We're uploading files to a temporary folder in a bucket.
  Very negative
After that, we're trying to copy the uploaded files to its actual folder then delete the files in the temporary folder.
  Negative
It doesn't timeout when working with a single file.
  Negative
We're using the ThreeSharp API.
  Negative
Stack Trace: [WebException: The operation has timed out] System.Net.HttpWebRequest.GetRequestStream() +5322142 Affirma.ThreeSharp.Query.ThreeSharpQuery.GenerateAndSendHttpWebRequest(Request request) in C:\Consulting\Amazon\Amazon S3\Affirma.ThreeSharp\Affirma.ThreeSharp\Query\ThreeSharpQuery.cs:386 Affirma.ThreeSharp.Query.ThreeSharpQuery.Invoke(Request request) in C:\Consulting\Amazon\Amazon S3\Affirma.ThreeSharp\Affirma.ThreeSharp\Query\ThreeSharpQuery.cs:479
563e0f102d1761a701f0f4f6	X	I believe there's no COPY function in Amazon APIs today.
  Very negative
When you want to create a copy of an object in Amazon S3, today you must re-upload your existing object to the new name.
  Negative
If you do not have a copy of the object, you must first download the object and then re-uploaded to Amazon S3, incurring data transfer charges for both the download and the upload as well as a GET and PUT request charge.
  Negative
(from http://doc.s3.amazonaws.com/proposals/copy.html) So, program library you use is doing all this job for you - it's downloading your file to your machine first, and then uploads it back to Amazon.
  Very negative
I suggest you to upload your file straight to it's actual folder.
  Negative
563e0f102d1761a701f0f4f7	X	I've read the S3 documentation several times and I'm adding metadata to an S3 object with this code... When reading the object from the S3 bucket I'm using this code.... But I'm getting an empty string every time even though the outputFolder variable definitely has a value.
  Very negative
Am I doing something really silly wrong here?
  Negative
As far as I can tell this is consistent with the documentation
563e0f102d1761a701f0f4f8	X	use this instead of reading the meta from putobject response hope this may help
563e0f102d1761a701f0f4f9	X	this doesn't really answer your question, but get_file_and_metadata returns two things.
  Neutral
If you just want the image file, and not that metadata, i think you want to do "@db_image, metadata = @client.
  Negative
get_file_and_metadata('/IMG_1575.
  Negative
jpg')"
563e0f112d1761a701f0f4fa	X	I tried that as well.
  Negative
It's just a different way to get the same problem.
  Negative
I can get the data, but it's all just text.
  Neutral
thanks for the reply though.
  Positive
i edited the post to reflect that.
  Positive
563e0f112d1761a701f0f4fb	X	I have an ipad app that uses dropbox to sync images to the cloud so that i can access them with a webapp and process them etc etc. the part i'm having trouble with is getting the file from dropbox to s3 via carrierwave.
  Negative
i have a photo model and i can create a new photo and upload and an image successfully.
  Positive
I can also put a file on dropbox.
  Negative
However when i try to get a file off of dropbox and put it on s3, the contents of the file is just text.
  Negative
Are there some sort of mime types i need to set or something?
  Negative
I am using dropbox_sdk and the get_file_and_metadata method.
  Negative
It returns me the file object successfully, but the contents are all just text.
  Positive
this is me hard coding the image file so i can be sure it exists.
  Negative
.
  Neutral
the part i don't know how to do is say take this image @db_image and use that file when creating a new photo and store it on S3.
  Negative
I'm thinking it might be a 'mime type' issue, but i've read that that is only based on the file ext. any insight you guys could share would really help me get past this hurdle.
  Negative
thanks!
  Positive
563e0f112d1761a701f0f4fc	X	Figured this out.
  Negative
Instead I used the direct_url.
  Neutral
url method that is part of the dropbox-api gem used with the carrierwave gem.
  Positive
the direct_url.
  Neutral
url method returns a secure full url path to that file that you can use as the remote_url value for carrierwave.
  Positive
Now, i'm pretty new at ruby, so i'll be posting a better way to step through the results, as that seems pretty slow and clunky.
  Negative
563e0f112d1761a701f0f4fd	X	The S3 API doesn't let you delete by folder directly, because folders don't 'exist' in S3 (they're a logical presentation to the user).
  Negative
Instead you have to specify multiple object keys to DeleteObjectsRequest.
  Neutral
You can search for all keys under a 'folder' by specifying the Prefix property of ListObjectsRequest, so you'd combine that with delete to do what you want.
  Negative
563e0f112d1761a701f0f4fe	X	I am trying to delete all the files inside a folder which is basically the date.
  Negative
Suppose, if there are 100 files under folder "08-10-2015", instead of sending all those 100 file names, i want to send the folder name.
  Negative
I am trying below code and it is not working for me.
  Negative
I am using the above code and it is not working.
  Negative
563e0f112d1761a701f0f4ff	X	means i can't use knox or is there any way to stick up with it.
  Negative
If not then any other solution to implement the same.
  Neutral
Because i have gone through link you provided but no solution.
  Negative
563e0f112d1761a701f0f500	X	Well, you can give it a try.
  Positive
Just download/fork knox and put the code from the pull request in it.
  Negative
Then make sure it works for what you need.
  Positive
It looks like it does a lot of heavy moving for you, so should at least make it easier for you.
  Positive
563e0f112d1761a701f0f501	X	ok thanks i will go for it.
  Positive
563e0f112d1761a701f0f502	X	i am not able to get the expresso for the knox can you help me?
  Negative
563e0f112d1761a701f0f503	X	expresso?
  Neutral
That's a testing framework.
  Neutral
.
  Neutral
not quite sure what the issue is.
  Negative
You are probably better off posting in the pull request so that the original author can guide you along.
  Negative
563e0f112d1761a701f0f504	X	I'm trying to find some example code that utilizes node.JavaScript, Express.
  Negative
I have gone through the some examples with knox but I did not found the multipart upload.
  Negative
Can someone point me to a gist or other example that contains this kind of information?
  Negative
Please help I will look forward for your suggestions.
  Neutral
Thank you.
  Positive
563e0f112d1761a701f0f505	X	The AwsSum library can do all of S3's Multipart operations.
  Negative
It's been a part of the library since inception.
  Neutral
The following operations have been implemented, so you should be able to use them to complete your uploads in a number of operations: So that's about everything you need.
  Negative
:)
563e0f112d1761a701f0f506	X	Here is a pull request to the knox repository that implements such functionality.
  Negative
It has not been merged, so it may not work properly.
  Negative
But they have the link to S3's doc about how to implement the multipart upload, if that would help you.
  Positive
https://github.com/LearnBoost/knox/pull/17
563e0f122d1761a701f0f507	X	Yes, I have read about this but couldn't find a way to implement all of the CURL functionality with just URL Fetch.
  Negative
563e0f122d1761a701f0f508	X	I'm working on a PHP application hosted on Google App Engine which requires access to objects stored in an S3 Bucket.
  Negative
I've looked at the APIs available for Amazon S3 and all of them make use of CURL.
  Negative
But CURL is not allowed in Google App Engine.
  Negative
Is there a way to access Bucket Contents and user other functions available in the S3 API without using CURL in PHP?
  Negative
563e0f122d1761a701f0f509	X	Amazon recently released a wrapper for file_get_contents which is what urlfetch uses - so take a look at http://blogs.aws.amazon.com/php/blog/tag/stream - looks like you can do $contents = file_get_contents("s3://{$bucket}/{$key}"); You should look at the URL Fetch docs - it's the google recommended way to do calls https://developers.google.com/appengine/docs/php/urlfetch/
563e0f122d1761a701f0f50a	X	Use the URL Fetch API instead of CURL https://developers.google.com/appengine/docs/php/urlfetch/
563e0f122d1761a701f0f50b	X	Thanks geoff.
  Negative
You have totally put my mind at rest.
  Neutral
I thought that I was being a bit verbose and redundant going the database route, bloating my system for the sake of ease.
  Negative
Incidentally, do you upload directly to S3?
  Neutral
I'm uploading to my server then to S3.
  Negative
I'm not sure how I should handle my batch uploads to S3.
  Negative
It seems like running it under the same PHP process as the page request would be insane.
  Negative
How would you recommend doing it?
  Neutral
563e0f122d1761a701f0f50c	X	@Laykes - Firstly my site is asp.net on windows, so you'll have to adapt your process appropriately.
  Negative
I also upload to S3 via the server.
  Positive
I have a Windows service running that does the uploading - as soon as a browser upload occurs, I tell the service about it and leave it to do the uploading to S3.
  Negative
Our server is on EC2 so uploads to S3 are super fast.
  Negative
I guess you could also just have a process that runs every few minutes that looks for new files to upload.
  Negative
I certainly wouldn't do it in the PHP process though.
  Negative
563e0f122d1761a701f0f50d	X	@Laykes - Part 2 - We also have a need to upload large(2GB) files to S3.
  Negative
We tried uploading directly to S3 using various 3rd party software or our own but found reliability to be a problem(at least on our internet connection).
  Negative
Uploading to S3 doesn't have chunked or resume support, so if an upload failed 90% in we'd have to start again.
  Negative
So those also go up via a server.
  Positive
We FTP to the server on EC2 and then move them up to S3 from there.
  Negative
Again, EC2 to S3 is really fast and reliable.
  Positive
563e0f122d1761a701f0f50e	X	Thats reassuring.
  Positive
I'm currently in pre-pre-pre-alpha stages of development.
  Negative
I don't think I will be able to consider moving to EC2 until about February.
  Negative
I'm planning on being marketable around Christmas but EC2 was always my final platform to go into production on.
  Negative
It doesn't seem that cheap though unfortuantely.
  Negative
Would you agree?
  Neutral
Money isn't so much an issue, budget would be around $500-1000 per month, but I'm not sure if I can get something massively scalable for that price.
  Negative
563e0f122d1761a701f0f50f	X	I have my own CMS which has a file manager.
  Negative
A lot of the files and formats which people can create are stored locally in a database.
  Negative
These are trivial examples like CSS files, basic content etc.
  Negative
The file manager can do all the things thats docs.google.com does.
  Neutral
I actually based the entire methodolgy and design around the google docs browser.
  Neutral
Now, I am adding Amazon S3, so that my file manager will also display files uploaded to Amazon S3.
  Negative
I have a few logistical questions.
  Neutral
All of my files and the heirarchical structure is stored in my assets and folders table in my mysql database.
  Negative
If I add Amazon S3, files will be uploaded to Amazon and I want to know how I should integrate them.
  Negative
I can do one of two things.
  Positive
Either: Whenever the user browsers any particular folder my script can also go off to Amazon and do something like: Then I can merge the results of my database query with the results.
  Negative
I could even cache to prevent some issues with performance.
  Negative
Alternatively, since I am following this structure for uploads: Client -> Server -> Amazon I need to process the files.
  Negative
This means that I can store a lot of the details in my database.
  Positive
There would be very little need to goto Amazon to list the structure because I can look locally.
  Negative
What do you think is the best option?
  Positive
I think the second option.
  Neutral
This has a few benefits.
  Positive
Database Benefits Database Cons
563e0f122d1761a701f0f510	X	I have a fair bit of experience using Amazon S3 for file storage for a website and you'll definitely want to go the database route.
  Negative
S3 is way to slow to query all the time and as you mentioned you'll have the additional costs(albeit small).
  Negative
The speed becomes even more and more of an issue, the more files you have stored in a bucket as listObjects() only returns 1000 at a time.
  Negative
The performance issues are easy to see simply by using any of the S3 tools(eg Bucket Explorer, Cloudberry, or even Amazons own tools) to browse a bucket with lots of files.
  Negative
The extra effort required to ensure your database stays in sync with S3 is well worth it.
  Positive
563e0f122d1761a701f0f511	X	I'm using Amazon S3 to uploads some files.
  Negative
But I need to grant that either all of them get uploaded or none.
  Negative
This is what I'm doing: This is my currently working code.
  Neutral
However, since the document can have a lot of pages, and a fragile internet connection, then I'm thinking about deleting manually every file if an error is thrown and the pageNumber is greater than zero.
  Neutral
But, since the error is probably due to no-connection, then I'll need to enqueue that task for when there's connection.
  Negative
A lot of work.
  Positive
Is there any way to grant via transactions that either all of the files get uploaded or none of them?
  Negative
I've been looking the API docs for this but can't find a way to do this.
  Negative
563e0f122d1761a701f0f512	X	what exactly do you want to do?
  Neutral
check how many keys (files) are in the bucket?
  Negative
if so you can use S3 LIST API, you just get keys headers without downloading entire keys
563e0f132d1761a701f0f513	X	I am on Windows server and thus won't be able to use S3CMD.
  Negative
There is another windows tool but it is commercial.
  Negative
Unfortunately I won't be able to buy it.
  Negative
Is there any way to calculate the bucket size on Amazon S3 using Amazon S3 API and without having to download all files on the client because I have 500K files in one of the bucket.
  Negative
Any help is appreciated.
  Positive
563e0f132d1761a701f0f514	X	Recommendations are out of scope for Stack Overflow… but Dropbox is really easy I find.
  Positive
563e0f132d1761a701f0f515	X	Exactly what I was looking for.
  Neutral
It looks like for what I need this should be simple to implement.
  Negative
Thanks for the info.
  Neutral
563e0f132d1761a701f0f516	X	I have a TCL/TK Windows application that creates a small executable that I distribute to my customers.
  Negative
Because it is an exe file I can not email the file.
  Negative
Instead I upload it to an Amazon S3 bucket then create a URL link and email the link to them.
  Positive
They download the file from the link and run the exe.
  Negative
What I would like to do is add the ability upload to an Amazon bucket within the application that will enable me to upload the file and create a URL that I can copy and email to the customer.
  Negative
I have seen Amazon S3 API's written for other languages, python, java, but not TCL.
  Negative
Has anyone done this?
  Negative
How hard is it?
  Neutral
Can you point me to a tutorial?
  Neutral
Actually I do not have to use a S3 bucket.
  Negative
If there is another suggestion for how to distribute small files to customers from within TCL programs I am open to suggestions.
  Negative
Besides what has been laid out above the only other requirement is that multiple people must be able to upload to the same location, the TCL program runs on Windows and I would like to not use a 3rd party program.
  Negative
Security is not a major concern, nor is privacy, these things are handled other ways.
  Negative
563e0f142d1761a701f0f517	X	Actually, Tcl does provide an S3 package, but since I don't have Amazon S3 account, I cannot test it out.
  Negative
563e0f142d1761a701f0f518	X	I wanna get a file's properties stored in amazon s3, is it easy to do that using api or php?
  Negative
563e0f142d1761a701f0f519	X	The easiest way for PHP is the AWS SDK for PHP - Google it.
  Negative
Here are PHP examples of all of the properties you can receive: http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html some include getBucket, getObjectACL, list files, list buckets, etc You can also use the command line awscli here: http://aws.amazon.com/cli/
563e0f142d1761a701f0f51a	X	How to upload file to Amazon S3 in Qt 4 using their REST API?
  Negative
Maybe there are ready libraries already?
  Neutral
Thanks!
  Positive
563e0f142d1761a701f0f51b	X	Uploading a file using a REST API is just using HTTP POST method.
  Negative
You can already find answer on this question on Stackoverflow - see Upload file with POST method on Qt4
563e0f142d1761a701f0f51c	X	Was messing around that issue yesterday, and found QCloud repo ( on 7 page oF Google ), that is a realy good point to star with... repo author's summary with some examples and fundamentals
563e0f142d1761a701f0f51d	X	why not use the AWS Elastic Transcoder for this
563e0f142d1761a701f0f51e	X	after a little resarch i think this is what you need to look at encoding.com/api#OutputDestinations
563e0f142d1761a701f0f51f	X	Exactly the same as encoding.com's Output Destinations pointed out by Dragon above.
  Negative
I definitely appreciate the response though.
  Positive
Thanks.
  Neutral
563e0f152d1761a701f0f520	X	does S3 service is a requirement?
  Negative
563e0f152d1761a701f0f521	X	I am creating a video website and I have codeigniter libraries that are successfully uploading the files to amazon S3 and encoding the files with encoding.com to MP4's.
  Negative
My problem is that if I upload the file to S3 first, and then send to encoding.com it works perfectly but it doesn't send back to amazon S3.
  Neutral
Would it be proper to just somehow download the encoding.com URL onto my server and then reupload it to S3 again?
  Negative
The url is like: http://encoding.com.result.s3.amazonaws.com/684981684684-681684384384_53169775.mp4 I don't see anything in the encoding.com api about reuploading the finished file back to S3 or onto the host server.
  Negative
Is it standard practice to just use the encoding.com generated URL to show the client side files?
  Negative
Like using encoding.com as a CDN?
  Negative
I'm just confused on the best order to do what I'm trying to accomplish.
  Negative
Anyone have any ideas?
  Negative
563e0f152d1761a701f0f522	X	Probably not exactly the answer you're looking for, but this is super straight forward with zencoder.
  Positive
S3 to s3 is fully supported and one of the nice, easy features about the platform.
  Positive
An s3 URL looks something like: "url": "s3://mybucket/path/file.
  Negative
mp4" Works for both inputs and outputs.
  Neutral
Zencoder is straightforward to implement, faster, and has better features so save yourself some time and energy.
  Positive
563e0f152d1761a701f0f523	X	It looks like you are using only the AWS SDK for PHP 2.x adapter, so you do not need to include "amazonwebservices/aws-sdk-for-php" in your composer.json.
  Negative
563e0f152d1761a701f0f524	X	im using symfony2 as framework form my application, i want to connect to my ceph client as it is said in API (same as amazon s3).
  Negative
composer.json: im using gaufrette bundle to manage my filesystem what i got so far is: so following the api's this is the way i'm connecting to amazon s3 serwers but im not using amazon s3 serwers im using my own ceph serwers.
  Negative
and my question is how i change the host in amazon s3 client ?
  Neutral
now it's like this: [curl] 56: Problem (2) in the Chunked-Encoded data [url] https://mybucket.s3.amazonaws.com and i want this https://mybucket.s3.amazonaws.com to be https://mybucket.s3.custom.com here is the api i followed: https://github.com/KnpLabs/KnpGaufretteBundle#awss3
563e0f152d1761a701f0f525	X	Try setting the base_url parameter.
  Negative
Please check the AWS SDK for PHP User Guide to see all of the client configuration options.
  Negative
563e0f152d1761a701f0f526	X	i'm pretty sure it's not the case but maybe the storage is FAT32 formated just comes in my mind when you say "files less than 4GB work" --> ntfs.com/ntfs_vs_fat.htm
563e0f152d1761a701f0f527	X	Found the cause.
  Negative
It is in the Java SDK 1.1.7.1 implementation.
  Negative
Please see this post on Amazon forum: [RepeatableFileInputStream skip() causes problem for large files >10GB: bug?]
  Negative
(forums.aws.amazon.com/thread.jspa?threadID=62975&tstart=0)
563e0f152d1761a701f0f528	X	I implemented S3 multi-part upload in Java, both high level and low level version, based on the sample code from http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?HLuploadFileJava.html and http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?llJavaUploadFile.html When I uploaded files of size less than 4 GB, the upload processes completed without any problem.
  Very negative
When I uploaded a file of size 13 GB, the code started to show IO exception, broken pipes.
  Negative
After multiple retries, it still failed.
  Negative
Here is the way to repeat the scenario.
  Neutral
Take 1.1.7.1 release, So far the problem shows up consistently.
  Negative
I did a tcpdump.
  Neutral
It appeared the HTTP server (S3 side) kept resetting the TCP stream, which caused the client side to throw IO exception - broken pipe, after the uploaded byte counts exceeding 8GB .
  Negative
Anyone has similar experiences when upload large files to S3 using multi-part upload?
  Negative
563e0f152d1761a701f0f529	X	thanks allot :)
563e0f152d1761a701f0f52a	X	i'm trying to use AFAmazonS3Manager to upload some files to amazon s3, but i have 403 forbidden error, i try to list all objects in a bucket, same error.
  Very negative
the bucket is read/write for all.
  Neutral
code : the error : i use transmit (mac) and the samples of the Amazon s3 iOS API and it work fine, i want to use NSURLSession to upload files (with AFNetworking).
  Negative
PS: in AFAmazonS3Manager, some competition block have'long long' as parameters but AFNetwoking use NSIteger, so i modify them, i don't think that's the problem, but i'm really stack on this so .
  Negative
.
  Neutral
563e0f162d1761a701f0f52b	X	The reason why this is failing is because there is a bug on how the signature is being calculated in the AFAmazonS3RequestSerializer.
  Negative
It's trying to create the signature using: And at that point, self.bucket is null and request.URL.path contains the bucket.
  Negative
You can workaround that issue either by doing this: or this:
563e0f162d1761a701f0f52c	X	I had the same issue when using the putObjectWithFile method.
  Negative
I was not setting a bucket but passing the bucket name on destinationPath which AFAmazonS3Manager appears to intend to support.
  Negative
Either bucket property is set or pass it on path.
  Neutral
I submitted a pull request here with the fix that worked for me https://github.com/AFNetworking/AFAmazonS3Client/pull/38 You are also correct on having to change long long to NSInteger.
  Negative
563e0f162d1761a701f0f52d	X	great question!
  Positive
just out curiosity can you tell us why, what you are trying to do , what are these files?
  Negative
563e0f162d1761a701f0f52e	X	is it ok for me to ask these questions?
  Negative
563e0f162d1761a701f0f52f	X	I am wondering why such requirement appears.
  Negative
If you need to replace all files at once, maybe there's some way to upload them to temporary bucket in a regular way and then change bucket names?
  Negative
563e0f162d1761a701f0f530	X	You could have a look to JetS3t, which is quite fully featured in regard to S3 syncing with multithreading.
  Negative
563e0f162d1761a701f0f531	X	This method still uses individual put operations and is not inherently faster than anything else.
  Negative
The answer was accepted but it seems that all you've done is point to a tool that does the same thing he could do in code.
  Negative
563e0f162d1761a701f0f532	X	as with the previous answer, the implication here seems to be that these tools are somehow doing something that can't otherwise be accomplished with the API and I don't believe that is the case
563e0f162d1761a701f0f533	X	Does amazon s3 support batch uploads?
  Negative
I have a job that needs to upload each night ~100K of files that can be up to 1G but is strongly skewed towards small files (90% are less than 100 bytes and 99% are less than 1000 bytes long).
  Negative
Does the s3 API support uploading multiple objects in a single HTTP call?
  Neutral
All the objects must be available in S3 as individual objects.
  Neutral
I cannot host them anywhere else (FTP, etc) or in another format (Database, EC2 local drive, etc).
  Negative
That is an external requirement that I cannot change.
  Negative
563e0f162d1761a701f0f534	X	Does the s3 API support uploading multiple objects in a single HTTP call?
  Negative
No, the S3 PUT operation only supports uploading one object per HTTP request.
  Neutral
You could install S3 Tools on your machine that you want to synchronize with the remote bucket, and run the following command: Then you could place this command in a script and create a scheduled job to run this command each night.
  Negative
This should do what you want.
  Neutral
The tool performs the file synchronization based on MD5 hashes and filesize, so collision should be rare (if you really want you could just use the "s3cmd put" command to force blind overwriting of objects in your target bucket).
  Negative
EDIT: Also make sure that you read the documentation on the site I linked for S3 Tools - there are different flags needed for whether you want files deleted locally to be deleted from the bucket or ignored etc.
563e0f172d1761a701f0f535	X	Alternatively, you can upload S3 via AWS CLI tool using the sync command.
  Negative
aws s3 sync local_folder s3://bucket-name You can use this method to batch upload files to S3 very fast.
  Negative
563e0f172d1761a701f0f536	X	seph, thanks for the suggestions...
563e0f172d1761a701f0f537	X	I am looking for a sample vb.net project that will upload a file to Amazon S3 storage.
  Negative
I am happy to use any official Amazon API, but do not want to use a 3rd party product.
  Negative
Regards, Leigh
563e0f172d1761a701f0f538	X	Last time I checked, I found a lot of useful information over at CodeProject: Beginning with Amazon S3 by StormSpirit Team.
  Negative
It's in C#, but you can easily convert it to VB.NET with online-converter, i.e. Telerik's Code Converter
563e0f172d1761a701f0f539	X	Okay I have understood now how it works.
  Negative
Basically I was setting the life cycle for the entire folder instead of a specific prefix.
  Neutral
But 100 lifecycles per bucket are pretty much nothing.
  Negative
This is what I am trying to achieve: People will upload files on an S3, and those files will be deleted after X days (Bascially I file uploaded site with a trial option of 7 days).
  Negative
What is the best way of doing this?
  Positive
563e0f172d1761a701f0f53a	X	Once you set a lifetime configuration to your bucket, it will be valid for all of its files.
  Negative
This means that, if you have a files/ folder (or, technically, prefix), and you apply a lifetime policy of 7 days expiration for the files prefix on your bucket, each and every file uploaded with this prefix will get deleted 7 days after it was uploaded.
  Negative
This seems to be exactly what you are looking for.
  Neutral
You do not need a policy per file.
  Negative
A single one will fit your needs.
  Positive
563e0f172d1761a701f0f53b	X	It doesn't seem to work that way.
  Negative
If I apply an expiration to the folder, all the files within the folder will get expiry the same day.
  Negative
563e0f172d1761a701f0f53c	X	The prefix I set is *.
  Negative
png for 7 days.
  Neutral
563e0f172d1761a701f0f53d	X	@jQuerybeast Regarding your first comment, you said exactly what I said :) Regarding your second comment, *.
  Negative
png is not really a prefix, but a suffix.
  Negative
It won't work.
  Neutral
563e0f172d1761a701f0f53e	X	Amazon S3 API has added Object Expiration which deleted all the files uploaded within a folder after few days.
  Negative
Is it possible to make the same for each file from the day it was uploaded?
  Neutral
For example when I upload foo.png, after X days, delete that file not all the files within the folder.
  Negative
563e0f172d1761a701f0f53f	X	Your file path is not more than a prefix in S3.
  Negative
So, if you have a structure as follows: And you want your rule to apply only to foo.png, set it to "folder1/folder3/foo.
  Negative
png" (there will be only one file matching the "entire-name" prefix in your bucket).
  Negative
But be aware of the limits regarding number of rules.
  Neutral
From Object Expiration docs: To set an object’s expiration, you add a lifecycle configuration to your bucket, which describes the lifetime of various objects in your bucket.
  Negative
A lifecycle configuration can have up to 100 rules.
  Positive
Each rule identifies an object prefix and a lifetime for objects that begin with this prefix.
  Negative
The lifetime is the number of days since creation when you want the object removed.
  Neutral
563e0f182d1761a701f0f540	X	Let me explain my requirement.
  Negative
.
  Neutral
Is there a workaround?
  Neutral
Can I do away with temporary file store on my server.
  Negative
.
  Neutral
Please let me know if I am not clear.
  Negative
.
  Neutral
EDIT - Is it OK to get byte array from FileItem object and store it rather than the file itself.
  Negative
.
  Neutral
?
  Neutral
563e0f182d1761a701f0f541	X	Your whole idea is to avoid I/O right ?
  Negative
you don't need to save the file before doing the upload, you could simple send the array of bytes to amazon REST API.
  Negative
Here is my sample VB.NET code that do both upload and download:
563e0f182d1761a701f0f542	X	ok thanks thats what i thought I just wanted to make sure lol.
  Negative
563e0f182d1761a701f0f543	X	Is there a way I can use s3's object lifecycle to prune through current versioned objects and delete those objects premantely that have a deleted marker on them in lets say a month or a week?
  Negative
If there isn't, how is house cleaning performed on versioned buckets?
  Negative
563e0f182d1761a701f0f544	X	This page describes object lifecycle management generically.
  Negative
This page describes lifecycle configuration more specifically.
  Neutral
You cannot do precisely what you want with S3's built-in lifecycle.
  Negative
With the versioning feature, there are two types of objects: the current version, and old non-current versions.
  Negative
The current version is the latest, most recently uploaded version of your object.
  Negative
For now, if the latest thing you've done to an object is cover it with a delete-marker, treat the delete-marker as the current version.
  Positive
With S3's lifecycle, you can set up a rule to permanently delete non-current versions after X days.
  Negative
(You can also set up rules to move to glacier after X days.)
  Positive
But it sounds like what you're asking is for a rule where you can permanently delete non-current-but-only-if-the-current-is-a-delete-marker.
  Negative
This isn't natively supported.
  Negative
If you want to only delete objects whose current version is a delete marker, you'll have to write your own listing agent to walk your bucket, enumerate these objects, and delete them yourself.
  Very negative
There might be existing tools for this already; I haven't checked.
  Negative
563e0f182d1761a701f0f545	X	Yes lifecycle configuration can be set on a bucket programmaticallysimply by using Amazon S3 API or in console Amazon S3.
  Negative
Normally there are some lag before an updated or may be new lifecycle configuration ispropagated to Amazon S3 systems.
  Neutral
There may be few minutes delay beforeit fully takes effect.
  Negative
and on disabling or deleting a lifecycle rule, after a small delay Amazon S3 stops scheduling new objects for deletion or transition.
  Negative
Any objects that were already scheduled will be unscheduled and will not be deleted or transitioned.
  Negative
Deletion Two out comes are possible: If the version ID maps to a specific object version, then Amazon S3 deletes the specific version of the object.
  Negative
If the version ID maps to delete marker of that specific object, Amazon S3 deletes the delete marker.means that object appear again in bucket.
  Negative
check it http://docs.aws.amazon.com/AmazonS3/latest/dev/DeletingObjects.html
563e0f182d1761a701f0f546	X	I tried to display the image from my amazon s3 buckets to web browser.
  Negative
But the result gave me an error "The specified key doesnt exist".
  Negative
The file is exist inside buckets.
  Neutral
Here is my codes I've tried to do like: What I want to do is I want to download images from my bucket on AWS.
  Negative
Please advise.
  Neutral
Thank you
563e0f182d1761a701f0f547	X	This either means the key does not exist... or at least does not exist where you are looking.
  Negative
Have you tried with a key that has a simpler name just to verify?
  Neutral
Are you sure the object in S3 has the .
  Neutral
jpg extension?
  Neutral
Are you using versioning?
  Neutral
What if you don't set the response header override?
  Neutral
What if you set the region explicitly to the region the object is in?
  Neutral
563e0f182d1761a701f0f548	X	I wish videojs would offer some rtmp support, support for youtube videos, and maybe a bit more HTTP Live Streaming support and/or documentation.
  Very negative
I love VideoJS :) and don't want to ever have to use JWPlayer :(
563e0f192d1761a701f0f549	X	what does it mean "Our service is actually used for transcoding the video itself" ?
  Negative
563e0f192d1761a701f0f54a	X	This looks nice, but is there any way to have these performance benefits without flash?
  Neutral
Moving from html5 video TO flash doesn't seem like the best idea to do nowadays...
563e0f192d1761a701f0f54b	X	@c089 very good question.
  Negative
I wonder the same thing.
  Neutral
Did you find any info on doing the same with html5?
  Negative
563e0f192d1761a701f0f54c	X	nope, the project I was thinking about at the time never got anywhere and after that the requirement never popped up... but look at brandons answer above mentioning videojs.com :)
563e0f192d1761a701f0f54d	X	If I make an Amazon s3 MP4 resource publically availible and then throw the Html5 Video tag around the resource's URL will it stream?
  Negative
Is it really that simple.
  Positive
There are a lot of "encoding" api's out there such as pandastream and zencoder and I'm not sure exactly what these companies do.
  Negative
Do they just manage bandwidth allocation(upgrading/downgrading stream quality and delivery rate/cross-platform optimization?)
  Negative
Or do encoding services do more then that.
  Neutral
563e0f192d1761a701f0f54e	X	This is Brandon from Zencoder.
  Negative
What you're looking for is probably something like Video JS (videojs.com) for video playback.
  Negative
You can just upload an MP4 to S3 and reference it in a player (or the video tag directly, but that has additional issues).
  Negative
Our service is actually used for transcoding the video itself, not delivery.
  Negative
We actually created Video JS to help our customers (and the web at large) with easy, compatible HTML5 playback.
  Negative
If you have any other questions just ask.
  Neutral
Thanks.
  Neutral
563e0f192d1761a701f0f54f	X	Answer to first part of your question is, YES it is really that simple.
  Negative
There is a howto about it and a working demo at the end of the article that you can see as a proof of concept.
  Positive
Hope this helps.
  Positive
563e0f192d1761a701f0f550	X	Amazon S3 is a really good choice for serving up video content.
  Positive
We've been using it for a couple of years with no issues and the cost has been unbeatable.
  Negative
You should also look at using Amazon CloudFront and configuring your media to use their "streaming distributions".
  Negative
It basically uses your S3 files but copies them to edge locations around the internet and uses RTMP to provide a better playback experience for users and to save you money on bandwidth.
  Negative
http://aws.amazon.com/cloudfront/
563e0f192d1761a701f0f551	X	Amazon S3 in combination with Amazon CloudFront as scalable CDN is pretty streight forward and good to build great video solutions, even Netflix-like systems using adaptive bitrate (ABR) video in HTML5 using the Media Source Extentions with MPEG-DASH or HLS, like done by Netflix or Youtube.
  Negative
Here you can find a pretty good tutorial on that: http://www.bitcodin.com/blog/2015/02/create-mpeg-dash-hls-content-for-amazon-s3-and-cloudfront/
563e0f192d1761a701f0f552	X	I realize this question has been asked here, however I'm still unable to get this to work.
  Negative
This works (forwarding) as expected.
  Positive
Now I'm trying to create the record in #3 using the SDK.
  Negative
This does create the record, however it is not forwarding.
  Positive
I notice when I examine the record in the web control panel, the Alias Hosted Zone ID is not appearing.
  Negative
Upon further examination I noticed that the Alias Target name has a .
  Negative
at end.
  Neutral
s3-website-us-east-1.
  Neutral
amazonaws.com.
  Neutral
If I manually remove that extra "."
  Neutral
the Hosted Zone ID appears.
  Negative
I have no idea how that extra dot is getting there (the code I'm using below doesnt have this).
  Negative
563e0f192d1761a701f0f553	X	What about the TripsCpnstants and How to get return server url to view the image.
  Negative
Please help
563e0f1a2d1761a701f0f554	X	Edited have you got it, its just the expiry date of your image
563e0f1a2d1761a701f0f555	X	No i didn't get any edits.
  Negative
What if i dont want any expiry date....is it mendatory?
  Neutral
563e0f1a2d1761a701f0f556	X	If you dont set the expiry date it will affect you when you reads the image.
  Positive
Better to set expiry date but not mandatory.
  Positive
563e0f1a2d1761a701f0f557	X	Ok fine.
  Positive
I will set it but can you please tell me how to receive server path where image is uploaded by this code.
  Positive
563e0f1a2d1761a701f0f558	X	I am trying to implement Amazon S3 SDK to upload image from Android application but always getting this 400 error, malformed xml bad request.
  Very negative
I've taken source code from this reference link I've correct access_key, secret_key and bucket_key.
  Negative
There is no error related to this.
  Negative
If anybody have a working sample in order to access this S3 API, please share.
  Positive
I am not able to find SDK error and stuck with the same.
  Negative
563e0f1a2d1761a701f0f559	X	I think this would help you to upload image EDIT Use this for generating URL
563e0f1a2d1761a701f0f55a	X	Is there a way to programmatically access an Amazon S3 account's usage data?
  Negative
I'm building an application that will charge end-users for their use of my Amazon S3 bucket.
  Negative
Because I'll be the middleman between AWS and the end user for PUT/DELETE operations, I'll be able to keep track of uploads and storage usage, but I'm allowing users to directly access their files with public access links so I won't be able to directly monitor downstream usage.
  Negative
As such, my plan is to check download usage regularly.
  Negative
Is there anywhere in the AWS API where I can access usage statistics?
  Negative
563e0f1a2d1761a701f0f55b	X	There is no way to get usage statistics with the API.
  Negative
This issue is actively discussed on the AWS forum over the years with no feedback from the AWS team.
  Negative
https://forums.aws.amazon.com/thread.jspa?messageID=277024 The alternatives would be to turn on Amazon S3 server log and to analyze it yourself.
  Negative
Another option would be to take advantage of Amazon DevPay service.
  Negative
Thanks Andy EDIT: Here's the official Amazon S3 documentation related to Amazon S3 access logs: http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?ServerLogs.html
563e0f1a2d1761a701f0f55c	X	Since December 2011, if you have premium support you can monitor your estimated charges (either aggregated or per-server) using CLoudWatch.
  Very negative
There's no official documentation right now but I wrote up an overview: http://blog.bitnami.org/2011/12/monitor-your-estimated-aws-charges-with.html
563e0f1a2d1761a701f0f55d	X	Has anyone had any experience using Amazon S3 file uploads on Xamarin?
  Negative
It should be simple, but I am having trouble getting it to work.
  Negative
I'm trying to use https://github.com/xamarin/amazon to do my upload of a file like so: But I'm getting this exception: System.ObjectDisposedException: The object was used after being disposed.
  Negative
at System.Net.WebConnection.BeginWrite (System.Net.HttpWebRequest request, System.Byte[] buffer, Int32 offset, Int32 size, System.AsyncCallback cb, System.Object state) [0x0001f] in /Developer/MonoTouch/Source/mono/mcs/class/System/System.
  Negative
Net/WebConnection.
  Neutral
cs:1033 at System.Net.WebConnectionStream.BeginWrite (System.Byte[] buffer, Int32 offset, Int32 size, System.AsyncCallback cb, System.Object state) [0x0026c] in /Developer/MonoTouch/Source/mono/mcs/class/System/System.
  Negative
Net/WebConnectionStream.
  Neutral
cs:541 The github repo hasn't been updated in a year so maybe it's just broken?
  Negative
All I want to do is PUT and DELETE files so my next step is to just hit the REST API with RestSharp rather than use a wrapper but surely this is something others have done, can anyone shed some light?
  Negative
563e0f1a2d1761a701f0f55e	X	One alternative which may be of interest, is to create a new AWS account (thus benefiting from the free tier) and add it alongside an existing account, via consolidated billing.
  Very positive
Then all stats/logs etc are tied solely to that account.
  Neutral
Naturally if you already have a large setup and whatnot then this may not be simple - thought I would put it out there anyway :)
563e0f1a2d1761a701f0f55f	X	That was my initial thought but i'm going to have dozens of clients to handle simultaneously.
  Negative
So this solution is not the right one for my specific issue.Thanks for the help though :)
563e0f1a2d1761a701f0f560	X	Thanks, i'll try that, this seems to be the right way to go !
  Negative
563e0f1a2d1761a701f0f561	X	I'd like to know if there's a way for me to have bucket-level stats in amazon s3.
  Negative
Basically i want to charge customers for storage and GET requests on my system (which is hosted on s3).
  Negative
So i created a specific bucket for each client, but i can't seem to get the stats just for a specific bucket.
  Neutral
I see the API lets me or But i just can't find how to get the number of requests issued to said bucket and the total size of the bucket.
  Negative
Thanks for help !
  Positive
Regards
563e0f1a2d1761a701f0f562	X	I don't think that what you are trying to achieve is possible using Amazon API.
  Negative
The GET Bucket request does not contain usage statistics (requests, etc) other than the timestamp of the latest modification (LastModified).
  Negative
My suggestion would be that you enable logging in your buckets and perform the analysis that you want from there.
  Neutral
S3 starting page gives you an overview on it: Amazon S3 also supports logging of requests made against your Amazon S3 resources.
  Neutral
You can configure your Amazon S3 bucket to create access log records for the requests made against it.
  Negative
These server access logs capture all requests made against a bucket or the objects in it and can be used for auditing purposes.
  Negative
And I am sure there is plenty of documentation on that matter.
  Negative
HTH.
  Neutral
563e0f1b2d1761a701f0f563	X	Thanks for instant reply but I'm using paperclip and want to generate multi sized images, In case If I saved file localy then I've to make local file for each size of image and then upload it on amazon which will become headache when I've to maintain url of amazon not changes and upload image to same url.
  Very negative
563e0f1b2d1761a701f0f564	X	I'm using Paperclip, ImagMagick, Rmagick, Amazon-s3 I'm getting this error when getting an image from url and after custom resizing image replacing the changed image to amazon.
  Negative
Magick::ImageMagickError (no encode delegate for this image format //s3.amazonaws.com/beu-dev/temp_images/final_images/000/000/377/original/template_37720121205-5921-99989h.
  Negative
png' @ error/constitute.
  Neutral
c/WriteImage/1153): app/models/temp_image.rb:38:inwrite' line#38 is last line before end of this method Note: One more thing This code works perfectly when using system file system, but when started using amazon s3 Error happening
563e0f1b2d1761a701f0f565	X	I think you need to use the local file name instead of URL.
  Negative
Imagemagick can't just write the file to URL via http.
  Negative
To replace the source file you need to use the Amazon S3 API.
  Negative
563e0f1b2d1761a701f0f566	X	Issue solved.
  Negative
1.
  Neutral
Check "identify -list format" If you don't see the .
  Negative
jpeg format in the list.
  Neutral
Then you need to add the .
  Neutral
jpeg library.
  Neutral
curl -O http://www.ijg.org/files/jpegsrc.v8c.tar.gz Sudo make install Now reinstall imagemagick then everything works fine now.
  Positive
563e0f1b2d1761a701f0f567	X	Hi!
  Positive
Welcome to StackOverflow!
  Neutral
StackOverflow is for programming questions, and this is not a question.
  Negative
If your question is "how do I do this?"
  Neutral
, please explain what you have already tried.
  Neutral
563e0f1b2d1761a701f0f568	X	Ok.
  Neutral
So I got response after successfully uploaded the files into amazon s3.
  Negative
In that response having location,date,success code etc..
  Positive
So What I need to pass to get the file with get request.
  Negative
563e0f1b2d1761a701f0f569	X	Sounds good @Michael but using api means sdk right
563e0f1b2d1761a701f0f56a	X	Yes, as far as I know you can't set up an expiring url using the AWS Console, so you have to use the api in some fashion.
  Positive
That could be via the command line tools though, if that helps.
  Negative
563e0f1b2d1761a701f0f56b	X	My requirement is download a files from Amazon s3 with out using Aws sdk for java and using rest api calls.
  Negative
563e0f1b2d1761a701f0f56c	X	If you make the file publicly accessible, you can use a simple url to download it.
  Negative
If you need it to be private you will need to use an api or a tool which handles the authorization for you.
  Negative
You can make the file public as part of the upload api call or after the fact using the AWS Console.
  Neutral
You can also create expiring urls that are only usable for a set period of time using the api.
  Negative
563e0f1b2d1761a701f0f56d	X	I further clarified this in my own answer.
  Negative
563e0f1b2d1761a701f0f56e	X	The question is how do you avoid distributing your secret key with your app when you provide S3 space for users?
  Negative
ASIHTTPRequest expects a secret key.
  Negative
563e0f1c2d1761a701f0f56f	X	let's start off with the problem statement: My iOS application has a login form.
  Negative
When the user logs in, a call is made to my API and access granted or denied.
  Negative
If access was granted, I want the user to be able to upload pictures to his account and/or manage them.
  Positive
As storage I've picked Amazon S3, and I figured it'd be a good idea to have one bucket called "myappphotos" for instance, which contains lots of folders.
  Negative
The folder names are hashes of a user's email and a secret key.
  Negative
So, every user has his own, unique folder in my Amazon S3 bucket.
  Negative
Since I've just recently started working with AWS, here's my question: What are the best practices for setting up a system like this?
  Negative
I want the user to be able to upload pictures directly to Amazon S3, but of course I cannot hard-code the access key.
  Negative
So I need my API to somehow talk to Amazon and request an access token of sorts - only for the particular folder that belongs to the user I'm making the request for.
  Negative
Can anyone help me out and/or guide me to some sources where a similar problem was addressed?
  Negative
Don't think I'm the first one and the amazon documentation is so extensive that I don't really know where to start looking.
  Negative
Thanks a lot!
  Positive
563e0f1c2d1761a701f0f570	X	Have you looked at the Amazon AWS SDK for iOS?
  Negative
From the docs: The AWSiOSDemoTVM and AWSiOSDemoTVMIdentity samples demonstrate a more secure mechanism for transferring AWS security credentials to a mobile client.
  Negative
These samples require a server application, in this case the token vending machine (TVM), which is provided as a separate download.
  Negative
The sample applications register with TVM, either anonymously or with a user-supplied user name and password.
  Negative
The TVM uses the AWS Security Token Service to get temporary security credentials and pass them to the mobile application.
  Negative
The TVM is available in two forms, one that supports anonymous registration and one that requires a user name and password to register a device and receive security tokens.
  Negative
To download and install the TVM for Anonymous Registration, go to http://aws.amazon.com/code/8872061742402990.
  Negative
To download and install the TVM for Identity Registration, go to http://aws.amazon.com/code/7351543942956566.
  Negative
From Authenticating Users of AWS Mobile Applications with a Token Vending Machine: This article discusses an architecture that enables applications running on a mobile device to more securely interact with Amazon Web Services such as Amazon Simple Storage Service (S3), Amazon SimpleDB, Amazon Simple Notification Service (SNS), and Amazon Simple Queue Service (SQS).
  Negative
The architecture discussed uses a "Token Vending Machine" to distribute temporary security credentials to the mobile application.
  Negative
Your token can limit access to a specific bucket on S3, so it appears to be the best option.
  Negative
563e0f1c2d1761a701f0f571	X	ASIHTTPRequest has direct support for Amazon S3.
  Negative
http://allseeing-i.com/ASIHTTPRequest/S3
563e0f1c2d1761a701f0f572	X	To further clarify Terry Wilcox's answer... You need to generate temporary security credentials on your server using AWS STS.
  Negative
STS is AWS' "Security Token Service".
  Negative
It allows you to create access keys programmatically and set specific permissions and expiration dates.
  Positive
Since you already have an API/backend for your app that authenticates your users, you can make an API call that will generate temporary AWS credentials that only have access to that user's folder.
  Negative
If you do not have a backend for your app, Amazon provides a Java app call TVM (Token Vending Machine) that you can easily deploy your own instance of to Elastic Beanstalk.
  Negative
Relevant AWS articles: http://aws.amazon.com/articles/4611615499399490 http://docs.aws.amazon.com/STS/latest/UsingSTS/STSUseCases.html#MobileApplication
563e0f1c2d1761a701f0f573	X	You can restrict the user access to folder level.
  Negative
Refer this sample Credential Management.
  Neutral
563e0f1c2d1761a701f0f574	X	Start by getting rid of the @, so you'll see any errors being displayed
563e0f1c2d1761a701f0f575	X	But readfile() is probably a better function to use if all you're doing with the file is spooling it direct to the browser
563e0f1c2d1761a701f0f576	X	yes, I know.
  Very negative
However, I will have to write the buffer to somewhere else as well.
  Neutral
that's why I am using reading it manually rather than using 'readfile'
563e0f1c2d1761a701f0f577	X	Just to check, I tried with readfile and interestingly, it is not working either.
  Negative
However, all three ways works just fine if I give a path for local version of the file.
  Neutral
But when its trying to retrieving from s3, only 'file_get_contents' is working, other two ways isn't.
  Negative
Just wondering, is this anything to do with the fact that, I am trying from my local system.
  Negative
It might work if I try on a ec2 server instance?
  Neutral
Do you think so?
  Neutral
563e0f1c2d1761a701f0f578	X	If you're seeing the binary data with the headers removed, then it suggests that the problem is with your headers, not with the file access from S3
563e0f1c2d1761a701f0f579	X	This article on the AWS PHP Development Blog might be helpful as well: Streaming Amazon S3 Objects From a Web Server
563e0f1c2d1761a701f0f57a	X	I am using amazon s3 API and setting the client to read as stream.
  Negative
It is working fine for me to use file_get_contents("s3://{bucket}/{key}"), which read the full data for the file(I am using video file & testing on my local system).
  Negative
However, I am trying to optimize the memory used by the script and thus trying to read and return data by chunk as below: This is not working on my local system.
  Negative
I am just wondering what might be the issue using this technique.
  Negative
by searching, I found that this is also a very widely used technique.
  Positive
So, if anybody can please give any suggestion about what might be wrong here or any other approach, I should try with, it will be very helpful.
  Negative
Thanks.
  Neutral
563e0f1c2d1761a701f0f57b	X	OK, finally got the solution.
  Positive
Somehow, some other output are being added to buffer.
  Negative
I had to put: In this way to clean anything if were added.
  Neutral
Now its working fine.
  Positive
Thanks Mark Baker for your valuable support through the debugging process.
  Neutral
563e0f1d2d1761a701f0f57c	X	It's probably not a good idea unless you either want to make all those files available for deletion by anyone, or have your private secret key in your public flash file (easily findable with a decompiler or even a hex editor).
  Negative
563e0f1d2d1761a701f0f57d	X	In addition to what @JonatanHedborg said, you may also encounter problems when including Authorization in your header.
  Negative
More info here: helpx.adobe.com/flash-player/kb/… The best approach is probably to communicate with an intermediate server-side service.
  Positive
563e0f1d2d1761a701f0f57e	X	according to Amazon S3 REST API deleting an object requires a DELETE request, like But the URLRequest.method field in Flash can be set only to GET or POST, so I cannot create such a request.
  Negative
Any idea?
  Neutral
563e0f1d2d1761a701f0f57f	X	Could it be possible to make a POST request to your server and then make a DELETE request from let's say php to amazon?
  Negative
563e0f1d2d1761a701f0f580	X	Check out the as3httpclient lib, it makes DELETE requests available
563e0f1d2d1761a701f0f581	X	Thanks Jamie, that did the trick, bounty coming in 16 hours
563e0f1d2d1761a701f0f582	X	Thanks for the lead Rocky, but I was unable to find a way to store the file as rrs.
  Very negative
563e0f1d2d1761a701f0f583	X	The zend framework provides a php wrapper for the amazon S3 api that simplifies the lower level REST functionality.
  Negative
http://framework.zend.com/manual/en/zend.service.amazon.s3.html For example to store a file in s3 all you need to do is By default, objects are stored in buckets as regular storage.
  Negative
Is there any functionality in the zend framework that will allow me specify objects to be stored as Reduced Redundancy Storage (RRS) in S3?
  Negative
If not is there any way I can set the default storage of all objects in a bucket as RRS?
  Negative
563e0f1d2d1761a701f0f584	X	http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectPOST.html Have you tried the following after RockyFords advice?
  Negative
The code below should upload it in the correct mode.
  Neutral
563e0f1d2d1761a701f0f585	X	It looks like you can pass a third argument to Zend_Services_S3::putFile and putObject that is $meta and it accepts a scalar or an array.
  Negative
and S3 just wants a header change to set an object or file as RRS: Q: How do I specify that I want to store my data using RRS?
  Negative
All objects in Amazon S3 have a storage class setting.
  Negative
The default setting is STANDARD.
  Neutral
You can use an optional header on a PUT request to specify the setting REDUCED_REDUNDANCY.
  Negative
you may have to dig in the API a bit to find exactly what to pass, but this should point you in the right direction.
  Positive
563e0f1e2d1761a701f0f586	X	Are the files already on S3?
  Negative
If not, can't you concatenate (or zip) before uploading?
  Negative
563e0f1e2d1761a701f0f587	X	Is there a way to concatenate small files which are less than 5MBs on Amazon S3.
  Negative
Multi-Part Upload is not ok because of small files.
  Negative
It's not a efficient solution to pull down all these files and do the concatenation.
  Negative
So, can anybody tell me some APIs to do these?
  Negative
563e0f1e2d1761a701f0f588	X	Amazon S3 does not provide a concatenate function.
  Negative
It is primarily an object storage service.
  Positive
You will need some process that downloads the objects, combines them, then uploads them again.
  Negative
The most efficient way to do this would be to download the objects in parallel, to take full advantage of available bandwidth.
  Positive
However, that is more complex to code.
  Neutral
I would recommend doing the processing on "in the cloud" to avoid having to download the objects across the Internet.
  Negative
Doing it on Amazon EC2 or AWS Lambda would be more efficient and less costly.
  Negative
563e0f1e2d1761a701f0f589	X	Thank you, sound good!
  Positive
563e0f1e2d1761a701f0f58a	X	I am developing an API using Codeigniter and I want to let users upload images to my Amazon S3 account.
  Negative
I am using Phils Restserver and Donovan Schönknecht S3 library (for Ci).
  Negative
It works perfectly to upload a local file to Amazon but how can I get the image file sent via normal external form?
  Positive
Using the built in Ci upload library it works fine but then I have to store the files locally on my own server and I want them on S3.
  Negative
Can the two be combined?
  Neutral
I guess what I am asking is how can I "get" the image file that is sent to the controller and resize it and then upload it to S3?
  Negative
Do I perhaps need to temporary save it on the local server, upload it to S3 and then remove it from the local server?
  Negative
This is my "upload" modal: Thankful for all input!
  Positive
563e0f1e2d1761a701f0f58b	X	If I understand you correctly, you want a user to upload an image via a form, resize that image, then transfer that to Amazon S3.
  Positive
You'll have to store the file locally (at least for a few seconds) to resize it with CI.
  Negative
After you resize it, then you can transfer it to Amazon S3.
  Negative
In your success callback from the transfer, you can delete the image from your server.
  Positive
563e0f1e2d1761a701f0f58c	X	You should definitely check out the CI S3 library.
  Negative
The "spark" is available here - http://getsparks.org/packages/amazon-s3/versions/HEAD/show
563e0f1e2d1761a701f0f58d	X	This does not provide an answer to the question.
  Negative
Please expand on what exactly solved the problem.
  Neutral
563e0f1e2d1761a701f0f58e	X	This does not provide an answer to the question.
  Negative
To critique or request clarification from an author, leave a comment below their post - you can always comment on your own posts, and once you have sufficient reputation you will be able to comment on any post.
  Negative
563e0f1e2d1761a701f0f58f	X	I have an issue with "submitted-image-url" option when post a share using LinkedIn API.
  Neutral
All my images stored on Amazon S3.
  Neutral
For example "https://s3.amazonaws.com/news-img/client_619/619_1424690228983-DarthVaderSEOToaster2.jpg".
  Neutral
When I try to use different source for image, from whatever another website, it works well.
  Positive
Could you please help me?
  Positive
Why images from Amazon S3 cannot be fetched by LinkedIn?
  Negative
Do I need add some exceptions in my S3 bucket?
  Neutral
One more, with other social networks like Twitter and Facebook everything works fine.
  Positive
Thank you, Oleg
563e0f1e2d1761a701f0f590	X	Tricky http headers... Need to set correct 'content-type' header when upload images to S3 bucket.
  Negative
E.g. 'Content-Type': image/' + imageExtension.
  Negative
Solved!
  Neutral
563e0f1f2d1761a701f0f591	X	I am using high level api for amazon s3 upload and getting a call back for progress but the method upload.getprogress().
  Negative
getPercentageTransferred() at times gives percentage above 100.
  Positive
.
  Neutral
Why does it shows above 100?
  Neutral
Are there any changes needed
563e0f1f2d1761a701f0f592	X	haha, thanks for the reply, I could be mistaken...but I'm so confused that I'm just not sure... $.
  Very negative
getJSON("api.github.com/users/githubuser/events") Works fine on a test server on say my desktop (or even from my other websites if I make it through the Chrome developer console) but when I try to make the same call after I've uploaded that website to Amazon S3 then it does not work because I get the XMLHttpRequests error.
  Very negative
Logically this suggests to me that it has something to do with my configuration on the Amazon S3 bucket since it did work in a test environment and certain websites?
  Neutral
563e0f1f2d1761a701f0f593	X	I'm hosting a static website via Amazon S3 and I have some Javascript on there that will make a call to another domain and parse the XML, JSON, or whatever that it gets.
  Negative
I followed the many posts on stackoverflow and various blog posts it linked to that claimed to get it working but even after following very closely I could never replicate the results.
  Negative
I even tried adding with and without the following to the rule, The following link allows you to test if CORS is enabled by sending XMLHttpRequests and it says it is not valid so CORS is not set up or recognized properly.
  Very negative
http://client.cors-api.appspot.com/client/ A possible lead is what is suggested in Amazon S3 documentation here, http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETcors.html that says we need to set the "s3:GetCORSConfiguration" permission, which I did via a line like... "Action": ["s3:GetObject", "s3:GetCORSConfiguration"], in the "edit bucket policy" section from the AWS control panel but it gives an error and cannot save because it doesn't recognize this action?
  Negative
A potentially similar post on stackexchange here, HTTP GET to amazon aws from jquery or XMLHttpRequest fails with Origin is not allowed by Access-Control-Allow-Origin seems to suggest that if I have a website hosted on S3 that it can not configure it to make XMLHttpRequests that are GET to a 3rd party resource?
  Very negative
I feel like I'm going in circles...anyone out there have any leads/advice?
  Negative
Thanks.
  Neutral
563e0f1f2d1761a701f0f594	X	I think you are confused or I misread your question.
  Negative
You enable CORS on your site so other sites can make requests to your page.
  Negative
Enabling CORS on your S3 site will allow example.com to talk to your S3 page.
  Negative
It does not allow your site to talk to example.com.
  Negative
In order for you to make requests to other domains, they have to enable the privileges.
  Negative
You can not magically turn it on for their domains.
  Negative
It is like saying that you give permission to yourself to walk into the whitehouse and use the president's bathroom.
  Negative
When you hop the fence, the secret service will deny that request with force.
  Positive
563e0f1f2d1761a701f0f595	X	There might be some relation here to the example we just worked through on our apps which was related to the https which was a hardcoded configuration on some of our clients machines.
  Negative
I don't know exactly what that means but it might be a place to check.
  Negative
instead of http:// try https:// XMLHttpRequest to Amazon S3 fails only on some computers
563e0f1f2d1761a701f0f596	X	but it failed, Its just that too many bugs will occur due to file naming, server timeout issues, etc.... This isn't too helpful, as it is unclear what you want your end result to be.
  Very negative
Do you want each video file to be a separate S3 object in the same bucket?
  Neutral
Do you want a zipped file of your whole 1 TB directory?
  Negative
Do you want the buckets to mirror your directory structure?
  Negative
Please clarify what your desired output is.
  Neutral
563e0f1f2d1761a701f0f597	X	You want to try something like s3cmd to handle the upload.
  Negative
563e0f1f2d1761a701f0f598	X	I know how to upload one single file to Amazon S3, I use this: I have a large 1TB directory of videos I want to upload them on Amazon S3.
  Very negative
I tried to loop recursively through each directory and upload each file alone, but it failed.
  Negative
Its just that too many bugs will occur due to file naming, server timeout issues, etc..
  Negative
.
  Neutral
I want the bucket to mirror my exact directory structure.
  Negative
The folder I want to copy is held on a dedicated server serving Apache.
  Positive
Is there a way I could just upload the folder through the API?
  Negative
It is also 1TB, so what's the best solution?
  Negative
563e0f1f2d1761a701f0f599	X	Even better, use the official SDKs or CLI tools.
  Negative
Also, if you're using PHP in 2014, but not using Composer, you're doing it wrong.
  Negative
563e0f1f2d1761a701f0f59a	X	Does the virus scanner on your machine check outgoing files?
  Negative
563e0f202d1761a701f0f59b	X	just for google: one of the best command line free anti-viruses is clamAV multi platform so you can work with any OS, and use Any programming lang that can execute system applications, php, java, c# most high level programming langs
563e0f202d1761a701f0f59c	X	How can I check files before uploading to Amazon S3,any tips?
  Negative
Updated: It need for my web-site when users upload a file i want to check whether it's not infected.
  Negative
Is there any API which allows for check files programmatically ?
  Neutral
Thanks in advance!!!
  Neutral
563e0f202d1761a701f0f59d	X	I think the best solution is @Joe's answer to a similar question for c#: I would probably just make a system call to run an independent process to do the scan.
  Negative
There are a number of command-line AV engines out there from various vendors.
  Negative
563e0f202d1761a701f0f59e	X	If you mean manually: try the online virusscanner at http://www.virustotal.com/ - It checks against a lot of antivirus programs for you.
  Negative
563e0f202d1761a701f0f59f	X	Our webserver can serve but we need some more complexity in php code, image uploaded status will be kept in db.
  Negative
It seems the only solution for the time being, though.
  Negative
563e0f202d1761a701f0f5a0	X	Well, how about you process the image on your server, add it to the queue database, display it to the user and then run a background task uploading it to S3?
  Negative
563e0f202d1761a701f0f5a1	X	Yes, a field in images table should be added to check if it's uploaded yet and set its address accordingly.
  Negative
563e0f202d1761a701f0f5a2	X	can anyone explain this part of the process please "add it to the queue database"
563e0f202d1761a701f0f5a3	X	I am looking for a best practice while uploading images to amazon s3 server and serving from there.
  Negative
We need four different sizes of an image.
  Negative
So just after image upload we convert the image and scale in 4 different widths and heights.
  Positive
And then we send them to the amazon s3 using official php api.
  Negative
But for a 1M image the client sometimes wait up to 30 seconds which is a very long time.
  Negative
Instead of sending images immediately to S3, it may be better to add them to a job queue.
  Negative
But the user should see the uploaded image immediately.
  Negative
563e0f202d1761a701f0f5a4	X	You could simply achieve that by queuing the files, in for example: a database, and then running a cron job, or having a constantly running php script.
  Negative
If you say you want the users to see the images instantly, should they see them instantly on S3?
  Negative
563e0f202d1761a701f0f5a5	X	Yeah, ideally you'd sign the request just as you're sending them instead of all-at-once at the beginning.
  Negative
563e0f212d1761a701f0f5a6	X	@RyanParman yes, that's right, but my library is in JS and I need to generate signatures server-side, and I'd rather make less requests to the server.
  Negative
563e0f212d1761a701f0f5a7	X	Ah, gotcha.
  Neutral
Yeah, that makes a difference.
  Positive
:)
563e0f212d1761a701f0f5a8	X	check out my edit
563e0f212d1761a701f0f5a9	X	I'm trying to upload a large file (1.5GB) to Amazon S3 by using the REST Api and HTML5 file slicing.
  Negative
Here's how the upload code looks like (code stripped down for readability): chunk_size is 6MB.
  Negative
After a chunk finishes uploading, the next one follows, and so on.
  Negative
But sometimes (every 80 chunks or so), the PUT request fails, with e.type == "error", e.target.status == 0 (which surprises me), and e.target.responseText == "".
  Negative
After a chunk fails, the code re-attempts to upload it, and gets the exact same error.
  Very negative
When I refresh the page and continue the upload (the same chunk!)
  Positive
, it works like a charm (for 80 chunks or so, when it gets stuck again).
  Positive
Here's how the request looks in chrome dev tools:  Any ideas why this might happen, or how to debug something like this?
  Negative
EDIT: Here is the OPTIONS response: 
563e0f212d1761a701f0f5aa	X	I finally found the issue by sniffing packets: there are two issues: for PUT requests that get a 4xx (didn't test for other non-2xx responses), the xhr request returns as aborted (status = 0); still haven't found an explanation for that, check out Why does a PUT 403 show up as Aborted?
  Negative
Amazon S3 responded with a 403 that said RequestTimeTooSkewed, because my signatures are generated when the upload starts, and after 15 minutes (the timeout that triggers the RequestTimeTooSkewed error), it fails, and the signatures have to be regenerated.
  Very negative
That 403 error is never seen in the dev tools console or by the js code, because of the first problem.
  Negative
.
  Neutral
After regenerating the signatures, everything works like a charm.
  Positive
563e0f212d1761a701f0f5ab	X	Did you verify whether browser is making any 'OPTIONS' request.
  Negative
If yes what is the response headers.
  Neutral
563e0f212d1761a701f0f5ac	X	Did you ever make any progress on this?
  Neutral
I am working on a similar issue.
  Neutral
563e0f212d1761a701f0f5ad	X	I'm developing a web application wrapped in Phonegap to upload recorded videos to Amazon S3.
  Negative
I've done some research, and have concluded there are two ways to do this: use the Amazon AWS JavaScript SDK, or using the Phonegap FileTransfer API (along with the Amazon S3 REST API).
  Negative
When you capture a video in a PhoneGap application, it's only possible to request the URI to the video file (not the file contents itself).
  Negative
You can use this URI in a FileTransfer to post to S3 like so: (From http://coenraets.org/blog/2013/09/how-to-upload-pictures-from-a-phonegap-app-to-amazon-s3/) When you use the AWS SDK, you can upload files like so: (From http://www.cheynewallace.com/uploading-to-s3-with-angularjs/) I have two questions:
563e0f212d1761a701f0f5ae	X	In my web application amazon s3 is using image uploading following code is used for uploading images.
  Very negative
But documents are not uploading using this earlier I was using same code for uploading both.
  Negative
Now I changed the API parameter SourceFile to Body then document is working fine but image is not in other case images working fine.
  Neutral
I gave Body for upload documents and SourceFile for upload images.
  Negative
How can I use same api for both?
  Neutral
563e0f212d1761a701f0f5af	X	May be specifying is creating problem for image.
  Negative
Also for image you will need to provide SourceFile.
  Neutral
Please check aws php sdk doc
563e0f212d1761a701f0f5b0	X	The Body parameter represents the actual content of the file you are uploading.
  Negative
This can be a string of data or a resource created with fopen().
  Neutral
The SourceFile parameter allows you to specify the path on disk to a file, and the SDK will take care of opening the file to retrieve and send its contents.
  Negative
Which one you choose to use has nothing to do with the type of file it is.
  Negative
For ContentType, the SDK will attempt to guess the content-type of the data/file you provide, and you only need to specify it, if the content-type cannot be determined.
  Negative
563e0f222d1761a701f0f5b1	X	This worked like a charm!
  Positive
Thank you SO much for your clear and straightforward answer!
  Positive
You're a lifesaver!
  Neutral
563e0f222d1761a701f0f5b2	X	I am trying to upload an image file from my Trigger.io mobile app directly to Amazon S3 (see here: http://aws.amazon.com/articles/1434).
  Negative
I'm able to do this on the web without any problems using jQuery and the FormData API as follows: However, I'm unable to get this working with the Forge request API.
  Negative
This is what I have tried: But, I receive the following error from Amazon: I would circumvent this forge.request entirely in favor of $.
  Negative
ajax, but my file was retrieved using the Forge File API and just shows up on S3 as [object Object] (I assume because it's a Forge file, not a true file object from an HTML <input />).
  Negative
So, how can I upload a file in Trigger.io to Amazon S3 using FormData and the Forge API?
  Negative
Any help is greatly appreciated!
  Positive
Thanks!
  Positive
563e0f222d1761a701f0f5b3	X	You are able to use the jQuery ajax $.
  Positive
ajax function and the FormData JavaScript API to upload an image file from your Trigger.io mobile app directly to Amazon S3 as you have done on the web.
  Negative
You will need to perform the following steps: As an example, once you have retrieved your image file (Step 1), you could use the following uploadImage function to upload your image to Amazon S3:
563e0f222d1761a701f0f5b4	X	A Bucket policy would do the trick: stackoverflow.com/questions/13169108/…
563e0f222d1761a701f0f5b5	X	I'd like to investigate possibility to filter uploads on Amazon S3 via filetype (extension).
  Negative
For example, I'd like to provide access to upload only *.
  Negative
jpg and *.
  Neutral
png files into specific bucket.
  Positive
Is it possible to restrict via standard S3 web-interface or via some API calls (I'll write my own little script for that).
  Negative
563e0f222d1761a701f0f5b6	X	When posting to s3 i expect a response from them but that never happens.
  Negative
The images post, and i get a request from my api, but i do not get one from S3.
  Negative
I get a 204 No Content error, failure to load response.
  Negative
but if i scroll down it actually shows the url in the location part also the images actually get posted to s3.
  Negative
the code looks like this: form.html main.js Amazon controller Amazon model
563e0f222d1761a701f0f5b7	X	dup?
  Negative
stackoverflow.com/questions/16485629/…
563e0f222d1761a701f0f5b8	X	also refer this netpalantir.it/news/index/…
563e0f222d1761a701f0f5b9	X	I'm writing an app that allows users to browse and upload large files to Amazon S3 via a web application.
  Negative
I'm using the Amazon AWS library for .
  Negative
Net.
  Neutral
However because the files are large I want to provide a 'Cancel' option on the website.
  Negative
I can't see anything in the API as to how to cancel a file that's uploading using S3.
  Negative
Does anyone know how to cancel file uploads?
  Neutral
Thanks.
  Neutral
563e0f232d1761a701f0f5ba	X	You can do it just by using Thread.Abort()
563e0f232d1761a701f0f5bb	X	I don't see in the second code example where the union between old and new metadata is taking place.
  Negative
When I execute the code, the metadata for the object gets overwritten with the latest metadata.
  Negative
563e0f232d1761a701f0f5bc	X	Seems to work, but the new key loses the Content-Type, which is really bad when using S3 to serve web images.
  Very negative
563e0f232d1761a701f0f5bd	X	For those of you using the AWS SDK you need to use AmazonS3Client.copyObject method
563e0f232d1761a701f0f5be	X	Does it copying the content of the key or only the metadata?
  Negative
Copying the whole key takes time, and decrease the efficiency, right?
  Positive
563e0f232d1761a701f0f5bf	X	The question asked for Python, but this was the only answer using the Java SDK that I've been able to find on SO.
  Negative
Thanks!
  Positive
563e0f232d1761a701f0f5c0	X	Been a while since I've used it, but I'm pretty sure that's among the things passing the original k.metadata field on the copy accomplishes.
  Negative
563e0f232d1761a701f0f5c1	X	If you have already uploaded an object to an Amazon S3 bucket, how do you change the metadata using the API?
  Negative
It is possible to do this in the AWS Management Console, but it is not clear how it could be done programmatically.
  Negative
Specifically, I'm using the boto API in Python and from reading the source it is clear that using key.set_metadata only works before the object is created as it just effects a local dictionary.
  Negative
563e0f232d1761a701f0f5c2	X	It appears you need to overwrite the object with itself, using a "PUT Object (Copy)" with an x-amz-metadata-directive: REPLACE header in addition to the metadata.
  Negative
In boto, this can be done like this: Note that any metadata you do not include in the old dictionary will be dropped.
  Negative
So to preserve old attributes you'll need to do something like: I almost missed this solution, which is hinted at in the intro to an incorrectly-titled question that's actually about a different problem than this question: Change Content-Disposition of existing S3 object
563e0f232d1761a701f0f5c3	X	In order to set metadata on S3 files,just don't provide target location as only source information is enough to set metadata.
  Very negative
563e0f242d1761a701f0f5c4	X	You can change the metadata without re-uloading the object by using the copy command.
  Negative
See this question: Is it possible to change headers on an S3 object without downloading the entire object
563e0f242d1761a701f0f5c5	X	For the first answer it's a good idea to include the original content type in the metadata, for example:
563e0f242d1761a701f0f5c6	X	If you want your metadata stored remotely use set_remote_metadata Example: key.set_remote_metadata({'to_be': 'added'}, ['key', 'to', 'delete'], {True/False}) Implementation is here: https://github.com/boto/boto/blob/66b360449812d857b4ec6a9834a752825e1e7603/boto/s3/key.py#L1875
563e0f242d1761a701f0f5c7	X	In Java, You can copy object to the same location.
  Negative
Here metadata will not copy while copying an Object.
  Negative
You have to get metadata of original and set to copy request.
  Negative
This method is more recommended to insert or update metadata of an Amazon S3 object
563e0f242d1761a701f0f5c8	X	here is the code that worked for me.
  Neutral
I'm using aws-java-sdk-s3 version 1.10.15
563e0f242d1761a701f0f5c9	X	I receive the binary file on Node.js, which means that I don't have an image path like I'd have if I had my image in a directory on my computer.
  Negative
Does this mean that I can't use the fs library because that deals with filesystem?
  Neutral
563e0f242d1761a701f0f5ca	X	Are you using express or connect (which both use formidable to parse file uploads) to handle 'localhost:3031/upload/image';?
  Negative
563e0f242d1761a701f0f5cb	X	I'm using express to handle 'localhost:3031/upload/image'.
  Negative
I can then access the uploaded file by looking at the req.body (request's body).
  Negative
Of course, it's a jumble.
  Neutral
Not sure how to turn that into a Buffer that Amazon S3's pushObject wants.
  Negative
563e0f242d1761a701f0f5cc	X	so then you don't need to convert it to a Buffer at all, you just use fs.stat(req.body) to get the file size and then fs.createReadStream(req.body) to create a stream to then pass that to S3 as the Body param
563e0f242d1761a701f0f5cd	X	fs.stat(req.body) complains that it needs a string (req.body is binary data, which is not a string).
  Negative
If I convert the binary .
  Neutral
jpg to a string, I won't be able to access it later using a web browser though.
  Negative
563e0f242d1761a701f0f5ce	X	I'm trying to take an image and upload it to an Amazon S3 bucket using Node.js.
  Negative
In the end, I want to be able to push the image up to S3, and then be able to access that S3 URL and see the image in a browser.
  Negative
I'm using a Curl query to do an HTTP POST request with the image as the body.
  Negative
curl -kvX POST --data-binary "@test.
  Negative
jpg" 'http://localhost:3031/upload/image' Then on the Node.js side, I do this: My file is 0 bytes, as shown on Amazon S3.
  Negative
How do I make it so that I can use Node.js to push the binary file up to S3?
  Neutral
What am I doing wrong with binary data and buffers?
  Neutral
UPDATE: I found out what I needed to do.
  Neutral
The curl query is the first thing that should be changed.
  Negative
This is the working one: curl -kvX POST -F foobar=@my_image_name.jpg 'http://localhost:3031/upload/image' Then, I added a line to convert to a Stream.
  Negative
This is the working code: So, in order to upload a file to an API endpoint (using Node.js and Express) and have the API push that file to Amazon S3, first you need to perform a POST request with the "files" field populated.
  Negative
The file ends up on the API side, where it resides probably in some tmp directory.
  Negative
Amazon's S3 putObject method requires a Stream, so you need to create a read stream by giving the 'fs' module the path where the uploaded file exists.
  Negative
I don't know if this is the proper way to upload data, but it works.
  Neutral
Does anyone know if there is a way to POST binary data inside the request body and have the API send that to S3?
  Neutral
I don't quite know what the difference is between a multi-part upload vs a standard POST to body.
  Negative
563e0f242d1761a701f0f5cf	X	I believe you need to pass the content-length in the header as documented on the S3 docs: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html After spending quite a bit of time working on pushing assets to S3, I ended up using the AwsSum library with excellent results in production: https://github.com/awssum/awssum-amazon-s3/ (See the documentation on setting your AWS credentials) Example: UPDATE So, if you are using something like Express or Connect which are built on Formidable, then you don't have access to the file stream as Formidable writes files to disk.
  Very negative
So depending on how you upload it on the client side the image will either be in req.body or req.files.
  Neutral
In my case, I use Express and on the client side, I post other data as well so the image has it's own parameter and is accessed as req.files.img_data.
  Negative
However you access it, that param is what you pass in as img_path in the above example.
  Negative
If you need to / want to Stream the file that is trickier, though certainly possible and if you aren't manipulating the image you may want to look at taking a CORS approach and uploading directly to S3 as discussed here: Stream that user uploads directly to Amazon s3
563e0f252d1761a701f0f5d0	X	I just answered this on stackoverflow.com/questions/12186993/…
563e0f252d1761a701f0f5d1	X	EDIT: Emerson Farrugia answered this at: What is the algorithm to compute the Amazon-S3 Etag for a file larger than 5GB?
  Very negative
As you may already know, when one uploads large files using the amazon multipart upload API: "Complete Multipart Upload" will return an ETag with the following format: where the number is the number of parts uploaded with "Upload Part".
  Negative
Now my question is how this multiparthash is calculated.
  Neutral
I find something in a blog stating that: where Ln is the index and Hn is the ETAg of part n.
  Negative
But this, for me at least, is not working.
  Negative
Is there any official documentation on this or any know-how?
  Neutral
563e0f252d1761a701f0f5d2	X	I'm using Amazon's provided High-Level API to upload files to Amazon S3.
  Negative
I use a lightly-modified version of the example provided: Meanwhile, from another thread, I'm measuring its progress: The progress reporting itself seems to be working well, as I'm getting progress that makes sense.
  Negative
However, once it reaches 100%, the upload stalls.
  Neutral
It looks a lot like the call to isDone() is blocking, as it simply will not update.
  Negative
The percentage, once it gets to 100%, will not update again.
  Negative
It appears to update twice and then hang.
  Neutral
If I check for the existence of the file externally, using Cyberduck, it appears to have uploaded the file successfully.
  Negative
563e0f252d1761a701f0f5d3	X	From your code snippet, you don't get any more updates because you have fallen through your loop, since upload.isDone() is true.
  Negative
If you add: after the end of your loop, you will see the Completed message.
  Positive
You probably see multiple 100% messages, because the TransferManager is waiting for the upload to complete.
  Negative
563e0f252d1761a701f0f5d4	X	Added the edgecast tag for you.
  Negative
Where are your images now?
  Neutral
What language would you like to do the upload with (you have to call curl from somewhere, even if it's a bash script).
  Negative
563e0f252d1761a701f0f5d5	X	I'd like to upload in PHP
563e0f252d1761a701f0f5d6	X	Don't you have to expose your secret key for this?
  Negative
563e0f252d1761a701f0f5d7	X	no of cause not!
  Negative
then it would not be secret.
  Neutral
You just have to use your secret key to generate a HASH, then send the HASH to S3 which will use your secret key to authenticate your HASH
563e0f262d1761a701f0f5d8	X	Here is a tutorial for php on howto do it: ioncannon.net/programming/1539/… if you want to search it then search "amazon s3 CORS" and there should be allot that pops up.
  Negative
563e0f262d1761a701f0f5d9	X	We have a web application which needs to store uploaded images with EdgeCast or Amazon S3.
  Negative
To do so we need to re-upload those images to EdgeCast / S3 first.
  Negative
There are a number of options I can think of: Which is the best solution, and are there any other ones?
  Positive
I suspect it is either 1 or 2.
  Neutral
Edit: My application is in PHP
563e0f262d1761a701f0f5da	X	Not sure about EdgeCast but I know with Amazon S3 the best way to do this was to POST the file directly to the file server.
  Negative
See http://doc.s3.amazonaws.com/proposals/post.html Doing it this way you would provide a HTML FORM with some field like folder id, file name, public key, timestamp etc to ensure it was secure and only you could upload to the server.
  Very negative
The server would redirect their browser once the upload was complete from that page they redirected to you could inspect the query string to find out if the upload was successful or not then record the FileID into your DB.
  Negative
Works great for reducing load on your server and gives the user a faster experience, however it can lead to orphan files if the upload succeeds but the DB insert fails.
  Negative
563e0f262d1761a701f0f5db	X	EdgeCast storage supports Rsync/sFTP configuration to automatically sync content from a storage server.
  Negative
EdgeCast also supports "reverse-proxy" or "Customer Origin" configuration to pull content from another web-server into cache automatically.
  Negative
You can use S3 as this Customer-origin location or use EdgeCast's own Cloud Storage service or any other Web-server outside the EdgeCast network.
  Negative
563e0f262d1761a701f0f5dc	X	What I do is upload the files to a temp directory then have a cron script run that PUT's the files onto AWS, so as not to cause the upload process to take any longer for the end-user.
  Very negative
563e0f262d1761a701f0f5dd	X	AWS provides an SDK for PHP to do this.
  Negative
It even has support for multi-part uploads, which is great news for developers.
  Positive
It should be noted that the AWS SDK for PHP will also do retry logic and (if you're using Multipart, which you should) can resume failed uploads.
  Negative
563e0f262d1761a701f0f5de	X	I have developed an iOS application which can upload larger videos to an Amazon S3 server In order to do that, I have used : I have integrated a MD5 checksum to my put request to in-order to validate after file uploaded to Amazon server.
  Very negative
Now I want to validate the checksum after the file has been uploaded to server.
  Negative
I need to know what is the Amazon API that I can get the checksum after upload to server.
  Negative
I've read below article too and it says we can measure the checksum for the data integrity.
  Negative
Which object gives the checksum after file uploaded?
  Negative
Update S3PutObjectResponse eTag function will give you md5 hash of the uploaded file , but now it gives me an error when i try to print value "ErrorCode:BadDigest, Message:The Content-MD5 you specified did not match what we received."
  Negative
563e0f262d1761a701f0f5df	X	Thanks for the reply.
  Neutral
I tried this approach but now, its not resuming the upload (java.awsblog.com/post/Tx2Y9J2CUDVJ5LZ/…).
  Negative
Following are the steps which are not working: 1.
  Neutral
Once the information from persistable Upload is serialized in a file, when the data is retrieved and deserialized, I call the method transferManager.resumeUpload(persistableUpload); This gives no error but doesn't do anything.
  Very negative
563e0f262d1761a701f0f5e0	X	I'm trying to upload a file on Amazon S3 using their APIs.
  Negative
I tried using their sample code and it creates various parts of files.
  Positive
Now, the problem is, how do I pause the upload and then resume it ?
  Neutral
See the following code as given on their documentation: I have also tried the TransferManager example which takes an Upload object and calls a tryPause(forceCancel) method.
  Positive
But the problem here is, it gets cancelled everytime I try and pause it.
  Negative
My question is, how do I use the above code with pause and resume functionalities ?
  Neutral
Also, just to note that I would also like to upload multiple files with same functionalities.... Help would be much appreciated.
  Negative
563e0f262d1761a701f0f5e1	X	I think you should use the Transfer Manager sample if you can.
  Negative
If it's being canceled, it's likely that it just isn't possible to pause it(with the given configuration of the TransferManager you are using).
  Negative
This might be because you paused it too early to make "pausing" mean anything besides canceling, you are trying to use encryption, or the file isn't big enough.
  Negative
I believe the default minimum file size is 16MB.
  Negative
However, you can change the configuration of the TransferManager to allow you to pause depending on tryPause is failing, except in the case of encryption where I don't think there's anything you can do.
  Neutral
If you want to enable pause/resume for a file smaller than that size, you can call the setMultipartUploadThreshold(long) method in TransferManagerConfiguration.
  Negative
If you want to be able to pause earlier, you can use setMinimumUploadPartSize to set it to use smaller chunks.
  Negative
In any case, I would advise you to use the TransferManager if possible, since it's made to do this kind of thing for you.
  Negative
It might be helpful to see why the transfer is not being paused when you use tryPause.
  Negative
563e0f262d1761a701f0f5e2	X	TransferManager performs the upload and download asynchronously and doesn't block the current thread.
  Negative
When you call the resumeUpload, TransferManager returns immediately with a reference to Upload.
  Negative
You can use this reference to enquire on the status of the upload.
  Negative
563e0f262d1761a701f0f5e3	X	Thanks so much for the thorough answer.
  Positive
I'll take a look into this info.
  Positive
563e0f272d1761a701f0f5e4	X	I have a huge number of objects on Amazon S3, of which only a small subset are accessed regularly.
  Negative
Thus, I'd really like to make use of a distributed caching system, like Ehcache.
  Negative
(Note, I'd use Cloudfront, but the data needs to be accessed from an API server, not from an enduser, and the Cloudfront does not support authentication last time I checked.)
  Negative
Can anyone tell me whether or not this is feasible, practical, or whether or not there exists a library or example of using Ehcache to cache objects from Amazon S3?
  Negative
Naturally, my app server is implemented in Java and is running on a Linux environment.
  Negative
Thanks much.
  Neutral
563e0f272d1761a701f0f5e5	X	Interesting idea - However, before diving into it eventually, I'd like to stress that authentication is available in Amazon CloudFront since September 2009, albeit likely not as you envision it, i.e. you can use a Signed URL to Serve Private Content: You can distribute private content with a static signed URL or a dynamic signed URL.
  Negative
You use a static signed URL when distributing private content to a known end user [...].
  Negative
In this case, you create a signed URL and make the URL available to your end users as needed.
  Positive
You use a dynamic signed URL to distribute content on-the-fly to an end user for a limited purpose [...] In this case, your application generates the signed URL.
  Negative
This is further detailed in the Overview of Private Content: A CloudFront private distribution is based on a policy statement that specifies any or all of the following constraints: [emphasis mine] Whether this approach is feasible for your use case depends on the architecture of your solution, insofar you'll need to generate these signed URLs by some means and use those from the API server in turn; given your 'end user' is the API server, you might pre generate static URLs as suggested, on the other hand the most obvious approach might be to perform the signed URL generation process dynamically in the API server itself and cache the generated URL<->Signed URL map for reuse eventually (i.e. via Memcached or Ehcache indeed).
  Very negative
This authentication scheme is obviously more cumbersome than simple HTTP authentication for example, on the other hand it provides more flexibility as well, see e.g. the tutorial Restricting Access to Files in a CloudFront Distribution Based on Geographic Location (Geoblocking) for an advanced use case, which is summarized in a Guest Post: Geo-Blocking Content With Amazon CloudFront on the AWS blog as well.
  Negative
563e0f272d1761a701f0f5e6	X	I ended up using JetS3t to read files from S3 and store them in a distributed ehcache cluster.
  Negative
The approach works quite well so far, although I find that JetS3t creates a large number of temporary files that have to be dealt with.
  Positive
563e0f272d1761a701f0f5e7	X	did you find a solution ?
  Neutral
563e0f272d1761a701f0f5e8	X	Any update here?
  Neutral
563e0f272d1761a701f0f5e9	X	I'm using Simpl3r, a simple high level Android API for robust and resumable multipart file uploads using the Amazon S3 service, to upload media files to my bucket.
  Negative
On some uploads, I'm getting a SSLException error.
  Negative
Here's the code where the exception is thrown: (My class is a subclass of an IntentService, as per the Simpl3r example) Here's the stack trace: The exception is not caught by my Exception clause.
  Negative
Meaning that the app is stuck in a "uploading" state that never ends.
  Negative
Any ideas?
  Neutral
563e0f272d1761a701f0f5ea	X	How long do these uploads take?
  Negative
Could you be getting reset?
  Negative
563e0f272d1761a701f0f5eb	X	Very simple: do not store any secrets client-side.
  Negative
You will need to involve a server to sign the request.
  Neutral
563e0f272d1761a701f0f5ec	X	You'll also find that signing and base-64 encoding these requests is much easier server-side.
  Negative
It doesn't seem unreasonable to involve a server here at all.
  Negative
I can understand not wanting to send all of the file bytes to a server and then up to S3, but there's very little benefit to signing the requests client-side, especially since that will be a bit challenging and potentially slow to do client-side (in javascript).
  Negative
563e0f272d1761a701f0f5ed	X	please note that this uses Signature v2 which will soon be replaced by v4: docs.aws.amazon.com/AmazonS3/latest/API/…
563e0f272d1761a701f0f5ee	X	Make very sure to add ${filename} to the key name, so for the above example, user/eric/${filename} instead of just user/eric.
  Negative
If user/eric is an already existing folder, the upload will silently fail (you will even be redirected to the success_action_redirect) and the uploaded content will not be there.
  Negative
Just spent hours debugging this thinking it was a permission issue.
  Negative
563e0f272d1761a701f0f5ef	X	I'm implementing a direct file upload from client machine to Amazon S3 via REST API using only JavaScript, without any server-side code.
  Negative
All works fine but one thing is worrying me... When I send a request to Amazon S3 REST API, I need to sign the request and put a signature into Authentication header.
  Negative
To create a signature, I must use my secret key.
  Negative
But all things happens on a client side, so, the secret key can be easily revealed from page source (even if I obfuscate/encrypt my sources).
  Negative
How can I handle this?
  Neutral
And is it a problem at all?
  Neutral
Maybe I can limit specific private key usage only to REST API calls from a specific CORS Origin and to only PUT and POST methods or maybe link key to only S3 and specific bucket?
  Negative
May be there are another authentication methods?
  Negative
"Serverless" solution is ideal, but I can consider involving some serverside processing, excluding uploading a file to my server and then send in to S3.
  Negative
563e0f282d1761a701f0f5f0	X	I think what you want is Browser-Based Uploads Using POST.
  Negative
Basically, you do need server-side code, but all it does is generate signed policies.
  Negative
Once the client-side code has the signed policy, it can upload using POST directly to S3 without the data going through your server.
  Negative
Here's the official doc links: Diagram: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html Example code: http://docs.aws.amazon.com/AmazonS3/latest/dev/HTTPPOSTExamples.html The signed policy would go in your html in a form like this: Notice the FORM action is sending the file directly to S3 - not via your server.
  Negative
Every time one of your users wants to upload a file, you would create the POLICY and SIGNATURE on your server.
  Negative
You return the page to the user's browser.
  Neutral
The user can then upload a file directly to S3 without going through your server.
  Negative
When you sign the policy, you typically make the policy expire after a few minutes.
  Positive
This forces your users to talk to your server before uploading.
  Negative
This lets you monitor and limit uploads if you desire.
  Negative
The only data going to or from your server is the signed URLs.
  Negative
Your secret keys stay secret on the server.
  Neutral
563e0f282d1761a701f0f5f1	X	You're saying you want a "serverless" solution.
  Positive
But that means you have no ability to put any of "your" code in the loop.
  Negative
(NOTE: Once you give your code to a client, it's "their" code now.)
  Neutral
Locking down CORS is not going to help: People can easily write a non-web-based tool (or a web-based proxy) that adds the correct CORS header to abuse your system.
  Negative
The big problem is that you can't differentiate between the different users.
  Negative
You can't allow one user to list/access his files, but prevent others from doing so.
  Neutral
If you detect abuse, there is nothing you can do about it except change the key.
  Negative
(Which the attacker can presumably just get again.)
  Negative
Your best bet is to create an "IAM user" with a key for your javascript client.
  Positive
Only give it write access to just one bucket.
  Neutral
(but ideally, do not enable the ListBucket operation, that will make it more attractive to attackers.)
  Neutral
If you had a server (even a simple micro instance at $20/month), you could sign the keys on your server while monitoring/preventing abuse in realtime.
  Negative
Without a server, the best you can do is periodically monitor for abuse after-the-fact.
  Negative
Here's what I would do: 1) periodically rotate the keys for that IAM user: Every night, generate a new key for that IAM user, and replace the oldest key.
  Negative
Since there are 2 keys, each key will be valid for 2 days.
  Negative
2) enable S3 logging, and download the logs every hour.
  Negative
Set alerts on "too many uploads" and "too many downloads".
  Negative
You will want to check both total file size and number of files uploaded.
  Negative
And you will want to monitor both the global totals, and also the per-IP address totals (with a lower threshold).
  Negative
These checks can be done "serverless" because you can run them on your desktop.
  Negative
(i.e. S3 does all the work, these processes just there to alert you to abuse of your S3 bucket so you don't get a giant AWS bill at the end of the month.)
  Negative
563e0f282d1761a701f0f5f2	X	If you don't have any server side code, you security depends on the security of the access to your JavaScript code on the client side (ie everybody who has the code could upload something).
  Negative
So I would recommend, to simply create a special S3 bucket which is public writeable (but not readable), so you don't need any signed components on the client side.
  Negative
The bucket name (a GUID eg) will be your only defense against malicious uploads (but a potential attacker could not use your bucket to transfer data, because it is write only to him)
563e0f282d1761a701f0f5f3	X	You can do this by AWS S3 Cognito try this link here : http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-examples.html#Amazon_S3 Also try this code Just change Region, IdentityPoolId and Your bucket name  
563e0f282d1761a701f0f5f4	X	To create a signature, I must use my secret key.
  Very negative
But all things happens on a client side, so, the secret key can be easily revealed from page source (even if I obfuscate/encrypt my sources).
  Negative
This is where you have misunderstood.
  Neutral
The very reason digital signatures are used is so that you can verify something as correct without revealing your secret key.
  Positive
In this case the digital signature is used to prevent the user from modifying the policy you set for the form post.
  Negative
Digital signatures such as the one here are used for security all around the web.
  Negative
If someone (NSA?)
  Neutral
really were able to break them, they would have much bigger targets than your S3 bucket :)
563e0f282d1761a701f0f5f5	X	I'm working on a javascript code that records images from webcam and sends them directly to amazon storage via REST API.
  Negative
I've checked some JS webcam plugins like JQuery webcam plugin (http://www.xarg.org/project/jquery-webcam-plugin/) and JPEGCam (http://code.google.com/p/jpegcam/), but I couldn't make them send compressed data as custom-formatted REST query.
  Very negative
Does anyone know how to: I know it is possible to get raw image data and compress it manually in javascript, but I hoped for a native method that would be both more efficient and easier to use...
563e0f292d1761a701f0f5f6	X	"Resource id #20" means that you're trying to echo the variable that contains some sort of resource (in your case perhaps it is handler to curl).
  Negative
563e0f292d1761a701f0f5f7	X	It's very, very unlikely that a shared server can handle uploading a 1.8GB file using PHP.
  Negative
563e0f292d1761a701f0f5f8	X	overriding the max execution time in your local file, or even in a local php.ini doesn't necessarily have any effect in a shared hosting environment, as your host may have those disabled.
  Very negative
563e0f292d1761a701f0f5f9	X	@Mihir, could you please share the php code that you tried.
  Positive
563e0f292d1761a701f0f5fa	X	I know about that but file is less than 2 GB.
  Negative
I get fopen error if file is bigger than 2 GB.
  Negative
So it's not the issue in this case.
  Negative
563e0f292d1761a701f0f5fb	X	I am trying to upload a big file (around 1.8GB) to amazon s3 using their php api.
  Negative
Somehow, upload fails.
  Negative
I have added lines to increase timeout, so I don't think it should be an issue.
  Positive
What I get is "Resource id #20" everytime.
  Neutral
I tried uploading small files and that worked fine.
  Negative
I am executing this script from a shared server, could it be due to resource limitation?
  Negative
Please help me solve this.
  Neutral
563e0f292d1761a701f0f5fc	X	Have you looked at using the multipart/large file upload support.
  Neutral
See the rest api or its easier if you use the php sdk
563e0f292d1761a701f0f5fd	X	PHP has a signed integer Problem by uploading files larger than 2GB - see here: http://bugs.php.net/bug.php?id=27792
563e0f292d1761a701f0f5fe	X	I am using LibS3, a C library that talks to my amazon S3 server and I have noticed that for checking the existence of a bucket libs3 sends a GET request with a query in the URL "?
  Very negative
location", and the amazon S3 server responds with an appropriate HTTP response if the bucket exists or not.
  Negative
The problem is that in the documentation of the Amazon S3 server for checking the existence of a bucket it's said that you must make a HTTP HEAD request, http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html, so I'm confused on what method it's used in the server... If the both of them are supported and how is list objects different from check existence using GET.
  Very negative
563e0f292d1761a701f0f5ff	X	You can send both GET and HEAD requests to check existence of a bucket.
  Negative
Amazon will return 404 HTTP code if it doesn't exist.
  Negative
The main difference is that HEAD does not contain request body (thus takes less bandwidth).
  Negative
563e0f292d1761a701f0f600	X	Let the server do all the work and just send back the URLs.
  Negative
It keeps the client lightweight and you won't have to reimplement functionality on every supported platform.
  Negative
563e0f2a2d1761a701f0f601	X	For my app I store/receive images from amazon s3.
  Negative
In the desktop client my API/server handles the file upload or download.
  Negative
But now I wonder if I should do the same when it comes to my iOS app?
  Negative
Should I download the S3 sdk for the iOS and make my phone scale/upload the images to amazon s3 / making api calls to S3 in order to get the image urls?
  Negative
Or should I first send the images from the phone to the server.
  Neutral
Then let the server/backend scale and upload the images to S3 / let my server send the images urls to the iOS app?
  Neutral
563e0f2a2d1761a701f0f602	X	As a general rule of thumb, keep your client as dumb as possible.
  Negative
If you already have this functionality on your server, you don't want to re-implement it.
  Negative
563e0f2a2d1761a701f0f603	X	I am very much new to java NIO.
  Negative
Since I only send the request and socket connections are handled by amazon API itself, can we still use Selectors here?
  Negative
563e0f2a2d1761a701f0f604	X	If the entire connection is handled by a (Java) API then probably not.
  Negative
As far as I understood your question I thought you were doing the calls on your own and use a REST-API.
  Negative
563e0f2a2d1761a701f0f605	X	I am trying to list the objects from s3 bucket which has very large data.
  Negative
I am using "listObjects" method in multiple threads (50 to 100 threads).
  Neutral
For each thread, I am giving commonPrefix which the API will list all the objects under that commonprefix.
  Negative
I have to use many threads in pull the large data in reasonable time.
  Neutral
So I defined 100 threads but I am facing the following exceptions: I want to know the efficient use of multithreads for amazon s3 requests
563e0f2a2d1761a701f0f606	X	It sounds like Amazon doesn't want you to do 100 queries at a time.
  Very negative
In addition 100 connections will put a huge strain on the user's network connection and probably slow down the whole thing more than you think, at least on some systems.
  Negative
It would be smarter to use a NIO Selector instead of 100 threads.
  Negative
563e0f2a2d1761a701f0f607	X	Connect (docs.aws.amazon.com/redshift/latest/mgmt/…) and submit your CREATE TABLE and COPY commands
563e0f2a2d1761a701f0f608	X	Did you manage to get this working ?
  Negative
do you have any blog post or anything related to how this is done ?
  Negative
tx
563e0f2a2d1761a701f0f609	X	The correct way is using a jdbc driver and treating redshift as a psql database.
  Negative
Here is an example I posted for a ruby programmer.
  Neutral
stackoverflow.com/questions/24438238/…
563e0f2a2d1761a701f0f60a	X	for my own learning, why did you decide to go from: S3 -> Redshift instead of S3 -> Kinesis -> Redshift?
  Negative
563e0f2a2d1761a701f0f60b	X	Kinesis is not a bridge between S3 and Redshift.
  Negative
Kinesis is an endpoint you would stream data to... process it.
  Negative
.
  Neutral
and place that process data into S3 and/or Redshift
563e0f2a2d1761a701f0f60c	X	I currently have a file in S3.
  Negative
I would like to issue commands using the Java AWS SDK, to take this data and place it into a RedShift table.
  Negative
If the table does not exist I would like to also create the table.
  Negative
I have been unable to find any clear examples on how to do this so I am wondering if I am going about it the wrong way?
  Negative
Should I be using standard postgres java connectors instead of the AWS SDK?
  Negative
563e0f2b2d1761a701f0f60d	X	Connect (http://docs.aws.amazon.com/redshift/latest/mgmt/connecting-in-code.html#connecting-in-code-java) and submit your CREATE TABLE and COPY commands
563e0f2b2d1761a701f0f60e	X	I am wondering the best way to achieve de-duplicated (single instance storage) file storage within Amazon S3.
  Negative
For example, if I have 3 identical files, I would like to only store the file once.
  Negative
Is there a library, api, or program out there to help implement this?
  Neutral
Is this functionality present in S3 natively?
  Negative
Perhaps something that checks the file hash, etc.
  Neutral
I'm wondering what approaches people have use to accomplish this.
  Positive
Thanks!
  Positive
563e0f2b2d1761a701f0f60f	X	You could probably roll your own solution to do this.
  Negative
Something along the lines of: To upload a file: To upload subsequent files: To read a file: You could also make this technique more efficient by uploading files in fixed-size blocks - and de-duplicating, as above, at the block level rather than the full-file level.
  Negative
Each file in the virtual file system would then contain one or more hashes, representing the block chain for that file.
  Negative
That would also have the advantage that uploading a large file which is only slightly different from another previously uploaded file would involve a lot less storage and data transfer.
  Negative
563e0f2b2d1761a701f0f610	X	That's because "folders" in S3 are not really folders.
  Negative
Why don't you just include the client-notification as part of the upload?
  Negative
563e0f2b2d1761a701f0f611	X	yes, this is also possible.
  Neutral
Nevertheless, I'm still curious how to do this
563e0f2b2d1761a701f0f612	X	I'm working on an auto-update solution, and I'm using Amazon S3 for distribution.
  Negative
I would like for this to work like follows: To do this, I somehow need to list all files in an amazon bucket's folder, and find the one which has been added last.
  Very negative
I've tried $s3->list_objects("mybucket");, but it returns the list of all objects inside the bucket, and I don't see an option to list only files inside the specified folder.
  Negative
What is the best way to do this using Amazon S3 PHP api?
  Neutral
563e0f2b2d1761a701f0f613	X	To do this, I somehow need to list all files in an amazon bucket's folder, and find the one which has been added last.
  Negative
S3's API isn't really optimized for sort-by-modified-date, so you'd need to call list_buckets() and check each timestamp, always keeping track of the newest one until you get to the end of the list.
  Negative
An automatic PHP script detects that a new file has been added and notifies clients You'd need to write a long-running PHP CLI script that starts with: Maybe throw an occasional sleep(1) in there so that your CPU doesn't spike so badly, but you essentially need to sleep-and-poll, looping over all of the timestamps each time.
  Negative
I've tried $s3->list_objects("mybucket");, but it returns the list of all objects inside the bucket, and I don't see an option to list only files inside the specified folder.
  Negative
You'll want to set the prefix parameter in your list_objects() call.
  Negative
563e0f2b2d1761a701f0f614	X	S3 launched versioning functionality of files in bucket http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html.
  Negative
You could get latest n files by calling s3client.listVersions(request) and specifying n if you want.See http://docs.aws.amazon.com/AmazonS3/latest/dev/list-obj-version-enabled-bucket.html Example is in java.
  Negative
Not sure if the API for PHP is added for versioning.
  Negative
563e0f2b2d1761a701f0f615	X	I am using Amazon S3 as images server.
  Negative
I use php API to put and delete objects to bucket with versions.
  Negative
Now i want to rename folder at S3, i understand that physically i will have to copy paste those files but with another path in folder-name place, and after copy i want all pics from old folder to be erased(in order not to occupy memory).
  Negative
However, my bucket has versions, so that means my erase iteration will just set delete markers and do nothing physically.
  Neutral
Is there a way to get all versions of i-th file as a list, and iterate over them?
  Negative
563e0f2b2d1761a701f0f616	X	Yes it is possible but you wil have to write some code.
  Positive
There is an API that let you retrieve all versions of an object in a bucket.
  Positive
http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html Based on this (and higher level SDK or AWS CLI equivalent) developing the behaviour you describe is relatively easy.
  Neutral
563e0f2c2d1761a701f0f617	X	I am using a bucket policy that denies any non-SSL communications and UnEncryptedObjectUploads. }
  Negative
This policy works for applications that support SSL and SSE settings but only for the objects being uploaded.
  Negative
I ran into these issues: CloudBerry Explorer was able to RENAME objects with the full SSL/SSE bucket policy only after I enabled in Options – Amazon S3 Copy/Move through the local computer (slower and costs money).
  Negative
All copy/move inside Amazon S3 failed due to that restrictive policy.
  Negative
That means that we cannot control copy/move process that is not originated from the application that manipulates local objects.
  Negative
At least above mentioned CloudBerry Options proved that.
  Neutral
But I might be wrong, that is why I am posting this question.
  Negative
Is there something wrong with my bucket policy?
  Negative
I do not know those Amazon S3 mechanisms that used for objects manipulating.
  Negative
Does Amazon S3 treat external requests (API/http headers) and internal requests differently?
  Negative
Is it possible to apply this policy only to the uploads and not to internal Amazon S3 GET/PUT etc..
  Negative
?
  Neutral
I have tried http referer with the bucket URL to no avail.
  Negative
The bucket policy with SSL/SSE requirements is a mandatory for my implementation.
  Negative
Any ideas would be appreciated.
  Negative
Thank you in advance.
  Positive
563e0f2c2d1761a701f0f618	X	IMHO There is no way to automatically tell Amazon S3 to turn on SSE for every PUT requests.
  Negative
So, what I would investigate is the following : write a script that list your bucket for each object, get the meta data if SSE is not enabled, use the PUT COPY API (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) to add SSE "(...) When copying an object, you can preserve most of the metadata (default) or specify new metadata (...)" If the PUT operation succeeded, use the DELETE object API to delete the original object Then run that script on an hourly or daily basis, depending on your business requirements.
  Negative
You can use S3 API in Python (http://boto.readthedocs.org/en/latest/ref/s3.html) to make it easier to write the script.
  Negative
If this "change-after-write" solution is not valid for you business wise, you can work at different level use a proxy between your API client and S3 API (like a reverse proxy on your site), and configure it to add the SSE HTTP header for every PUT / POST requests.
  Negative
Developer must go through the proxy and not be authorised to issue requests against S3 API endpoints write a wrapper library to add the SSE meta data automatically and oblige developer to use your library on top of the SDK.
  Negative
The later today are a matter of discipline in the organisation, as it is not easy to enforce them at a technical level.
  Negative
Seb
563e0f2c2d1761a701f0f619	X	This question applies to both Desktop browsers as well as mobile apps using Phonegap / Cordova.
  Negative
For uploads, the signature has to have the amazon secret encoded and hence created on the server for safety purposes.
  Negative
The alternative would be to generate the signature in the browser but that would mean exposing the Amazon Secret key.
  Negative
Is there an alternative?
  Neutral
I found apisigning.com, which forwards requests sent to them to the Amazon Advertising API.
  Negative
Is there something similar for Amazon S3?
  Neutral
563e0f2c2d1761a701f0f61a	X	RE your presigning link which links to the S3GetPreSignedURLRequest method, how are you supposed to fill in the SecretKey info w/o having it stored on the client?
  Negative
Update: I re-looked at the reference and it says AccessKey, not SecretKey.
  Negative
Is the idea here that you just store one of the keys and it's used like a public key of sorts?
  Negative
563e0f2c2d1761a701f0f61b	X	I want to make calls to the Amazon S3 rest API through an iPhone app.
  Negative
It means that I will have to write in my iPhone app the secretAccessKey and the accessKey of the Amazon S3 service.
  Negative
If my app goes on the appstore, is it going to be dangerous for me?
  Negative
Maybe some people will extract my secretKey and my key to use it for other purposes?
  Neutral
Is there a way to protect my app from this kind of attacks?
  Neutral
Thanks!
  Positive
Martin
563e0f2c2d1761a701f0f61c	X	If possible you shouldn't store your keys in your app.
  Negative
You can see a lengthy discussion of the topic here: Architectural and design question about uploading photos from iPhone app and S3 (check out Adrian Petrescu's answer).
  Neutral
There are a couple of options here.
  Positive
First, upload your data to a central server and then onto S3.
  Negative
Your keys stay private on your server.
  Neutral
Or you can look at presigning your URLs.
  Neutral
563e0f2c2d1761a701f0f61d	X	I know the time cannot be wrong because the script I am running to connect to Walrus is run from the same virtual machine that is hosting Walrus.
  Negative
As you might have noticed the server_url points to localhost.
  Neutral
I did try localtime() instead of gmtime() anyways just to be sure, and to no avail.
  Negative
There was nothing helpful in cloud-error.
  Negative
log unfortunately.
  Negative
Thanks for checking my signature though.
  Neutral
563e0f2c2d1761a701f0f61e	X	Also do you have any evidence that Walrus does not support SOAP?
  Negative
563e0f2c2d1761a701f0f61f	X	From the way you talk, I am assuming that you have worked on the project.
  Negative
If that is so, thanks for giving evidence.
  Neutral
In the future though where would I look for a changelog for Eucalyptus?
  Negative
563e0f2c2d1761a701f0f620	X	@KevinTindall Yes, I'm the lead on Eucalyptus storage.
  Negative
I should have made that more clear.
  Neutral
If you have more questions you can also find us on freenode IRC in #eucalyptus-devel
563e0f2c2d1761a701f0f621	X	@KevinTindall, with regard to the changelog, it is here.
  Negative
It looks like the S3 SOAP support removal was not included, that is a problem and I will work to get that fixed in the doc.
  Negative
563e0f2c2d1761a701f0f622	X	Thanks, for the update :)
563e0f2d2d1761a701f0f623	X	I have been learning how to use Amazon S3 API by using the open source Eucalyptus.
  Negative
So far I have been able to successfully use REST, but now I would also like to use SOAP.
  Positive
I seem to be having trouble generating the correct signature for my request.
  Negative
The service is giving me a 403 Forbidden error: My code is in Python 2 and uses the SUDS-Jurko library for sending SOAP requests: I changed the server endpoint, because otherwise the WSDL points to the regular S3 servers at Amazon.
  Negative
I also have two different ways of creating the signature in my create_signature function.
  Neutral
I was swapping between one and the other by simply commenting out the second one.
  Negative
Neither of the two seem to work.
  Neutral
My question is what am I doing wrong?
  Neutral
SOAP Authentication: http://docs.aws.amazon.com/AmazonS3/latest/dev/SOAPAuthentication.html SUDS-Jurko Documentation: https://bitbucket.org/jurko/suds/overview Edit: I realized I forgot to include an example of what timestamp and signature is printed for debugging purposes.
  Negative
Edit 2: Also, I know that the download_file function does not download a file :) I am still in testing/debug phase Edit 3: I am aware that REST is better to use, at least according to Amazon.
  Negative
(Personally I think REST is better also.)
  Negative
I am also already aware that SOAP is deprecated by Amazon.
  Negative
However I would like to go down this path anyways, so please do me a favor and do not waste my time with links to the deprecation.
  Very negative
I assure you that while writing this SOAP code, I was already well aware of the deprecation.
  Negative
In fact one of the links I posted has the deprecation notice printed at the top of its page.
  Negative
However, if you have evidence showing that Walrus completely ditches SOAP or that they stopped working on the SOAP portion, I would like to see something like that.
  Neutral
But please do not tell me Amazon has deprecated SOAP.
  Negative
563e0f2d2d1761a701f0f624	X	The S3 SOAP API does not support "new" features so the REST API should be used where possible: http://docs.aws.amazon.com/AmazonS3/latest/dev/SOAPAPI3.html https://forums.aws.amazon.com/message.jspa?messageID=77821 IIRC recent versions of Eucalyptus do not support SOAP with S3.
  Negative
That said, the signature looks good to me so I would check if the client/service hosts have the correct time, if there is a difference of more than 15 minutes authentication would fail.
  Negative
You could also check the cloud-error.
  Neutral
log on the Walrus service host as there may be more details on the failure there.
  Negative
563e0f2d2d1761a701f0f625	X	Eucalyptus does not support SOAP for S3 as of Eucalyptus version 4.0.0.
  Negative
If you are using an older version of Eucalyptus (pre 4.0), then SOAP should work.
  Negative
Note, however, that the wsdl provided by S3 is not necessarily current or accurate for even their own service.
  Negative
S3 is notorious for changing the API without version or wsdl bumps, particularly since they stopped updating the SOAP API.
  Negative
So, there are likely responses from Walrus that do not conform to the published WSDL because our XML was updated based on the responses we see from S3 (via REST) and the SOAP and REST responses diverged.
  Negative
The signatures should be compatible, however, so worst case you would see 500 or 400 errors, but not 403 responses.
  Negative
My recommendation is if you really want to learn the S3 SOAP API, you'll have to use S3 proper.
  Negative
The S3 SOAP support in Eucalyptus is there pre-4.0 but may not be compliant with the current state of S3 SOAP--we stopped testing against the S3 SOAP API directly when the AWS SDKs stopped using SOAP in favor of better REST support since that was the API moving forward.
  Negative
563e0f2d2d1761a701f0f626	X	Eucalyptus does support SOAP (see ZachH's comment about deprecation): https://www.eucalyptus.com/docs/eucalyptus/4.0.2/schemas/aws-apis/s3/index.html I decided to scrap using python, and I produced some working code with C# instead.
  Negative
Turns out C# has better SOAP libraries.
  Negative
563e0f2d2d1761a701f0f627	X	Check out the S3 documentation for the ListBucket operation: docs.amazonwebservices.com/AmazonS3/2006-03-01/….
  Negative
To obtain a1-a5000 specify prefix="/l1/" delimeter="/".
  Negative
To obtain /l1/a123/* specify prefix="/l1/a123/", delimeter="/".
  Negative
Is that what you had in mind?
  Neutral
563e0f2d2d1761a701f0f628	X	Oren, You are right it is working now.
  Positive
Thanks a lot.
  Positive
Maybe the test bucket structure I created was wrong.
  Negative
563e0f2d2d1761a701f0f629	X	it is amazing that this wasn't built in still, but this saved me a lot of time.
  Positive
thanks.
  Positive
563e0f2d2d1761a701f0f62a	X	I improved it somewhat here: stackoverflow.com/questions/4849939/… (it also lists individual files now).
  Positive
Still can't believe that this isn't built into one of the many Ruby S3 gems.
  Negative
563e0f2d2d1761a701f0f62b	X	I am storing two million files in an amazon S3 bucket.
  Negative
There is a given root (l1) below, a list of directories under l1 and then each directory contains files.
  Negative
So my bucket will look something like the following l1/a1/file1-1.
  Negative
jpg l1/a1/file1-2.
  Neutral
jpg l1/a1/... another 500 files l1/a2/file2-1.
  Negative
jpg l1/a2/file2-2.
  Neutral
jpg l1/a2/... another 500 files .... l1/a5000/file5000-1.
  Negative
jpg I would like to list as fast as possible the second level entries, so I would like to get a1, a2, a5000.
  Very negative
I do not want to list all the keys, this will take a lot longer.
  Neutral
I am open to using directly the AWS api, however I have played so far with the right_aws gem in ruby http://rdoc.info/projects/rightscale/right_aws There are at least two APIs in that gem, I tried using bucket.keys() in the S3 module and incrementally_list_bucket() in the S3Interface module.
  Negative
I can set the prefix and delimiter to list all of l1/a1/*, for example, but I cannot figure out how to list just the first level in l1.
  Negative
There is a :common_prefixes entry in the hash returned by incrementally_list_bucket() but in my test sample it is not filled in.
  Negative
Is this operation possible with the S3 API?
  Neutral
Thanks!
  Positive
563e0f2e2d1761a701f0f62c	X	right_aws allows to do this as part of their underlying S3Interface class, but you can create your own method for an easier (and nicer) use.
  Positive
Put this at the top of your code: This adds the common_prefixes method to the RightAws::S3::Bucket class.
  Positive
Now, instead of calling mybucket.keys to fetch the list of keys in your bucket, you can use mybucket.common_prefixes to get an array of common prefixes.
  Negative
In your case: I must say I tested it only with a small number of common prefixes; you should check that this works with more than 1000 common prefixes.
  Negative
563e0f2e2d1761a701f0f62d	X	I am starting to develop an app to access the Amazon S3 storage using the SOAP API.
  Negative
I have read the documents that says the the method PutObject must be used if the file size is greater than 1 MB.
  Neutral
Now PutObject uses DIME attachment.
  Neutral
Is there a sample code or example or a fragment of code that someone can show me on how to do DIME attachement with GSOAP for the PutObject method of Amazon S3.
  Negative
I want to use GSOAP because of portability and to make it generic.
  Negative
I do not want to use the .
  Negative
NET API provided by Amazon for the same reason.
  Negative
I want in GSOAP particularly as I have worked in GSOAP earlier.
  Negative
Thanks, david
563e0f2e2d1761a701f0f62e	X	I put together something that uploads files larger than 1MB using PutObject, it should also work for smaller files.
  Negative
I share it for others who might find it useful.
  Positive
Also see my previous post on using GSOAP to access S3 AMAZON AWS S3 using GSOAP C C++ The link also contains the method to generate the signature.
  Negative
Here is the code for PutObject.
  Neutral
It uses the latest GSOAP from sourceforge.
  Neutral
After wsdl2h to generate the header and soapcpp2 to generate the gsoap client code the following will be the code to access the service PutObject......
  Negative
Requirements : OpenSSL GSOAP Build with the compiler preprocessor directive WITH_OPENSSL.
  Negative
Include the library files libeay32 and ssleay32.
  Negative
Take the methods to generate signature from the link above.
  Neutral
Hope it helps.
  Neutral
Thanks, david
563e0f2e2d1761a701f0f62f	X	Say I have the following directories and files in an Amazon S3 bucket (files are in bold): How can I list all objects and immediate subdirectories of a given directory with the .
  Negative
NET AWS S3 API, without recursively getting everything below that directory?
  Negative
In other words, how can I "browse" the contents of a directory at a single level?
  Negative
For example, imagine I want to browse the contents of bucketname/folder1/.
  Negative
What I would like to see is the following: ...and nothing else.
  Negative
I don't want to list the files and directories in subdirectories, I just want to list the files and subdirectories at the folder1 level.
  Negative
Is there a way to apply filters to a single AWS API call so that it doesn't return everything and force me to manually parse only what I need?
  Negative
I've found that this code let's me get just the immediate subdirectories (as intended), but I can't figure out how to include the immediate files too:
563e0f2e2d1761a701f0f630	X	I had the opposite problem (I knew how to get the files in the specified folder, but not the subdirectories).
  Negative
The answer is that Amazon lists files differently than it does sub-folders.
  Negative
Sub-folders are listed, as your example shows, in the ListObjectsResponse.CommonPrefixes collection.
  Negative
Files are listed in the ListObjectsResponse.S3Objects collection.
  Negative
So your code should look like this: my google search turned up this post on the burningmonk blog with this in the comment section: When you make the Lis­tO­b­jects request, to list the top level fold­ers, don’t set the pre­fix but set the delim­iter to ‘/’, then inspect the ‘Com­mon­Pre­fixes’ prop­erty on the response for the fold­ers that are in the top folder.
  Negative
To list the con­tents of a ‘root­folder’, make the request with pre­fix set to the name of the folder plus the back­slash, e.g. ‘rootfolder/’ and set the delim­iter to ‘/’.
  Negative
In the response you’ll always have the folder itself as an ele­ment with the same key as the pre­fix you used in the request, plus any sub­fold­ers in the ‘Com­mon­Pre­fixes’ property.
  Negative
563e0f2e2d1761a701f0f631	X	amazon have sdk for it aws.amazon.com/articles/3051?_encoding=UTF8&jiveRedirect=1
563e0f2e2d1761a701f0f632	X	I've seen the SDK, I was clarifying if there was a built in sync command, as I can't find any information on it.
  Negative
563e0f2e2d1761a701f0f633	X	i doubt it there is no sync command.
  Negative
but you can create one for your own.
  Positive
check this link codeproject.com/Articles/131678/Amazon-S3-Sync
563e0f2e2d1761a701f0f634	X	Cheers for this, I actually wrote my own using the SDK yesterday.
  Negative
And it's working quite well.
  Positive
563e0f2e2d1761a701f0f635	X	I have an Amazon s3 bucket, and wish to have the app data folder for my .
  Negative
net application sync itself to reflect changes in the bucket.
  Negative
So, for instance if a user uploads a photo through my web application.
  Negative
When they launch the desktop application I want the app data folder to sync with the bucket state, so it would automatically download the uploaded photo etc.
  Negative
Is there any api or sdk made for this?
  Neutral
I've seen examples of an application updating a bucket to reflect changes to local storage but I wish to do it the other way around.
  Negative
563e0f2e2d1761a701f0f636	X	So the question is simple, what is the biggest number of CommongPrefixes displayed when you list s3 objects, and what is the biggest MaxKeys.
  Positive
Default is 1000.
  Neutral
Amazon s3 api / bucket get
563e0f2e2d1761a701f0f637	X	To answer my own question, maximum number of CommonPrefixes and MaxKeys is 1000.
  Negative
Caution, TOGETHER 1000.
  Negative
This means that you can have 0 Keys displayed, and maximum 1000 CommonPrefixes or 990 Keys displayed, and maximum 10 CommonPrefixes
563e0f2f2d1761a701f0f638	X	HmacSHA1 generates different signatures on different systems using same secret, maybe?
  Negative
563e0f2f2d1761a701f0f639	X	According to C# in Depth - Strings in C# and .
  Negative
NET, linked from Determine a string's encoding in C#, .
  Negative
NET strings are UTF-16.
  Neutral
563e0f2f2d1761a701f0f63a	X	I do not see a response from Amazon confirming the issue.
  Negative
563e0f2f2d1761a701f0f63b	X	I am following the official Amazon S3 REST API documentation here and am having problems computing the same authorization values they show in their examples.
  Negative
The base64 HMAC-SHA1 hash they show for the first example is: But I keep coming up with: I am tearing my hair out here.
  Negative
What can I possibly be doing wrong?
  Neutral
From their very first example:
563e0f2f2d1761a701f0f63c	X	Okay I found the problem.
  Negative
The keys used in the examples are wrong.
  Negative
563e0f2f2d1761a701f0f63d	X	do you want to make the images publicly readable?
  Neutral
563e0f2f2d1761a701f0f63e	X	@Sebastian I dont know almost nothing about security.
  Negative
So, any help is really welcome.
  Positive
My guess to your question is actually not.
  Negative
I guess I just want to make the images public to request coming from my domain.
  Negative
Does it make sense?
  Neutral
I have read there is something more you can add to the policy in order to make this.
  Neutral
But anyway, Im not even able to make them public to everyone.
  Negative
563e0f2f2d1761a701f0f63f	X	@Sebastian something came to my mind.
  Neutral
Is it possible to check who is the user or owner of an object?
  Neutral
I have two AWS accounts, and its possible that I uploaded the files with the credentials of the other user.
  Negative
In my policy I granted access to put to everyone, right?
  Negative
Does it make sense or Im getting crazy...
563e0f2f2d1761a701f0f640	X	Its working!!
  Neutral
I really apologize for my mistake.
  Negative
I will explain it below.
  Positive
563e0f302d1761a701f0f641	X	Sorry, I have replaced my policy with yours, but still the same.
  Negative
I cannot open any object created with the aws sdk api from the browser :(
563e0f302d1761a701f0f642	X	Im developing an AngularJS (frontend) + Rails API (backend) website.
  Negative
Im using Amazon S3 to store images.
  Negative
Im able to upload an image from Rails to S3 using the aws sdk.
  Positive
I see the image in my S3 bucked.
  Positive
However when I try to get it later from an AngularJS view, my GET request gets a Forbidden: Access denied error.
  Negative
I guess the problem is related to the bucket or image object permissions, but I cant figure out what.
  Negative
This is a summary of how I upload the image: NOTE: I dont use any permissions in the upload request (when I check the image in S3, I see in properties there isnt any permissions set).
  Negative
And this is my bucket policy: This is my GET request: Just in case it helps, when I create an object in a bucket manually, I can also set it as public (with make public menu).
  Positive
However, when I upload the image from the aws sdk API, I cant do it (I get the following objects were not make public due to error).
  Neutral
563e0f302d1761a701f0f643	X	Make sure you set this policy and try to access your image from a browser: When this works set this CORS config and try from Angular:
563e0f302d1761a701f0f644	X	I finally fixed it.
  Negative
The problem was that I have two different AWS accounts.
  Negative
As defined in my bucket policy (above), so far, I grant read and write access to everyone.
  Negative
So, when I upload the images I was using the key credentials for the first user.
  Negative
I was able to upload.
  Positive
However, I couldnt open the files because I was using the S3 AWS account of second user.
  Neutral
I just changed the key credentials and everything is working perfectly.
  Positive
563e0f302d1761a701f0f645	X	i have never used image upload before, don't really understand the mechanism .
  Negative
Trying to upload image from backend, to amazon s3 buckets.
  Negative
does the image have to be converted to binary in this case?
  Neutral
the following code is what i got from amazon tutorial (can't find a good document&tutorial of how to use their api.
  Negative
.)
  Neutral
what i need to add on the code to upload image ?
  Neutral
var params = {Bucket:bucketName, Key: keyName, Body: "?"}
  Negative
; s3.putObject(params, function(err,data){ if(err) console.log(err) else console.log("Successfully uploaded data to " + bucketName + "/" + keyName); })
563e0f302d1761a701f0f646	X	there's alternative such as readasurl, to convert the image into a string, when the image is small.
  Negative
so.
  Neutral
.
  Neutral
no need to even use Amazon S3, which is so complicated...
563e0f302d1761a701f0f647	X	I have an application that allows users to upload contents to Amazon S3, and returns the link of the uploaded content.
  Positive
I have been wondering how to allow only users that own the content to access it, and i got into http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using-authorization-header.html the authorization header.
  Negative
A way to use it i thought is: generating a link to my application host for each content (e.g: from bucket.s3.amazonaws.com/29347524.jpg to -> myapp.com/image/154155.jpg) and serve it to user.
  Negative
When i receive a request i'll be checking if the user is authenticated in my application or not, and in successful match i'll allegate the authorization header to the request and forward it to amazon.
  Negative
I would like not to download the content from amazon's server from my application's server and serve the content to the client.
  Negative
I think this is a useless waste of band.
  Negative
Is there maybe any way to forward the request after adding some headers?
  Negative
So that the client is answered by Amazon when he requests the content but the request is made to my server and modified in some parts.
  Negative
Do you know any other way to perform an authentication like this on Amazon's S3 content ?
  Negative
Any suggestion will be appreciated
563e0f302d1761a701f0f648	X	I would look at the pre-signing facility in S3: http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html Your application server can generate a time-bound URL as described there, and redirect the user using HTTP 302 with the corresponding Location header.
  Negative
If you Google for "amazon s3 presign url", you'll find a few more resources: both blog posts and official Amazon docs.
  Negative
563e0f302d1761a701f0f649	X	I am trying to figure out the best way to download a file from Dropbox and upload it to Amazon s3 using carrierwave.
  Negative
Can use the Dropbox Chooser?
  Neutral
Or do I have to use the Core API from dropbox to get the file?
  Neutral
If I can do it through just the Chooser app, then how do I pass that file into Amazon s3?
  Negative
Do I even need carrierwave?
  Negative
563e0f312d1761a701f0f64a	X	To confuse people.
  Negative
563e0f312d1761a701f0f64b	X	You can execute commands, like checking something or initializing.
  Positive
But I confess, I would never use it as it is not readable.
  Negative
In for loops it is more common to increase/decrease multiple variables.
  Positive
563e0f312d1761a701f0f64c	X	The language allows a lot of things that are known to cause undefined behavior.
  Positive
At least these are harmless.
  Neutral
563e0f312d1761a701f0f64d	X	It depends on the types of a and b. For user defined types, operator== can be overloaded and have side effects.
  Negative
One such side effect could be to deposit a certain amount of money in your bank account.
  Negative
With that in mind, you could consider it advantageous to invoke that operator many times.
  Negative
563e0f312d1761a701f0f64e	X	@juanchopanza: If you are going the operator overloading for obfuscation sake way, they you can also override the operator,() and have extra fun!
  Positive
563e0f312d1761a701f0f64f	X	wellcome on SO!
  Positive
just gave you some reputation ...
563e0f312d1761a701f0f650	X	Nice example, but I'd rather use a while - loop header for demonstration.
  Negative
for the if case, I don't see a reason, why I wouldn't just write those statements in front of the if statement.
  Negative
563e0f322d1761a701f0f651	X	Agreed.
  Neutral
Added some words to make that clear.
  Positive
563e0f322d1761a701f0f652	X	:-) I am clear about for loop , I am specifically asking about if and while statement.
  Negative
563e0f322d1761a701f0f653	X	@AbdulRehman: Well in that case the anser probably is: because it if and while statements require an expression that evaluates to true and (<ex1>,<ex2>) is an expression.
  Negative
563e0f322d1761a701f0f654	X	If it is the case why only last condition of an expression matters ?
  Negative
563e0f322d1761a701f0f655	X	@Abdul: Well, because you have to pick one and someone decided that it should be the last.
  Negative
You have to understand, that the comma operator doesn't have any special behavior in an if statement compared to using it at other positions in your code.
  Negative
563e0f322d1761a701f0f656	X	@MikeMB in decltype you can use it as a simple SFINAE.
  Negative
decltype( test_expression, void(), result_we_want) will test the first expression (determine if it is valid), and if it is, will return result_we_want type.
  Negative
Parameter pack expansion tricks -- using discard=int[]; (void)discard{ 0, ( (std::cout << ts << '\n'), void(), 0 )... }; will expand the statement std::cout << ts << '\n' for each element in the ts... pack (in order) and discard the result.
  Very negative
(not all , are comma operator in this case, but some are)
563e0f322d1761a701f0f657	X	they could simply use assignment expression rather than including expression list in c++ GRAMMAR of If statement.
  Negative
563e0f322d1761a701f0f658	X	@AbdulRehman you could write if ( foo(), a == b ) to call a function and then do the test
563e0f322d1761a701f0f659	X	@AbdulRehman I don't understand: assignment is already an expression...
563e0f322d1761a701f0f65a	X	yes, but it is different than expression in c++ grammar, expression is a list of assignment expressions, while assignment expression is the one without commas in it.
  Negative
563e0f322d1761a701f0f65b	X	I think I we both agree that the comma expression is unnecessary ;) in case of the for-loop, the grammar for initialise and increment could have been simply a list of statements instead of an expression and we wouldn't have this confusion...
563e0f322d1761a701f0f65c	X	perhaps you could add some code examples?
  Negative
as it stands, this is more than confusing.
  Negative
563e0f322d1761a701f0f65d	X	superb while examples.
  Positive
So is it better than duplication of code ?
  Negative
563e0f322d1761a701f0f65e	X	@AbdulRehman: Yes, duplication of code is bad.
  Negative
Especially the code way down at the end of the while loop is disconnected from it purpose, and you could very well forget to update it when for some reason the first instance is updated.
  Negative
Note that the "disconnected" argument is the sole reason why the third (increment) component of a for(.
  Negative
.
  Neutral
;.
  Neutral
.
  Neutral
;.
  Neutral
.)
  Neutral
clause exists, and I've never heard anybody complain that that is useless.
  Negative
563e0f322d1761a701f0f65f	X	no need for comma: while(Success == fallibleCalculation(result) && result.isBared()) { ... } and for void functions (in this case bad api in the first place) use a for to counter the disconnect argument for while
563e0f322d1761a701f0f660	X	I dunno; there could be a void operator==(int rhs) { std::cout << rhs; }.
  Very negative
This is a code base where people are using , in an if statement, I wouldn't rule it out.
  Negative
563e0f332d1761a701f0f661	X	We can write an if statement as and only the last condition should be satisfiable to enter the if body.
  Negative
What is the advantage of commas in an if or while statement, and why is it allowed?
  Negative
563e0f332d1761a701f0f662	X	Changing your example slightly, suppose it was this (note the = instead of ==).
  Negative
In this case the commas guarantee a left to right order of evaluation.
  Negative
In constrast, with this you don't know if f(5) is called before or after f(6).
  Negative
More formally, commas allow you to write a sequence of expressions (a,b,c) in the same way you can use ; to write a sequence of statements a; b; c;.
  Negative
And just as a ; creates a sequence point (end of full expression) so too does a comma.
  Negative
Only sequence points govern the order of evaluation, see this post.
  Neutral
But of course, in this case, you'd actually write this So when is a comma separated sequence of expressions preferrable to a ; separated sequence of statements?
  Negative
Almost never I would say.
  Neutral
Perhaps in a macro when you want the right-hand side replacement to be a single expression.
  Neutral
563e0f332d1761a701f0f663	X	In short: Although it is legal to do so, it usually doesn't make sense to use the comma operator in the condition part of an if or while statement (EDIT: Although the latter might sometimes be helpful as user5534870 explains in his answer).
  Negative
A more elaborate explanation: Aside from its syntactic function (e.g. separating elements in initializer lists, variable declarations or function calls/declarations), in C and C++, the , can also be a normal operator just like e.g. +, and so it can be used everywhere, where an expression is allowed (in C++ you can even overload it).
  Very negative
The difference to most other operators is that - although both sides get evaluated - it doesn't combine the outputs of the left and right expressions in any way, but just returns the right one.
  Positive
It was introduced, because someone (probably Dennis Ritchie) decided for some reason that C required a syntax to write two (or more) unrelated expressions at a position, where you ordinarily only could write a single expression.
  Negative
Now, the condition of an if statement is (among others) such a place and consequently, you can also use the , operator there - whether it makes sense to do so or not is an entirely different question!
  Negative
In particular - and different from e.g. function calls or variable declarations - the comma has no special meaning there, so it does, what it always does: It evaluates the expressions to the left and right, but only returns the result of the right one, which is then used by the if statement.
  Negative
The only two points I can think of right now, where using the (non-overloaded) ,-operator makes sense are: If you want to increment multiple iterators in the head of a for loop: If you want to evaluate more than one expression in a c++11 constexpr function.
  Negative
To repeat this once more: Using the comma operator in an if or while statement - in the way you showed it in your example - isn't something sensible to do.
  Negative
It is just another example where the language syntaxes of c and c++ allow you to write code, that doesn't behave the way that one - on first glance - would expect it to.
  Negative
There are many more....
563e0f332d1761a701f0f664	X	There is no advantage: the comma operator is simply an expression with type of the last expression in its expression list and an if statement evaluates a boolean expression.
  Negative
It's a weird operator true, but there's no magic to it - except that it confuses lists of expressions with argument lists in function calls.
  Negative
Note that in the argument list, comma binds stronger to separating arguments.
  Negative
563e0f332d1761a701f0f665	X	For an if statement, there is no real point in putting something into a comma expression rather than outside.
  Negative
For a while statement, putting a comma expression to the condition executes the first part either when entering the loop, or when looping.
  Negative
That cannot easily be replicated without code duplication.
  Negative
So how about a s do...while statement?
  Neutral
There we have only to worry about the looping itself, right?
  Neutral
It turns out that not even here a comma expression can be safely replace by moving the first part into the loop.
  Negative
For one thing, destructors for variables in the loop body will not have already been run then which might make a difference.
  Negative
For another, any continue statement inside the loop will reach the first part of the comma expression only when it indeed is in the condition rather than in the loop body.
  Negative
563e0f332d1761a701f0f666	X	What follows is a bit of a stretch, depending on how devious you might wish to be.
  Negative
Consider the situation where a function returns a value by modifying a parameter passed by reference or via a pointer (maybe from a badly designed library, or to ensure that this value is not ignored by not being assigned after returning, whatever).
  Negative
Then how do you use conditional statements that depend on result?
  Negative
You could declare the variable that will be modified, then check it with an if: This could be shortened to Which is not really worth while.
  Negative
However, for while loops there could be some small advantages.
  Neutral
If calculateValue should/can be called until the result is no longer bar'd, we'd have something like: and could be condensed to: This way the code to update result is in only one place, and is near the line where its conditions are checked.
  Negative
maybe unrelated: Another reason why variables could be updated via parameter passing is that the function needs to return the error code in addition to modify/return the calculated value.
  Negative
In this case: then
563e0f332d1761a701f0f667	X	None whatsoever.
  Negative
The comparisons on a in that code are completely redundant.
  Negative
563e0f332d1761a701f0f668	X	My question is what is the advantage of commas in if or while statement?
  Neutral
Why is it allowed ?
  Negative
It exists because statements and expressions are different things in C.
  Neutral
A compound expression is a construct that is understood from theory (and some other languages) and would be missing without having added it in the form of the comma.
  Negative
Its use in the for statement was the original justification of why they needed it.
  Negative
But, by making the language more complete from a sound theoretical point of view, it later finds uses that nobody planned.
  Negative
The early C++ was a translator that generated C as its output, and having a sequential expression was absolutely essential in allowing inline functions to really generate "in line" logic in the C code.
  Negative
That includes any place the expression appears, including the condition of an if statement.
  Negative
Similarly, it has been used in "interesting" macros.
  Negative
And as much as C++ did away with macros by providing inline functions, as late as up-to-x11 compilers found the Boost FOREACH range loop (eventually, an emulation of the feature added to the language in x11) very handy, and that was a devilishly clever set of macros that involved the comma operator.
  Negative
(Hmm, the current version expands into multiple statements using chained if/else, rather than cramming it all into a single while.)
  Negative
Now, there is another way to put any statement into an expression (lambdas), so future crazy business of macros that emulate even newer language features or domain-specific embedded languages might not need to use that anymore.
  Negative
So, don't write code like that.
  Negative
Unless it's clear and indeed simpler than writing helper functions or splitting into multiple statements.
  Negative
But it may be just the thing for a macro that you want to easily use in one place and that place is inside the parens of an if or while.
  Negative
That could be justified in a domain-specific language hosted inside the C++ source code, or a language emulation feature like (perhaps) an alternative to exception handling used in an embedded real-time system.
  Negative
In short, it doesn't have a normal good usage.
  Negative
But it's there for completeness and you never know when someone will find it useful.
  Negative
563e0f342d1761a701f0f669	X	Could you post the exact wording of the error message?
  Negative
563e0f342d1761a701f0f66a	X	The specified bucket does not exist
563e0f342d1761a701f0f66b	X	Have you tried specifying region in the client factory?
  Negative
563e0f342d1761a701f0f66c	X	Hi Antony thanks for your help but its not that i can copy a file to a different bucket but not the same one to rename.
  Negative
563e0f342d1761a701f0f66d	X	Looking around, this kind of problem can be caused by the naming rules of the bucket.
  Negative
But the name looks right in your example.
  Positive
563e0f342d1761a701f0f66e	X	Really struggling to rename a file within the same bucket with Amazon S3 SDK can anyone help, i am referring to copy object here in the api docs.
  Negative
http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.S3.S3Client.html#_copyObject So here is my call but it keeps returning specifed bucket does not exsist?
  Negative
Before someone points out the obvious and says does your bucket exist yes it definitely exists i can run a call with the same keys and get all my files from that bucket?
  Negative
I really want to be able to rename a file via the api you can do it within the Amazon S3 Browser.
  Positive
Please help Thanks UPDATE WORKING VERSION you have to include the bucket in copy source???
  Neutral
563e0f342d1761a701f0f66f	X	I'd like to load an image directly from a URL but without saving it on the server, I want to upload it directly from memory to Amazon S3 server.
  Negative
This is my code: The Amazon API gives me the error "Could not determine content length".
  Negative
The stream fileStream ends up as "System.Net.ConnectStream" which I'm not sure if it's correct.
  Negative
The exact same code works with files from the HttpPostedFile but I need to use it in this way now.
  Positive
Any ideas how I can convert the stream to become what Amazon API is expecting (with the length intact)?
  Negative
563e0f342d1761a701f0f670	X	Have you updated your bucket policy?
  Neutral
563e0f342d1761a701f0f671	X	No, I new with amazon, what is need to be added there?
  Negative
563e0f342d1761a701f0f672	X	Still does not upload , maybe something else?
  Negative
563e0f352d1761a701f0f673	X	Try printing the error
563e0f352d1761a701f0f674	X	there is no errors.
  Negative
The uploadRequest.key = "filename" is ok?
  Neutral
I have just opened a bucket without any internal folders.
  Neutral
It is ok?
  Neutral
563e0f352d1761a701f0f675	X	The code looks like it should be working.
  Negative
563e0f352d1761a701f0f676	X	i find an error AWSRegion is not valid for bucket , thank you a lot!
  Negative
563e0f352d1761a701f0f677	X	Hi i try to upload a file to my bucket at AWS s3 without successes.
  Negative
I just need to upload a file from some application from some phones.
  Negative
i use AWS SDK version2 on ios 8.
  Negative
this is my function for Upload a DB: UPLOAD AppDelege.m ** ** Thanks
563e0f352d1761a701f0f678	X	Try changing your bucket policy to this (replace bucketName): This allows you to Add/Read objects from your bucket, you should read AWS S3 Documentation in order to know what exactly you want your policy to be, but for now, this should do the trick.
  Very negative
563e0f352d1761a701f0f679	X	I give up on using AWS SDK.
  Negative
I have tried with AFAmazonS3Manager and It works well.
  Positive
AFAmazonS3Manager: AFNetworking Client for the Amazon S3 API.
  Negative
563e0f352d1761a701f0f67a	X	ok i see, so a follow up question, how can I use the link I got to download that file to my server?
  Negative
the server is not authnticated with the user credentials, so how can he access the file to download it?
  Negative
563e0f352d1761a701f0f67b	X	you dont.
  Neutral
you download it on the client side using Javascript, then post to s3.
  Negative
bo server involved.
  Neutral
563e0f352d1761a701f0f67c	X	I have a working upload form in my webapp that lets user upload files from their computer.
  Positive
The file is uploaded directly to amazon s3 without going through my server first this is how my form looks like, and Im using jquery file upload plugin.
  Negative
I want to give users the option for users to choose files from their google drive, using the drive api I added the google picker which lets user pick files from there.
  Negative
after they pick the file, I get in the callback the file metadata (including a file url).
  Negative
How can I use that to upload the file directly to s3?
  Neutral
I dont want to upload the file to my server and then copy it to s3, I want to upload the file dircetly to s3.
  Negative
Is that poosible without having to download the file on client side and then upload it?
  Neutral
Thx
563e0f352d1761a701f0f67d	X	The only way to upload a file to S3 is to send the file to S3.
  Negative
S3 does not have the capability to fetch an object from an external URL.
  Negative
Download to your server directly from Google and then upload to S3, or you could use AWS Lambda invoked through AWS API Gateway to handle the actual download/upload transfer in the background (sync or async).
  Negative
563e0f352d1761a701f0f67e	X	JS library in backbone, require.js web application to render PDF page.
  Negative
The PDF page exist on Amazon S3 server, for fetching that I am calling REST API which will return that https://s3.amazonaws.com/bucket/asjkasjkasj.pdf.
  Negative
But I am getting XMLhttp request can not load   No Access-Control-Allow-Origin header is present on requested source.
  Negative
563e0f362d1761a701f0f67f	X	Based on PDF.js documentation, you can load a PDF from another server by: Enable CORS (Cross Origin Resource Sharing) To enable CORS in AWS S3, you can add this in Edit CORS Configuration inside bucket Permission Properties: Test your CORS using: Make sure you see Access-Control-Allow-Origin: *.
  Negative
And then test using your PDF.js.
  Negative
Create a proxy inside your server to download the PDF file from S3
563e0f362d1761a701f0f680	X	i am using amazon s3 javascript browser api, when i'm trying to download one file it works fine, when i'm trying to download multiple files some of the files downloaded as expected, but some of the files doesn't downloaded and i got 403 The request signature we calculated does not match the signature you provided.
  Negative
Check your key and signing method on some of the files.
  Neutral
all the files are in the same bucket , and all the files are good.
  Positive
for example: in the bucket i have 3 files test1.jpg , test2.jpg and test3.jpg if i try to download all the 3 files sometimes test1 and test2 downloaded and for test3 i got 403 and somtimes test2 and test 3 was download and i got 403 for test1.
  Negative
I send several requests in parallel through this code(to amazon s3).
  Negative
Thanks.
  Neutral
563e0f362d1761a701f0f681	X	Yes I have the values beforehand but maybe S3 changes something on upload (not sure if that ever happens...) The $response is actually an array with a lot of values returned from the api.
  Negative
563e0f362d1761a701f0f682	X	None of the values you've mentioned will ever be changed on upload.
  Negative
563e0f362d1761a701f0f683	X	I'm uploading images to amazon s3 using PHP.
  Negative
When I upload a file I want to store information about it in my database so I'll make less API requests when I need to get info about the file (file name, size, type) Creating new objects: After a successful upload, is it possible to get the file name, size, and file type from the $response variable without making new GET requests?
  Negative
563e0f362d1761a701f0f684	X	Those values aren't in $response, but you don't need them.
  Negative
You know everything you want to know already:
563e0f362d1761a701f0f685	X	I can access the returned array from $response using header For example to get the file size
563e0f362d1761a701f0f686	X	I'm currently adding the "Range" header with a value of "bytes = 0" so it will only download the response headers and 0 bytes from the response body, is this valid/optimal?
  Negative
563e0f362d1761a701f0f687	X	Can you be more specific why you think that it doesn't work for using the "HEAD" option - as per my answer below, I'm not aware of any special restrictions for a HEAD request in comparison to a GET request.
  Negative
563e0f372d1761a701f0f688	X	Well, I changed the "GET" for a "HEAD" on my string to encode and it doesn't work, that's my problem.
  Negative
But if I do GET and get 0 bytes from the response body I think that work as well.
  Negative
563e0f372d1761a701f0f689	X	By the way, I can't use the Amazon S3 SDK for .
  Negative
NET because I'm using MONO and Unity3D
563e0f372d1761a701f0f68a	X	I've been using this method https://coderwall.com/p/kmodkq but I think that it doesn't work for using the "HEAD" option (which is supposed to get the file metadata but not the file body).
  Negative
Any help would be appreciated.
  Neutral
563e0f372d1761a701f0f68b	X	Using a HEAD request for an Amazon S3 object is fully supported and the method of choice for retrieving the information you are looking for: The HEAD operation retrieves metadata from an object without returning the object itself.
  Negative
This operation is useful if you are interested only in an object's metadata.
  Negative
To use HEAD, you must have READ access to the object.
  Negative
A HEAD request has the same options as a GET operation on an object.
  Negative
The response is identical to the GET response except that there is no response body.
  Negative
[emphasis mine] Section Examples in the referenced documentation features a Sample Response which surfaces the desired Last-Modified HTTP header:
563e0f372d1761a701f0f68c	X	really good question +1
563e0f372d1761a701f0f68d	X	Good Q, any luck figuring it out?
  Negative
563e0f372d1761a701f0f68e	X	I am trying to upload files to Amazon S3 storage using Amazon’s Java API for it.
  Negative
The code is When I run the code after commenting the first two lines and uncommenting the third one, ie stream is a FileoutputStream, the file is uploaded correctly.
  Negative
But when data is a base64 encoded String, which is image data, the file is uploaded but image is corrupted.
  Negative
Amazon documentation says I need to create and attach a POST policy and signature for this to work.
  Negative
How I can do that in java?
  Neutral
I am not using an html form for uploading.
  Negative
563e0f372d1761a701f0f68f	X	First you should remove data:image/png;base64, from beginning of the string: Sample Code Block:
563e0f372d1761a701f0f690	X	Did a little more digging and found that my original assumption of the transfer manager was pretty much correct, it does remain open until you shut it down via tx.shutDownNow() but this should only be called when everything is done as it will leave a partial file if shutDownNow() is called during a transfer.
  Very negative
Funny how I can't find the answer to something until AFTER I ask others about it...
563e0f372d1761a701f0f691	X	I'm having trouble with the Amazon S3 download hanging after the last file completes its download, "locking" the file from being deleted as it is "in use" still by the java app, otherwise they work fine.
  Neutral
Additionally, the progress doesn't appear to be updating correctly as the largest file simply says 100 once for its progress then proceeds to download without any further updates until it is completed, at which point it says "State: Completed" before the script hangs.
  Negative
My code is below: I pretty much hacked apart the sample code for the Amazon S3 Transfer Progress Sample that comes with the SDK to create a download version of the method without a GUI, so I'm surprised it even works.
  Negative
I'm not that great with Java and even worse with the AWS API, so any pointers are welcome.
  Negative
563e0f372d1761a701f0f692	X	I found the solution to the issue of the hang.
  Positive
Apparently tx = new TransferManager(credentials); should be tx = new AmazonS3Client(credentials);.
  Negative
I'm not sure how the transfer manager works, but I'm assuming the reason it's hanging is that it's not closing the connection when it's done, but that's probably a whole other topic...
563e0f382d1761a701f0f693	X	This doesn't look like it is meant to scale in a web application.
  Negative
My app can have potentially 100's if not 1000's of users and groups.
  Neutral
563e0f382d1761a701f0f694	X	You didn't mention that this was for an app.
  Negative
Added a reference to Security Token Service in my answer.
  Negative
563e0f382d1761a701f0f695	X	Yes it is a web application.
  Negative
'GetFederationToken' matches what i had in mind.
  Positive
docs.aws.amazon.com/STS/latest/UsingSTS/CreatingFedTokens.html
563e0f382d1761a701f0f696	X	I have a requirement where users are part of groups.
  Negative
.
  Neutral
user1 is part of group1, user2 is part of group2, user3 is part of group1 and group2.
  Negative
.
  Neutral
etc..
  Neutral
each group has static content which i want to host in Amazon S3.
  Negative
So far in my research, i concluded that i will need to create a bucket for each group.
  Positive
My question is: How do i control access to users ?
  Neutral
user1 should only be able to access resources of bucket belonging to group1.
  Neutral
(upload, download etc.) users should not be able to access resources of groups they are not part of.
  Negative
I imagine this is a typical scenario, but so far my google-fu is not yeilding any fruitful results.
  Negative
I have a NodeJS/Express REST API as my middle tier.
  Negative
Please advise me on how to engineer for this requirement.
  Negative
Thanks
563e0f382d1761a701f0f697	X	You do not need a bucket per group.
  Negative
You could create one bucket, then in each bucket create a directory for each group and grant access only to the corresponding group.
  Positive
You can use IAM to create the groups and users.
  Neutral
Refer to this AWS doc for a walkthrough on this scenario.
  Negative
If you need to scale to many users and groups, you should look at the AWS Security Token Service which scales to millions of users and doesn't require IAM credentials at all.
  Negative
Your application is responsible for authenticating users and managing their accounts and group membership, but the token service can allow access to AWS resources including S3.
  Negative
563e0f382d1761a701f0f698	X	Note that S3 is a dynamic system, so doing a 'quick benchmark' will give you terrible numbers.
  Positive
Here is an article about a similar test run on ELB: rightscale.com/blog/cloud-management-best-practices/…
563e0f382d1761a701f0f699	X	I have developed a cloud storage system that uses the same API structure as Amazon S3.
  Negative
Now I want to run some performance tests on getting object data and object metadata.
  Negative
In such a way that I can compare my system with Amazon S3, OpenStack storage and other systems.
  Negative
I have looked at some common file system benchmark tools, there is too much work to convert them for Cloud Storage systems.
  Negative
I am looking for some benchmark tools similar to SIEGE, that not only can performance http requests, but also have some workload simulation features.
  Negative
For example, one simulation can be storing an entire static HTML website in the Cloud Storage then performance some workload stress test etc.
  Negative
Can someone help and suggest some existing framework or tools that can be relatively easy to be fit for such cloud storage system benchmark scenario?
  Neutral
563e0f382d1761a701f0f69a	X	As you are the provider of the cloud system.
  Neutral
There is many aspects you should want to benchmark.
  Negative
as provider For all thoses things there is specifics tools/api/controls.
  Negative
Sometime it is closely related to your hardware, sometime less.
  Negative
But the connexion between hardware to software result to specifics measure and integration problems.
  Negative
Defining what is a benchmark or routing a 'end to end' query from the 'objet storage api' to the disks can be just crazy hard.
  Negative
If your goal is to get a benchmark(in higther level of api) that could end in improving your system then you only solution is to have a total control(and understanding) of your cloud system; Nagios like tools, aren't fitted for this kind of tests.
  Very negative
You need CMDB and some fetching tools to an big data oriented storage.
  Negative
You need to understand that all solutions of benchmark are primary data, and as cloud can be very complex, there is a lot of data.
  Negative
What you will learn from your data isn't just some graphicals data, but also some how to ask your questions.
  Negative
Even getting the rights questions will ask you work.
  Negative
As I said in my first short answer we use VMware VMmark to conduct this kind of test, but that just a small part.
  Neutral
There is a so greate numbers of tool (juste to do some real time monitoring - benchmarking that) that one person can't know them all.
  Negative
A work, I'm doing some AI progs (bayesian network for failure detection, evolutionnary algorithms for repartion ...) to enable better management of those things.
  Negative
Just to tease you : Do you expect to conduct benchmark when you install a new client, swap storage of two others and running the emergency plan of a last one, all in same time ?
  Negative
A correct benchmark should cover so many cases.
  Negative
Today cloud must manage the complexity of the world, every chaotic event; nothing should distrub the service.
  Negative
So just to say what is a benchmark is pretty hard.
  Negative
(feeding the cmdb is itself a challenge) as client yep :-) I'm also client of cloud providers like every human will do in near future.
  Negative
Just a little background.
  Neutral
Openstack as been initialy release by organizations with very specific needs(Just to think that, in the 'Compute' part of the 'openstack' api there is nothing related to share/cluster processing that look like what lhc consume).
  Negative
So what is a normal website ?
  Neutral
Youtube ?
  Neutral
Amazon ?
  Neutral
Even if it is just for an example a "entire static HTML website" could hardly be use to compare cloud solution.
  Negative
This week I have work as well on the translation of vCloud api into openstack (loose loose game), vCloud is mush well defined, with mush more objects that openstack, but even with this we just cover so little needs of applications management.
  Negative
So how the client could compare two cloud solutions ?
  Neutral
In fact, before trying hes own solution he can't.
  Negative
That why clients, come to visit us, ask what we are using and how, our process ... In the end the commercials to the job, generaly few months free of charges just to install the client and find what we should do to reconfigure our cloud to his applications.
  Negative
Very few clients know how many cpu/ram/disk/iops they use; some of them buys dedicated resources(as it is dedicted we can't share to other client) they will never use.
  Negative
Then any benchmark tool for normal web site should do the job.
  Neutral
If you want to play you can openstack 'inner' tools like swiftstack and tempest to get some sort of feedback, but you have to define what a normal use of a web site should look.
  Negative
If you look for openstack products related you should also look at the wiki.
  Negative
But if you want just more than A is faster than B is the condition you set, it will be near to impossible as a client.
  Negative
I believe to have explain why not any 'client' have answser to your question until now, while your question is vital in many commercials/industrials/ecologicals aspects.
  Negative
563e0f382d1761a701f0f69b	X	You can probably look into COSBench, which is a tool to benchmark object storage cloud services.
  Negative
563e0f382d1761a701f0f69c	X	Can you show all your code if possible trying to figure out how to grab the info sent back from the response of the signature of Amazon S3
563e0f382d1761a701f0f69d	X	Hey do you have any idea how I can go about getting the signed request from retrofit?
  Negative
This is my question on SO with my attempt stackoverflow.com/questions/30411672/…
563e0f382d1761a701f0f69e	X	I'm attempting to convert all my asynctasks and HttpPost code to use Retrofit, so far so good, but I'm having trouble uploading user files to an amazon s3 bucket.
  Negative
The file upload has two parts: Here's an example list of parameters provided to me from the first call.
  Negative
According to the documentation, these values can change or not be included, and the 2nd api call must call use these params in the exact order they were provided.
  Negative
In order to deal with the dynamic content.
  Neutral
I created a custom converter to have retrofit return me a LinkedHashMap in my first API call.
  Negative
Then in the second api call, Once I have these values I create a FormUrlEncodedTypedOutput by iterating over the HashMap and adding each item.
  Negative
Everything up to here seems to be working.
  Neutral
I'm getting the necessary upload params and the ordering seems to be consistent.
  Negative
I'm a bit less sure about how I have my multipart retrofit call setup.
  Negative
I then use that inside a synchronous retrofit call inside an intentservice.
  Negative
This results in a Amazon error.
  Negative
I've been googling and it seems like Amazon prefers the "key" value to be first?
  Negative
However, if i put the "key" in front of "AWSAccessKeyId" I get a 403 unauthorized error.
  Neutral
Do i have my retrofit call setup correctly?
  Negative
If someone could help me figure this out, I'd appreciate it.
  Negative
It's taken a few days to convert most of my uploading code over to retrofit and if I've been stuck on this for a while.
  Negative
Thanks!
  Positive
563e0f392d1761a701f0f69f	X	Solution was to use a @PartMap instead of the FormUrlEncodedTypedOutput.
  Negative
563e0f392d1761a701f0f6a0	X	I am currently using an HTTP handler in an attempt to serve up a file from my S3 bucket.
  Negative
I am not getting an error, but the first run through always fails.
  Negative
The file gets "served" to the webpage and the typical "Open or Save" message appears.
  Negative
If I click "Save", everything works as it should.
  Positive
If I click "Open", however, it simply says that the file could not be downloaded.
  Negative
"Retry" is an option at this point.
  Positive
If I click "Retry" the file opens normally.
  Negative
This happens -EVERY- time.
  Neutral
I'm not sure what I'm doing wrong.
  Negative
If anyone has any insight after reviewing my code below, PLEASE help me out.
  Negative
A bit of background.
  Neutral
Here's the standard message that comes up when I click the file link.
  Negative
Here's the message that comes up when I click "Open" in the previous image.
  Neutral
If I click "Retry" from here, it works just fine.
  Positive
563e0f392d1761a701f0f6a1	X	I am having trouble uploading images from my ios app to amazon s3 using their newest sdk.
  Negative
How can I upload images without using their cognito service?
  Neutral
For example, I have an api in my website that returns the following information Now my question is, even without setting up those Cognito credentials that is in the sample app of amazon sdk, how do i use the above information to upload to the bucket (assuming I know the bucket name)?
  Negative
Thank you
563e0f392d1761a701f0f6a2	X	This question already has an answer here: Hello :) I'm looking at the feasibility of having my node application stream HTTP POST file uploads directly through to an Amazon S3 Bucket.
  Negative
I'm looking at using Formidable and Knox for this.
  Negative
The part I'm unsure about is that S3 requires that you know the total number of bytes in the file before transmission.
  Negative
Would I be right in thinking that the only way to accomplish this then would be to use the HTML5 File API (And possibly an ActiveX control on Internet Explorer) to check the file size on the client and send this as part of the HTTP POST request ?
  Neutral
563e0f392d1761a701f0f6a3	X	With the recent CORS support, you can easily send files directly to s3, without your server having to handle anything.
  Negative
I recently wrote a short tutorial, with rails, but again the server is just used to compute some keys, so adapting it to express shouldn't be hard at all.
  Negative
But with such a solution, you'll need to use the jQuery File Upload plugin, and you probably won't need knox http://pjambet.github.com/blog/direct-upload-to-s3/ Hope it'll help you.
  Negative
563e0f392d1761a701f0f6a4	X	Maybe this can help - I made this to use the JQuery File Upload plugin upload directly to S3.
  Negative
I did not need to check the size of the file.
  Negative
https://gist.github.com/3995819
563e0f392d1761a701f0f6a5	X	Nice.
  Negative
Thanks!!!
  Neutral
It's interesting that I can get the MD5 using eTag even if it is a folder but it gives me errors when I try to do Files.getDigest() on a folder.
  Negative
This works well though!
  Very positive
Thanks again!
  Positive
563e0f3a2d1761a701f0f6a6	X	There is no concept of folder on amazon s3.
  Negative
simpli filenames that contain /.
  Negative
So, i dont really understand what do you mean by "even it is a folder"
563e0f3a2d1761a701f0f6a7	X	Well, suppose you have a file in S3 called file1.txt and there will be a "folder" called /folder1.
  Negative
If there are no concepts of folders, that means that this file contains a others files which in my mind is a folder
563e0f3a2d1761a701f0f6a8	X	@Yko, not really, your file1.txt is not actually called file1.txt but folder/file1.
  Negative
txt.
  Neutral
The proof of that is that you can not list only *** the files in ***"current folder", but you can list files by prefix, and if your prefix is folder/, then you will get folder/a.
  Negative
txt and folder/dir/b.
  Neutral
txt
563e0f3a2d1761a701f0f6a9	X	I am building a java application as a learning exercise.
  Negative
For now, I want to make an app that will take the contents in a folder and replicate it to my Amazon S3 bucket.
  Negative
By searching around, I found out that the best way to tell if 2 files are identical is to take the MD5 value.
  Positive
How do I iteratively take the MD5 of each file in my bucket?
  Neutral
According to this link: http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?ListingObjectKeysUsingJava.html The ObjectListing class can list the object keys and (metadata?)
  Positive
of your files in the bucket.
  Neutral
But, it doesn't really specify how I can interact with the files.
  Negative
For example, for my local files, I use guava and does something like this: Looking at the AWS S3 API, you can use S3Object.getObjectContent() which will return an InputStream.
  Negative
Is that the best way to work with the objects directly on S3?
  Positive
Finally, what is the best way to sync from my local folder to a bucket in S3?
  Neutral
Any tips is welcomed.
  Neutral
Thanks!!!!
  Neutral
563e0f3a2d1761a701f0f6aa	X	S3ObjectSummary has memeber called eTag which is md5 hash of the object
563e0f3a2d1761a701f0f6ab	X	Are you really sure you want to do this over a browser?
  Negative
Besides the complexity, uploading a 10GB+ file means having the page open for some time.
  Negative
Is that realistic (what if the page is closed)?
  Neutral
Aren't your better of making a small app dat can access the S3 from the desktop (or even buy one)?
  Negative
563e0f3a2d1761a701f0f6ac	X	@Rogier you are correct it's probably not the best solution (if even a feasible solution).
  Negative
I was trying to avoid having to install anything on the client side but that doesn't look very realistic for extremely large files.
  Negative
The route you mentioned is pretty much the path I was leaning towards.
  Neutral
563e0f3a2d1761a701f0f6ad	X	Yeah.
  Neutral
Depending on the platform, you could use for example Transmit (im on Mac) and make a droplet (drag an drop file for uploading).
  Negative
Cost a bit of money, but in terms of hours you can't beat that ;-)
563e0f3a2d1761a701f0f6ae	X	I'm uploading files via AJAX to Amazon S3 (using the browser File API and storing the actual upload script on Amazon S3 as an iframe to get around the Amazon S3 cross-site issues, courtesy to jquery-file-upload for the idea).
  Negative
I have that working and it works great for small files (< 50 MB or so).
  Positive
However, I'm looking to store extremely large files on Amazon S3.
  Neutral
I'd like to store things like a configured virtual machine, which could be 10+ GB in size.
  Negative
From my understanding of the HTML5 file API, large files can be chunked up into small bits on the client and uploaded.
  Negative
It is then the responsibility of the server code to join the files together and move the file to S3.
  Negative
I understand the concept but am not sure of the best implementation.
  Negative
I'm using Heroku for the app server and I normally upload files directly to Amazon S3, skipping Heroku's servers completely.
  Negative
However, if I chunk the upload into small bits, I would have to have some code that joins the parts before actually putting it in S3.
  Neutral
But Heroku has some limitations on how much data can be used with them, and I don't think that joining a 10 GB file would work out effectively on their servers (not 100% sure but doubtful).
  Negative
So my current thought is that I have to have a web service app setup on an Amazon EC2 server where my app posts the upload parts to.
  Negative
The EC2 app is then responsible for joining the upload parts and putting the final joined file into S3.
  Negative
Once the file is loaded into S3, S3 sends a response to the original app hosted on Heroku, which then creates a resource that points to the stored file in S3.
  Negative
Is there any realistic way of getting around having a separate EC2 server to join the files?
  Negative
There's no cost for sending files between EC2 and S3, but I don't want to have to maintain 2 apps to accomplish what I want (main app on Heroku and the file-joining app on EC2).
  Negative
563e0f3a2d1761a701f0f6af	X	Amazon S3 API supports a multipart upload.
  Negative
File is automatically merged on the S3 side.
  Negative
I don't know how flexible the new html5 file API is.
  Negative
.
  Neutral
if you managed to upload a file directly talking to S3 (wow) you might also be able to use the multipart feature.
  Positive
May I ask for a sample of your current implementation?
  Neutral
Makes me curious How multipart uploads to S3 are working http://aws.typepad.com/aws/2010/11/amazon-s3-multipart-upload.html REST API for multipart uploads http://docs.amazonwebservices.com/AmazonS3/latest/dev/UsingRESTAPImpUpload.html The trickiest thing (if possible) probably will be to split a (large) file in the browser Before you start developing something for the sake of coolness make sure there isn't a more practical/pragmatic solution for your original problem EDIT: File slicing is possible - indeed: html5 rocks!
  Neutral
If you implement this well you can probably go for unlimited filesizes without exploding the users memory https://developer.mozilla.org/en/docs/DOM/Blob http://www.html5rocks.com/en/tutorials/file/dndfiles/#toc-slicing-files
563e0f3a2d1761a701f0f6b0	X	I have tried to upload a file to Amazon S3 using a presigned url, which works.
  Negative
Now I would like to use the Amazon API to upload the file.
  Negative
For convenience I added the library using the maven repository like this: I also wrote some colde that is supposed to upload the file to the server: However, I am never reaching the point where I can debug the code.
  Very negative
The compiler in Android Studio gives me this error, which I do not understand how to solve: Error:(36, 53) error: cannot access AmazonServiceException class file for com.amazonaws.AmazonServiceException not found I hope you can help :)
563e0f3b2d1761a701f0f6b1	X	Found the problem.
  Very negative
I Also had to include the aws-android-sdk-core library.
  Neutral
563e0f3b2d1761a701f0f6b2	X	I would take a look at the AWS Java SDK source for the same thing.
  Negative
Should be easy to find at github.com/aws/aws-sdk-java.
  Neutral
563e0f3b2d1761a701f0f6b3	X	why don't you want to use the SDK ?
  Negative
It contains many boiler plate code to handle signature etc that will avoid you to handle these low level details
563e0f3b2d1761a701f0f6b4	X	Do you have a solution for this?
  Negative
I'm trying to upload to S3 without the SDK.
  Negative
563e0f3b2d1761a701f0f6b5	X	@sebsto: The Amazon SDK for Android has 20K methods which would likely put you over the Dex limit.
  Negative
563e0f3b2d1761a701f0f6b6	X	@user2744821, hi.
  Negative
Did you made upload to S3 without SDK?
  Neutral
Please share some more code.
  Neutral
563e0f3b2d1761a701f0f6b7	X	I've been working on using the REST API of Amazon's S3 to upload a file from my Android device to a bucket I have.
  Negative
I have the KEY and SECRET_KEY, but am not sure how to properly generate the signatureValue they are looking for in their requests.
  Negative
I'm using a HttpPut to their servers, but am not sure how to properly generate the signatureValue.
  Negative
So far here's what I have: Then here are the methods I use to generate the signature value: I used the Amazon S3 Signature tester, and my string was correct, but I never got the right encoded value.
  Negative
Thanks for any help or a push in the right direction.
  Positive
563e0f3b2d1761a701f0f6b8	X	[Ref: https://aws.amazon.com/articles/1434] The link above also describes the input parameteres in the HTTP request.
  Negative
563e0f3b2d1761a701f0f6b9	X	I would double check that the date matches what is expected and sent in the http headers (are you setting the "x-amz-date" header?)
  Negative
, it gave me some headache when signing requests "manually".
  Negative
Also, adding the error message from S3 might help us to understand what is wrong and help you.
  Negative
563e0f3b2d1761a701f0f6ba	X	So I can!
  Positive
Thanks :-) do you know if this is a supported format?
  Neutral
563e0f3c2d1761a701f0f6bb	X	According to their doco it is.
  Positive
(docs.amazonwebservices.com/AmazonS3/latest/API/…)
563e0f3c2d1761a701f0f6bc	X	Disadvantage is that https won't work then
563e0f3c2d1761a701f0f6bd	X	I have created the file in an amazon S3 bucket.
  Negative
I know that the url format is: However I want to be able to work out what the 's3-eu-west-1' bit is without having to explicitly know this in my application.
  Negative
I have seen that in the API there is a call which I can make to get the location... But this only returns 'eu' so im wondering how to get the other parts.
  Negative
The less the user has to configure on the application the better :-)
563e0f3c2d1761a701f0f6be	X	You can just use {bucket}.
  Negative
s3.amazonaws.com.
  Neutral
i.e. http://ptedotnet.s3.amazonaws.com/UserContent/Uploads/54/26.jpg
563e0f3c2d1761a701f0f6bf	X	Use S3SignURL to make a signed URL
563e0f3c2d1761a701f0f6c0	X	I am building a Rails 4 app with admin uploads of large files to Amazon S3.
  Negative
To validate the transfer of the large files, I would like to include the Content-MD5 field in the request header per the Amazon docs: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html I started with Paperclip + S3 and the MD5 verification is working: With Paperclip, the large file is first uploaded to my server, then transferred to Amazon S3.
  Negative
This blocks the Rails process and consumes redundant bandwidth, so I am trying to use the S3UploadDirect gem to upload the file directly the S3 bucket: https://github.com/waynehoover/s3_direct_upload This gem is a wrapper around the code from Railscasts episode 383 and uses jquery-fileupload-rails for the actual upload: I can upload the file, but I cannot figure out how to pass the Content-MD5 information into the upload request header.
  Negative
563e0f3c2d1761a701f0f6c1	X	It's open sourced now
563e0f3c2d1761a701f0f6c2	X	Is there an open-source equivalent to the Amazon S3 storage service running under Linux?
  Negative
For example a bucket-based file system like: Thanks.
  Positive
563e0f3c2d1761a701f0f6c3	X	Riak CS is a new Amazon S3 API-compatible product for your own cloud.
  Negative
It's not open-source, but it may be a viable alternative for your consideration.
  Negative
563e0f3c2d1761a701f0f6c4	X	Eucalyptus is an open source attempt to provide EC2 and S3 type clouds, up to cloning the API of Amazon, as far as possible (i.e. you don't just get the functionality you mention, you get it in the same format as you'd make the calls to the real S3).
  Negative
I believe Walrus is what you're after.
  Neutral
OpenStack also does object storage.
  Negative
563e0f3c2d1761a701f0f6c5	X	Thank you for your answers and sorry for not being clear about my problem.
  Positive
Currently i have about 4TB of image files spread over 3 servers.
  Negative
My application spreads these files randomly.
  Negative
On which server the file is located, is stored at the mysql database (replicated to each of these servers).
  Negative
The image files are served by nginx and php directly from these servers (no proxy).
  Negative
When one of these servers crashes, i have no failover and no redundancy.
  Negative
Of course i can recover anything from backups, but 1 to 1.5TB of data on each server need much time to recover.
  Negative
After a bit of research, i found MogileFS as the optimal solution.
  Positive
563e0f3d2d1761a701f0f6c6	X	Im more concerned with am I sending the data right at the moment such as the use of "bucketString" in my little example.
  Negative
Also I notice in your example you have "data:" twice, doesn't one over write the other?
  Negative
563e0f3d2d1761a701f0f6c7	X	Sorry your right, I will update.
  Negative
563e0f3d2d1761a701f0f6c8	X	Since you appear new to Javascript, I would definitely recommend reading and listening to any content from douglas crockford, I't will enlighten you to the fundamentals (and nature) of Javascript - just don't take him as too seriously tho.
  Negative
563e0f3d2d1761a701f0f6c9	X	Also have a good look at the parameters in $.
  Positive
ajax http://api.jquery.com/jQuery.ajax/, you man not want to full REST due to browser compatibility and the parameter URL is just that - A url.
  Negative
Other HTTP headers are set though the other properties.
  Negative
563e0f3d2d1761a701f0f6ca	X	I am wanting to try and do a handful of things with the use of jQuery and Amazons S3 API via REST.
  Negative
My key issue is not being familiar with REST well enough (or not as well as I thought I knew) to know if this approach would even remotely work right.
  Negative
I have tried searching endlessly for some tidbit of an example and came up fruitless, maybe I am searching for the wrong things I don't know, but as a last ditch effort I figured I'd hit up my new favorite place, here.
  Positive
.
  Neutral
What I need to do is send.
  Neutral
PUT a request to the API to create the bucket.
  Positive
Based on the S3 API docs I came up with though concept isn't complete with the above I am just starting it out, and I started questioning if this idea of approach was even going to work.
  Negative
.
  Neutral
And if it is to work with the above or in any means provided here for help how would I also work with the response to know if it was successful or not?
  Negative
I know if I can nail this one piece down I can handle for the most part the rest of my issues to come.
  Positive
Its just tackling the first hump and figuring out if I am going about it the right way.
  Neutral
Its also worth mentioning that I have been tasked with doing this purely javascript style with or without the help of a lib like jquery.
  Positive
I can't use PHP, or the like in this concept.
  Negative
So if anyone can throw me a bone i'd be greatly appreciative.
  Negative
On a side note, does anyone know if theres a means of actually testing something like this stuff out without actually having a S3 account, cause I can't afford to pay for an account just for the sake of testing let alone any other reason.
  Negative
563e0f3d2d1761a701f0f6cb	X	Firstly, I am getting the feeling that you are quite new to consuming web-services client side.
  Negative
It is often best to start with something simple.
  Very positive
If I have a resource that returns a string... say test.html -> "Hello World!"
  Positive
And the URL for this web-service is some-realy-long-id.
  Negative
s3.amazonaws.com then we have the following: You must remember that requests from the browser follow the same-origin policy, so unless you are planning to use JSOP, or some other cross-domain hack you will run into trouble.
  Negative
p.s. another little piece of advice is to use right hand braces in Javascript as it performs semi-colon insertion (which will bite you if you return a object literal).
  Negative
Oh yes and a lot of old browsers do not support 'PUT' which you may need to consider.
  Negative
563e0f3d2d1761a701f0f6cc	X	Are you using an EC2 instance on Amazon?
  Neutral
If so, is it in the same region as your S3 bucket?
  Negative
Which instance type?
  Neutral
I handle image resizing in a similar way and it doesn't take that 23 secs, even for large images.
  Negative
563e0f3d2d1761a701f0f6cd	X	@user1091949 They are in the same region us-east.
  Negative
May be bandwidth can be an issue!For me its variable between 1mbps-2mbps.
  Negative
Your say?
  Neutral
563e0f3d2d1761a701f0f6ce	X	How big are the images?
  Neutral
Also, is the 23secs the length of time to transfer from your computer, plus the time it takes to resize?
  Negative
563e0f3d2d1761a701f0f6cf	X	The size may vary...The numbers I have put are for images upto 2 mb and around.But the application has no restriction for size yet(may be we have it later to around 10-12 mb) as it is in development right now.
  Negative
563e0f3d2d1761a701f0f6d0	X	I will study both the approaches rineez.The first looks more apt but will need a POC to check the use.Actually I ask the user to select a size from a set of defined sizes like original,1024x768,800x600,100x100 and then I show the image.So I will have to keep them ready before the user selects the options.That's what I do,I create the versions before the user can select and when he selects they come up easily.
  Very negative
563e0f3e2d1761a701f0f6d1	X	This question says something about image resizing and in memory representation,but I did not get how can I implement it in my scenario.
  Negative
563e0f3e2d1761a701f0f6d2	X	I have another issue with amazon and its related to file uploads.I am using jqueryFileUpload and amazon API's to uplaod files to amazon S3.I have succeeded in uploading it,but it involves a trick.
  Negative
I had to store the image on my server and then move it to S3 from there using putObjectFile method of S3.Now the plugin comes with great functions to crop/resize images and I have been using them since long.Now when I integrate the plugin with AWS,i am facing performance issues with upload.The time taken for uploads is longer than normal and this raises questions of us using AWS S3 over traditional way.
  Very negative
I had to make changes to my UploadHandler.php file to make it work.These are the changes made.i added a part of AWS upload code to the file from line 735 to 750 Here is a link to s3 class on git.
  Negative
The normal upload to my current server(not amazon),same image uploads in 15 secs,but on amazon S3 it takes around 23 secs and I am not able to figure out a better solution.I have to store the image on my sever before uploading to S3 as I am not sure if I can process them on the fly and upload directly to S3.
  Negative
Can anyone suggest the right way to approach the problem?Is it possible to resize the images to different sizes in memory and upload directly to S3 avoiding the overhead of saving it to our server?If yes can anyone guide me in the right direction?
  Negative
Thank you for the attention.
  Positive
563e0f3e2d1761a701f0f6d3	X	I believe the approximate 8secs is the overhead here for creating versions of image in different sizes.
  Negative
You may take different approaches to get rid of the resizing overhead at time of upload.
  Negative
The basic idea will be to allow the uploading script to finish execution and return the response, and do the resizing process as a separate script.
  Positive
I like to suggest following approaches: Approach 1.
  Negative
Don't resize during the upload!
  Neutral
Create resized versions on-the-fly only when it is being requested for the first time and cache the generated images to serve directly for later requests.
  Negative
I saw a few mentions of Amazon CloudFront as a solution in some other threads in Stackoverflow.
  Negative
Approach 2.
  Neutral
Invoke the code for creating resized versions as a separate asynchronous request after the upload of original image.
  Negative
There will be a delay in scaled versions being available.
  Negative
So write necessary code to show some place holder images in the website until the scaled versions become available.
  Neutral
You will have to figure out some way to identify whether scaled version is available yet or not(For example check file is existing, or set some flag in database).
  Negative
Some ways for making asynchronous cURL requests are suggested here if you would like to try it out.
  Negative
I think both approaches will have equal level of complexity.
  Neutral
Some other approaches are suggested as answers for this other question.
  Neutral
563e0f3e2d1761a701f0f6d4	X	Be careful, as stated here docs.google.com/support/bin/answer.py?answer=50092.
  Negative
"Important notes: Your storage quota includes plenty of bandwidth for ordinary use.
  Negative
If there's excessive bandwidth use, we may limit your access for a period of time."
  Negative
.
  Neutral
So Google Docs (G-Drive) might not be the right solution in this case
563e0f3e2d1761a701f0f6d5	X	really useful info!
  Negative
thx a lot
563e0f3e2d1761a701f0f6d6	X	If you use S3 or G or any other external provider, make sure you have them on your own servers as well, and have an automatic process to switch over in case the external provider stop working, catches fire, goes out of business, or just mucks up your account.
  Very negative
This requires that your image index has the local and external URLs on hand
563e0f3e2d1761a701f0f6d7	X	thank you again
563e0f3e2d1761a701f0f6d8	X	where did you find the =s parameter option?
  Negative
possible values I've found are 512-800.
  Positive
Is there any doc for the supported parameters?
  Neutral
thx
563e0f3e2d1761a701f0f6d9	X	Just look at the web pages that Google generates.
  Negative
Either in Google docs browser or Picasa (which uses same storage system)
563e0f3e2d1761a701f0f6da	X	I'd like to save some of my site monthly bandwidth allocation and I'm wondering if I can use Flickr PRO or I should rely on Amazon S3 as an hosting service for my web site images.
  Negative
(My Web Application allows users to upload their own pictures and at the moment it's managing around 40GB of data) I've never used Amazon's services and I like the idea of using Flickr REST Api do dynamically upload images from my webApp.
  Negative
I like also the idea of having virtually unlimited space to store images on Flickr for only 25$/year but I'm not sure if I can use their service on my web site.
  Negative
I think that my account can be banned if I use Flickr services to store images (uploaded by users of my website) that are not only for 'personal use'.
  Negative
What's your experience and would you suggest other services rather than Amazon's S3 or is this the only available option at the moment?
  Neutral
Thanks edit: Flickr explicitly says 'Don’t use Flickr for commercial purpose', you could always contact them to ask to evaluate your request but it sounds to me like I can't use their services to achieve what I want.
  Negative
S3 looks like the way to go then... Even though a rough estimate of what I'm going to spend every month is still scaring is there any cheaper place to host my images?
  Negative
563e0f3f2d1761a701f0f6db	X	400 images per user seems high?
  Negative
Is that figure from actual stats?
  Neutral
Amazon S3 is great and it just works!
  Very positive
A possible cheaper option is Google.
  Positive
Google docs now supports all file types, so you can load the images up to a Google docs folder, and share the folder for public access.
  Negative
The URL's are kind of long e.g. http://lh6.ggpht.com/VMLEHAa3kSHEoRr7AchhQ6HEzHVTn1b7Mf-whpxmPlpdrRfPW216UhYdQy3pzIe4f8Q7PKXN79AD4eRqu1obC7I Add the =s paramter to scale the image, cool!
  Positive
e.g. for 200 pixels wide http://lh6.ggpht.com/VMLEHAa3kSHEoRr7AchhQ6HEzHVTn1b7Mf-whpxmPlpdrRfPW216UhYdQy3pzIe4f8Q7PKXN79AD4eRqu1obC7I=s200 Google only charge USD5/year for 20GB.
  Negative
There is a full API for uploading docs etc
563e0f3f2d1761a701f0f6dc	X	I love amazon S3.
  Negative
There are so many great code libraries (LitS3) and browser plugins (S3Fox) and upload widgets (Flajaxian) that make it really easy to use.
  Positive
And you only pay for what you use.
  Negative
I use it a lot and have only ever experienced down time on one occasion.
  Negative
Nivanix is an s3 competitor.
  Negative
I haven't used them, but they have a bit more functionality (image resizing) etc.
  Negative
Edit:The link about Nivanix is dead now(2015/07/21), because Nivanix was dead.
  Negative
563e0f3f2d1761a701f0f6dd	X	Have you already gone through this solution: stackoverflow.com/questions/3871430/… ?
  Negative
563e0f3f2d1761a701f0f6de	X	It wasnt that particular post; but similar.
  Neutral
I ended up using the TransferUtility + the ConfigOptions to split the parts into the correct size
563e0f3f2d1761a701f0f6df	X	I am trying to upload a large file to Amazon S3.
  Negative
I first used the PutObject and it worked fine but took about 5 hours to upload a 2GB file.
  Negative
So I read some online suggestions and tried it with the TransferUtility.
  Negative
I have increased the timeout but this TransferUtility API always give me "The request was aborted.
  Negative
The request was canceled."
  Neutral
error.
  Negative
code sample:
563e0f3f2d1761a701f0f6e0	X	Try This
563e0f3f2d1761a701f0f6e1	X	I tried this and it fixes the problem.
  Negative
563e0f402d1761a701f0f6e2	X	This isn't work.
  Negative
The only way to set new cache-control headers is by "moving" the file, as the updated question says.
  Neutral
563e0f402d1761a701f0f6e3	X	My Django project uses django_compressor to store JavaScript and CSS files in an S3 bucket via boto via the django-storages package.
  Negative
The django-storages-related config includes This works except that when I visit the objects in the S3 management console I see the equals sign in the Cache-Control header has been changed to %3D, as in max-age%3D100000, and this stops caching from working.
  Negative
I wrote a little script to try to fix this along these lines: but this does not change the metadata as displayed in Amazon S3 management console.
  Negative
(Update.
  Neutral
The documentation for S3 metadata says After you upload the object, you cannot modify object metadata.
  Negative
The only way to modify object metadata is to make copy of the object and set the metadata.
  Negative
For more information, go to PUT Object - Copy in the Amazon Simple Storage Service API Reference.
  Negative
You can use the Amazon S3 management console to update the object metadata but internally it makes an object copy replacing the existing object to set the metadata.
  Negative
so perhaps it is not so surprising that I can’t set the metadata.
  Negative
I assume get_metadata is only used when creating the data in the first place.
  Negative
end update) So my questions are, first, can I configure django-storages so that it creates the cache-control header correctly in the first place, and second, is the metadata set with set_metadata the same as the metadata viewed with S3 management console and if not what is the latter and how do I set it programatically?
  Negative
563e0f402d1761a701f0f6e4	X	Use ASCII string as values solves this for me.
  Negative
563e0f402d1761a701f0f6e5	X	cache_control is a property of key, not part of metadata.
  Negative
So to set cache-control for all the objects in a bucket, you can do this:
563e0f402d1761a701f0f6e6	X	I am working on file sharing for objects stored on amazon S3.Now the path for the object stored on S3 is default like this https://s3.amazonaws.com/bucket_name/path_to_file/file_name.jpg/docx etc.Now I want to share these file URLs via email through my app.
  Negative
Currently when I share I see the entire URL as is in the email.I want it to be sent in an encoded form so that its hard to guess the exact location of the file.
  Negative
I am using PHP and I was planning to use base_64_encode/decode functions or md5 the URLs but not sure if thats the right way to go.
  Negative
So,I am looking for some tool or API (by amazon ot 3rd party) that can do it for me.
  Negative
I would also like to shorten the URLs while sharing.
  Negative
Would like to seek advice and guidance from someone implemented something similar.
  Neutral
Not sure if it comes under URL-REWRITING but tagging it under it.
  Negative
Thank you
563e0f402d1761a701f0f6e7	X	option 1: You can map your url and s3's url in your server, and give your url to user.
  Negative
When user make request to your url, redirect it to s3's.
  Negative
You can ref http://en.wikipedia.org/wiki/URL_redirection option 2: You can call API provided by url redirect service provider, e.g., http://tiny.cc/api-docs
563e0f412d1761a701f0f6e8	X	yes - should work.
  Negative
563e0f412d1761a701f0f6e9	X	It seems that amazon S3 .
  Negative
NET SDK is not supported in windows 8 store apps because of some restrictions in WinRT.Is there any other work around?
  Negative
In case I use REST api, can i use the WinJS.xhr function to call and process the web service?
  Negative
563e0f412d1761a701f0f6ea	X	This will work with the REST api.
  Neutral
Check out: Uploading Image to Amazon s3 with HTML, javascript & jQuery with Ajax Request (No PHP) http://aws.amazon.com/articles/1434 Note that in WinJS you DO NOT have to deal with CORS here as HTML/JS Windows Store apps do not have the cross domain restriction that you have in the browser.
  Very negative
http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html You can also test this out using Fiddler's composer (http://fiddler2.com/) to ensure you have the proper request and then recreate in your JavaScript
563e0f412d1761a701f0f6eb	X	would you mind sharing the upload script (sans API docs).
  Negative
It would be helpful to see how you are sending the temporary file up to S3
563e0f412d1761a701f0f6ec	X	I wrote that for my previous employer, I don't have access to the code anymore.
  Negative
It was pretty basic usage of the API though, nothing too fancy!
  Negative
563e0f412d1761a701f0f6ed	X	I'm generating a PDF file using the PdfLib PHP extension.
  Negative
I start a new file, load an image into it, and call end_document to finish the file.
  Positive
All is working fine, the file is generated correctly, but I'm trying to upload the file to the Amazon S3 cloud immediately (in the same function that generates the file) after generating it.
  Negative
The file is uploaded but it is not complete, seems like an empty pdf is uploaded.
  Negative
The location where I keep the file locally contains the file as it should be: the image is loaded into it and the file is about 600KB in size.
  Negative
The uploaded file is like an empty skeleton, the file that was generated when I called PdfLib->begin_document but before anything is actually placed inside it, I can't open it and it is 104 Bytes in size.
  Negative
I tried sleeping for several seconds after end_document and before the upload, because I thought maybe PdfLib needed some time to finish things up, but this has no effect, the results are exactly the same.
  Negative
Does anyone have any ideas on how to fix this, what can cause this, where to look for a solution?
  Neutral
UPDATE The problem seems to be the S3 api class I'm using.
  Negative
I was using this one.
  Neutral
Using the official SDK the upload succeeds as expected.
  Positive
I will continue using the official SDK.
  Positive
The original question, why it failed using the other API class, remains.
  Negative
563e0f422d1761a701f0f6ee	X	I'd like to securely display a grid of thumbnail images to an authenticated user on our site.
  Negative
All the images will be stored in Amazon S3.
  Neutral
One way, I suppose, is to implement "security by obscurity" by uploading these images with public read access, and making the keys long and random.
  Negative
I also could set up ACLs, but then I'd have to disclose the access key in the url (I think), or pull the image into my application via the API and display it securely through the web server.
  Negative
Is there a preferred way to do this?
  Neutral
And to be able to display the images quickly without requiring tremendous requests to S3 from the server every time a page is generated?
  Negative
Thanks in advance
563e0f422d1761a701f0f6ef	X	You can generate urls to s3 with an expiry date.
  Negative
Generating such a URL does not require a request to S3 and does not result in the disclosure of your secret key: you use your secret key to generate a signature that is appended to the URL (the access key id is in that URL but that's ok) See the docs on query string authorization
563e0f422d1761a701f0f6f0	X	is there has any methods to limit incoming buffer size ?
  Very negative
563e0f422d1761a701f0f6f1	X	You can manage the buffers sizes and bandwidth by using response.pause() and response.resume().
  Negative
This will cause the TCP algorithms to back off appropriately.
  Negative
(see nodejs.org/api/http.html#http_response_pause )
563e0f422d1761a701f0f6f2	X	thanks for help, I will try it :)
563e0f432d1761a701f0f6f3	X	I want provide a rest service api by restify ( or express ) solution1: using request stream pipe to put files to amazon by knox solution2: using formdiable and fs to pre store file on server local temp folder and then put files to amazon by knox which way is better between solution 1 and solution 2 or other better way ??
  Very negative
and does here has any performance issues need to know?
  Neutral
563e0f432d1761a701f0f6f4	X	Depending on the server load and your available bandwidth, your diskio is your biggest potential bottleneck.
  Negative
I would say go with solution 1 that streams the file directly and doesn't store it to physical media.
  Negative
Make sure you don't attempt to store the entire object to memory either or you could run into a system resources problem.
  Neutral
563e0f432d1761a701f0f6f5	X	is there a way to use this with django-storages?
  Negative
563e0f432d1761a701f0f6f6	X	@DataGreed I am not sure, I understand what you mean, but I can tell you that boto works with django-storages and I use the latter in my application to redirect default file storage to Amazon S3
563e0f432d1761a701f0f6f7	X	In my Django application (RESTful API with DjangoRestFramwework), I have to store images on Amazon S3.
  Negative
Due to some technical constraints, we have to use direct upload by a client application using signed URLs.
  Negative
Everything works fine but after the upload is performed by the client, the content type is always set to binary/octet-stream, while I naturally want it to be, say, image/jpeg.
  Positive
As it is mentioned in the Amazon S3 documentation here, response-content-type header has to be added to the signed URL in order to ensure the Content-Type of the response by S3 will be correct.
  Negative
Here is my code for signing the URL (written according to boto documentation): This code works almost fine: it generates a URL which I then successfully use for upload.
  Negative
The curl request is: Unfortunately, when I access the file, the content type is still wrong (binary/octet-stream).
  Negative
Am I doing anything wrong?
  Negative
563e0f432d1761a701f0f6f8	X	Today, after a lot of experiments I found an answer.
  Negative
I was inspired by... official Amazon S3 documentation :) Here is the URL signing code: The principal change here is the headers section, where I put 'Content-Type': 'image/jpeg' ('x-amz-acl' : 'public-read' is also useful since it makes your file publicly available for reading).
  Negative
Then you have to place Content-Type header to your HTTP request.
  Neutral
In my case, I used curl as follows: Enjoy!
  Negative
563e0f432d1761a701f0f6f9	X	I am not sure if HEAD OBJECT will get it but I found a solution to it.
  Negative
Please find my answer below.
  Negative
563e0f432d1761a701f0f6fa	X	It is actually very easy to get the MIME type of objects stored in S3.
  Positive
All you have to do is response = _s3.GetObject(request); string MIMEtype = response.ContentType; response.ContentType will then get the MIME type from s3.
  Negative
We are using Cloud berry explorer pro to upload objects and to maintain our s3 account.
  Negative
It has a lot of default MIME types and it assigns that to objects as you upload them.
  Positive
You can also add your own MIME types.
  Neutral
You can specify the extension and the MIME type, so the next time you upload something it automatically assigns a MIME type to that object.
  Positive
563e0f432d1761a701f0f6fb	X	@Abhi.
  Neutral
Net: While GetObject() works fine in principle, @Roman is correct to suggest HEAD resp.
  Neutral
GetObjectMetadata() here, which is significantly more efficient for your use case - see my update to his answer for details.
  Positive
563e0f442d1761a701f0f6fc	X	Thank you guys for your solution.
  Positive
Now, I know when to choose GetObject() and GetobjectMetaData().
  Neutral
Although in my case I have to retrieve the objects(Diff types- css, html,jpeg, gif, flash) so I will have to go with GetObject() but 'll keep GetGetobjectMetaData() in my mind in case I just need the meta data.
  Negative
563e0f442d1761a701f0f6fd	X	Is there a way to retrieve the MIME types of the objects in S3.
  Neutral
I am trying to implement a solution in which I will be getting multiple objects from S3.
  Negative
Instead of using there key and then getting a sub string to calculate the MIME type, can I get the MIME type from Amazon S3 in some way?
  Negative
I am using cloud berry explorer pro and I know it let's you set the MIME type, but how do we retrieve this information using the AWS SDK for .
  Negative
NET or the REST API?
  Neutral
563e0f442d1761a701f0f6fe	X	The REST API offers the HEAD Object operation for this purpose and the AWS SDK for .
  Negative
NET conveniently wraps the very same functionality via the GetObjectMetadata() method: The GetObjectMetadata operation is used to retrieve information about a specific object or object size, without actually fetching the object itself.
  Negative
This is useful if you're only interested in the object metadata, and don't want to waste bandwidth on the object data.
  Negative
The response is identical to the GetObject response, except that there is no response body.
  Negative
[emphasis mine]
563e0f442d1761a701f0f6ff	X	To get the file and mimeType of the file in the same request...
563e0f442d1761a701f0f700	X	Use this: blueimp.github.com/jQuery-File-Upload
563e0f442d1761a701f0f701	X	@apneadiving that worked exactly how I wanted it to.
  Negative
The plugin page had some very helpful examples that pointed me in the right direction and I've implemented exactly what I want.
  Positive
If you make an answer to the question instead of a comment, I will gladly accept your answer.
  Positive
Thank you very much, you helped save me a lot of effort.
  Positive
563e0f442d1761a701f0f702	X	Nice to read :)
563e0f442d1761a701f0f703	X	Can you share how you did this with multiple files and S3?
  Negative
I can upload single files, but can't quite figure out how to upload more than one at a time.
  Negative
563e0f452d1761a701f0f704	X	If you want to write code to make all of the different S3 API calls client side, generate the policies, handle the specific S3 errors, etc, yourself then use jquery file upload.
  Negative
If you want something that just works for simple to extremely complex upload to S3 workflows, use Fine Uploader.
  Negative
563e0f452d1761a701f0f705	X	Whilst this may theoretically answer the question, it would be preferable to include the essential parts of the answer here, and provide the link for reference.
  Negative
563e0f452d1761a701f0f706	X	@IlmariKaronen I posted upon asker request.
  Negative
But feel free to downvote if you like
563e0f452d1761a701f0f707	X	Someone flagged your answer as "not an answer".
  Negative
I don't quite agree, but it is pretty borderline.
  Neutral
That said, the question isn't exactly the clearest one ever, either.
  Neutral
563e0f452d1761a701f0f708	X	This issue has been bothering me for many hours and I can't seem to find a solution to it.
  Negative
I have a rails 3.2 app that allows users to upload files to an Amazon S3 account using carrierwave_direct, fog, and carrierwave (dependency for carrierwave_direct).
  Negative
Using carrierwave_direct allows the user to skip uploading the file to the server by POSTing it directly to Amazon S3 (saves server processing and timeouts like Heroku for large files).
  Negative
It works fine if all you do is select 1 file, upload it to Amazon, and want a redirect_to a URL you provide Amazon.
  Positive
It does this by POSTing the form to Amazon S3, and Amazon responds to a provided URL (you specify this URL in your form) with some params in the URL, which are then stored as a pointer to the file on Amazon in your model.
  Negative
So the lifecycle is: select 1 file, POST to Amazon, Amazon responds with a URL that sends you to another page, and you can then save a record with a pointer to the Amazon file.
  Negative
What I've been trying to figure out is how do I allow multiple files to be selected and uploaded and update the upload progress?
  Negative
I'm trying to do this with pure javascript (using the file API provided by modern browsers) so I don't want any 3rd party tools.
  Negative
Also, in the pursuit of learning this in-depth, I'm avoiding any plugins and am trying to write the code myself.
  Negative
The functionality I'm trying to obtain is: At this point, I could even do without an individual progress bar; I'd be happy just to get multiple files POSTed to Amazon S3 without page refreshes.
  Negative
I am not partial to any of the gems.
  Negative
I'm actually afraid I'm going to have to write what I want to do from scratch if I really want it done in a specific way.
  Negative
The goal is multiple file uploads to an Amazon S3 account via AJAX.
  Negative
I would be ecstatic with even general concepts of how to approach the problem.
  Negative
I've spent many hours googling this and I just haven't found any solutions that do what I want.
  Negative
Any help at all would be greatly appreciated.
  Neutral
EDIT 2014-03-02 Raj asked how I implemented my multiple upload.
  Negative
It's been so long I don't recall all the "why" behind what I did (probably bad code anyway as it was my first time), but here is what I had going on.
  Negative
The model I was uploading was a Testimonial, which has an associated image being stored in Amazon S3.
  Negative
It allowed a user to select multiple images (I think they were actually PDF files I converted to images) and drag/drop them onto the screen.
  Negative
While uploading, I displayed a modal that gave the user feedback about how long it would take.
  Negative
I don't pretend to remember what I was doing on a lot of this, but if it helps feel free to use it.
  Negative
Here's the view: And the JS: The Testimonial model:
563e0f452d1761a701f0f709	X	As advised in comment, use jQuery Upload: http://blueimp.github.com/jQuery-File-Upload/
563e0f452d1761a701f0f70a	X	I have started writing a basic library for this functionality.
  Negative
I have a working version on github, and am writing a series of blog posts to detail how to achieve this.
  Negative
'Working' code can be found at : https://github.com/joeandrews/s3multipartupload.
  Neutral
I am in the process of writing a series of blog posts, which when complete will be posted on http://blog.fuuzik.com.
  Negative
I hope this helps in the mean time though.
  Neutral
563e0f452d1761a701f0f70b	X	Thanks!
  Positive
Now, as I am trying to create this policy and later sign this using my amazon access key.
  Neutral
How do I ensure that my indentation and the indentation which will be generated at Amazon's end will be same?
  Neutral
So that, ultimately, my signature matches with that of Amazon's.
  Positive
563e0f452d1761a701f0f70c	X	@TriptiR Just use the formatting what you see in Amazon's text (e.g. 2 spaces for indenting, arrays in one line, space after ':' and ',' etc.).
  Negative
563e0f452d1761a701f0f70d	X	I found one issue with in the JSON policy itself(1 parameter was different).
  Negative
I tried something else after your suggestion, I decoded the original base64 string from amazon tutorial by using [link](base64encode.org) and re-encoded that using my program.
  Negative
Though I cant see any difference in indentation now, the base64 encoded strings are still different.
  Negative
563e0f462d1761a701f0f70e	X	@TriptiR Edited the answer, there are 2 differences besides indentation.
  Negative
563e0f462d1761a701f0f70f	X	Thanks, as it was unsuitable to change the question itself.
  Negative
Still, I couldn't get it working even after correcting this.
  Negative
In steps for creating a POST policy, there is a requirement - "Create a policy using UTF-8 encoding".
  Negative
I believe, I am missing something at this point.
  Negative
How to ensure that my JSON-policy is created using UTF-8 encoding?
  Neutral
563e0f462d1761a701f0f710	X	I am trying to upload a image file to amazon s3.
  Negative
The setup is as follows: Web server: golang Front-end: simple html form for testing In reference with this document : http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html I referred to the example provided in the above document and tried this: http://play.golang.org/p/3zn5fSDasK The result I got with this The base 64 encoded policy which is on link of amazon tutorial: Why my base64 encoded policy doesn't match with that of Amazon's?
  Negative
563e0f462d1761a701f0f711	X	Simply because the 2 JSON source texts which produce those base64 strings have different indentation and different content as seen below (they are not equal char-by-char).
  Negative
Decode both of the base64 strings, and you will see the differences.
  Positive
You can do that either with a program (not necessarily Go), or simply use an online service like this one.
  Negative
You shouldn't worry about indentation, it doesn't matter in JSON.
  Negative
But when you encode a text using Base64, that encodes all characters of the source including spaces and tabs used to indent, so different indentation will result in different Base64 encoded forms.
  Negative
But comparing the 2 decoded JSON, there are also other differences: First one: 2nd one: The complete decoded JSON texts: The first one: And the 2nd one:
563e0f462d1761a701f0f712	X	Seems like you are having same issue : stackoverflow.com/questions/30778579/…
563e0f462d1761a701f0f713	X	possible duplicate of Transport Security has Blocked a cleartext HTTP
563e0f462d1761a701f0f714	X	You should take a look at a post on AWS Mobile Development Blog.
  Very negative
563e0f462d1761a701f0f715	X	Thanks it worked!
  Positive
But is that enough?
  Neutral
Looking at: mobile.awsblog.com/post/Tx2QM69ZE6BGTYX/… they suggest to add some more stuff
563e0f462d1761a701f0f716	X	I am trying to load images from amazon s3 (async).
  Negative
But I get these errors in my log: NSURLSession/NSURLConnection HTTP load failed (kCFStreamErrorDomainSSL,-9802) I am making all my api calls with "https", it worked in ios 8.3 / xcode6 So
563e0f472d1761a701f0f717	X	You are running into the HTTPS errors of App Transport Security read here: Transport Security has Blocked a cleartext HTTP But the basic jist is add the following to your info.plist You can do that via a text editor or in XCODE as such: 
563e0f472d1761a701f0f718	X	Add the following to your info.plist exactly as typed.
  Negative
NSAppTransportSecurity Once you create that make its value a dictionary called NSAllowsArbitraryLoads Set the value to true or yes
563e0f472d1761a701f0f719	X	You are running into the HTTPS errors of App Transport Security read here: Transport Security has Blocked a cleartext HTTP But the basic jist is add the following to your info.plist
563e0f472d1761a701f0f71a	X	Correct, There is no limitation over number of object stored on Amazon S3 Bucket.
  Negative
For large file you can upload your data in parts and then merge when all parts get uploaded.
  Positive
I am one of the Developer of Bucket Explorer, Supporting Multipart operation for Amazon S3 Object using Amazon Multipart API for Upload as well as Exclusive Multipart Download Operation.
  Negative
563e0f472d1761a701f0f71b	X	What is the size of data that can be sent using the GET PUT methods to store and retrieve data from amazon s3 cloud and I would also like to know where I can learn more about the APIs available for storage in Amazon S3 other than the documentation that is already provided.
  Very negative
563e0f472d1761a701f0f71c	X	The PUT method is addressed in the respective Amazon S3 FAQ How much data can I store?
  Negative
: The total volume of data and number of objects you can store are unlimited.
  Negative
Individual Amazon S3 objects can range in size from 1 byte to 5 terabytes.
  Negative
The largest object that can be uploaded in a single PUT is 5 gigabytes.
  Negative
For objects larger than 100 megabytes, customers should consider using the Multipart Upload capability.
  Negative
[emphasis mine] As mentioned, Uploading Objects Using Multipart Upload API is recommended for objects larger than 100MB already, and required for objects larger than 5GB.
  Negative
The GET method is essentially unlimited.
  Neutral
Please note that S3 supports the BitTorrent protocol out of the box, which (depending on your use case) might ease working with large files considerably, see Using BitTorrent with Amazon S3: Amazon S3 supports the BitTorrent protocol so that developers can save costs when distributing content at high scale.
  Negative
[...]
563e0f472d1761a701f0f71d	X	Possible related: stackoverflow.com/questions/23098223/…
563e0f472d1761a701f0f71e	X	I'm building an app on the MEAN (MongoDB, Express, AngularJS, node.js) stack that requires uploading image files to Amazon S3.
  Very negative
I'm doing it in the following way: First, an http get is sent to my API, which specifies the 'policy document' for the interaction and returns it to the AngularJS frontend.
  Negative
That backend code looks like this (with the variables filled in): The frontend (AngularJS) code for this process looks like this: However, as it is currently, if I post an image I get the following error: If I change the http POST call to the AngularJS shorthand and remove the "content-type": "multipart/form-data" specification, like such: I get the following error instead: I have tried any combination of conditions and specifications that I can think of.
  Very negative
What am I missing here?
  Neutral
563e0f472d1761a701f0f71f	X	I believe the problem was with the format of the "file" I was attempting to upload.
  Negative
I was using this AngularJS directive, and the format it returns resized images in isn't as an actual file.
  Negative
I'm still using that same directive, but no longer using the image resizing.
  Negative
I changed the structure of my upload process to do most of the work on the node.js backend.
  Negative
The AngularJS function looks like this: And the node.js/express endpoint to post looks like this: This is now using Amazon's AWS sdk for node.js.
  Negative
563e0f482d1761a701f0f720	X	Recently i also faced to the same problem.
  Negative
You need to do convert a dataURI to a Blob.
  Negative
And upload.
  Positive
Upload the file returned from this function.
  Negative
Its uploading fine with S3 upload. }
  Positive
563e0f482d1761a701f0f721	X	I got that much, however it doesn't solve the real problem of SL4 not accessing the clientaccesspolicy.xml file.
  Negative
I'll add an edit, that if I force the end point to point to http instead of https required by amazon, fiddler does show that it access the file, so it appears to only be an https issue.
  Very negative
563e0f482d1761a701f0f722	X	Did you check out the answers here?
  Neutral
stackoverflow.com/questions/2449737/…
563e0f482d1761a701f0f723	X	This is an odd one.
  Negative
I'm using Amazon S3 for storage of files in my Silverlight 4 application.
  Negative
Because of the file restrictions associated with the REST API and S3 (files have to be < 1mb for REST), I'm trying to get the SOAP calls to work.
  Negative
I followed the tutorial written by Tim here http://timheuer.com/blog/archive/2008/07/05/access-amazon-s3-services-with-silverlight-2.aspx minus the parts about CNAME's since he updated and said it was bad to do that for security, but kept having issues connecting till it just magically started working this morning and I was able to get a list of all my buckets!
  Negative
So I thought it was fixed, until a few minutes ago when I restarted Chrome and then tried the application again, and it no longer connected to the SOAP endpoint and VS gave me the cross-domain error.
  Negative
However, I thought about all the stuff I had done earlier to get it working, and the only thing I could think of was that I had a tab open with the clientaccesspolicy.xml file open via bucket.s3.amazonaws.com/clientaccesspolicy.xml.
  Neutral
So I tried opening it up again in a new tab, opened my application in another, and then the SOAP calls started working!
  Very positive
It only works when the file is open in a tab!!!
  Neutral
I've tried it in Firefox and IE as well, same thing!
  Negative
I have Fiddler, and it doesn't seem to actually ever make a call to the clientaccesspolicy.xml, unless it's hidden inside one of the SSL calls which then there's no way to tell, but there's no straight out calls to .
  Negative
s3.amazonaws.com/clientaccesspolicy.xml going through Fiddler like some other questions on here said there would be.
  Negative
Would really appreciate some help here guys, thanks.
  Positive
Edit: Since someone will probably ask for it, this is the clientaccesspolicy.xml file I'm currently using.
  Negative
I know it's not the most secure, just trying to get this to work before I take the wildcards out Edit 2: This appears to be an issue HTTPS.
  Negative
If I force my endpoint to be http, instead of the https required by Amazon, Fiddle does show SL accessing the clientaccesspolicy.xml file.
  Negative
563e0f482d1761a701f0f724	X	When you open the clientaccesspolicy.xml file in another tab I'm guessing you are passing some credentials which allows you to access it.
  Positive
This sets a cookie which Silverlight can then use to access the clientaccesspolicy.xml file as well.
  Negative
When you close the browser you lose the cookie and thus the access to the file.
  Negative
563e0f482d1761a701f0f725	X	So I figured it out.
  Negative
The first problem, about why it would work if I opened it, was not because of a cookie being set (per say), but that accessing it that way over https made me accept the SSL security policy for amazon.
  Negative
The second problem, I shouldn't have had to accept it.
  Negative
The SSL wildcard amazon uses, *.
  Negative
s3.amazonaws.com, doesn't match buckets that contain periods in them.
  Negative
So as I was following Tim's tutorial I made all my buckets like this bucketname.domain.com and when I tried to access it that way through SOAP (and subsequently https), it wasn't working because the wildcard wasn't being matched.
  Negative
Changed all my buckets to contain no buckets, and it worked.
  Negative
Should also note that Tim's tutorial no longer works as he's using http and in June of this year Amazon forced SOAP calls over https, so http calls no longer work.
  Negative
563e0f482d1761a701f0f726	X	What does $e->getMessage() say?
  Negative
563e0f482d1761a701f0f727	X	Does the user own the bucket?
  Negative
Also, are the credentials from an IAM user?
  Negative
If so, does their policy allow the putObject operation?
  Neutral
563e0f482d1761a701f0f728	X	Do not use file_get_contents(), that will load your whole file into memory unnecessarily.
  Negative
Do 'Body' => fopen(drupal_realpath($file_path), 'r') or 'SourceFile' => drupal_realpath($file_path)
563e0f482d1761a701f0f729	X	I am trying to integrate PHP Amazon SDK - s3 api.
  Negative
I am giving the correct credentials.
  Neutral
But I am not sure about putObject parameters.
  Negative
This is the code that I tried: After make the request I am getting this Response:
563e0f492d1761a701f0f72a	X	If you are looking to code it, why are you talking about clones?
  Negative
I don't see anything special in your requirements.
  Negative
What's the question?
  Neutral
563e0f492d1761a701f0f72b	X	I really don't know why I mentioned the clone, but my question pretty much is what would you recommend for modules and or programs for my requirements.
  Negative
563e0f492d1761a701f0f72c	X	Are you asking for advice about setting up the service or advice about programming an interface to such a service?
  Neutral
What is your question?
  Neutral
563e0f492d1761a701f0f72d	X	Advice about programming such a service to do what I want.
  Positive
For example for the USERDB, I could use PostgreSQL and DBIx::PgLink from cpan but id like advice of what would be better to use than PostgreSQL and the module I mentioned if there is even something better to use.
  Negative
563e0f492d1761a701f0f72e	X	Thanks, But I was already planning on using MogileFS for the storage.
  Negative
563e0f492d1761a701f0f72f	X	Good stuff.
  Positive
Two great mind think a like ;-) Or fools never differ :(
563e0f492d1761a701f0f730	X	As MogileFS isn't mentioned anywhere here uptil now then this answer is very useful as a reference for future seekers.
  Negative
563e0f492d1761a701f0f731	X	I am rethinking about using mogilefs for this, because it seems to be for a few servers with not a whole lot of storage.
  Negative
I am planing to be running 400+TB per rack.
  Negative
But now I am not sure what to use for storage.
  Negative
563e0f492d1761a701f0f732	X	I am looking to code a file storage application in perl similar to amazon s3.
  Negative
I already have a amazon s3 clone that I found online called parkplace but its in ruby and is old also isn't built for high loads.
  Negative
I am not really sure what modules and programs I should use so id like some help picking them out.
  Negative
My requirements are listed below (yes I know there are lots but I could start simple then add more once I get it going): Thanks in advance
563e0f492d1761a701f0f733	X	Perhaps MogileFS may help?
  Negative
Also there was a recent discussion about MogileFS performance on the Google Groups / Mailing list which maybe of interest to you.
  Negative
/I3az/
563e0f492d1761a701f0f734	X	here I found a ruby impl https://github.com/jubos/fake-s3 hope that helps mike
563e0f492d1761a701f0f735	X	I have created a super simple server, see the put routine in Photo::Librarian::Server.pm it supports the s3cmd put of a file, nothing more for now.
  Negative
https://github.com/h4ck3rm1k3/photo-librarian-server https://github.com/h4ck3rm1k3/photo-librarian-server/commit/837706542e57fbbed21549cd9e59257669d0220c
563e0f4a2d1761a701f0f736	X	The output of Hive on EMR is a file named 000000_0 (perhaps a different number if there is more than 1 reducer).
  Negative
How do I get this file to be named differently?
  Neutral
I see two options: 1) Get Hive to write it differently 2) Rename the file(s) in S3 after it is written.
  Negative
This is could be a problem: from what I've read S3 doesn't really have a "rename".
  Negative
You have to copy it, and delete the original.
  Negative
When dealing with a file that is 1TB in size, for example, this could cause performance problems or increase usage cost?
  Neutral
563e0f4a2d1761a701f0f737	X	The AWS Command Line Interface (CLI) has a convenient mv command that you could add to a script: Or, you could do it programmatically via the Amazon S3 COPY API call.
  Negative
563e0f4a2d1761a701f0f738	X	I have my bucket on Amazon S3 filled with lot of images.
  Negative
I want to develop a API that would hotlink all the images to my website.
  Negative
For this i want to write a code that would fetch the URL`s for all the images from bucket into a PHP - array.
  Neutral
I could`nt find code that would dynamically fetch the URL of all the files in bucket without passing the file name.
  Negative
Waiting for help !!
  Neutral
563e0f4a2d1761a701f0f739	X	The best place to get the info is here: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/service-s3.html Basically you create an iterator off of the bucket and then for each one you get the object URL.
  Positive
563e0f4a2d1761a701f0f73a	X	How can i transfer file amazon s3 to youtube using php api, also is there available to set bitrate when uploading.
  Negative
563e0f4b2d1761a701f0f73b	X	Relevant link on Stack Overflow: stackoverflow.com/questions/2576923/dropbox-com-api-for-net and external link dkdevelopment.net/2010/05/18/…
563e0f4b2d1761a701f0f73c	X	Is there an online backup service (like Dropbox), which supports a C# or JAVA API?
  Negative
I want to use this as part of a build process to host my code offsite.
  Neutral
Thanks
563e0f4b2d1761a701f0f73d	X	You can take a look at Amazon S3 API, it can be accessed with WebServices.
  Negative
If you just want to store code and keep different versions you should use versionning system such as Subversion, git or mercurial.
  Negative
Look at the links below for more informations.
  Neutral
Resources : On the same topic :
563e0f4b2d1761a701f0f73e	X	If it is for code, why not use a hosted SVN like Assembla or Codesion?
  Negative
563e0f4b2d1761a701f0f73f	X	If you want to just store your data so please look at this below link.
  Negative
Login to this site and store your data.
  Negative
It's look like windows desktop with my computer and drives.
  Negative
http://www.theskypc.com
563e0f4b2d1761a701f0f740	X	I have a scenario in which multiple lines of text are to be appended to an existing text file... Is it possible to do this using Jclouds?
  Negative
(That would be ideal for me as jclouds supports a lot of cloud providers)... Even if this is not doable using jclouds, does the native API of Amazon S3/Rackspace Cloudfiles/Azure storage support appending content to existing blobs?
  Negative
If this is doable, then kindly point me to good working examples which show the same...
563e0f4b2d1761a701f0f741	X	This is not possible in the underlying blob stores I know of.
  Negative
563e0f4c2d1761a701f0f742	X	This typically happens if the remote server doesn't speak SSL.
  Negative
563e0f4c2d1761a701f0f743	X	It is working fine at another server.
  Positive
563e0f4c2d1761a701f0f744	X	Your cURL wasn't compiled with support to perform SSL requests.
  Negative
Recompile using proper binaries.
  Neutral
563e0f4c2d1761a701f0f745	X	Should I need to reinstall cURL ?
  Negative
563e0f4d2d1761a701f0f746	X	Could you add to your question the configure line?
  Neutral
E.g. .
  Neutral
/configure --with-curl ... etc from the php info page.
  Negative
563e0f4d2d1761a701f0f747	X	it actually needs to save other libraries.
  Negative
I have found this solutions a year later :)
563e0f4d2d1761a701f0f748	X	Issue is generally occurred due to upgrade PHP 5.2 to 5.5
563e0f4d2d1761a701f0f749	X	I'm trying to test in PHP Amazon S3 on my localhost on Ubuntu system but keep getting the same error: S3::listBuckets(): [35] error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol It is the function to display bucket list.
  Very negative
Here is the Amazon API function that has been called by this function.
  Negative
563e0f4d2d1761a701f0f74a	X	It seems that you have changed your PHP version as this bug is occurred several time in PHP 5.4 but it works perfectly in previous versions.
  Positive
You can re-install cURL with Open SSL again.
  Negative
563e0f4d2d1761a701f0f74b	X	Is there some reason the CMS deployment can't bundle the files?
  Negative
Validating/Downloading one bundle of 3MB will be far more efficient than verifying 100 files and downloading each individually.
  Negative
563e0f4d2d1761a701f0f74c	X	I already do that to some extent.
  Negative
saving the date of the last download and, the next time, downloading only those files which have been modified on the server since.
  Negative
563e0f4e2d1761a701f0f74d	X	Well, an approach of that sort is probably all you can really do.
  Negative
563e0f4e2d1761a701f0f74e	X	I have an iphone app that sometimes has to download a set of files from a bucket on a Amazon AWS S3 account.
  Negative
A typical such download will involve maybe 100 files.
  Negative
Most of these files are very small though and all combined, we are still under 3MB.
  Negative
At the moment, I use the listObjectsInBucket function and then loop on all files and use the API/SDK function getObject to get them one by one.
  Negative
The problem is that it takes a very long time to do it that way so I would like to have some advice regarding a faster strategy that would work in my scenario (many small files that have to stay available individually so that they can be modified by a CMS).
  Negative
Thanks in advance.
  Neutral
563e0f4e2d1761a701f0f74f	X	If you want the files to remain resident in the application's local storage, it might make sense to store all the files locally along with metadata on the files (i.e. checksum, last modified timestamp, etc.).
  Negative
You could then compare this metadata against metadata you store in S3 metadata fields, syncing up only those files where the metadata differs.
  Negative
563e0f4e2d1761a701f0f750	X	Great answer - thank you for that.
  Positive
Max.
  Neutral
563e0f4e2d1761a701f0f751	X	I can set up my EC2 instances so that certain users other than myself are allowed to SSH in.
  Negative
Is there anyway of achieving a similar situation with S3 in giving certain users access to buckets without revealing the Access ID and Secret keys?
  Negative
Thanks for any help.
  Positive
Max.
  Neutral
563e0f4e2d1761a701f0f752	X	Yes, the method to do this is described in the Access Control Lists documentation.
  Negative
Note that when using ACLs, the other users to whom you grant permissions must have their own Amazon S3 account.
  Negative
If your users don't have their own account, see:
563e0f4e2d1761a701f0f753	X	Why is $bucket = 'BUCKET'?
  Negative
Shouldn't it be 'gf-redshift-upload-dev'?
  Negative
563e0f4f2d1761a701f0f754	X	Apologies simply missed putting the arn as BUCKET not to display what it actually was.
  Negative
563e0f4f2d1761a701f0f755	X	Add "Version":"2012-10-17" to your policy, and change "arn:aws:s3:::BUCKET*" to "arn:aws:s3:::BUCKET/*" then retry.
  Negative
You may also want to use the IAM policy simulator or install the awscli and test basic S3 commands while you debug your policy.
  Negative
563e0f4f2d1761a701f0f756	X	First off Im not php so please be kind !
  Negative
due to failings within SSIS and losing patience I thought of using php as a go between to help move and retrieve files from the AMAZON S3 bucket.
  Negative
Theres an access issue that we cant seem to get round Steps so far WE use IIS The Amazon PHP SDK was downloaded user C:\inetpub\wwwroot\phpaws a index.php file was created the code Then when the API is ran we get on both post and get error msg Error executing "GetObject" AWS HTTP error: Client error: 403 AccessDenied (client): Access Denied - AccessDeniedAccess the permissions on account are as follows: Which means we can do all commands any suggestions on how to get around this error would be great?
  Very negative
Many thanks Robert TO be clear This is the full permissions listed { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": "s3:", "Resource": "arn:aws:s3:::BUCKET/" } { "Effect": "Allow", "Action": "s3:ListAllMyBuckets", "Resource": "arn:aws:s3:::*" } , { "Effect": "Allow", "Action": [ "s3:ListBucket", "s3:GetBucketLocation" ] , "Resource": "arn:aws:s3:::BUCKET" } ] }
563e0f4f2d1761a701f0f757	X	You can use mc tool and use mc share command to give presignedURL access to your private s3 objects.
  Negative
563e0f4f2d1761a701f0f758	X	Requirement is to share a file, stored in Amazon's S3 to any user, who may or may not have amazon access, using an expiring temporary link preferably.
  Negative
We could use the Presigned URLs generated using the access key and secret key of an IAM User with restricted roles.
  Negative
Generating Expiring Presigned URLs was pretty straight forward and I was able to achieve it using the API from Amazon's Java SDK.
  Negative
This URL contains the Access Key ID, visible to anybody who has access to the URL.
  Negative
Though it is not of much security concern, as I understand from this post,are there any other ways of achieving this scenario, where the access key id is not shared?
  Negative
What about the temporary security credentials concept?
  Neutral
I would also like to know about other ways to share files in S3 to any external user.
  Negative
563e0f4f2d1761a701f0f759	X	Thanks.
  Neutral
I will try out your script.
  Negative
I had the same issues with trying to add the parameters.
  Neutral
563e0f4f2d1761a701f0f75a	X	I've pushed a commit to the repo that should resolve this bug: github.com/aws/aws-sdk-ruby/commit/…
563e0f4f2d1761a701f0f75b	X	Awesome.
  Negative
Thanks for updating the gem so quickly.
  Positive
I really appreciate it.
  Positive
563e0f4f2d1761a701f0f75c	X	Is there any chance that S3 could be changed to take the key etc. in a query string for a signed url?
  Negative
That would make it significantly easier to use these signed urls from a browser.
  Negative
Or is there a recommended technique for how to deliver these encrypted files directly to an end user in a browser - ajax w/ CORS or ...?
  Neutral
563e0f502d1761a701f0f75d	X	The presigned url does accept the key, just not much else.
  Negative
For cors, you will need to configure your bucket to accept these headers using #put_bucket_cors.
  Negative
I agree, it would be better if these values could be sent as part of the querystring though.
  Negative
563e0f502d1761a701f0f75e	X	I am currently using the Ruby aws-sdk, version 2 gem with server-side customer-provided encryption key(SSE-C).
  Negative
I am able to upload the object from a rails form to Amazon S3 with no issues.
  Neutral
But I'm having some issues with retrieving the object and generating a signed url link for the user to download it.
  Negative
The link is generated, but when I clicked on it, it directs me to an amazon page saying.
  Positive
The error message is not helping as I'm unsure what parameters I need to add.
  Negative
I think I might be missing some permissions parameters.
  Negative
Get Method http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Object.html#get-instance_method Presigned_Url Method http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Object.html#presigned_url-instance_method
563e0f502d1761a701f0f75f	X	When you generate a pre-signed GET object URL, you need to provide all of the same params that you would pass to Aws::S3::Object#get.
  Negative
This means you need to pass the same sse_customer_* options to #presigned_url: This will ensure that the SDK correctly signs the headers that Amazon S3 expects when you make the final GET request.
  Negative
The next problem is that you are now responsible for sending those values along with the GET request as headers.
  Negative
Amazon S3 will not accept the algorithm and key in the query string.
  Negative
Please note - while testing this, I found a bug in the presigned URL implementation of the current v2.0.33 version of the aws-sdk gem.
  Negative
This has been fixed now and should be part of v2.0.34 once it releases.
  Negative
See the following gitst for a full example that patches the bug and demonstrates: You can view the sample script here: https://gist.github.com/trevorrowe/49bfb9d59f83ad450a9e Just replace the bucket_name and object_key variables at the top of the script.
  Positive
563e0f502d1761a701f0f760	X	"direct upload" vs "normal upload" isn't a well-defined concept.
  Negative
Those aren't terms I would use, since they are too vague... but I would assume they are referring to user uploads, and allowing the browser to upload directly to S3, instead of uploading to your server and then your server uploading the file to S3.
  Very negative
Does that fit the context of whatever you are reading?
  Neutral
563e0f502d1761a701f0f761	X	@Michael-sqlbot yes I am talkin about that exactly (user uploads), but I am just not sure what's the best approach !!
  Negative
if browser direct upload to S3 will reduce things like bandwidth and cpu like John says in his answer, so why should I upload to server ?
  Negative
I am just confusing .
  Negative
.
  Neutral
563e0f502d1761a701f0f762	X	So should I always upload directly to Amazon S3 because I will get those benefits ?
  Positive
also I would like to know what most people use for their application !
  Negative
563e0f502d1761a701f0f763	X	I've seen many posts talking about direct uploading to amazon S3 but none of them tell when we should really do that !
  Negative
Is "direct upload to S3" always better than uploading to S3 via Rails app using gems like paperclip or carrierwave... ?
  Negative
And when I should use direct upload vs normal upload to S3 ?
  Negative
563e0f502d1761a701f0f764	X	Uploading directly to Amazon S3 has the benefit of reducing load on your back-end web servers (for both CPU and bandwidth).
  Negative
It is great for mobile apps that can call the Amazon S3 API directly, but is also applicable to web pages that can upload via a POST to Amazon S3.
  Positive
563e0f502d1761a701f0f765	X	Amazon features a "Edit Redirection Rules" in the S3 management console:  But I wanted to automate that and update it via command line, which I can't find.
  Negative
Even on Amazon's documentation page, it shows screenshots from the S3 interface, and there's no mention to any web API.
  Negative
563e0f502d1761a701f0f766	X	The various AWS SDKs call this putBucketWebsite:
563e0f512d1761a701f0f767	X	I am using Amazon S3 to host images for a public REST API and serve them.
  Negative
Currently, my bucket allows anyone to enter in the URL to the image, without any signature included in the params, and the image will be accessible.
  Positive
However, I'd like to require an expiring signature in each image request, so users will have to go through the API to fetch images.
  Neutral
How do I do this?
  Neutral
Is this a bucket policy?
  Neutral
563e0f512d1761a701f0f768	X	You simply set all of the files to private.
  Negative
If you want to be able to give out pre-signed URLs, you'll need to generate them as you hand them out.
  Negative
AWS’ official SDKs make this trivial.
  Negative
563e0f512d1761a701f0f769	X	Indeed it's looking like the way we are going to go.
  Negative
It just feels expensive and unnecessary somehow.
  Negative
Chat thread ID's are unique, generated from an md5 hash of the particpants userids (the userid's are ordered and its a little more complex than this) This way when a user begins talking to someone they have an existing history with we can perform the threadID algorithm and get the correct ID everytime, which could be a file.
  Negative
I'm actually leaning towards using DynamoDB for everything and keeping a cached copy of the most recent 20 items in a memcached elasticache instance to keep costs down.
  Negative
563e0f522d1761a701f0f76a	X	I suppose going the DB route would also allow us to potentially add multi-user chat if needed sometime in the future.
  Negative
I guess we could use our thread_id algorithm to generate the hash key in dynamodb, using a timestamp as the range key and also using the thread_id as the key for the cached items.
  Negative
Does this sound like a reasonable solution to you?
  Neutral
While using an RDB is attractive cost wise, we have fallen in love with the no hassle scaling that dynamodb offers us.
  Negative
563e0f522d1761a701f0f76b	X	Seems reasonable.
  Negative
My going in position is always that you will ALWAYS find uses for data once you have it accumulated, and its better to have it in a way that makes it easy to use/implement that new stuff when the opportunity arises.
  Positive
563e0f522d1761a701f0f76c	X	We are using a cluster of node.js servers to power our chat system.
  Negative
We would like users to be able to see their previous conversations with any other given user and as such need to store chat logs.
  Negative
Since these never need to be edited or queried in any way it does not make sense to store them in a database and as such we would like to simply keep them as simple json files stored on Amazon S3.
  Negative
Using Amazons PHP api it is now possible to use standard fopen/fwrite commands making streaming to S3 possible.
  Negative
But in this case we need to do it with Node.js.
  Neutral
Essentially we would like to be able to open a stream when the users begin chatting, and append to the end of a chatlog file on S3, live while the users are chatting.
  Neutral
If this is not possible, what are our other options?
  Negative
We have considered creating local writeStreams on the node.js servers, using them to essentially buffer the data, detect the end of a conversation and then upload/replace the file to S3.
  Negative
It just seems overly verbose and given the the node.js servers are within Amazons network it feels like a poor mans solution when clearly streaming to S3 is now a possibility in PHP.
  Negative
Thanks in advance
563e0f522d1761a701f0f76d	X	I don't agree with your logic here: Just because you only want to write once and read-many, doesn't mean a DB is not an appropriate solution.
  Negative
You could probably use S3, but given the brief description of your problem I would think a classic DB solution is going to serve you better in the long run.
  Negative
If you go the DB route, you have lots of well supported options within AWS (RDS-SQL Server, Postgres, MYSQL), DynamocDB and SimpleDB.
  Negative
Even if you end up storing the chats on S3, its not unreasonable to think you may end up storing the metadata about each chat in a DB such as DynamoDB.
  Negative
How many chats are you going to store?
  Negative
If you end up with 100's of thousands or millions of seperate chat files, how will you navigate a user to the right one when they want it?
  Positive
This is a where a DB will come in handy, and then it begs the question, if you are going to use a DB to store the metadata, why not just store the content as well?
  Negative
563e0f522d1761a701f0f76e	X	There are several design decisions that could avoid the usage of a full scan, such as indexes, or a simple prefix tree, for example.
  Negative
563e0f522d1761a701f0f76f	X	you'r right Viccari, the index part is the most challenging one.
  Positive
Note it's not just a prefix issue, many records with a common prefix but differ in after-delimiter-part have to be skipped in order to list a "folder".
  Negative
This skip process could introduce additional overhead.
  Negative
563e0f522d1761a701f0f770	X	Thanks Viccari.
  Positive
Sure, it's kv fashion with s3 file's namespace, i'm curious of the s3 architecture part, how did s3 did this: you could use any string as the delimiter and the total file number is unlimited.
  Very negative
Take this example dataset: folder1/[level2]/[level3], we have one million level2 each with one million level3, and do a ls with prefix=folder1/ and delimiter=/, all [level3] would be folded up.
  Negative
This is a real challenging work to implement.
  Positive
563e0f522d1761a701f0f771	X	You can have unlimited objects in an Amazon s3 bucket, and use getBucket api to list your objects.
  Negative
The funnies part is you can use any character as a delimiter(which likes "/" in linux file systems).
  Negative
With large quantities of objects, how could the list api response in real-time?
  Negative
you cannot expect a full scan, right?
  Negative
what are the technologies behind s3 storage architecture?
  Neutral
Here is some search work results, anyone knows more detail?
  Negative
563e0f522d1761a701f0f772	X	I am adding as an answer because there is not enough room in the comments section: There are several design decisions that could avoid the usage of a full scan, such as indexes, or a simple prefix tree, for example.
  Very negative
Even though several S3 client apps will list prefixes as folders, there is not such concept in S3.
  Negative
Within a bucket, all files are hierarchically in the same level.
  Negative
The organization of the files is in a key/value fashion, rather than in a tree fashion (like one would expect in a "folder"-like system).
  Positive
Please see this related question for more information.
  Neutral
So, if you want to list your "folders", yes, it is likely that you will need to list your prefixes, i.e. get objects based on prefix and skip the ones having additional information.
  Negative
563e0f522d1761a701f0f773	X	To no mess up with the files names, you could create another file with the MD5 content, like image_file_1.
  Negative
tif.md5 .
  Neutral
563e0f522d1761a701f0f774	X	We put hundreds of image files on Amazon S3 that our users need to synchronize to their local directories.
  Negative
In order to save storage space and bandwidth, we zip the files stored on S3.
  Negative
On the user's end they have a python script that runs every 5 min to get a current list of files, and download new/updated files.
  Negative
My question is what's the best way determine what is new or changed to download?
  Neutral
Currently we add an additional header that we put with the compressed file which contains the MD5 value of the uncompressed file... We start with a file like this: We compress it (with 7zip) and put it to S3 (with Python/Boto): The problems is we can't get a large list of files from S3 that include the x-amz-meta-uncompressedmd5 header without an additional API for EACH one (SLOW for hundreds/thousands of files).
  Very negative
Our most practical solution is have users get a full list of files (without the extra headers), download the files that do not exist locally.
  Negative
If it does exist locally, then do and additional API call to get the full headers to compare local MD5 checksum against x-amz-meta-uncompressedmd5.
  Negative
I'm thinking there must be a better way.
  Negative
563e0f522d1761a701f0f775	X	You could include the MD5 hash of the uncompressed image into the compressed filename.
  Negative
So image_file_1.
  Neutral
tif could become image_file_1.
  Negative
xxxx1234.tif.z Your user python file which does the synchronising would therefore have the information needed to determine if it needed to go get the file again from S3, and could either strip out the MD5 part of the filename, or maintain it, depending on what you wanted to do.
  Negative
Or, you could also maintain, on S3, a single file containing the full file list including the MD5 metadata.
  Neutral
So the python script just need to fetch that single file, parse that, and then decide what to do.
  Negative
563e0f522d1761a701f0f776	X	I have an existing salesforce implementation that uses web services to upload attachments into a Amazon S3 bucket.
  Negative
This process has been working properly for the last 2 years or so.
  Positive
Now, quite suddenly, we are getting intermittent failures in this system (the system is only used 4 times a year, so it may have been broken for some time).
  Neutral
I've looked into it quite a bit, and I'm at a total loss.
  Negative
The system generates PutObjectInline requests through the S3 Soap API.
  Negative
It may generate several hundred requests over the period that it runs (usually 10-15 minutes).
  Negative
Of the requests made, about 50% fail (more on this below).
  Negative
Each failure is given a HTTP 400 status from the server with the message "Invalid URI".
  Negative
The body of the response is blank.
  Negative
Successful transmissions use the same URI as the failures.
  Negative
The entire transmission set is being uploaded into the same bucket.
  Negative
The failures form an odd pattern, from the look of it, every other (pass, fail, pass, fail) transmission is failing, with occasional chains of 3-4 transmissions succeeding.
  Negative
I looked into the idea that we might be transmitting the data too quickly, but AWS has a very specific code for that: 503.
  Negative
Also the error itself seems to point to a connection issue of some kind.
  Negative
Does anyone know what would cause this kind of issue?
  Neutral
This is an example of one of the failed requests (I've sripped out some of the information to save space and protect privacy):
563e0f532d1761a701f0f777	X	Seems like you are not the only one having the same problem.
  Very negative
This reported yesterday, also getting 400 errors with no body in the response.
  Negative
s3 upload, 400 response, no body
563e0f532d1761a701f0f778	X	I'm trying to upload a file using J2ME code to Amazon S3 through the Amazon REST API(POST object).
  Negative
This is my code: After executing it.
  Neutral
I'm getting this below response: But the file is not uploaded to Amazon S3.
  Negative
Please help me to fix it.
  Neutral
563e0f532d1761a701f0f779	X	It looks like your file line is commented out, your are not even sending a file.
  Negative
Having said that I strongly suggest you used the AWS Java SDK: http://aws.amazon.com/sdkforjava/
563e0f532d1761a701f0f77a	X	there isn't any fixed set of files, user may select random files to be download every time...
563e0f532d1761a701f0f77b	X	@kami998 That will be fine.
  Negative
You pass the file set into the job parameters.
  Neutral
563e0f532d1761a701f0f77c	X	I am using Amazon S3 storage for my files and i need to provide download functionality in which user can select multiple files and download them at once (as dropbox)... I tried to implement this functionality by downloading each file in memory stream and create a zip file and returned to user, but its too much time consuming, I need to know that is there any way that this process can be implemented asynchronously that user don't have to be wait longer and downloading starts immediately as dropbox do... I am using MVC Web API... Thanks in advance...
563e0f532d1761a701f0f77d	X	From a high level, I would probably do something like this.
  Very negative
Trigger a job with the file set the user wants to download.
  Negative
Have a worker that downloads the files from s3, compresses them into a zip file and then uploads it back to a temporary location in s3.
  Negative
Once the job is completed, send the user a signed URL to the zip file itself.
  Negative
Clean up the zip file after a certain amount of time.
  Positive
Maybe 24 hours?
  Neutral
563e0f532d1761a701f0f77e	X	Good answer and thanks for pointing out that this is not documented.
  Neutral
I have created an issue to track this: github.com/boto/boto/issues/729
563e0f532d1761a701f0f77f	X	This is now documented!
  Positive
boto.readthedocs.org/en/latest/ref/… And should probably note that the last byte in the last chunk should be the file size - 1, to head off any off-by-one errors.
  Negative
563e0f532d1761a701f0f780	X	Amazon S3 REST API documentation says there's a size limit of 5gb for upload in a PUT operation.
  Negative
Files bigger than that have to be uploaded using multipart.
  Neutral
Fine.
  Neutral
However, what I need in essence is to rename files that might be bigger than that.
  Neutral
As far as I know there's no rename or move operation, therefore I have to copy the file to the new location and delete the old one.
  Negative
How exactly that is done with files bigger than 5gb?
  Neutral
I have to do a multipart upload from the bucket to itself?
  Neutral
In that case, how splitting the file in parts work?
  Neutral
From reading boto's source it doesn't seem like it does anything like this automatically for files bigger than 5gb.
  Negative
Is there any built-in support that I missed?
  Negative
563e0f532d1761a701f0f781	X	As far as I know there's no rename or move operation, therefore I have to copy the file to the new location and delete the old one.
  Negative
That's correct, it's pretty easy to do for objects/files smaller than 5 GB by means of a PUT Object - Copy operation, followed by a DELETE Object operation (both of which are supported in boto of course, see copy_key() and delete_key()): This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3.
  Negative
A PUT copy operation is the same as performing a GET and then a PUT.
  Negative
Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket.
  Negative
However, that's indeed not possible for objects/files greater than 5 GB: Note [...] You create a copy of your object up to 5 GB in size in a single atomic operation using this API.
  Neutral
However, for copying an object greater than 5 GB, you must use the multipart upload API.
  Neutral
For conceptual information [...], go to Uploading Objects Using Multipart Upload [...] [emphasis mine] Boto meanwhile supports this as well by means of the copy_part_from_key() method; unfortunately the required approach isn't documented outside of the respective pull request #425 (allow for multi-part copy commands) (I haven't tried this myself yet though): You might want to study the respective samples on how to achieve this in Java or .
  Very negative
NET eventually, which might provide more insight into the general approach, see Copying Objects Using the Multipart Upload API.
  Negative
Good luck!
  Positive
Please be aware of the following peculiarity regarding copying in general, which is easily overlooked: When copying an object, you can preserve most of the metadata (default) or specify new metadata.
  Negative
However, the ACL is not preserved and is set to private for the user making the request.
  Neutral
To override the default ACL setting, use the x-amz-acl header to specify a new ACL when generating a copy request.
  Negative
For more information, see Amazon S3 ACLs.
  Negative
[emphasis mine]
563e0f542d1761a701f0f782	X	The above was very close to working, unfortunately should have ended with mp.complete_upload() instead of the typo "upload_complete()"!
  Negative
I've added a working boto s3 multipart copy script here, based of the AWS Java example and tested with files over 5 GiB: https://gist.github.com/joshuadfranklin/5130355
563e0f542d1761a701f0f783	X	Were you hoping to use an existing package or write your own?
  Negative
Since you're willing to use a subdomain, this is very do-able if you use the proper CORS headers, but I am not aware of any packages which can do this for you
563e0f542d1761a701f0f784	X	I can write my own.
  Negative
So I can host the entry point on S3, a back end on a subdomain and I just need to use proper CORS headers.
  Negative
I need to do some research about CORS then, since I barely know what it is, but I don't know how to implement it.
  Negative
Do you have any recommended reading?
  Neutral
563e0f542d1761a701f0f785	X	It is possible to set the root domain on S3?
  Negative
I.e. mysite.com instead of www.mysite.com?
  Negative
This is for a completely new site.
  Positive
563e0f542d1761a701f0f786	X	Yes.
  Neutral
Simply follow Amazon's documentation.
  Neutral
I recommended the www only from the fact that this is fairly standard.
  Negative
563e0f542d1761a701f0f787	X	Basically I want to make a blog.
  Negative
Since the actual blog content is static, it seems fitting to upload it to S3 - cheap & basically infinitely scalable in case Hacker News or Reddit ever hit it (unlikely, but you never know).
  Negative
However, I want to have some dynamic parts to it, like search.
  Neutral
My current thinking is that the blog would be a HTML uploaded to a domain on S3 while the actual server side component that would be called from Javascript and would return the search results would be hosted somewhere else (probably in a subdomain).
  Negative
Basically the first hit is always to S3.
  Neutral
If the user wants to get more "interactive" only then does he actually query a server.
  Negative
But as long as the access is read only, no extra interaction (most likely scenario in case of a traffic spike), S3 can handle things gracefully, unlike a puny VPS.
  Negative
According to this question/answer: Serving Django API on Heroku and single-page app on Amazon S3 on the same domain it is not possible.
  Negative
Is this still the case now?
  Neutral
Thank you.
  Positive
563e0f542d1761a701f0f788	X	I would recommend that you configure www.yoursite.com to point to your S3 bucket and run the dynamic content on a subdomain (dynamic.yoursite.com).
  Negative
I don;t know how you plan to do the dynamic portion, but you have several options: Keep them as separate sites which link to each other.
  Negative
This may work well, for example, if you have your pages on S3 and your search index on your VPS.
  Neutral
When the user does a search, the search request is POSTed to a page on dynamic.yoursite.com, which returns the results list.
  Negative
This results list can reference CSS, scripts, and images from www.yoursite.com as needed, and each result can link back to the static content.
  Negative
For more complex sharing, both S3 or your VPS can be configured to allow CORS.
  Negative
This will allow AJAX requests to be sent as well.
  Neutral
(Note that while browser support is very wide, there are still a few that do not support CORS yet)
563e0f542d1761a701f0f789	X	Are you trying to do this programatically?
  Negative
Is it a one-off transfer or will you have to do repeatedly?
  Neutral
563e0f542d1761a701f0f78a	X	Yes I want to do it programatically and I want to do it repeatedly.
  Negative
Whenever user request for file I have to upload that file to his server.
  Negative
563e0f542d1761a701f0f78b	X	unfortunately I don't think this is possible.
  Negative
The only way to get a file from S3 is via the API or a Direct Url.
  Negative
The only suggestions I have are to get the user to pull the file from a url or use EC2 as the 'proxy' server.
  Negative
Transfers from S3 to EC2 are very quick and there are no extra bandwidth charges.
  Negative
563e0f542d1761a701f0f78c	X	Did you find a solution for this?
  Neutral
I'd love to hear it, since I have a very similar (if not the same) requirement.
  Positive
This is my question here: stackoverflow.com/q/8403401/291915
563e0f542d1761a701f0f78d	X	+1 to hear whether you found a solution
563e0f552d1761a701f0f78e	X	I have some files that are stored on S3.
  Negative
On users request, I want to transfer them to FTP server of a third party site.
  Negative
Amazon S3 does not support FTP/SFTP.
  Negative
Currently I am downloading the file from S3 to my local server using S3 APIs and then transferring it to third party FTP server.
  Negative
S3 --API--> Local --FTP--> Third party FTP Now, instead I want to transfer the files directly to third party FTP server directly from S3 without downloading it to my local server.
  Negative
S3 ---CloudFront or Other Service---> Third Party FTP How can I do it using cloudfront or any other services?
  Negative
Any Help will be appreciated.
  Neutral
Thanks in advance.
  Neutral
563e0f552d1761a701f0f78f	X	S3 only has APIs to get data to it and from it.
  Negative
It also has an API function to copy data between two buckets, but that's about it.
  Negative
If you require to transfer data from S3 to other places and want to save the download from S3 to your local machine I suggest you start a t1.micro instance and put a script on it to download the files to it (you won't pay the bandwidth because between S3 and EC2 instance on the same region you don't pay anything and its significantly faster) and then upload from that instance to the remote 3rd party FTP site.
  Negative
563e0f552d1761a701f0f790	X	To eliminate the possibility that the client code is somehow making valid credentials invalid, can you try using your key and secret in a gsutil config file, and see if you can perform gsutil operations with those credentials?
  Negative
Also, did you create your HMAC credentials this way?
  Neutral
cloud.google.com/storage/docs/migrating#keys
563e0f552d1761a701f0f791	X	Perhaps signature => 'v2' in the constructor?
  Negative
Google doesn't seem to have implemented v4 and you might be defaulting to that.
  Negative
The wording of the error suggests an invalid format to the auth header, rather than a mismatched signature.
  Negative
563e0f552d1761a701f0f792	X	@MikeSchwartz Yes, I created my credentials through the Interoperability.
  Negative
Yes, they work when I use gsutil.
  Neutral
I can run "gsutil ls gs://devtest" and it properly lists the files in the bucket.
  Positive
I added some additional info to my post.
  Positive
Any help would be greatly appreciated.
  Positive
563e0f552d1761a701f0f793	X	@Michael-sqlbot I tried that but I don't think that config actually gets used anymore.
  Negative
I updated my post with more info about the Auth header.
  Negative
563e0f552d1761a701f0f794	X	I suspect this is something about the way that PHP SDK is producing the credentials on the wire.
  Negative
That error message happens when the credentials GCS receives are neither AWS nor GOOG1 credentials.
  Negative
I suggest running gsutil -D ls and looking at the Authorization header it sends; then capture the Authorization header the PHP SDK is sending, and compare them.
  Negative
I see gsutil sending credentials like: Authorization: GOOG1 GOOG<...>
563e0f552d1761a701f0f795	X	I'm in the process of migrating from Amazon S3 to Google Storage and I can't seem to get my credentials to work.
  Very negative
Here's some sample code that I put together to test my credentials: Here's what I get back: InvalidSecurity Error executing "PutObject" on "https://storage.googleapis.com/devtest/test"; AWS HTTP error: Client error response [url] https://storage.googleapis.com/devtest/test [status code] 403 [reason phrase] Forbidden InvalidSecurity (client): The provided security credentials are not valid.
  Very negative
- InvalidSecurityThe provided security credentials are not valid.
  Neutral
Incorrect Authorization header I've tried googling 100 different combinations of this issue and can't find anything.
  Negative
I have Interoperability enabled, at least I think I do since I don't think I can get the key/secret without it being enabled first.
  Negative
And I have the Google Storage API enabled.
  Negative
Any help would be greatly appreciated.
  Positive
Edit: here's the Authentication Header in case that helps: AWS4-HMAC-SHA256 Credential=GOOGGUxxxxxxxxxxx/20150611/US/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=9c7de4xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx I noticed it stays "aws4_request" even when I specify 'signature' => 'v2'.
  Negative
Not sure if that matters.
  Neutral
I took a look at the S3Client code and it doesn't use the 'signature' config key as far as I can tell.
  Negative
The only thing I found was 'signature_version' which when set to v2, I get this error: Unable to resolve a signature for v2/s3/US.
  Negative
Valid signature versions include v4 and anonymous.
  Negative
I'm using Laravel 5.1 with composer package aws/aws-sdk-php version 3.0.3 Any ideas?
  Negative
563e0f552d1761a701f0f796	X	S3 only supports v4 signatures, and this requirement is enforced by the PHP SDK.
  Negative
It seems that Google Cloud Storage only supports v2 signing, so you wouldn't be able to use the same library to talk to both.
  Negative
Google does provide their own PHP SDK, which might make talking to Cloud Storage a bit easier.
  Negative
563e0f552d1761a701f0f797	X	One problem is that you're using request['file_count'].
  Negative
Should be @request['file_count'].
  Negative
563e0f552d1761a701f0f798	X	That was something I missed as well.
  Negative
Thanks Mischa!
  Positive
563e0f562d1761a701f0f799	X	I did find that a little while after I posted the question and it solved my initial problem and then more came up.
  Negative
I figured everything out though after a bit more debugging so thanks!
  Positive
I'm so used to Javascript automatically interpreting Integers correctly when building strings that I had a pretty major brain fail.
  Negative
563e0f562d1761a701f0f79a	X	I am working on a website that requires a document tree for file management and have been running into trouble getting files into my Amazon S3 bucket.
  Negative
My approach is to have the user upload files from the client side into a jQuery plugin I have written.
  Negative
The plugin takes all of the files and packages them up into a Ajax call that gets sent to my internal API that then handles the files.
  Negative
From here, the files make their way to my S3 uploading logic, but here is where I start having problems.
  Negative
Once the object has come into my server, it's in a split apart form and I am not sure which parts are necessary or not.
  Negative
Ajax did it's magic and broke apart my files into a JSON readable array, but now that its in a new form, I'm not sure how to process it.
  Negative
Whenever I go to upload the file(s) to the S3 server it says the following.
  Negative
Given I am just throwing the new formatted array of the file into the S3 logic, but I don't know how else to do this.
  Negative
Every other part of my solution so far works in that I can save strings into my S3 bucket no problem.
  Negative
So my question is, how can I take a file object passed into Rails and then pass it into S3?
  Negative
What format do I need?
  Neutral
Below is my code for my API method to upload the files.
  Neutral
Here is a screen shot of the data that is coming through to the server for the files.
  Negative
563e0f562d1761a701f0f79b	X	The error occurs in this line: To fix it, do this: Or:
563e0f562d1761a701f0f79c	X	Thanks, that will do nicely :)
563e0f562d1761a701f0f79d	X	I am hoping to access the Amazon S3 API to determine how much data has been transferred on a single S3 Object.
  Negative
Is this possible?
  Neutral
I have looked through the documentation yet can not find anything.
  Negative
563e0f562d1761a701f0f79e	X	This is not possible via the API as such, however, you should be able to calculate this yourself by facilitating Server Access Logging: [...] An Amazon S3 bucket can be configured to create access log records for the requests made against it.
  Negative
An access log record contains details about the request such as the request type, the resource with which the request worked, and the time and date that the request was processed.
  Negative
[...] This is can be configured via the AWS Management Console as well, see Managing Bucket Logging for details.
  Negative
Any decent analytics package should be able to provide respective aggregates for the generated Server Access Log Format, ideally including the transferred data already - otherwise you would need to do the math yourself from the number of requests (still inconvenient, but likely automatable one way or another).
  Positive
Good luck!
  Positive
563e0f562d1761a701f0f79f	X	Amazon S3 API for Ruby only streams objects when passing a block to the read method.
  Negative
I'm developing a driver for em-ftpd, and It needs an IOish object to stream the S3 data to the client.
  Negative
If simply pass myS3Object.read, whole file is loaded into the memory, as stated by S3 API.
  Negative
Is there any way of encapsulating that into something like a custom IO class, so I can pass the stream to em-ftpd?
  Negative
Here is my code: Here is the code inside em-ftpd that gets the result of my code, preferably as an IOish object, using a block to get its data chunks: Thank you.
  Negative
563e0f562d1761a701f0f7a0	X	Turns out that I needed to create a class with a buffer of the S3 data and the methods read and eof?
  Negative
.
  Neutral
563e0f562d1761a701f0f7a1	X	That's great I'll take a look.
  Positive
563e0f562d1761a701f0f7a2	X	This is perfect - thanks.
  Positive
563e0f562d1761a701f0f7a3	X	@John.
  Neutral
No problem
563e0f562d1761a701f0f7a4	X	We're moving from Nirvanix to Amazon S3.
  Negative
What I need is to simulate Nirvanix style child accounts for S3 storage.
  Negative
(In a nutshell these provide isolated storage with predefined limit, and a separate authentication for each sub-user, still managed by the same master account).
  Negative
We'll have more than 100 users so the bucket-per user won't work (that's still limited at 100 right?)
  Negative
.
  Neutral
The storage is used directly from a desktop application (and not, for example, via our servers, though there is a central server if that helps).
  Negative
We want a single S3 billing account that pays for everything, but we want our customers objects safely segmented from each other.
  Negative
Nirvanix provides this out of the box (http://developer.nirvanix.com/sitefiles/1000/API.html#_TocCreatingChildAccounts) - this is essentially what I'm trying to replicate with S3.
  Negative
I understand how to segment objects for each sub-user, e.g. using the "prefix" notation of Objects (E.g. "USER1/object1", "USER2/something_else).
  Negative
What I can't work out: 1) How can I set permissions so that each customer can only access his files?
  Neutral
If I give "the app" access to the S3 storage, then that obviously means that every user of the app could access anyones files.
  Negative
It seems like you can set rich ACLs, but what I can't understand is "who" you can set permissions against.
  Positive
Is it only AWS users?
  Neutral
Does that mean the only way to do this is to have my customers each have an AWS account?
  Neutral
If so, can I create accounts on their behalf?
  Positive
E.g. through an API call?
  Negative
What we certainly cannot allow is having every user create an account through the AWS website (yuck!)
  Neutral
.
  Neutral
2) Any ideas about the best way to manage quotas for each customer?
  Positive
This concerns me because from what I can tell, we'd have to limit this from the desktop application.
  Negative
This is obviously ripe for abuse because S3 will just keep allowing more data.
  Negative
I guess we could probably live with having a script we run daily which sanity checks the storage limits for "abuse", but just wondered if there was a better way.
  Negative
Thanks all!
  Neutral
John
563e0f562d1761a701f0f7a5	X	Amazon has a new beta service called AWS Identity and Access Management (IAM) that will allow you to segment your buckets.
  Negative
In the using with S3 section of the documentation, there are examples describing your use case: Example 1: Allow each User to have a home directory in Amazon S3 In this example, we create a policy that we'll attach to the User named Bob.
  Negative
The policy gives Bob access to the following home directory in Amazon S3: my_corporate_bucket/home/bob.
  Negative
Bob is allowed to access only the specific Amazon S3 actions shown in the policy, and only with the objects in his home directory.
  Negative
Unfortunately I don't think you can currently enforce quotas using IAM.
  Negative
Also, depending on your platform, you probably want to use one of the SDK's available to simplify your interactions with these services.
  Negative
You absolutely don't want to distribute your standard secret key in a desktop application without taking some serious precautions.
  Negative
Any user with your secret key could have full access to all your AWS services.
  Negative
563e0f562d1761a701f0f7a6	X	Yes, definitely.
  Positive
Here are samples : docs.aws.amazon.com/AmazonS3/latest/dev/UploadInSingleOp.html
563e0f572d1761a701f0f7a7	X	@TJ- I want users of my product to upload to my bucket, like this - docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html
563e0f572d1761a701f0f7a8	X	http://aws.amazon.com/articles/1434 "If you have an AWS account, you can interact with the S3 service using specialized tools to upload and manage your files.
  Negative
It is very convenient to have access to this online storage resource for yourself, but there may be situations where you would like to allow others to upload files into your account.
  Negative
For this purpose, S3 accepts uploads via specially-crafted and pre-authorized HTML POST forms.
  Negative
You can include these forms in any web page to allow your web site visitors to send you files using nothing more than a standard web browser."
  Neutral
This excerpt shows that we can use web pages to allow users of a product to upload directly to s3.
  Negative
However, is there a way we can do this without using a browser based approach.
  Neutral
As in, can i develop a desktop application to let my users upload to s3 directly?
  Negative
563e0f572d1761a701f0f7a9	X	Uploading via a browser is just one way of interacting with Amazon S3.
  Negative
It is also possible to call Amazon S3 APIs directly from your application by taking advantage of a Software Development Kit (SDK) for your preferred language.
  Negative
See: AWS Tools and SDKs You also interact via the AWS Command-Line Interface (CLI), available for Windows, Linux and Mac.
  Negative
563e0f572d1761a701f0f7aa	X	I'm using these codes for getting metadata from key of buckets.
  Negative
But I'm looking for bucket not the keys
563e0f572d1761a701f0f7ab	X	I see.
  Negative
I'm not too sure about the bucket itself.
  Negative
I have updated the answer with something else that you may be able to try.
  Positive
563e0f572d1761a701f0f7ac	X	I'd like to get the list of metadata/headers of an amazon s3 bucket?
  Negative
I can get the metadata in OpenStack Swift Storage like following -> the curl command that request the metadata: response for above curl command: In openstack swift I can get the above metadata of a swift container(bucket) by using the python-swiftclient API.
  Negative
I want to do same thing for AmazonS3 bucket.
  Negative
so, how can I get the metadata of an AmazonS3 bucket with Python ?
  Negative
(I am using boto library) Is it possible or am I "beating the air"?
  Negative
UPDATE: I'm trying to get metadata of a bucket not the keys of bucket.
  Negative
563e0f572d1761a701f0f7ad	X	You can try this boto snippet code: You can read more information for boto on : https://boto.readthedocs.org/en/latest/s3_tut.html Hope that helps.
  Negative
UPDATE: Something else that you can try: Hope you are seeing some metadata that you wanted in the response headers.
  Negative
563e0f572d1761a701f0f7ae	X	I was using the services from Parse a while back, and they had implemented an amazing feature for uploading data, with a method something like this: Which let me keep track of the file-upload.
  Negative
Since Parse only let me upload max 10mb files, I chose to move to the cloud-area to explore a bit.
  Negative
I've been testing with Amazon's S3-service now, but the only way I can find how to upload data is by calling [s3 putObject:request];.
  Negative
This will occupy the main thread until it's done, unless I run it on another thread.
  Negative
Either way, I have no idea of letting my users know how far the upload has come.
  Negative
Is there seriously no way of doing this?
  Negative
I read that some browser-API-version of S3's service had to use Flash, or set all uploads to go through another server, and keep track on that server, but I won't do either of those.
  Negative
Anyone?
  Negative
Thanks.
  Neutral
My users are supposed to be uploading video with sizes up to 15mb, do I have to let them stare at a spinning wheel for an unknown amount of time?
  Negative
With a bad connection, they might have to wait for 15 minutes, but they would stare at the screen in hope the entire time.
  Negative
563e0f572d1761a701f0f7af	X	Seems like I didn't quite do my homework before posting this question in the first place.
  Negative
I found this great tutorial doing exactly what I was asking for.
  Positive
I would delete my question, but I'll let it stay just in case it might help other helpless people like myself.
  Negative
Basically, it had a delegate-method for this.
  Neutral
Do something like this: Then use this appropriately named delegate-method: It will be called for every packet it uploads or something.
  Neutral
Just be sure to set the delegates.
  Neutral
(Not sure if you need both delegates to be set though)
563e0f582d1761a701f0f7b0	X	Test with two small ~1kB files.
  Negative
563e0f582d1761a701f0f7b1	X	@istruble: well, multipart only works for 5BM+ chunks.
  Negative
But still, I can only test on a few files and hope that the amazon part is free of any bug and that my tests were exhaustive.
  Negative
563e0f582d1761a701f0f7b2	X	Thank you.
  Positive
I learned something new today.
  Positive
563e0f582d1761a701f0f7b3	X	I doubt this will work remotely with amazon S3...
563e0f582d1761a701f0f7b4	X	You're just downloading data to calculate a hash, but not storing the data.
  Very negative
As far as any given program is concerned, S3 is just another data source, accessed via web requests.
  Negative
563e0f582d1761a701f0f7b5	X	I see, it's time to try EC2 :)
563e0f582d1761a701f0f7b6	X	I need to move large files (>5GB) on amazon S3 with boto, from and to the same bucket.
  Negative
For this I need to use the multipart API, which does not use md5 sums for etags.
  Negative
While I think (well only 98% sure) that my code is correct, I would like to verify that the new copy is not corrupted before deleting the original.
  Negative
However I could not find any method except downloading both objects and comparing them locally, which for 5GB+ files is quite a long process.
  Neutral
For the record, below is my code to copy a large file with boto, maybe this can help someone.
  Negative
If there is no good solution to my problem maybe someone will find a bug and prevent me from corrupting data.
  Negative
This code only works for original key sizes >= 5368709121 bytes.
  Negative
563e0f582d1761a701f0f7b7	X	You should be able to compute a SHA-1 hash on a data stream (see this SO thread for C++ code, which could give hints for a python approach).
  Negative
By redirecting your hashed data stream to the equivalent of /dev/null, you should be able to compare SHA-1 hashes of two files without first downloading them locally.
  Negative
563e0f582d1761a701f0f7b8	X	There is no way to do what you want without knowing how AWS calculates the etag on multipart uploads.
  Negative
If you have a local copy of the object, you can calculate the md5 of each part that you are copying on the local object and compare it to the etag in the key that each mp.copy_part_from_key() returns.
  Negative
Sounds like you have no local object though.
  Negative
You also have a small non-obvious problem hiding in boto that may or may not cause you to lose data in a very rare case.
  Negative
If you look at the boto source code, you'll notice that mp.complete_upload() function actually doesn't use any of the etags for any of the parts returned by AWS when uploading.
  Negative
When you use multipart_complete, it actually does a totally new multipart list itself and gets a new list of parts and etags from S3.
  Negative
This is risky because of eventual consistency and the list may or may not be complete.
  Negative
The multipart_complete() should ideally use the etags and part info that was returned by each remote copy to be completely safe.
  Negative
This is what Amazon recommends in its documentation (See the Note under Multipart Upload Listings).
  Negative
That said, it's less likely a problem if you confirm the file size of both objects to be the same.
  Negative
The worst case I believe is that a part is not listed in the multipart upload listing.
  Very negative
A listed part should never be incorrect itself.
  Negative
563e0f582d1761a701f0f7b9	X	I'm also interested in this question.
  Positive
I found uploads.im website which seems free.
  Neutral
Interesting of someone has experience with it?
  Positive
563e0f592d1761a701f0f7ba	X	hmm amazon seems way too hard to use .
  Negative
.
  Neutral
and imgur api isnt free for commercial use
563e0f592d1761a701f0f7bb	X	Does Flickr not work?
  Negative
563e0f592d1761a701f0f7bc	X	I am building an Image Sharing feature for my website, where users can upload images and show it off on my website, through their own profile gallery.
  Negative
Except I dont want to physically host the uploaded images.
  Positive
Are there API's that can allow me to descretly save the uploaded images (from the users) to another Host, such as Flickr, Google etc..
  Negative
I know Flickr has a good API, but could I use it for this?
  Neutral
And does the Flickr Upload API only work with authorised users ?
  Negative
becuase I dont want them to upload to their Flickr account, but just host them with 1 big account so to speak thanks.
  Positive
563e0f592d1761a701f0f7bd	X	You could use Amazon S3, also Imgur has an API
563e0f592d1761a701f0f7be	X	I hoped for a more detailed answer with a short snippet.
  Negative
I don't want to risk my data by making some newbie mistake, since I never programmed for S3 Api.
  Negative
But this at least point to the minimum information so I'll award you the bounty.
  Negative
But if you have some more help/suggestions please let me know.
  Negative
563e0f592d1761a701f0f7bf	X	I find it strange that after looking everywhere I don't find any tool to delete all the versions of a file older than X days (not the actual file) of a S3 bucket that has versioning enabled.
  Negative
I would believe this is a very common issue because without it the buckets with time would become huge.
  Negative
Is there any existing solution (even commercial)?
  Neutral
If there is no ready made way, could you point me to some info or give me suggestions on how to code this myself in C#?
  Negative
I guess I have to use recursion for this kind of problem.
  Negative
Thanks
563e0f592d1761a701f0f7c0	X	If you use the amazon s3 API, you can do that.
  Negative
I use AmazonS3Client + DeleteObjectRequest method on amazon SDK: http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html http://docs.amazonwebservices.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/model/DeleteObjectRequest.html Should really be straightforward.
  Negative
Hope it helps
563e0f592d1761a701f0f7c1	X	I am using Amazon S3 in my android app for uploading files to the cloud storage.
  Negative
Every user can upload and download files from the cloud storage i.e. S3 I want to keep track of every user's upload and download... for eg User abc uploaded 26MB and downloaded 94MB One solution to this is to implement this on my mobile app, store/track size while uploading and downloading.
  Very negative
Does AWS lets us know via any analytics or is there any other 3rd party api which gives transfer details.
  Negative
563e0f592d1761a701f0f7c2	X	AWS Console does not provide any useful method to track user's upload and download, One suggestion is that you should post these information using locally if you have a local database, but if you have a remote database to save your records you may send these informations to your server using web service.
  Negative
563e0f592d1761a701f0f7c3	X	OP didn't get around to mentioning it, but the aurguable/theoretical benefit in uploading through CloudFront is to get the traffic into Amazon's high-bandwidth, low-latency network as close to the end user as possible, potentially improving connection quality, stability, performance.
  Very negative
With the CloudFront edge operating as a TCP proxy, retries related to the end user's Internet connection quality would be handled more quickly.
  Negative
563e0f592d1761a701f0f7c4	X	Thanks for your comment
563e0f592d1761a701f0f7c5	X	I'd like to upload image to S3 via CloudFront.
  Negative
If you see the document about CloudFront, you can find that cloud front offers put method for uploading to cloudFront There could be someone to ask me why i use the cloud front for uploading to S3 If you search out about that, you can find the solution What i wanna ask is whether there is method in SDK for uploading to cloud front or not As you know , there is method "putObejct" for uploading directly to S3 but i can't find for uploading cloud front ... please help me.
  Positive
.
  Neutral
563e0f5a2d1761a701f0f7c6	X	Data can be sent through Amazon CloudFront to the back-end "origin".
  Negative
This is used for using a POST on web forms, to send information back to web servers.
  Negative
It can also be used to POST data to Amazon S3.
  Negative
If you would rather use an SDK to upload data to Amazon S3, there is no benefit in sending it "via CloudFront".
  Negative
Instead, use the Amazon S3 APIs to upload the data directly to S3.
  Negative
So, bottom line:
563e0f5a2d1761a701f0f7c7	X	Is there any way I can ask the CloudFront API for the name of the bucket it uses on Amazon S3?
  Negative
563e0f5a2d1761a701f0f7c8	X	This is possible via the GET Distribution action: To get the information about a distribution, you do a GET on the 2012-03-15/distribution/ resource.
  Negative
Have a look at the sample syntax in the Responses section, which specifically includes fragments for either S3Origin or CustomOrigin, e.g. abbreviated: Please note that The S3Origin element is returned only if you use an Amazon S3 origin for your distribution, whereas The CustomOrigin element is returned only if you use a custom origin for your distribution.
  Negative
Furthermore, for more information about the CustomOrigin element and the S3Origin element, see DistributionConfig Complex Type.
  Negative
563e0f5a2d1761a701f0f7c9	X	You are correct in your belief.
  Neutral
The AWS SDK for .
  Neutral
NET does not currently support creating presigned URLs for multipart uploads.
  Negative
563e0f5a2d1761a701f0f7ca	X	Thank you.
  Positive
Can you post the same as answer so that I can accept it.
  Positive
In case anybody's reading this thread and interested, I wrote a blog post about consuming Amazon S3 REST API for multi part upload using C#: gauravmantri.com/2014/01/06/….
  Negative
563e0f5a2d1761a701f0f7cb	X	Hi, and how can I create presigned URLs for multipart uploads use java sdk?
  Negative
thank you.
  Positive
563e0f5a2d1761a701f0f7cc	X	Our requirement is to upload objects in Amazon S3 using a browser based interface.
  Negative
For this we're utilizing Query String Authentication mechanism (we don't have end-user's credentials during the upload process and we're using ASP.Net to write code to do so.
  Negative
I'm running into issues when trying to do multipart uploads.
  Neutral
I'm using AWS .
  Negative
Net SDK (version 2.0.2.5) to create a query string using the following code below: This works great if I don't do multi-part upload.
  Negative
However I'm not able to figure out how to do a multipart upload using Query String.
  Neutral
Problems that I'm running into are: I am inclined to believe that .
  Negative
Net SDK does not support this scenario and I have to resort to native REST API to create query string.
  Negative
Is my understanding correct?
  Neutral
Any insights into this would be highly appreciated.
  Negative
Happy New Year in advance.
  Positive
563e0f5a2d1761a701f0f7cd	X	You are correct in your belief.
  Neutral
The AWS SDK for .
  Neutral
NET does not currently support creating presigned URLs for multipart uploads
563e0f5a2d1761a701f0f7ce	X	What happens if you try accessing it from the AWS console at console.aws.amazon.com/s3 ?
  Negative
563e0f5a2d1761a701f0f7cf	X	The AWS console interface says "an error occurred" if I try to view the folder, or try to delete the folder.
  Negative
563e0f5a2d1761a701f0f7d0	X	Very interesting...
563e0f5a2d1761a701f0f7d1	X	I just began to use S3 recently.
  Negative
I accidentally made a key that contains a bad character, and now I can't list the contents of that folder, nor delete that bad key.
  Very negative
(I've since added checks to make sure I don't do this again).
  Negative
I was using an old "S3" python module from 2008 originally.
  Negative
Now I've switched to boto-2.0, and I still cannot delete it.
  Negative
I did quite a bit of research online, and it seems the problem is I have an invalid XML character, so it seems a problem at the lowest level, and no API has helped so far.
  Negative
I finally contacted Amazon, and they said to use "s3-curl.
  Positive
pl" from http://aws.amazon.com/code/128.
  Neutral
I downloaded it, and here's my key: I think I was doing a quick bash for loop over some files at the time, and I have "lscolors" set up, and so this happened.
  Negative
I tried .
  Neutral
/s3curl.pl --id <myID> --key <myKEY> -- -X DELETE https://mybucket.s3.amazonaws.com/info/&#x1b;[01 (and also tried putting the URL in single/double quotes, and also tried to escape the '[').
  Negative
Without quotes on the URL, it hangs.
  Negative
With quotes, I get "curl: (3) [globbing] error: bad range specification after pos 50".
  Negative
I edited the s3-curl.pl to do curl --globoff and still get this error.
  Negative
I would appreciate any help.
  Neutral
563e0f5a2d1761a701f0f7d2	X	You can use the s3cmd tool from here.
  Negative
You first need to run You can then delete the file using
563e0f5b2d1761a701f0f7d3	X	Using Amazon REST API, is there way to download multiple files perhaps in one Bucket with a single operation?
  Negative
For example to zip all the objects and download them as one file?
  Negative
Note that I am looking for operation efficiency and than custom solutions.
  Negative
For example of I was to download each individual file from S3 and Create the ZIP file myself the operation could take up to 40 seconds for 100 files of 1.5MB size , I am looking for a solution that allow me to do this in less 10 seconds.
  Negative
563e0f5b2d1761a701f0f7d4	X	There is no REST call that can do this in S3.
  Negative
As you no doubt realize, the transfer bandwidth doesn't likely account for much of the wall-clock time your process is taking.
  Negative
The most likely solution would be to use either multiple threads or asynchronous I/O (depending on your language and environment) to send the requests to S3 in parallel groups, combining the results when all of the desired objects have been successfully fetched.
  Negative
Of course, if what you are doing, now, does not reuse the user agent's connection so that you can take advantage of HTTP keep-alives, you should see some level of performance improvement if you can enable that functionality, by avoiding unnecessarily repeated connection setups (and potentially, SSL negotiations).
  Negative
563e0f5b2d1761a701f0f7d5	X	Amazon S3 is a storage service AFAIK so don't look into that.
  Negative
I would look into web services.
  Neutral
563e0f5b2d1761a701f0f7d6	X	I need to be able to pass a book title into some amazon api function and pull out the proper image, isbn ect to then store in my own DB.
  Negative
Any particular web service you'd recommend for that?
  Negative
563e0f5b2d1761a701f0f7d7	X	Yes, I think anything else is overkill.
  Negative
Thanks!
  Positive
563e0f5b2d1761a701f0f7d8	X	I don't think I need a relational database.
  Negative
I just need to be able to pass a book title into some amazon api function and pull out the proper image, isbn ect to then store in my own DB.
  Negative
Any particular web service you recommend for that?
  Negative
563e0f5b2d1761a701f0f7d9	X	those services are for your own sake.
  Negative
they are totally independent from Amazon stores.
  Negative
I really don't have any clue on how to access Amazon's store via API.
  Negative
563e0f5b2d1761a701f0f7da	X	Thanks for your help!
  Positive
563e0f5b2d1761a701f0f7db	X	I am working on a site (ASP.NET MVC) that will ultimately display millions of book.
  Negative
I already have the titles and authors of these books in my MySQL database.
  Negative
The goal is when a user searches for book, the top 20 matches (title and author) will appear on the page.
  Positive
I then plan to use the Amazon API to get more information (isbn, image, description etc) for these 20 books and flesh out these items via Ajax.
  Negative
I would then also add this info to MySQL so next time these specific books are requested, I already have the data.
  Negative
My question is what Amazon Web Service should I use?
  Negative
There are so many like Amazon S3, Amazon SimpleDB etc.
  Negative
I just don't know which would be best for my needs.
  Negative
Cost is also a factor.
  Negative
Any guidance would be greatly appreciated.
  Negative
563e0f5b2d1761a701f0f7dc	X	The API you're looking for is Amazon's Product Advertising API: https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html
563e0f5b2d1761a701f0f7dd	X	in short, Amazon S3 is a technology oriented on large data storage whilst SimpleDB is a non relational database (as mongoDB and raven could be).
  Very negative
We use the first for storing the static files (javascript, css and pictures).
  Negative
The first is cheaper but you can only retrieve a "file" at once.
  Neutral
The second gives you some degree of support to queries.
  Positive
If you need a relational database, you could use Amazon RDS which is a MySql database ready for replicas.
  Negative
563e0f5c2d1761a701f0f7de	X	"Eucalyptus Walrus" would be a good name for a band.
  Negative
(Sorry I can't help with the question, though!)
  Negative
563e0f5c2d1761a701f0f7df	X	It does not look like this would be the problem.
  Negative
I was able to go one step farther by removing the http:// at the beginning of the ServiceURL.
  Negative
But now the problem is that I cant connect ... .
  Negative
LoginException: Login Failure !
  Positive
My guess would be that AWS SDK is not using the :8773/services/Walrus into the signing part !
  Negative
So I am still searching.
  Negative
563e0f5c2d1761a701f0f7e0	X	I have downloaded the Amazon AWS SDK for C#, I have no problem accessing the EC2 part of our private cloud running Eucalyptus, I can list, Images, Instances, Zones ... This is working fine : But I need to access the Walrus (S3) part of our Cloud.
  Negative
This is how I try to access the Walrus, the code is almost identical, but with this call I will get an exception.
  Negative
This is not working: I will receive this exception : From Eucalyptus site : Eucalyptus implements an IaaS (Infrastructure as a Service) private cloud that is accessible via an API compatible with Amazon EC2 and Amazon S3 What am I missing ?
  Negative
Note: The same code work flawlessly with Amazon S3, the problem is to access Eucalyptus Walrus.
  Negative
563e0f5c2d1761a701f0f7e1	X	You can access Walrus using the current Amazon AWS SDK for C#.
  Negative
It just doesn't work quite the way you would expect if your Walrus URL contains a path component like... http://eucalyptus.test.local:8773/services/Walrus Here's how to setup the AmazonS3Client and a request.
  Negative
Notice that the path portion of the service url gets put into the bucketname.
  Negative
I've tested this setup with presigned urls and with the DeleteObjectRequest.
  Negative
I'm using version 1.5.19.0 of the SDK from https://github.com/aws/aws-sdk-net
563e0f5c2d1761a701f0f7e2	X	AWSSDK for .
  Negative
NET doesn't seem to understand port numbers in ServiceUrls... I just wrote a patch for this in the SimpleDB client code the other day, but I haven't actually looked at it in the S3 client... You could try temporarily hosting you Walrus service on port 80 and retesting to verify if this is the problem.
  Very negative
563e0f5c2d1761a701f0f7e3	X	beware CORS not supported in all browsers
563e0f5c2d1761a701f0f7e4	X	thx, i'm ok with enable-cors.
  Negative
org/client.
  Neutral
html
563e0f5c2d1761a701f0f7e5	X	I have an angularjs app that I want to host on amazon s3, is there a way to get it to play nice with a sinatra api hosted on heroku?
  Negative
I need to use ng-resource to make put requests to the heroku app, e.g. at a subdomain rest.site.com where site.com is s3.
  Negative
.
  Neutral
anticipate same origin policy issues.
  Neutral
.
  Neutral
Edit: The following looks like a start; http://beckyl11.blogspot.co.uk/2012/07/allowing-cross-origin-requests-cors-in.html
563e0f5c2d1761a701f0f7e6	X	You talking about downloading from your logging bucket?
  Negative
563e0f5c2d1761a701f0f7e7	X	See the AWS docs on Server Access Logging?
  Negative
563e0f5c2d1761a701f0f7e8	X	I guess this is not exaclty what i am looking for, what it really give an global idea of working with ASW.
  Negative
Thanks gray, your comment was the most constructive of all :)
563e0f5d2d1761a701f0f7e9	X	I am looking for a way to track downloads of files that are stored in the amazon S3 service.
  Negative
I downloaded the API, and then I get pretty lost.
  Negative
I just want to know what objects and methods I have to use from the API in order to get the information of the files stored there.
  Negative
I want to track downloads including the addresses of those that make the requests.
  Neutral
Thanks for advance.
  Neutral
563e0f5d2d1761a701f0f7ea	X	If you are talking about downloading the files in a logging bucket, we use something like the following: Here's the sample code: If this isn't what you want, please edit your post so we can help you better.
  Negative
563e0f5d2d1761a701f0f7eb	X	I have tried using the SDK as it has a signature creater, and it is where I originally started with this process ... I have even asked Amazon to help directly and they say "here, read our web page" .
  Very negative
.
  Neutral
so much for a paid support plan.
  Neutral
563e0f5d2d1761a701f0f7ec	X	I'm confused, here.
  Negative
Are you trying to make the S3 requests through Flex, or through PHP?
  Negative
563e0f5d2d1761a701f0f7ed	X	My original attempt and current succesful file upload is through Flex, but I then realized that if two people upload the same file name, it will overwrite the current file on S3.
  Very negative
I decided to try and use a REST interface with PHP to "rename" the file but extensive attempts have failed.
  Negative
I was hoping to head back to the original working Flex application I had written to rename the file during upload so I could avoid the REST (PHP) solution entirely.
  Negative
I have not found a Flex solution, so am still persuing all avenues.
  Negative
REST still is failing for me.
  Negative
I can provide a URL to the PHP REST I am using
563e0f5d2d1761a701f0f7ee	X	a Copy worked with the SDK... and to tell you the truth ... I was using the SDK that was available when i started this project (1.2.6) .
  Negative
.
  Neutral
they must had made some changes since then, when i grabbed 1.3.1 and tried the script .
  Neutral
.
  Neutral
viola.
  Neutral
You sir earn points.
  Neutral
THANK YOU
563e0f5d2d1761a701f0f7ef	X	tl;dr, but +1 to @Charles for all the hard work!
  Negative
563e0f5d2d1761a701f0f7f0	X	I have been trying for weeks to properly format a REST request to the Amazon AWS S3 API using the available examples on the web but have been unable to even successfully connect.
  Negative
I have found the code to generate a signature, found the proper method for formatting the "string to encode", and the http headers.
  Positive
I have worked my way through the signatureDoesNotMatch errors just to get a Anonymous users can not perform copy functions, Please authenticate message.
  Negative
I have a working copy of an Adobe Flex application that successfully uploads files, but with their "original" file name.
  Positive
The point of using the REST with the Amazon API is to perform a PUT (copy) of the file, just so I can rename it to something my back end system can use.
  Positive
If I could find a way to get this REST submission to work, or perhaps a way to specify a "new" filename within Flex while uploading I could avoid this whole REST situation all together.
  Negative
If anyone has successfully performed a PUT/Copy command on the Amazon API via REST I would be very interested in how this was accomplished - OR - if anyone has been able to change the destination file name using the Flex fileReference.browse() method I would also be eternally grateful for any pointers.
  Negative
When I submit a malformed or incorrect header I get the corresponding error message as expected: Query: PUT /bucket/1-132-1301047200-1.
  Negative
jpg HTTP/1.1 Host: s3.amazonaws.com x-amz-acl: public-read Connection: keep-alive Content-Length: 34102 Date: Sat, 26 Mar 2011 00:43:36 +0000 Authorization: AWS -removed for security-:GmgRObHEFuirWPwaqRgdKiQK/EQ= HTTP/1.1 403 Forbidden x-amz-request-id: A7CB0311812CD721 x-amz-id-2: ZUY0mH4Q20Izgt/9BNhpJl9OoOCp59DKxlH2JJ6K+sksyxI8lFtmJrJOk1imxM/A Content-Type: application/xml Transfer-Encoding: chunked Date: Sat, 26 Mar 2011 00:43:36 GMT Connection: close Server: AmazonS3 397 SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided.
  Negative
Check your key and signing method.50 55 54 0a 0a 0a 53 61 74 2c 20 32 36 20 4d 61 72 20 32 30 31 31 20 30 30 3a 34 33 3a 33 36 20 2b 30 30 30 30 0a 78 2d 61 6d 7a 2d 61 63 6c 3a 70 75 62 6c 69 63 2d 72 65 61 64 0a 2f 6d 6c 68 2d 70 72 6f 64 75 63 74 69 6f 6e 2f 31 2d 31 33 32 2d 31 33 30 31 30 34 37 32 30 30 2d 31 2e 6a 70 67A7CB0311812CD721ZUY0mH4Q20Izgt/9BNhpJl9OoOCp59DKxlH2JJ6K+sksyxI8lFtmJrJOk1imxM/AGmgRObHEFuirWPwaqRgdKiQK/EQ=PUT Sat, 26 Mar 2011 00:43:36 +0000 x-amz-acl:public-read /bucket/1-132-1301047200-1.
  Negative
jpg-removed for security- 0 but when sending properly formatted requests, it says I'm not authenticated: Query being used: PUT /1-132-1301047200-1.jpg HTTP/1.1 Host: bucket.s3.amazonaws.com Date: Sat, 26 Mar 2011 00:41:50 +0000 x-amz-copy-source: /bucket/clock.
  Negative
jpg x-amz-acl: public-read Authorization: AWS -removed for security-:BMiGhgbFnVAJyiderKjn1cT7cj4= HTTP/1.1 403 Forbidden x-amz-request-id: ABE45FD4DFD19927 x-amz-id-2: CnkMmoF550H1zBlrwwKfN8zoOSt7r/zud8mRuLqzzBrdGguotcvrpZ3aU4HR4RoO Content-Type: application/xml Transfer-Encoding: chunked Date: Sat, 26 Mar 2011 00:41:50 GMT Server: AmazonS3 AccessDenied Anonymous users cannot copy objects.
  Negative
Please authenticate ABE45FD4DFD19927CnkMmoF550H1zBlrwwKfN8zoOSt7r/zud8mRuLqzzBrdGguotcvrpZ3aU4HR4RoO 0 Date: Sat, 26 Mar 2011 00:41:50 GMT Connection: close Server: AmazonS3
563e0f5d2d1761a701f0f7f1	X	I have been trying for weeks to properly format a REST request to the Amazon AWS S3 API using the available examples on the web Have you tried the Amazon AWS SDK for PHP?
  Very negative
It's comprehensive, complete, and most importantly, written by Amazon.
  Positive
If their own code isn't working for you, something's gonna be really wrong.
  Negative
Here is example code using the linked SDK to upload example.txt in the current directory to a bucket named 'my_very_first_bucket'.
  Negative
Appropriate API docs: You can navigate the rest of the methods in the left menu.
  Negative
It's pretty comprehensive, including new bucket creation, management, deletion, same for objects, etc.
  Negative
You should be able to basically drop this in to your code and have it work properly.
  Positive
PHP 5.2-safe.
  Neutral
Edit by Silver Tiger: Charles -      The method you provide is using the API SDK functions to upload a file from the local file system to a bucket of my choosing.
  Negative
I have that part working already via Flex and uploads work like a charm.
  Positive
The problem in question is being able to submit a REST request to AWS S3 to change the file name from it's current "uploaded" name, to a new name more suited name that will work with my back end (database, tracking etc, which I handle and display seperately in PHP with MyySQL).
  Negative
AWS S3 does not truly support a "copy" function, so they provided a method to re-"PUT" a file by reading the source from your own bucket and placing a new copy with a different name in the same bucket.
  Negative
The difficulty I have been having is processing the REST request, hence the HMAC encryption.
  Negative
I do appreciate your time and understand the example you have provided as i also have a working copy of the PHP upload that was functioning before I designed the Flex application.
  Negative
The reason for the Flex was to enable status updates and a dynamically updated progress bar, which is also working like a charm :).
  Negative
I will continue to persue a REST solution as from the perspective of Amason zupport, it will be the only way i can rename a file already existing in my bucket per thier support team.
  Negative
As always, if you have input or suggestions regarding the REST submission I would be greatful for any feedback.
  Negative
Thanks, Silver Tiger Proof copy/delete works: Edit by Silver Tiger: Charles -      No REST needed, no bothers ... SDK 1.3.1 and your help solved the issue.
  Negative
the code I used to test looks a lot like yours : Now I will implement the delete after the copy, and we're golden.
  Negative
Thank you sir for your insight and help.
  Positive
Silver Tiger
563e0f5d2d1761a701f0f7f2	X	I´ve got JWPlayer installed and I´ve got an Amazon S3 bucket with some videos.
  Negative
Those videos are meant to be private, allowing only my site´s visitors to watch them.
  Neutral
Now, I´ve got this code, that´s working: What I don´t know how to manage is the Amazon´s video itself.
  Negative
I mean, I have a public and a private key, where should I put them?
  Negative
Because if I just post the video´s links (https://...) it won´t play, because it salys "forbidden" (naturally).
  Positive
Any ideas on what should I do?
  Neutral
I´ve look into jwplayer´s API and didn´t found anything, not at the setup wizard... Any help will be very much appreciated!
  Negative
Rosamunda
563e0f5d2d1761a701f0f7f3	X	I think you should provide a proxy for auth, setting a server side cookie for user's session in order to let him watch your video but avoid embedding it or linking it on different domains.
  Negative
There's no other way to do it without letting videos to be public.
  Negative
Sorry if this comes a little late (I just saw your question).
  Negative
563e0f5e2d1761a701f0f7f4	X	I think this is the reason!
  Neutral
563e0f5e2d1761a701f0f7f5	X	sorry folks, it doesn't work.
  Negative
same problem, same error message too.
  Very negative
I know that the file is being read as I get a file not found error if I change it's name.
  Negative
I also know that the ENV variables are being read as I get a exception if I change the name of the key in S3.rb.
  Negative
Anyway, it's not the end of hte world and I think I will just move on from here and leave it alone.
  Negative
It just means that I will have to be very careful when putting stuff on github but I can live with that.
  Neutral
Thanks for your help.
  Positive
If I get it working I'll let you all know.
  Positive
563e0f5e2d1761a701f0f7f6	X	also - puts ENV['S3_KEY'] from rails console works fine - so I reckon it might just be a paperclip issue - thanks anyway
563e0f5e2d1761a701f0f7f7	X	I appologise in advance for the length of this post.... I'm developing a rais app that uses paperclip to store stuff on Amazon S3.
  Negative
The app is hosted on Heroku.
  Negative
I'm developing on Ubuntu Karmic.
  Negative
The problem that I am about to describe occurs in development (on my localhost) and production (on Heroku).
  Negative
The standard way of passing S3 creds to paperclip is by putting them in config/s3.
  Negative
yml like so: When I do this, everything works just fine.
  Positive
But this makes it difficult to share my code with others so Heroku suggest an alternative method - http://docs.heroku.com/config-vars.
  Negative
They advise that you should put your S3_KEY and S3_SECRET into your .
  Negative
bashrc like so: They then suggest that you create config/initializers/s3.
  Negative
yml (note the slightly different path) and put the following into that file: BUT, When I do this, paperclip throws a wobbler and spits out the following error message: So clearly it's all kicking off inside the storage.rb module.
  Negative
Stepping through the stack trace: The parse_credentials method on Line 176 is flagged - here's the call as it appears in the code: The parse_credentials method attempts to call another method, find_credentials, and this is where I believe the problem lies.
  Very negative
Heres the code for find_credentials: I can't see how the find_credentials method is equipped to read values from my .
  Negative
bashrc file.
  Neutral
It's got two cases where it can read from YAML and one where it's looking for a hash.
  Negative
My model references the credentials like so: If I remove the :s3_credentials hash from the model, the stringify_keys error goes away and the rails console throws the error message that appears at the end of the find_credentials method: i.e. "Credentials are not a path, file, or hash".
  Negative
So I'm stumped.
  Negative
I realise that this is possibly a question for the guys at Heroku (who I am actually going to email this link to in the hope that they can answer it) and it's also possibly a question for the doods at thoughtbot, but I thought that StackOverflow might be the best place to ask this as it's proven to be quite a reliable forum for me in the past.
  Positive
As I said at the beginning, my app works fine when I take the standard approach of sticking my key and secret into config/s3.
  Positive
yml, but I would prefer to use the method that Heroku suggest because it makes things WAY easier for me and it means I can store my repo on my public github page for others to use without having to write any customer merge drivers in Git to keep my api keys out of the public domain.
  Negative
Once again, sorry for the lengthy post, and if you've made it this far, I applaud you.
  Negative
Anyone got any ideas?
  Negative
I've tried sticking the ENV variables in etc/bash.
  Negative
bashrc as well as ~/.
  Negative
bashrc and after rebooting, I still have the same problem.
  Negative
The problems occur on development machine as well as on Heroku.
  Negative
I've made sure to push my config-vars to Heroku as well.
  Negative
I've been at it for 8 hours straight!!
  Negative
I'm gonna go watch the football now.
  Negative
563e0f5e2d1761a701f0f7f8	X	After much searching I found the answer here - http://tammersaleh.com/posts/managing-heroku-environment-variables-for-local-development The trick is to remove the S3.rb file altogether and just refer to the ENV variables in the model like so: Anyway, David, thanks for your suggestion.
  Negative
I don't know if you want to update the Heroku docs to say that some users have had to do it this way.
  Negative
Thanks again though.
  Positive
563e0f5e2d1761a701f0f7f9	X	Rename the file config/initializers/s3.
  Negative
yml to config/initializers/s3.
  Neutral
rb and give it a try.
  Positive
563e0f5e2d1761a701f0f7fa	X	Here's your problem: needs to be i.e. the assignments are not being interpreted.
  Negative
563e0f5e2d1761a701f0f7fb	X	I want to upload multiple files in to amazon using S3 API.
  Negative
This is my code uploadcontentintoamazone.php API integration is working perfectly.
  Positive
But temp file is uploaded instead of my uploaded file.
  Negative
Thanks in advance
563e0f5e2d1761a701f0f7fc	X	try this way:
563e0f5e2d1761a701f0f7fd	X	You are iterating over the wrong array, you need to do it like this: (you also didn't set the destination filename properly)
563e0f5f2d1761a701f0f7fe	X	If the bucket has an ACL that permits anonymous access then you can retrieve a file by the following pattern: //s3.amazonaws.com/<bucket name>/<full key name with slashes>.
  Negative
Is that what you as looking for?
  Neutral
563e0f5f2d1761a701f0f7ff	X	@JasonSperske It is in a private bucket.
  Positive
563e0f5f2d1761a701f0f800	X	Have you tried this?
  Neutral
I did something similar and got a 403 signature doesn't match.
  Negative
Same was described in that forum post.
  Neutral
"Unfortunately it's not quite correct to say you can do it both ways.
  Negative
It's fine if you're using the URL generated by the Java SDK in a generic fashion.
  Neutral
Unfortunately if you hand that URL off to a .
  Negative
NET app and it uses it's standard WebRequest class to consume the URL, .
  Negative
NET decodes the %2F to a / and the request then fails with a 403 - Signature does not match."
  Negative
563e0f5f2d1761a701f0f801	X	As I understood your post it works if you manually use the slahes, but you don't want to write the code to do this, no?
  Negative
Using url.getQuery should do exactly this.
  Neutral
563e0f5f2d1761a701f0f802	X	You can't mix and match.
  Negative
When generatePresignedUrl is called it takes the entire key and escapes it, and then creates a signature using the escaped key.
  Positive
You cannot use the signature from the escaped key and combine it with the unescaped key in the URL.
  Negative
Hypothetically, if you could perform the signature code without escaping, you could use the key without escaping.
  Negative
I was just wondering if someone has an easy way of doing this.
  Negative
563e0f5f2d1761a701f0f803	X	Hey, sorry for this.
  Negative
I responded too fast.
  Negative
Doing some more research and looking at the code, I came to the same conclusion.
  Negative
See my recent edit.
  Positive
Long story short I think the easiest way would be to override the erronous method or alternatively implement your own version by checking out how Amazon does it internally.
  Negative
563e0f5f2d1761a701f0f804	X	Yes, I came to the same conclusion and walked through the code and its at a frustratingly inconvenient place.
  Negative
That's why I came here :) Figured someone probably did it already.
  Negative
563e0f5f2d1761a701f0f805	X	I am using the Java Amazon SDK to work with S3 for storing uploaded files.
  Negative
I would like to retain the original filename, and I am putting it at the end of the key, but I am also using virtual directory structure - something like <dirname>/<uuid>/<originalFilename>.
  Very negative
The problem is that when I want to generate a presigned URL for downloading using the api like: The sdk url escapes the entire key, including the slashes.
  Negative
While it still works, it means that the name of the file downloaded includes the entire key instead of just the original filename bit at the end.
  Negative
I know that it should be possible to do this without escaping the slashes, but I'm trying to avoid rewriting a lot of the code already in the SDK.
  Negative
Is there a common solution to this?
  Neutral
I know I've used web apps that follow the same pattern and do not have the slash escape problem.
  Negative
563e0f5f2d1761a701f0f806	X	This is a bug in the current Java SDK: If you look at https://github.com/aws/aws-sdk-java/blob/master/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L2820 The method presignRequest which is called internally has the following code: The key is URL encoded here before signing which I think is the error.
  Very negative
You might be able to inherit from the AmazonS3Client and override the funcion to fix this.
  Neutral
In some places it is suggested to use url.getQuery() and prefix this with your original awsURL (https://forums.aws.amazon.com/thread.jspa?messageID=356271).
  Negative
However as you said yourself this will produce an error, because the resource key will not match the signature.
  Negative
The following problem might also be related, I did not check out the proposed workarround: How to generate pre-signed Amazon S3 url for a vanity domain, using amazon sdk?
  Negative
Amazon recognized and fixed a similar bug before: https://forums.aws.amazon.com/thread.jspa?messageID=418537 So I hope it will be fixed in the next version.
  Negative
563e0f5f2d1761a701f0f807	X	I'm still hoping for a better solution than this, but seeing as @aKzenT has confirmed my conclusion that there is not an existing solution for this I wrote one.
  Negative
Its just a simple subclass of AmazonS3Client.
  Neutral
I worry it's brittle because I had to copy a lot of code from the method I overrode, but it seems like the most minimal solution.
  Negative
I can confirm that it works fine in my own code base.
  Positive
I posted the code in a gist, but for the sake of a complete answer: UPDATE I updated the gist and this code to fix an issue I was having with ~'s.
  Negative
It was occurring even using the standard client, but unescaping the ~ fixed it.
  Negative
See gist for more details/track any further changes I might make.
  Negative
563e0f5f2d1761a701f0f808	X	version 1.4.3 of the Java SDK seems to have fixed this problem.
  Negative
Perhaps, it was fixed earlier, but I can confirm that it is working correctly in 1.4.3.
  Negative
563e0f5f2d1761a701f0f809	X	We have a large number of people (10k+) who return to my clients' sites on a regular basis to use a web app we built, improve, and host for them.
  Negative
We have been making fairly frequent backward-incompatible updates to the web app's javascript as our app has improved and evolved.
  Negative
During deployments, the javascript is minified and concatenated into one file, loaded in the browser by require.js, and is uploaded to and hosted on Amazon S3.
  Negative
The file name & url currently doesn't change at all during updates.
  Negative
This last week we deployed a major refactor to the web app and got a few (but not a lot) of reports back that the app stopped working for some people, particularly in firefox.
  Negative
It seemed like a caching issue.
  Neutral
We were able to see it initially in a few browsers in testing but it seemed to go away after a refresh or two.
  Neutral
It dawned on me that I really don't know what browser-caching ramifications deploying a new version of a javascript file (with the same name) on S3 will have and whether this situation warrants cache-busting or manipulating S3's headers or anything.
  Negative
Can someone help me get a handle on this?
  Neutral
Are there actions I should be taking during deployments to ensure that browsers will immediately get the new version of a javascript file?
  Negative
If not, we run the risk of the javascript and the server API being out of sync and failing, which I think happened here.
  Negative
Not sure if it matters, but the site's server runs Django and the app and DB are deployed to Heroku.
  Negative
Static files are deployed to S3 using S3Boto via Django's collectstatic command.
  Negative
563e0f5f2d1761a701f0f80a	X	This depends a lot on the behaviour of S3 and the headers it sends when requesting files on S3.
  Negative
As you experienced, browsers will show different caching behaviour - so the best option is to use unique filenames.
  Positive
I would suggest to use cachebuster hashes - in this way you can be sure that the new file always gets requested by browsers and you can use long cache-lifetime headers if you host the files on your own server.
  Negative
You can for example create a MD5 hash of your minified file and append it (like mycss-322242fadcd23.
  Negative
css).
  Neutral
Or you could use the revision number of your source control system.
  Neutral
You have to use the cache buster in all links to this file, but you can normally easily do this in your templates where you embed your static resources.
  Negative
Depending on your application, you could probably use this Django plugin that should do this work for you.
  Negative
563e0f602d1761a701f0f80b	X	Say we want a REST API to support file uploads, and we want uploads to be done directly on S3.
  Negative
According to this solution Amazon S3 direct file upload from client browser - private key disclosure, we have to create POLICY and SIGNATURE for user to be allowed to upload to S3.
  Negative
However, we want a single entry point for the API, including uploads.
  Neutral
Can we: 1.
  Neutral
in our API, catch POST https://www.example.org/users/1234/objects 2.
  Negative
calculate POLICY and SIGNATURE to allow direct upload to S3 3.
  Negative
return a 307 "Temporary Redirect" to https://s3-bucket.s3.amazonaws.com How to pass POLICY and SIGNATURE in the redirect?
  Negative
What is best practice here?
  Positive
563e0f602d1761a701f0f80c	X	You dont redirect, instead your API should return the policy and signature in the response (say in JSON).
  Negative
Then the browser can use these values to directly upload to S3 as in the document.
  Neutral
This is a two step process.
  Neutral
563e0f602d1761a701f0f80d	X	Is there an error ?
  Negative
563e0f602d1761a701f0f80e	X	Its not an error, but i guess this is not correct way of authenticating endpoint...
563e0f602d1761a701f0f80f	X	I've amazon aws s3 endpoint, access key and secret key, i want to validate my endpoint using these key in java, can someone help how to test my endpoint if you can share some sample code will be very helpful.
  Very negative
Thanks I tried the below code but its not authenticating:
563e0f602d1761a701f0f810	X	An endpoint is an Amazon supplied value.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html E.g. s3.amazonaws.com for US Standard.
  Negative
Or s3-<region>.
  Neutral
amazonaws.com, E.g. s3-eu-west-1.
  Negative
amazonaws.com You are using an incorrect endpoint.
  Negative
Plus in most situations you want to set the Amazon Region, not the endpoint specifically.
  Negative
See http://docs.aws.amazon.com/AmazonS3/latest/dev/create-bucket-get-location-example.html for an example.
  Positive
With your Account credentials, Region and Bucket name, Amazon S3 API has all it needs to find your files.
  Negative
563e0f602d1761a701f0f811	X	Thanlks for the reply.
  Negative
I put my libraries which come with jets#t in Applibs folder and my jars in classes folder.
  Negative
I am still getting the above error.
  Negative
Am I placing these files correctly?
  Negative
I am a total noob to Glassfish btw.
  Negative
563e0f602d1761a701f0f812	X	What is applib?
  Negative
Why you put your jar in classes?
  Negative
Have you actually read my answer?
  Neutral
563e0f602d1761a701f0f813	X	Applibs and Classes are two of the directories I found in the domains/domain1/lib folder ... but anyway , I followed your method and dumped everything related to jets3t in it and it worked.
  Very negative
Thanks for the effort.
  Positive
563e0f602d1761a701f0f814	X	Where do I put the Amazon JetS3t and the related jar files in Glassfish 3?
  Negative
I have a sample Restful Web application which currently uses a Arraylist to maintain objects at back-end, I would like them to be stored as text files on Amazon S3.
  Negative
So I am using jets@t API for that.
  Negative
When I deploy the application onto Glassfish, it throes an error like this Thanks in advaance for your help.
  Negative
563e0f602d1761a701f0f815	X	GlassFish v3 has a well defined Class Loader hierarchy which identifies the common class loader as the proper way to deal with shared libraries.
  Negative
So to make a long story short, putting you libraries and other framework JARs in domains/domain1/lib is all you need to do.
  Negative
Good luck!
  Positive
563e0f602d1761a701f0f816	X	Can I use redis for caching images as I have that installed.
  Negative
Memcache is there too as part of Mysql 5.6 distro.
  Negative
563e0f612d1761a701f0f817	X	Go ahead with Redis then, it is cool.
  Positive
563e0f612d1761a701f0f818	X	I am using Amazon S3 for storing images and YII CListView is used to list each page item.
  Negative
Inside partial view that represents each item of CListView, there are 2 calls to Amazon S3 api , once for checking if file exists and second to actually fetch image and show.
  Negative
Since page size is 20 , and average latency is 200ms per image and therefore for 20 images, total of 20 * 200ms * ( 2 requests per item ) = 8 seconds are taken Since in javascript , I hide the view till all page elements are loaded, so it keeps on loading for 8 to 10 seconds when page size is 20 and it is quite slow.
  Very negative
.
  Neutral
Do we have some YII extension that has solved this problem with CListView and S3 download integration which does it faster ?
  Negative
563e0f612d1761a701f0f819	X	Considering that when you cold launch your app, you don't know nothing about your images, the images might or not exists on S3, I don't see much options you can take here, just one: cache.
  Negative
Having cached the retrieved items would make the difference.
  Negative
Memcache would suit if your item size is not bigger than 1M.
  Negative
On the other hand, I would reduce drastically the calls to S3 (as they cost), maybe keep locally a flag to know if the image exists on S3 or not.
  Negative
And would directly link to CDN rather than fetch it.
  Negative
You can even preload the next pages, based on where the user is.
  Negative
You can use a Message Queue as Beanstalkd to manage background jobs.
  Negative
563e0f612d1761a701f0f81a	X	thanks for your reply.
  Negative
There is no example for multipart upload in given samples of SDK that's why I'm asking this.
  Negative
But I have found some code from somewhere else which is given below, but its giving exceptions which are given below.
  Negative
Please check it if you can help me in that.
  Neutral
563e0f612d1761a701f0f81b	X	thanks for your reply.
  Negative
Please check Edit part of my question if you can help me in solving my problem.
  Negative
563e0f612d1761a701f0f81c	X	in response you must be getting string that need tobe signed.
  Negative
.
  Neutral
just double check the string you are signing is correct.
  Neutral
563e0f612d1761a701f0f81d	X	I have changed base64 conversion method, now its giving response "HTTP/1.0 411 Length Required".
  Negative
Any idea what I'm doing wrong??
  Negative
563e0f612d1761a701f0f81e	X	are u setting "Content-Length" in header?
  Negative
563e0f612d1761a701f0f81f	X	No I'm not setting Content-Length in header.
  Negative
563e0f612d1761a701f0f820	X	I'm searching on google from last two days regarding using amazon multipart upload api in my cocoa project for mac.
  Negative
I have downloaded AWS sdk for ios.
  Negative
But didn't find how to use that sdk in cocoa project.
  Negative
Can anyone provide me some example code to achieve multipart uploading using amazon S3 multipart upload???
  Negative
Edit: As AWS SDK for IOS is not compatible with Cocoa applications, I'm using Rest api to upload file using libcurl.
  Negative
I'm using following code (by taking reference from http://dextercoder.blogspot.in/2012/02/multipart-upload-to-amazon-s3-in-three.html): But its giving response "< HTTP/1.0 403 Forbidden SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided.
  Negative
Check your key and signing method."
  Neutral
563e0f612d1761a701f0f821	X	You have a basic examples for AWS services like S3, SDB etc provided with the SDK itself: http://docs.amazonwebservices.com/mobile/sdkforios/gsg/Welcome.html?r=1498
563e0f612d1761a701f0f822	X	you can refer java code from http://dextercoder.blogspot.in/2012/02/multipart-upload-to-amazon-s3-in-three.html.
  Negative
tells how to upload file in multiparts without AWS.
  Neutral
563e0f642d1761a701f0f823	X	Some off-topic things: Another way to get that dict: __import__('collections').
  Negative
Counter(len(str(__import__('random').
  Negative
random())) for i in range(0, 1000000)).
  Negative
563e0f642d1761a701f0f824	X	if you look at Math.random().
  Neutral
toString(2).
  Neutral
length you'll see a different pattern, perhaps more to what you'd expect
563e0f642d1761a701f0f825	X	@KevinGuan And the JavaScript ES6 counterpart (sortof): Array.apply(null, Array(1e5)).
  Neutral
map(() => Math.random()).
  Negative
reduce((records, n) => records[String(n).
  Negative
length] ?
  Neutral
(records[String(n).
  Negative
length] += 1, records) : (records[String(n).
  Negative
length] = 1, records), {})
563e0f642d1761a701f0f826	X	A JS version shorter than the previous: Array.apply(null, Array(1e5)).
  Negative
map(() => Math.random()).
  Negative
reduce((records, n) => (records[String(n).
  Negative
length] = records[String(n).
  Negative
length] + 1 || 1, records), {})
563e0f642d1761a701f0f827	X	@Jaromanda X, binary representation will show a similar effect.
  Negative
It has to do with the difference between number of significant digits and the number of digits needed to represent tiny numbers without using exponents.
  Positive
563e0f642d1761a701f0f828	X	It should be noted that Javascript has a Number.toFixed() method when one want to make a mathematical use of the string representation of the number.
  Negative
563e0f642d1761a701f0f829	X	I encounter this curious phenomenon trying to implement a UUID generator in JavaScript.
  Negative
Basically, in JavaScript, if I generate a large list of random numbers with the built-in Math.random() on Node 4.2.2: The numbers of digits have a strange pattern: I thought this is a quirk of the random number generator of V8, but similar pattern appears in Python 3.4.3: And the Python code is as follows: The pattern from 18 to below is expected: say if random number should have 20 digits, then if the last digit of a number is 0, it effectively has only 19 digits.
  Negative
If the random number generator is good, the probability of that happening is roughly 1/10.
  Negative
But why the pattern is reversed for 19 and beyond?
  Negative
I guess this is related to float numbers' binary representation, but I can't figure out exactly why.
  Negative
563e0f642d1761a701f0f82a	X	The reason is indeed related to floating point representation.
  Negative
A floating point number representation has a maximum number of (binary) digits it can represent, and a limited exponent value range.
  Negative
Now when you print this out without using scientific notation, you might in some cases need to have some zeroes after the decimal point before the significant digits start to follow.
  Negative
You can visualize this effect by printing those random numbers which have the longest length when converted to string: This prints only the 23-long strings, and you will get numbers like these: Notice the zeroes before the first non-zero digit.
  Negative
These are actually not stored in the number part of a floating point representation, but implied by its exponent part.
  Negative
If you were to take out the leading zeroes, and then make a count: ... you'll get results which are less strange.
  Negative
However, you will see some irregularity that is due to how javascript converts tiny numbers to string: when they get too small, the scientific notation is used in the string representation.
  Negative
You can see this with the following script (not sure if every browser has the same breaking point, so maybe you need to play a bit with the number): This gives me the following output: So very small numbers will get a more fixed string length as a result, quite often 22 characters, while in the non-scientific notation a length of 23 is common.
  Very negative
This influences also the second script I provided and length 22 will get more hits than 23.
  Neutral
It should be noted that javascript does not switch to scientific notation when converting to string in binary representation: The above will print a string of over 450 binary digits!
  Negative
563e0f642d1761a701f0f82b	X	It's because some of the values are like this: And thus they're longer.
  Negative
563e0f652d1761a701f0f82c	X	I am developing an iOS app that uses a large amount of images that are needed for animations for short videos.
  Negative
I want to save my application assets as static files in cloud and once they are needed download them using secure API call (either JSON, XML or any other alternative for that matter).
  Negative
What is the best option for that.
  Positive
I have checked Parse, Dropbox, iCloud, Google Drive, but I am puzzled since I see only instructions for dynamic data that lets users access content they have created and not static assets.
  Very negative
What would be best option for that?
  Neutral
563e0f652d1761a701f0f82d	X	If you just want an easy way to serve static files I would take a look at Amazon S3.
  Negative
You can just upload files through the online console and then get the public URL to those files to use in your app.
  Negative
You can also use the S3 API to upload files through your web service or iOS app.
  Negative
Hope this helps!
  Positive
563e0f652d1761a701f0f82e	X	I'd go for Parse (basically because it is fast to learn and develop), you can create a table with the images and change the writing permissions if you are afraid somebody could modify the table.
  Negative
Another option that you can check it's the special Config table so you can upload custom files (zip files i.e.) and download them in demand.
  Negative
563e0f652d1761a701f0f82f	X	Choose an answer ?
  Neutral
563e0f652d1761a701f0f830	X	doesnt seem to work.
  Negative
both.get empty page, despite all files public and not running as website bucket.
  Negative
563e0f652d1761a701f0f831	X	@devdude, can you turn on Javascript debugging to see if the script is failing anywhere?
  Negative
563e0f652d1761a701f0f832	X	Firebug: response is null handleList()list.html (line 104) [Break On This Error] filex = response.getElementsByTagName('Contents');
563e0f652d1761a701f0f833	X	Didn't worked for me neither, Rufus Pollock's solution worked well
563e0f652d1761a701f0f834	X	Great job, works perfectly and very well documented
563e0f662d1761a701f0f835	X	Are these APIs callable from Javascript?
  Negative
Any samples?
  Neutral
otherwise I'm not sure how I'd use them from within a browser.
  Negative
I was hoping for something I could serve up from within the S3 space.
  Neutral
563e0f662d1761a701f0f836	X	I have an Amazon S3 account where I would like to store several directories of files.
  Negative
I would like a visitor to this site to be able to see and download the files and folders I have placed there.
  Negative
These files and folders will change regularly and I would prefer not to have to rewrite any html each time I added or removed files.
  Negative
How can I arrange for the viewers of my site to be presented with a simple list of files/ folders?
  Neutral
563e0f662d1761a701f0f837	X	You can use Javascript to list the files.
  Negative
Here is the solution provided by Amazon: http://aws.amazon.com/code/Amazon-S3/1713 You place list.html in every directory you want to list.
  Negative
I have made my own listing file that provides a collapsible tree view: https://github.com/phatmann/jS3Tree/blob/master/index.html Neither of these files will work if you are using the S3 website feature.
  Negative
563e0f662d1761a701f0f838	X	I've created a simple bit of JS that creates a directory index in HTML style that would fit what you are looking for: https://github.com/rgrp/s3-bucket-listing You can install this either directly into your s3 bucket or into a separate website (thanks to the fact that the S3 REST API supports CORS!)
  Negative
.
  Neutral
The README has full instructions on this: https://github.com/rgrp/s3-bucket-listing
563e0f662d1761a701f0f839	X	you should use the Amazon S3 API to list the buckets and files within them a bucket can represent a folder (will be easier than using prefix's on the file name) after creating your buckets and uploading the files to them you can present the buckets on a page with a List All My Buckets request once a user clicks on a given bucket you can get the files in it using a List Bucket request another last click on the file to generate a url for the object so the user can download it without wasting your bandwidth you can find many implementations to the amazon s3 api here
563e0f662d1761a701f0f83a	X	in addition to what jnoller bellow suggests, boto also supports S3 and CloudFiles as well as Google Cloud Storage.
  Very negative
stackoverflow.com/questions/7624900/…
563e0f662d1761a701f0f83b	X	I got to know that Rackspace Cloud Files is based on OpenStack Object Storage service (Swift).
  Negative
As OpenStack allows configuring/manipulating object storage using S3 APIs through swift3 http://docs.openstack.org/trunk/openstack-object-storage/admin/content/configuring-openstack-object-storage-with-s3_api.html I am thinking whether Rackspace Cloud Files provides S3 API support as well.
  Negative
I have a client written for Amazon Web Services using S3 RESTful APIs so was thinking to reuse it for Rackspace Cloud Files as well.
  Negative
563e0f662d1761a701f0f83c	X	The S3 plugin for Swift is not deployed as part of Rackspace Cloud Files (most production deploys of openstack don't deploy it by default).
  Negative
However, if you want better flexibility in the app, you can use a cross cloud toolkit such as libcloud (python), fog (ruby), jclouds (java), pkgcloud (node/js).
  Neutral
This means you can use a simpler abstraction and support multiple providers within your application.
  Neutral
563e0f662d1761a701f0f83d	X	For your information: Google App Engine only supports JDK 5 & 6.
  Negative
developers.google.com/appengine/docs/java/gettingstarted/…
563e0f672d1761a701f0f83e	X	but in this link they told that problem fixed in app engine 1.7.3 but still i have problem so do you know about that thing may be some where my mistake in my code or configuration.thanks for your response
563e0f672d1761a701f0f83f	X	It seems that the bug-report differs to your problem.
  Negative
You are getting a NullPointerException at VersionInfo line 124.
  Neutral
I think System.getProperty throws the NullPointerException (os.name or os.version).
  Neutral
But in their documentation they write that System.getProperty is allowed.
  Negative
Try to set the properties on your own as explained HERE.
  Neutral
563e0f672d1761a701f0f840	X	ya you are right, i also try to put <system-properties> <property name="os.name" value="..." /></system-properties> this property but i don't know what i have write in value field?????
  Negative
i put different value than upload on app engine still its give me same error as above.
  Negative
563e0f672d1761a701f0f841	X	Debug your application on a local server and look which property throws the exception.
  Negative
Then try different strings that match the property.
  Neutral
I think most of the values are optional (but not all of them).
  Neutral
If that is not working try to LOG the properties (load them on startup and write them into your log file).
  Negative
563e0f672d1761a701f0f842	X	Hey da_re thanks above error gone when i write following code in appengine-web.xml <system-properties> <property name="java.util.logging.config.file" value="WEB-INF/logging.properties"/> <property name="os.name" value="Linux" /> <property name="os.version" value="6.1" /> <property name="java.vm.name" value="Java HotSpot(TM) Client VM" /> <property name="java.vm.version" value="23.5-b02" /> <property name="user.language" value="en" /> <property name="user.region" value="us" /> </system-properties>
563e0f672d1761a701f0f843	X	i am working on gwt2.4, jre7 and GAE 1.7.3.
  Negative
in development mode my code working properly but when i uploaded my apps on app engine than i am getting following error i am also find out solution on following link but still it didn't works (http://code.google.com/p/googleappengine/issues/detail?id=8166)
563e0f672d1761a701f0f844	X	//EDIT: He added the following XML to his appengine-web.xml to get it work: You can't use the Amazon Libs because of restrictions in the Google App Engine (no Threads, ...).
  Negative
But there are few solutions that should help you: [1] http://socialappdev.com/using-amazon-s3-with-google-app-engine-02-2011 [2] https://github.com/handstandtech/s3-simple-appengine Why don't you use Google Cloud Storage with the GAE Java SDK?
  Negative
The GCS API is almost the same as the S3 API.
  Negative
563e0f672d1761a701f0f845	X	some time app engine didn't get value of os.name,os.version,java.vm.name etc so we have to specify manually in appengine-web.xml file 
563e0f682d1761a701f0f846	X	What platform(s)
563e0f682d1761a701f0f847	X	App in iOS and Android backend using rails
563e0f682d1761a701f0f848	X	On iOS you might want to look at using AFNetworking for uploading your image to a rails RESTful API.
  Negative
563e0f682d1761a701f0f849	X	I have a web app in Rails that has a profile image upload, I`m using Refile gem to upload the photo direct to Amazon S3.
  Negative
But now I need to do this upload from a mobile app too.
  Negative
Witch is the best way to do that?
  Positive
I think about a Rest API in Rails that receives the binary data and then uses Async Jobs (Sidekiq) to upload to Amazon S3, but and not sure if the approach is best way.
  Negative
Can anyone help me?
  Neutral
Thanks!
  Positive
563e0f692d1761a701f0f84a	X	first sorry for my english I am new to this topic as well as mentioned in the title, I have to build this ios-app and after day-long search I found nothing useful.
  Negative
I already can get the facebook data.
  Negative
now I have to build a connection with mongodb.
  Neutral
and there I should call a method (http://myurl.com/api/auth).
  Negative
This method assumes the storage of facebook-data: -name -Birthday ... -and the url of the facebook-profile-picture.
  Negative
I have to call up another method that stores the image in amazon s3 (using the url).
  Negative
The method is called "createPhoto" and so must be called: http://myurl.com/api/createPhoto Now I'm looking for example code and how I can solve it best.
  Positive
I would be very happy if you could suggest me some links or codes.
  Negative
Thank you very much in advance
563e0f692d1761a701f0f84b	X	Any ideas?
  Negative
563e0f692d1761a701f0f84c	X	You could implement the front-end in pretty much anything that you can code to speak native S3 multipart upload... which is the approach I'd recommend for this, because of stability.
  Negative
With a multipart upload, "you" (meaning the developer, not the end user, I would suggest) choose a part size, minimum 5MB per part, and the file can be no larger that 10,000 "parts", each exactly the same size (the one "you" selected at the beginning of the upload, except for the last part, which would be however many bytes are left over at the end... so the ultimatel maximum size of the uploaded file depends on the part-size you choose.
  Negative
The size of a "part" essentially becomes your restartable/retryable block size (win!)
  Positive
... so your front-end implementation can infinitely resend a failed part until it goes through correctly.
  Negative
Parts don't even have to be uploaded in order, they can be uploaded in parallel, and if you upload the same part more than once, the newer one replaces the older one, and with each block, S3 returns a checksum that you compare to your locally calculated one.
  Negative
The object doesn't become visible in S3 until you finalize the upload.
  Negative
When you finalize the upload, if S3 hasn't got all the parts (which is should, because they were all acknowledged when they uploaded) then the finalize call will fail.
  Negative
The one thing you do have to keep in mind, though, is that multipart uploads apparently never time out, and if they are "never" either finalized/completed nor actively aborted by the client utility, you will pay for the storage of the uploaded blocks of the incomplete uploads.
  Negative
So, you want to implement an automated back-end process that periodically calls ListMultipartUploads to identify and abort those uploads that for whatever reason were never finished or canceled, and abort them.
  Negative
I don't know how helpful this is as an answer to your overall question, but developing a custom front-end tool should not be a complicated matter -- the S3 API is very straightforward.
  Negative
I can say this, because I developed a utility to do this (for my internal use -- this isn't a product plug).
  Neutral
I may one day release it as open source, but it likely wouldn't suit your needs anyway -- its essentially a command-line utility that can be used by automated/scheduled processes to stream ("pipe") the output of a program directly into S3 as a series of multipart parts (the files are large, so my default part-size is 64MB), and when the input stream is closed by the program generating the output, it detects this and finalizes the upload.
  Negative
:) I use it to stream live database backups, passed through a compression program, directly into S3 as they are generated, without ever needing those massive files to exist anywhere on any hard drive.
  Negative
Your desire to have a smooth experience for your clients, in my opinion, highly commends S3 multipart for the role, and if you know how to code in anything that can generate a desktop or browser-based UI, can read local desktop filesystems, and has libraries for HTTP and SHA/HMAC, then you can write a client to do this that looks and feels exactly the way you need it to.
  Very negative
You wouldn't need to set up anything manually in AWS for each client, so long as you have a back-end system that authenticates the client utility to you, perhaps by a username and password sent over an SSL connection to an application on a web server, and then provides the client utility with automatically-generated temporary AWS credentials that the client utility can use to do the uploading.
  Negative
563e0f692d1761a701f0f84d	X	Something like S3Browser would work.
  Negative
It has a GUI, a command line and works with large files.
  Positive
You can use IAM to create a group, grant that group access to a specific S3 bucket using a policy, then add IAM users to that group.
  Positive
Your IAM group policy would look something like this : Adding an IAM user to this group will allow them to use S3Browser and only have read-write access to YOUR_BUCKET_NAME.
  Negative
However, they will see a list of your other buckets, just not be able to read/write to them.
  Neutral
You'll also need to generate an AWS Access Key and Secret for each IAM user and provide those 2 items to whoever is using S3Browser.
  Negative
563e0f692d1761a701f0f84e	X	Thanks for the answer, found the solution.
  Positive
I added three libraries - aws-java-sdk-s3, aws-java-sdk-kms and aws-java-sdk-core
563e0f692d1761a701f0f84f	X	I am working on amazon S3 file upload operation in java.
  Negative
Amazon provides aws java sdk for this purpose but the thing is the sdk is of 20 mb including its dependencies.
  Negative
What I want is, a library which have only functionality of S3 related operation.
  Negative
P.S. I read that amazon provides REST API also.
  Neutral
So, did anyone tried that api in java.
  Negative
If yes, please help me by sharing the code snippet.
  Negative
Thanks
563e0f692d1761a701f0f850	X	aws now provides separate libraries too.
  Negative
If you are using maven then you can include just the s3 library.
  Negative
Below is the link for the same.
  Positive
http://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3/1.9.33
563e0f692d1761a701f0f851	X	What version of boto are you using?
  Negative
563e0f692d1761a701f0f852	X	I'm using AWS Cloudtrail.
  Negative
I would like to display the audit trail for a particular file on S3 using Python.
  Negative
How do I do this?
  Neutral
563e0f692d1761a701f0f853	X	AWS CloudTrail does not keep an audit of accesses made to Amazon S3 objects.
  Negative
CloudTrail only records API calls that related to Buckets.
  Negative
See: Logging Amazon S3 API Calls By Using AWS CloudTrail To obtain information about accesses made to Amazon S3 objects, enable Server Access Logging: In order to track requests for access to your bucket, you can enable access logging.
  Negative
Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and error code, if any.
  Positive
Access log information can be useful in security and access audits.
  Negative
It can also help you learn about your customer base and understand your Amazon S3 bill.
  Neutral
Access logs are similar to web server logs, showing each access to objects within Amazon S3.
  Negative
563e0f6a2d1761a701f0f854	X	Does it really have to use S3?
  Negative
Or can I specify other sources of input?
  Neutral
563e0f6a2d1761a701f0f855	X	@beefjerky yes, you do have to.
  Positive
see link to faq in edited answer
563e0f6a2d1761a701f0f856	X	Seems a bit weird to restrict input source to S3 though, but yeah, it does mention it in general faq - "In the request form, you specify the name of your cluster, the location in Amazon S3 of your input data, your processing application...".
  Negative
Thanks!
  Positive
563e0f6a2d1761a701f0f857	X	I'm just beginning to learn about Big Data and I'm interested in Hadoop.
  Negative
I'm planning on building a simple analytics system to make sense of certain events that occurs in my site.
  Neutral
So I'm planning to have code (both front and back end) to trigger some events that would queue messages (most likely with RabbitMQ).
  Negative
These messages will then be processed by a consumer that would write the data continuously to HDFS.
  Negative
Then, I can run a map reduce job at any time to analyze the current data set.
  Negative
I'm leaning towards Amazon EMR for the Hadoop functionality.
  Negative
So my question is this, from my server running the consumer, how do I save the data to HDFS?
  Negative
I know there's a command like "hadoop dfs -copyFromLocal", but how do I use this across servers?
  Negative
Is there a tool available?
  Neutral
Has anyone tried a similar thing?
  Negative
I would love to hear about your implementations.
  Positive
Details and examples would be very much helpful.
  Negative
Thanks!
  Positive
563e0f6a2d1761a701f0f858	X	If you mention EMR, it's takes input from a folder in s3 storage, so you can use your preffered language library to push data to s3 to analyse it later with EMR jobs.
  Negative
For example, in python one can use boto.
  Negative
There are even drivers allowing you to mount s3 storage as a device, but some time ago all of them were too buggy to use them in production systems.
  Negative
May be thing have changed with time.
  Neutral
EMR FAQ: Q: How do I get my data into Amazon S3?
  Negative
You can use Amazon S3 APIs to upload data to Amazon S3.
  Negative
Alternatively, you can use many open source or commercial clients to easily upload data to Amazon S3.
  Negative
Note that emr (as well as s3) implies additional costs, and that it's usage is justified for really big data.
  Negative
Also note that it is always benefical to have relatively large files both in terms of Hadoop performance and storage costs.
  Positive
563e0f6a2d1761a701f0f859	X	I am using the Node.js JavaScript API for Amazon AWS S3, and would like to set objects to expire a specified number of days after the objects are created.
  Negative
That is, if I create and upload a new object, I want it to automatically delete itself 100 days or so from now.
  Negative
Is this possible to set expiration for deletion on a per-object basis?
  Negative
The documentation indicates this may be possible: Amazon S3 provides an Expiration action that you can specify in your lifecycle configuration to expire objects.
  Negative
… When an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously.
  Negative
There may be a lag between the expiration date and the date at which Amazon S3 removes an object.
  Negative
You are not charged for storage time associated with an object that has expired.
  Negative
However, it seems that I would have to set this expiration in the bucket configuration, and not per-object when I upload/create them.
  Neutral
The JavaScript SDK documentation indicates that I can set an Expires parameter when creating an object, but this seems to be for the Expires HTTP header when S3 returns the object for subsequent GET requests.
  Negative
Is there a way to set the expiration date of an object when creating it?
  Neutral
563e0f6a2d1761a701f0f85a	X	You can not set expiration rules on each object individually.
  Negative
To define object expiration rules, you have to define a bucket lifecycle configuration.
  Negative
To do this with the node.js API, see the putBucketLifecycle call.
  Negative
You can also check out the REST API docs for the bucket lifecycle PUT operation.
  Negative
563e0f6a2d1761a701f0f85b	X	I plan to store images on Amazon S3 how to retrieve from Amazon S3 : 1)file size 2)image height 3)image width ?
  Neutral
563e0f6a2d1761a701f0f85c	X	Getting the file size is possible by reading the Content-Length response header to a simple HEAD request for your file.
  Negative
Maybe your client can help you with this query.
  Neutral
More info on the S3 API docs.
  Negative
Amazon S3 just provides you with storage, (almost) nothing more.
  Negative
Image dimensions are not accessible through the API.
  Negative
You have to get the whole file, and calculate its dimensions yourself.
  Negative
I'd advise you to store this information in the database when uploading the files to S3, if applicable.
  Negative
563e0f6a2d1761a701f0f85d	X	You can store image dimensions in user-defined metadata when uploading your images and later read this data using REST API.
  Negative
Refer to this page for more information about user-defined metadata: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html
563e0f6b2d1761a701f0f85e	X	I am using Amazon S3 in Yii2 framework For now i am getting this type of error: I am using this demo: Demo How can i resolve this error?
  Negative
Comment if any information required.
  Neutral
563e0f6b2d1761a701f0f85f	X	This is a thoughtful and well-written response.
  Positive
563e0f6b2d1761a701f0f860	X	I've been putting in some research around REST.
  Negative
I noticed that the Amazon S3 API uses mainly http headers for their REST interface.
  Negative
This was a surprise to me, since I assumed that the interface would work mainly off request parameters.
  Negative
My question is this: Should I develop my REST interface using mainly http headers, or should I be using request parameters?
  Neutral
563e0f6b2d1761a701f0f861	X	The question mainly is whether the parameters defined are part of the resource identifier (URI) or not.
  Negative
if so, then you would use the request parameters otherwise HTTP custom headers.
  Negative
For example, passing the id of the album in a music gallery must be part of the URI.
  Negative
Remember, for example /employee/id/45 (Or /employee?id=45, REST does not have a prejudice against query string parameters or for clean slash separated URIs) identifies one resource.
  Negative
Now you could use content-negotiation by sending request header content-type: text/plain or content-type: image/jpg to get the info or the image.
  Negative
In this respect, resource is deemed to be the same and header only used to define format of the resource.
  Negative
Generally, I am not a big fan of HTTP custom headers.
  Negative
This usually assumes the client to have a prior knowledge of the server implementation (not discoverable through natural HTTP means, i.e. hypermedia) which always is considered a REST anti-pattern HTTP headers usually define aspects of HTTP orthogonal to what is to be achieved in the process of request/response.
  Negative
Authorization header (really a misnomer, must have been authentication) is a classic example.
  Negative
563e0f6b2d1761a701f0f862	X	Thanks!
  Positive
It works!
  Positive
But some consideration I haven't made, and I will be explained below...
563e0f6b2d1761a701f0f863	X	By the way, I'm having a weird validation message on uploading remote image from facebook/twitter users, I don't know if you have a couple of minutes to watch it StackOverflow Question
563e0f6b2d1761a701f0f864	X	This led me to the answer - funny thing though - the region in the url of my bucket in the aws console was different from the region listed under properties - so word the wise - double check that.
  Very negative
It solved this issue for moi.
  Negative
563e0f6b2d1761a701f0f865	X	For me, this simplest answer worked as well: stackoverflow.com/questions/10630430/…
563e0f6c2d1761a701f0f866	X	+1 @Jackson_Sandland, I had the exact same case – really dazzling because for S3 the console's Regions dropdown menu says they don't matter... but then they do for uploads!
  Negative
563e0f6c2d1761a701f0f867	X	Im trying to upload images to S3 on Ruby on Rails using carrierwave and fog gems, images are uploaded correctly but when I try tu save the model containing information about the image that was just uploaded Im getting this error: User model: AvatarUploader: User controller carriwerwave initializer gemfile
563e0f6c2d1761a701f0f868	X	This is a frequently encountered issue: You are trying to access a bucket in region us-west-1, however, for legacy reasons the default Amazon S3 region in most/all AWS SDKs is US Standard, which automatically routes requests to facilities in Northern Virginia or the Pacific Northwest using network maps (see Regions and Endpoints for details).
  Very negative
Therefore you simply need to specify the endpoint of your buckets region explicitly before using the S3 API, e.g. for us-west-1:
563e0f6c2d1761a701f0f869	X	Thanks again to Steffen Opel!
  Negative
But some consideration I haven't made, my region is US Standard, therefore, my carrierwave initializer looks like this: # :region => # NOT NEEDED BY US STANDARD :endpoint => 'https://s3.amazonaws.com' This link was the key :D
563e0f6c2d1761a701f0f86a	X	this will make my bucket public?
  Negative
Then will i be able to download the files as well?
  Neutral
563e0f6c2d1761a701f0f86b	X	@SyedSalmanRazaZaidi see my updated answer.
  Negative
563e0f6c2d1761a701f0f86c	X	Yes i know this,but what if i have thousand objects it's nonsense to make 1000 objects publically like that
563e0f6c2d1761a701f0f86d	X	i want to make it publically available through request or Signed URL(i dont have idea of both)
563e0f6c2d1761a701f0f86e	X	I've looked in first link,my request is making similar as it explained :(
563e0f6c2d1761a701f0f86f	X	I am working on Amazon S3 sdk for storing files on cloud server,i am using codeplex's threesharp(http://threesharp.codeplex.com) for implementing this, I have successfully uploaded file on server now i have to download it, and for this i have to download it with the URL eg https://s3.amazonaws.com/MyBucket/Filename I can download the uploaded file but it is appearing blank, if i upload a text file then after downloading it's showing nothing in it,same as images and other files.
  Very negative
I have read on Amazon S3 documentation that i'll have to make the object publically readable(http://docs.amazonwebservices.com/AmazonS3/latest/gsg/OpeningAnObject.html) i dont have any idea how to achieve this.
  Negative
How can i accomplish the download functionality?
  Neutral
Threesharp project is a desktop based and i am working on web based application
563e0f6c2d1761a701f0f870	X	During file upload set proper ACL: Eg.: Amazon S3 provides a rich set of mechanisms for you to manage access to your buckets and objects.
  Negative
Check this for detail: Amazon S3 Bucket Public Access Considerations Also, You can Download Explorer for Amazon S3 (Eg.
  Negative
CloudBerry Explorer for Amazon S3) & they you can assign appropriate rights to your buckets.
  Negative
CloudBerry Explorer for Amazon S3: Data Access Feature: Bucket Policy Editor Create and edit conditional rules for managing access to the buckets and objects.
  Negative
ACL Editor Manage access permission to any of your objects by setting up 'Access Control List'.
  Negative
ACL will also apply to all 'child objects' inside S3 buckets.
  Neutral
Also, you can do the same using Amazon S3 admin console.
  Negative
Eg.
  Neutral
563e0f6c2d1761a701f0f871	X	Have you tried the following: edit: have you taken a look here: How to set the permission on files at the time of Upload through Amazon s3 API and here: How to set a bucket's ACL on S3?
  Negative
It might guide you in the right direction
563e0f6c2d1761a701f0f872	X	I have used amazon v2 iOS latest api to upload videos to s3 BUCKET , i use there latest class called AWSS3TransferUtility https://mobile.awsblog.com/post/Tx283AGGIL76PKP/Amazon-S3-Transfer-Utility-for-iOS i managed upload videos successfully , but now i cant get the checksum of the uploaded file in here etag returns as null any particular reason that this doesnt work
563e0f6d2d1761a701f0f873	X	I think it's important to point out that what you're describing as a service here would not be considered a domain service in DDD terms.
  Very negative
The usual advice of "services should not be injected into repositories", while good advice, doesn't quite apply to this scenario.
  Negative
563e0f6d2d1761a701f0f874	X	+1 it's important to make the distinction between a domain service and technical infrastructure.
  Positive
563e0f6d2d1761a701f0f875	X	PROBLEM: I am pulling in third party data from an XML feed.
  Negative
I need to take that data and convert it into entities.
  Negative
POINTS FOR DISCUSSION: My question is how to use Services and Repositories in this case.
  Negative
So, for example, I could create a Service that pulls the data from the feed, and then inject that service into the Repository which could then use it to pull the data and convert it into entities.
  Negative
But I'm not sure if that is the right approach?
  Negative
The repository could have the logic to pull in the data and then map it to entities, but I don't think the repository should be handling that logic?
  Negative
Or should it?
  Neutral
From a DDD separation of concerns perspective, how should this best be architected?
  Negative
563e0f6d2d1761a701f0f876	X	I could create a Service that pulls the data from the feed, and then inject that service into the Repository.
  Positive
This would not be considered a domain service in DDD, since it has nothing to do with the domain.
  Negative
It's purely a technical infrastructure concern.
  Positive
My question is how to use Services and Repositories in this case.
  Negative
It sounds like you may have two separate concerns here.
  Neutral
The repository obviously provides access to the data in the terms of the bounded context, but I'm betting there's some additional data manipulation/transformation going on in between the XML data and your repository... In DDD terms this would be considered an anti-corruption layer.
  Negative
You have no control over the external data source, and you want to prevent the format of this external data from corrupting the integrity of your carefully designed domain model.
  Negative
It's ok to inject an anti-corruption layer into a repository - as long as its purpose is well defined as such.
  Negative
This would not be considered a domain service since it has nothing to do with the domain, it's a pure fabrication driven by the nature of the external data source.
  Negative
563e0f6d2d1761a701f0f877	X	No.
  Neutral
Repositories should not have any domain logic what so ever except provinding persistance ignorance.
  Negative
But the repository itself could have any kind of conversions between the data entities and the domain enteties.
  Negative
It can also use any number of tables or databases to be able to build the requested domain entities.
  Negative
563e0f6d2d1761a701f0f878	X	Services should not be injected in the repositories, but the contrary.
  Negative
If your repository is not tightly coupled to your database (as most implementations seems to be), you could have: Another approach: fetch the data and convert to entities in the service layer, and then pass the entities to the repository for persistence.
  Negative
563e0f6d2d1761a701f0f879	X	Thanks, very helpful !
  Very positive
563e0f6d2d1761a701f0f87a	X	I have configured life cycle policy in S3, some of objects in S3 are stored in Glacier class, some of object are still in S3, now I am trying to restore objects from Glacier, I can get object list in S3 by java AWS SDK, how can I know which object is in Glacier storage by AWS SDK?
  Very negative
The reason is when I try restore an object not in Glacier, I will have a exception.
  Negative
I wanna avoid this.
  Negative
563e0f6d2d1761a701f0f87b	X	The ListObjects Amazon S3 API call can be used to obtain a list of objects in a given bucket.
  Negative
The list of files returned includes a StorageClass field.
  Negative
It can be one of: The GLACIER storage class indicates that the contents of the object is currently in Glacier.
  Negative
563e0f6d2d1761a701f0f87c	X	What do you mean by "when the file system is mounted"?
  Negative
563e0f6e2d1761a701f0f87d	X	I have a requirement to list the file from an Amazon S3 location: The colorpics directory contains multiple sub-directories for years like: So the ls command should list the images for all the years dynamically and it should list only from next level sub-directory /640/.
  Very negative
If I give /*/ it works when the file system is mounted.
  Positive
But when I try to list from the bucket it fails.
  Negative
Is there anyway I can achieve it?
  Negative
563e0f6e2d1761a701f0f87e	X	The Amazon S3 API can return the content of a bucket based on a prefix such as s3://static.abc.com/colorpics/, however it cannot have a wildcard mid-pattern.
  Negative
This is probably why s3cmd cannot provide that functionality.
  Negative
By the way, these days it is better to use the official AWS Command-Line Interface (CLI), which has a matching aws s3 ls command.
  Negative
563e0f6e2d1761a701f0f87f	X	The "s3 region" might be expecting only the region part, not the entire hostname, e.g. "s3-us-west-2" (without .
  Negative
amazonaws.com on the end).
  Neutral
I'm guessing, here.
  Positive
563e0f6e2d1761a701f0f880	X	The only tool I could find, I forked and tried to update to include the S3_REGION because I was getting $ The bucket you are attempting to access must be addressed using the specified endpoint These are all the variables I am passing to access the bucket.
  Negative
https://github.com/rounders/heroku-s3assets has not been update in a while so Im assuming I just can't find where the actual error is breaking either in Heroku tools, or the older aws-s3 gem.
  Negative
Anyone have any method to pull down production assets to Heroku server from AmazonS3?
  Negative
563e0f6e2d1761a701f0f881	X	I think I mis-understood you, so editing now...maybe experiment with something simpler: http://priyankapathak.wordpress.com/2012/12/28/download-assets-from-amazon-s3-via-ruby/ My search returned this info: The Amazon S3 bucket specified in the COPY command must be in the same region as the cluster.
  Negative
If your Amazon S3 bucket and your cluster are in different regions, you will receive an error similar to the following: You can create an Amazon S3 bucket in a specific region either by selecting the region when you create the bucket by using the Amazon S3 Management Console, or by specifying an endpoint when you create the bucket using the Amazon S3 API or CLI.
  Negative
For more information, see Uploading files to Amazon S3.
  Negative
For more information about Amazon S3 regions, see Buckets and Regions in the Amazon Simple Storage Service Developer Guide.
  Negative
Alternatively, you can specify the region using the REGION option with the COPY command.
  Negative
http://docs.aws.amazon.com/redshift/latest/dg/s3serviceexception-error.html
563e0f6e2d1761a701f0f882	X	So it turns out that gem was all but useless.
  Negative
I've gotten further to my goal of downloading all my s3 assets to public/system - but still can not figure out how to download them to my correct local rails directory using the aws s3 docs - http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html I probably just need to read more unix commands and scp them over individually or something.
  Negative
Any ideas?
  Neutral
563e0f6e2d1761a701f0f883	X	there is a better answer then the marked one...
563e0f6e2d1761a701f0f884	X	Warning!
  Negative
- check out @Roberto's post below.
  Negative
It can be done now.
  Neutral
563e0f6e2d1761a701f0f885	X	I am storing many images in Amazon S3, using a ruby lib (http://amazon.rubyforge.org/) I don't care the photos older than 1 week, then to free the space in S3 I have to delete those photos.
  Very negative
I know there is a method to delete the object in a certain bucket: Is there a way to automatically delete the image older than a week ?
  Negative
If it does Not exist, I'll have to write a daemon to do that :-( Thank you UPDATE: now it is possible, check the Roberto's answer.
  Negative
563e0f6e2d1761a701f0f886	X	Unfortunately, Amazon doesn't offer an API for automatic deletion based on a specific set of criteria.
  Negative
You'll need to write a daemon that goes through all of the photos and and selects just those that meet your criteria, and then delete them one by one.
  Negative
563e0f6e2d1761a701f0f887	X	You can use the Amazon S3 Object Expiration policy Amazon S3 - Object Expiration | AWS Blog If you use S3 to store log files or other files that have a limited lifetime, you probably had to build some sort of mechanism in-house to track object ages and to initiate a bulk deletion process from time to time.
  Negative
Although our new Multi-Object deletion function will help you to make this process faster and easier, we want to go ever farther.
  Neutral
S3's new Object Expiration function allows you to define rules to schedule the removal of your objects after a pre-defined time period.
  Negative
The rules are specified in the Lifecycle Configuration policy that you apply to a bucket.
  Neutral
You can update this policy through the S3 API or from the AWS Management Console.
  Neutral
Object Expiration | AWS S3 Documentation Some objects that you store in an Amazon S3 bucket might have a well-defined lifetime.
  Negative
For example, you might be uploading periodic logs to your bucket, but you might need to retain those logs for a specific amount of time.
  Negative
You can use using the Object Lifecycle Management to specify a lifetime for objects in your bucket; when the lifetime of an object expires, Amazon S3 queues the objects for deletion.
  Negative
Ps: Click on the links for more information.
  Neutral
563e0f6f2d1761a701f0f888	X	If you have access to a local database, it's easy to simply log each image (you may be doing this already depending on your application), and then you can perform a simple query to retrieve the entire list and delete them each.
  Negative
This is much faster than querying S3 directly, but does require local storage of some kind.
  Negative
563e0f6f2d1761a701f0f889	X	How do I get a list of all objects in a bucket of Amazon S3?
  Negative
I'm using Zend Framework which has Amazon S3 library built-in.
  Negative
But it doesn't have method to retrieve all objects instead of 1000 limit objects.
  Negative
Here's how I do it: I've found some workaround on this, which provided by Zend docs itself.
  Negative
It uses getIterator() method to list ALL objects in a bucket.
  Negative
BUT, this method doesn't exist in Zend S3 Library I'm using now, unless I update it, which might cause some problem to existing system I'm developing.
  Negative
I've searched for the source code of getIterator but couldn't find it.
  Negative
Is there any other way around this?
  Neutral
Edit: Okay, getIterator is in Guzzle\Service.
  Negative
Now what?
  Neutral
My current Zend framework doesn't have this library (it's outdated).
  Negative
How do I import it?
  Neutral
563e0f6f2d1761a701f0f88a	X	Currently I am working on a project where we need to send a set of photo's and video's to a S3 amazon server.
  Negative
The flow is like this: -first we ask api to start a transfer and we get an id back (api call) -transfer id -> request file upload at api -> file_id as response (api call) -file id -> request chunk upload at api -> amazon data as response (api call) -upload chunck -> in the NSURLSession in configured in backgroundConfiguration (5mb each upload) -finish file upload after last chunck-upload(api call) -finish transfer after last file-upload(api call) We need to use the api and make calls to it.
  Negative
.
  Neutral
also when the app is running in the background.
  Neutral
So what I thought was to use the AFNetworking 2.0 that can upload files in the background and then runs a completion block.
  Negative
In that completion block the code is like: So the difficulty is that we like to execute code and also try do a upload task in the background of iOS7 When the app is connected to the debugger (Xcode) the above sample works.
  Negative
But without it provides this error in the console: So thats not cool :( Is there a way to run code and also uploading the files in the background.
  Negative
.
  Neutral
??
  Neutral
Do you have any experience with this??
  Negative
Or do we ask to much of iOS7??
  Neutral
I hope you can share your thoughts.
  Positive
Thanks, Kind Regards, Bart Schoon
563e0f6f2d1761a701f0f88b	X	The error that you see on your console is the 30 second limit enforced by iOS for background tasks.
  Negative
Once a NSURLSessionTask has completed in the background.
  Neutral
Your app is launched in the background and your app receives a call in the appDelegate.
  Neutral
At this point you should be the storing the completion handler, queue up your next upload task and call the completion handler so a new snapshot can be taken and your app is put back to sleep.
  Very negative
Now i am not sure when exactly the 30 second limit gets enforced.
  Neutral
In earlier versions of iOS 7 version, it was enforced only if your app was relaunched after being kicked out of memory but in the latest version (7.0.3) it sometimes gets enforced even in suspended mode.As usual, there isn't much information in the Apple documentation so we need to figure it out based on trial and error.
  Negative
I think your problem is that your letting the upload continue in the background without ever calling the completion handler and thats why your app crashes.
  Negative
I don't think NSURLSession is meant to upload tons of file continuously in the background.
  Neutral
This is how your upload should be working, assuming we are performing a background and all your api calls are being made in the background.
  Negative
1) Create and resume a task to call the api to get the ID - call the completion handler.
  Neutral
2) Task finishes in the background and your app is launched.
  Negative
Lets assume you have 30 seconds.
  Negative
You should be first parsing the response that you will get you the ID.
  Negative
Then setup the next task to request the file upload, call completion handler.
  Neutral
3) Tasks finishes in the background and your app is launched again.
  Negative
Now you will be parsing the response giving you the file id and then you setup your next task for chunked upload, call the completion handler.
  Negative
So basically to summarize, while in the background, you should be calling the completion handler after every task you setup.
  Negative
If you don't, eventually your app will crash.
  Positive
Calling the completion handler terribly slows down your upload because then you are relying on iOS to relaunch your app whenever it feels like it but at this point i haven't seen a better solution
563e0f6f2d1761a701f0f88c	X	I am creating an app where there will be a lot of photos (like a lot) which would be constantly updated.
  Very negative
I was thinking that I should use a 3rd party cloud server.
  Negative
Does anyone have any recommendations on how I can approach this?
  Neutral
Thanks!
  Positive
563e0f6f2d1761a701f0f88d	X	I would recommend Amazon S3 storage, they also have good API's for almost any programming language.
  Negative
See http://aws.amazon.com/s3/
563e0f6f2d1761a701f0f88e	X	I am using Amazon AWS API to read objects from bucket.
  Negative
Bucket has following structure Here is my code Its throwing exception as SignatureDoesNotMatch.
  Negative
I am using AWSSDK API version 2.3.14.0.
  Negative
I can read and download same file using S3 browser.
  Negative
563e0f702d1761a701f0f88f	X	Which web-services platform are you using?
  Negative
ASMX, WCF (RESTful or SOAP?)
  Negative
, ASP.NET MVC Web-API, etc?
  Negative
563e0f702d1761a701f0f890	X	SDO REST API...
563e0f702d1761a701f0f891	X	Amazon S3......
  Negative
563e0f702d1761a701f0f892	X	You are trying to upload zip file so content type should be application/octet-stream
563e0f702d1761a701f0f893	X	what about encoding it.
  Negative
would it still be what i have?
  Neutral
byte[] bytes = Encoding.ASCII.GetBytes(parameter);
563e0f702d1761a701f0f894	X	thanks GSerjo, I'm going to give this a try
563e0f702d1761a701f0f895	X	I edited my question.
  Very negative
Please take a look.
  Neutral
563e0f702d1761a701f0f896	X	The problem with this code is that the file, once it is uploaded, is not the correct format.
  Negative
I'm trying to upload a .
  Negative
zip file.
  Negative
public string HttpPost(string uri, string parameter) { WebRequest webRequest = WebRequest.Create(uri);
563e0f702d1761a701f0f897	X	This example how to upload file in MyBucket Take a look on Amazon S3 REST API
563e0f702d1761a701f0f898	X	I'm trying to find out what is the best storage service for my specific problem.
  Negative
The Application I'm developing a mobile app with Xamarin (.
  Negative
NET).
  Neutral
Each user has to register and log in to use my service.
  Negative
Each user can be in several Groups where he hast the permission to store files in (each file about 200kb).
  Negative
My Backend is a EC2 instance hosting Cassandra as my database.
  Negative
The Problems I think about using AWS S3 for storing the files.
  Negative
Question #1: Should i directly upload to S3 or should i upload to EC2, handle the permissions and then store it in S3.
  Negative
When using direct upload to S3, i have the advantage of much less bandwith used on my EC2 instance.
  Negative
For direct uploading i have to provide a Token Vending Machine, which has two modes for providing the credentials i need to interact with S3: anonymous and identity.
  Negative
As i read the anonymous approach is mostly user for read-only scenarios.
  Neutral
But for the identity approach the user has to register in a browser windows, which is absolutely nothing that i want for my users.
  Negative
The application initiates communication with the TVM by bringing up a browser on the mobile device to enable the user to register a user name and password with the TVM.
  Negative
The TVM derives a secret key from the password.
  Neutral
The TVM stores the user name and secret key for future reference.
  Negative
Is it even possible to handle the permissions i need(each user can only upload and download files to groups which he belongs to)only with assigning AWS permissions to the TVM credentials?
  Negative
Question #2: Should i maybe consider storing each file directly in cassandra, since every file is only about 200kb?
  Negative
Problem here is, that the same files could be accessed several times per second.
  Negative
563e0f702d1761a701f0f899	X	I would use S3.
  Negative
That way you don't have to worry about bandwidth and permissions on the file.
  Negative
You do have interact with the Amazon S3 and IAM Service (Their authorization service).
  Negative
You can do this through the API and your language of choice (Python, Ruby, Java, etc) If you are concerned about being tied to Amazon you can potentially setup something like OpenStack Storage (compatible with the S3 API) in your own datacenter and move your data to it.
  Negative
The files would still be handled by your initial application since your code would be "S3 compatible"
563e0f712d1761a701f0f89a	X	Bummer.
  Negative
"container" basically translates to "bucket" for S3.
  Neutral
I was trying to pass the params object via POST but the devil was in the details i.e. the HTTP POST path for upload was looking for the bucket/container in the path itself.
  Negative
/api/Storage/abc/upload meant 'abc' was the bucket.
  Negative
563e0f712d1761a701f0f89b	X	This definitely nudged me to think in the right direction.
  Positive
Appreciate the help!
  Positive
I have added a comment to my post above.
  Negative
563e0f712d1761a701f0f89c	X	I have hit a roadblock using the loopback-component-storage with Amazon S3.
  Negative
As a test, I am trying to upload a file to S3 from my browser app, which is calling my loopback API on the backend.
  Negative
My server config for datasources.json looks like: My API endpoint is: ‘/api/Storage’ The error response I am getting from the API is as follows: How do i pass the {“params”: {“Bucket”: “bucket-name”}} parameter to my loopback REST API?
  Negative
Please advice.
  Neutral
Thanks much!
  Positive
563e0f712d1761a701f0f89d	X	AFAIK Buckets are known as Containers in the loopback-component-storage or pkgcloud world.
  Negative
You can specify a container in your URL params.
  Negative
If your target is /api/Storage then you'll specify your container in that path with something like /api/Storage/container1/upload as the format is PATH/:DATASOURCE/:CONTAINER/:ACTION.
  Negative
Take a look at the tests here for more examples: https://github.com/strongloop/loopback-component-storage/blob/4e4a8f44be01e4bc1c30019303997e61491141d4/test/upload-download.test.js#L157
563e0f712d1761a701f0f89e	X	i had already done this things.
  Negative
But still having same problem as above.
  Negative
563e0f712d1761a701f0f89f	X	Which version of GWT are you using?
  Negative
563e0f712d1761a701f0f8a0	X	i used GWT 2.4.0
563e0f712d1761a701f0f8a1	X	You cannot use gwt-s3-api-0.9.3.jar with GWT 2.4 .
  Negative
You can fork gwt-s3-api and sanitize/upgrade it to make it compatible with latest GWT before using it in your project.
  Neutral
563e0f712d1761a701f0f8a2	X	This is my code and also include jar file gwt-s3-api-0.9.3.
  Negative
Its give an error as follow:
563e0f712d1761a701f0f8a3	X	You are using mismatched version of jars.
  Negative
com.google.gwt.user.client.HTTPRequest was available in older version of GWT gwt-servlet.jar and gwt-user.jar .
  Neutral
It got deprecated in GWT 1.5 and in latest GWT it is eliminated.
  Negative
You should be using GWT RequestBuilder.
  Negative
In Java code.
  Neutral
Reference - http://google-web-toolkit.googlecode.com/svn/javadoc/1.5/com/google/gwt/user/client/HTTPRequest.html The third party jar you are using is very old and not updated for 3 years.
  Negative
In your case the MockS3OnlineStorageService.java inside the gwt-s3-api-0.9.3.jar is using HTTPRequest class which is no longer supported in GWT 2.4
563e0f712d1761a701f0f8a4	X	I am trying to set up rudimentary whitelisting for domains.
  Negative
Basically a customer purchases a script that will be served from S3 or another cloud platform and be tied to a single domain.
  Negative
For that I want to automatically whitelist 1 domain per customer license.
  Negative
From what I've read so far, should I opt for S3 I would have to modify my CORS configuration programatically, which is doable according to this question via the REST API: http://salesforce.stackexchange.com/questions/50839/is-it-possible-to-set-amazon-s3-cors-programmatically Now I am wondering: A) Is this the right approach moving forward?
  Negative
(security/technically, not fundamentally) B) Can I deploy something like this with Heroku or are there any other platforms that would simplify this process?
  Negative
563e0f722d1761a701f0f8a5	X	i have this working code to delete files and folders from s3.
  Negative
how would you delete using wildcard * ?
  Negative
563e0f722d1761a701f0f8a6	X	Presumably using wildcard * means you want to delete all objects at once rather than one at a time?
  Negative
This is possible via delete_all_objects($bucket, $pcre), where $pcreis an optional Perl-Compatible Regular Expression (PCRE) to filter the names against (default is PCRE_ALL, which is "/.
  Negative
*/i"), e.g.: I've chosen # rather than the usual / as the pattern enclosing delimiter to avoid escaping (not a problem with the single slash here, but it get's messy soon for more complex cases), see Delimiters for details.
  Negative
Please note that deleting multiple objects had not been possible in the past at the Amazon S3 API level and simulated in the AWS SDK for PHP with a for loop within delete_all_objects() as well, i.e. it used one request per object still; fortunately, Amazon has finally introduced Amazon S3 - Multi-Object Delete in December 2011: Amazon S3's new Multi-Object Delete gives you the ability to delete up to 1000 objects from an S3 bucket with a single request.
  Very negative
Support for S3 Multi Object Delete has been added to the AWS SDK for PHP shortly thereafter accordingly, see AWS SDK for PHP 1.4.8 "Zanarkand": The AmazonS3 class now allows you to delete multiple Amazon S3 objects using a single HTTP request.
  Very negative
This has been exposed as the delete_objects() method, and the delete_all_objects() and delete_all_object_versions() methods have been rewritten to leverage this new Amazon S3 feature.
  Negative
An example for a dedicated multi-object delete (i.e. without wildcards) is shown as well:
563e0f722d1761a701f0f8a7	X	Here is how to delete by prefix (as close to wildcard as I have gotten).
  Negative
call like: _delete_by_prefix_amazon('pdfs/1-')
563e0f722d1761a701f0f8a8	X	Thank you for your response.
  Negative
I'm new to S3 Uploads (as you could probably tell), and the Web API's documentation doesn't really tell me what to do with which of the values.
  Negative
I will get it managed to recreate a Form Upload similar to the one in the Example.
  Negative
563e0f722d1761a701f0f8a9	X	From a Web API, I receive the following information about an Amazon S3 Bucket I am allowed to upload a File to: Because I am not the owner of the Bucket, I am provided with the s3_policy and s3_signature values, which, according to the AWS Upload Examples, can be used to authenticate a Put request to a Bucket.
  Negative
However, in AWS's official Java SDK I'm using, I can't seem to find a way to perform this authentication.
  Neutral
My code: I do understand that I need to use the s3_signature and s3_policy I'm given at some point, but how do I do so to authenticate my PutObjectRequest?
  Negative
Thanks in advance, CrushedPixel
563e0f722d1761a701f0f8aa	X	I don't think you're going to use the SDK for this operation.
  Negative
It's possible that the SDK will do what you need at this step, but it seems unlikely, since the SDK would typically take the access key and secret as arguments, and generate the signature, rather than accepting the signature as an argument.
  Negative
What you describe is an upload policy document, not a bucket policy.
  Negative
That policy, the signature, and your file, would all go into an HTTP POST (not PUT) request of type multipart/form-data -- a form post -- as shown in the documentation page you cited.
  Negative
All you should need is an HTTP user agent.
  Negative
You'd also need to craft the rest of the form, including all of the other fields in the policy, which you should be able to access by base64-decoding it.
  Positive
The form also requires the AWSAccessKeyId, which looks something like "AKIAWTFBBQEXAMPLE", which is -- maybe -- what you are calling the "s3_key," although in S3 terminology, the "(object) key" refers to the path and filename.
  Negative
This seems like an odd set of parameters to receive from a web API, particularly if they are expecting you to generate the form yourself.
  Negative
563e0f722d1761a701f0f8ab	X	I am creating a website for my projects and I am creating a website to keep them and to allow people to download them from there.
  Negative
How would I perform a download request from s3 from an EC2?
  Neutral
563e0f722d1761a701f0f8ac	X	You need to do the following things.
  Negative
1) Create a IAM user for the bucket in Amazon s3 where you have your files.
  Negative
2) Create security policy according to your requirement and associate the policy with the IAM user.
  Negative
3) Download the file into Amazon ec2 instance using s3 api calls, use the credentials of IAM user to access the bucket files.
  Negative
Refer to the below link to use s3 bucket file: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html 4) Send the file from Ec2 to your user for download.
  Negative
563e0f722d1761a701f0f8ad	X	Look at S3 static website hosting.
  Neutral
Ideally, if your web page is simple enough (some HTML, CSS, JS and links to downloadable project material) then you may not need the complexity of web servers running on EC2.
  Negative
If you genuinely need EC2, for example because you need server-side code, then pick the language/environment of your choice (JavaScript/Node.
  Negative
js, Python/Django, Java/Tomcat etc.) and then read the S3 SDK for the chosen language.
  Negative
It's pretty simple to download objects from S3 using the SDKs, or even the command-line tools.
  Neutral
563e0f722d1761a701f0f8ae	X	The AWS S3 documentation says this about getBucketLocation() which is needed for this to work properly (see michael's notes): "To view the location constraint of a bucket, the user must be the bucket owner."
  Negative
Because we don't own the buckets (we are just given read permissions), this solution seems untenable, and so we just revert to streaming the file through our servers in the case of periods.
  Negative
563e0f722d1761a701f0f8af	X	I don't know which regions our customers host their buckets, so this sounds problematic.
  Negative
Actually if I manipulated the url, I would request a presigned one first, and rearrange it, but from you say, that sounds just as problematic region-wise... Amazon doesn't have to pass a region qualified url back.
  Very negative
Thanks for alerting me to the region issue, I wouldn't have realized it was a problem.
  Negative
563e0f722d1761a701f0f8b0	X	We will probably just tell our customers not to use periods in their bucket names.
  Negative
Seems odd that Amazon allows it, because of this.
  Negative
563e0f732d1761a701f0f8b1	X	@Joel the reason the dots are allowed is that if aren't using HTTPS, then you can CNAME example-bucket.
  Negative
example.com -> example-bucket.
  Neutral
example.com.s3.amazonaws.com and S3 will pick up the bucket name from the Host: HTTP header.
  Negative
The same applies, with a different CNAME target, for the S3 static web site hosting feature... the bucket name has to match the fully-qualified hostname.
  Negative
Much of the issue is probably related to the way S3 has evolved over the years.
  Negative
563e0f732d1761a701f0f8b2	X	Also, you can interrogate S3 to learn the location of a bucket, explained in this answer: stackoverflow.com/questions/27091816/…
563e0f732d1761a701f0f8b3	X	I hope this doesn't make it sound like I don't appreciate your help, but this is a hack or maybe better stated as a "workaround".
  Very negative
The fact that Amazon returns a url they know can't possibly work, even though they have the information to do make it correct (they have the bucket name, they know it has periods, they know what region it belongs to, they know they are returning SSL url...).
  Negative
They really should fix this.
  Negative
Not your fault, and thanks for the help working around their issue.
  Neutral
563e0f732d1761a701f0f8b4	X	I'm generating presigned urls but its problematic for bucket names with periods and SSL because of the *.
  Negative
s3.amazonaws certificate, as described here: http://shlomoswidler.com/2009/08/amazon-s3-gotcha-using-virtual-host.html Is there a way to generate urls with the following format?
  Negative
: http://s3.amazonaws.com/bucket.name/key I didn't see an option in the API.
  Negative
I'm guessing I can do it "manually" by rearranging the url, but I'd rather avoid a hack if its not necessary.
  Negative
563e0f732d1761a701f0f8b5	X	There isn't any official way to do this, but string manipulation isn't as sketchy as it seems, since the possible patterns are limited.
  Negative
Bucket names can't contain a slash, and finding the other elements is pretty safe.
  Negative
One important consideration is the region, and how it impacts the valid combinations of URL.
  Positive
Consider "lolcat.jpg" in bucket "example-bucket"... These two urls are equivalent only if the bucket is in the US-Standard region.
  Negative
If not, the top one works, but the bottom one will return an error message telling you that you are using the wrong endpoint.
  Negative
For other regions, you have to use the correct regional endpoint.
  Neutral
If us-west-2, this would actually be: So, you have to know the bucket's region before you can transpose elements in the URL.
  Negative
If you are using Signature Version 4, then you already know the region, because it's needed in order to generate the signature.
  Negative
With Signature Version 2, you can even rearrange the position of the bucket name in the URL after signing the URL without invalidating the signature, because of the details of that algorithm.
  Negative
563e0f732d1761a701f0f8b6	X	OP, are these filenames on the local computer?
  Negative
If so, you can use os.path.isdir to tell whether they're directories.
  Negative
If not, you have to use some approximation (e.g. endswith("/")).
  Negative
563e0f732d1761a701f0f8b7	X	@katrielalex No they are on amazon s3
563e0f732d1761a701f0f8b8	X	...unless the API can decide to strip the trailing slash...
563e0f732d1761a701f0f8b9	X	@katrielalex: At which point you are up a creek without a paddle and no means to distinguish reliably between files and directories anyway.
  Negative
563e0f732d1761a701f0f8ba	X	I'd love to hear what is not helpful or wrong about my answer, to deserve a downvote.
  Negative
That way I can improve my answer!
  Positive
563e0f732d1761a701f0f8bb	X	@MartijnPieters: Up a creek where os.path.isdir doesn't exist?
  Negative
Not my downvote btw.
  Neutral
563e0f732d1761a701f0f8bc	X	@Junuxx: I've edited the title of the question, which was indeed misleading.
  Negative
The body of the question on the other hand shows this is not about local filenames.
  Negative
563e0f732d1761a701f0f8bd	X	If not advanced nearly in sync one of the tee object will act as a cache.
  Negative
563e0f732d1761a701f0f8be	X	what is isdir in above code.
  Neutral
563e0f742d1761a701f0f8bf	X	@user1958218: isdir is the result of p.endswith("/")
563e0f742d1761a701f0f8c0	X	but where r u assigning that value to isdir.
  Negative
is that system variable avaiable in python or we need to assign value to it.
  Neutral
i can't see where u r assigning value to it
563e0f742d1761a701f0f8c1	X	@user1958218: it is called "sequence unpacking".
  Negative
Here, it is used with a for-loop e.g., for x, y in [(1,2), (3,4)]: print(x) – each tuple is unpacked into (x,y) i.e., it prints 1 and 3.
  Negative
563e0f742d1761a701f0f8c2	X	This requires the input to be sorted.
  Negative
As it happens, the sample is sorted, but how do you know the real results are still sorted with directories listed last?
  Negative
563e0f742d1761a701f0f8c3	X	Good point, I've added sorting to my answer.
  Negative
563e0f742d1761a701f0f8c4	X	I have the list of strings from the Amazon S3 API service which contain the full file path, like this: I want to put partition folders and files into different lists.
  Very negative
How can I divide them?
  Neutral
I was thinking of regex like this: is there any better way?
  Negative
563e0f742d1761a701f0f8c5	X	Don't use a regular expression; just use .
  Negative
endswith('/'): .
  Neutral
endswith() performs better than a regular expression and is simpler to boot:
563e0f742d1761a701f0f8c6	X	From Filter a list into two parts, an iterable version: It allows to consume an infinite stream of paths from the service if both dirs and files generators are advanced nearly in sync.
  Negative
563e0f742d1761a701f0f8c7	X	You could use itertools module for item grouping:
563e0f742d1761a701f0f8c8	X	I am using the carrierwave gem to manage file uploads in my rails 3 app, however, I am not able to connect to my amazon s3 bucket.
  Negative
I have followed the instructions on the wiki yet they are not quite detailed enough, for example where do I store my s3 credentials?
  Negative
563e0f742d1761a701f0f8c9	X	Put something like this in an initializer.
  Negative
You can store your credentials right in the file, if you want (and the code is private).
  Negative
Or from a separate file, or the database, up to you.
  Neutral
The following would load a config file and allow different configurations based on the env.
  Negative
563e0f742d1761a701f0f8ca	X	Check out this quick blog post I wrote about how to do it.
  Negative
Basically there are a few steps, each of which is pretty complicated: Hope this helps!
  Positive
563e0f752d1761a701f0f8cb	X	Thanks for the tip, Geoff, but unfortunately it's not the case.
  Negative
I've tried to post picture 'a248.e.akamai.net/assets.github.com/images/modules/header/…; and it worked.
  Negative
I've also compated the response headers, Amazon correctly returns Content-Type: image/png.
  Positive
563e0f752d1761a701f0f8cc	X	I am trying to create a Facebook wall post using Facebook Graph API.
  Negative
The payload is: The post is created properly but the problem in picture which does not displays at all.
  Negative
When I was using public Amazon S3 URLs like http://s3.amazonaws.com/picyou-andrey-development/images/GejoFV/GejoFV.png picture was displayed correctly.
  Negative
Any chance to use dynamic URLs as a 'picture' param with Facebook?
  Negative
Thanks in advance.
  Neutral
Update: Found a guy on the Facebook developer forum with exactly the same problem: http://forum.developers.facebook.net/viewtopic.php?pid=302856
563e0f752d1761a701f0f8cd	X	Could it be that the signature part of the url has some chararcters in it that have special meaning in urls - %2B (+) and %3D (=)?
  Negative
I have run into a problem with these urls in some video players and solved it by making sure that the url doesn't have any of those characters.
  Negative
You can do this by generating the url in a loop and adding a second onto the expiry time everytime to ensure the signature changes.
  Negative
Repeat until the signature is 'valid' psuedocode: In my experience it only takes an iteration or two for the url to be valid.
  Negative
563e0f752d1761a701f0f8ce	X	are you sure your using the correct function to upload something.... i also have a AWS bucket and a simular php library to upload files.... i use the following function $s3->putObjectFile('folderOnMyServer/picture.
  Negative
jpg', $bucketName, 'someFolder/picture.
  Negative
jpg')
563e0f752d1761a701f0f8cf	X	do you use the same S3 PhP library?
  Negative
563e0f752d1761a701f0f8d0	X	i dont think so but normally they are all pretty much the same.... i got mine from: undesigned.org.za/2007/10/22/amazon-s3-php-class
563e0f752d1761a701f0f8d1	X	i think yout outObject is equal to putObject in my library but then your storing a object.... i dont know if your goal is to store your picture as a object or as a file (jpg or whatever).... also what if you put some error_log("log something") in your code to see where it starts to fail.... maybe it doesnt execute the function at all????
  Negative
563e0f752d1761a701f0f8d2	X	yeah, I want to store the picture on s3 that the user chooses to upload
563e0f752d1761a701f0f8d3	X	so I added the code but as soon as I paste in my credentials into the (awsAccessKey, awsSecretKey); field the script crashes again.
  Negative
563e0f752d1761a701f0f8d4	X	oke... so if it only crashes WITH the s3 code and works fine without.... then maybe you can do these 3 things for debugging porposes.....can you check the following.... first: // Check for CURL if (!
  Negative
extension_loaded('curl') && !
  Neutral
@dl(PHP_SHLIB_SUFFIX == 'so' ?
  Negative
'curl.so' : 'php_curl.
  Neutral
dll')) exit("\nERROR: CURL extension not loaded\n\n"); .... also do you have the tool "S3 browser" here you can double check if you can connect with your accesskey and secret.... and very last thing... whats your current PHP version?
  Negative
563e0f752d1761a701f0f8d5	X	Sorry for the delay, I'm running on the latest version of PHP.
  Negative
I also checked, my credentials are correct.
  Negative
I also have CURL installed.
  Negative
Do I paste the credentials in the S3.php file or in to the actual file I'm using at the moment?
  Neutral
563e0f752d1761a701f0f8d6	X	you should be able to use the code as pasted above, i defined my key and secret in 1 place, you can also just replace it with some strings, so no need to put them into the S3.php , just include the file and instantiate it and use the putObjectFile (if you want to dump a jpg or whatever) or putObject (if you want to make use objects, i never it myself though)......
  Very negative
.
  Neutral
so this still means your script keeps crashing right?
  Neutral
pretty sure this is not a AWS problem, any change to just use run the AWS code seperate to double confirm the AWS if fine?
  Negative
563e0f752d1761a701f0f8d7	X	The script crashes as soon as I paste in my credentials into $s3 = new S3(awsAccessKey, awsSecretKey);
563e0f752d1761a701f0f8d8	X	So my goal was it to implement Amazon S3 image uploads to the PhPDolphin script, unfortunately I've run into a few Issues, if I add the code in the script just doesn't load and since the script doesn't have an error log I'm clueless as to what went wrong, for licensing reasons I'm unable to publish the entire script here but I will post a snipped of the affected area.
  Very negative
/includes/classes.
  Neutral
php [Default (Just a small snippet of the 4000 lines of code within this file)] And then my edited version that is supposed to upload the images automatically to Amazon S3.
  Negative
/includes/classes.
  Neutral
php [edited] (the s3 code is on the far bottom of the snippet) And yes I did add my own access and secret key :) Any help would be greatly appreciated and will be credited!
  Negative
Links to the products and API used in this: [PhPDolphin] [S3.php API on Github]
563e0f752d1761a701f0f8d9	X	This is how you can use the library i am using, i hope it will fix your problems (make sure the folder on the bucket actually exists otherwise just upload something to the root of the bucket)
563e0f762d1761a701f0f8da	X	Digging a bit further, I realized that Amazon Cloudfront is more like a CDN.
  Very negative
Am I correct ?
  Neutral
Also, does that mean, if I don't want my audio files to be distributed across a delivery network and want to always stream from a static location, Can I do without Cloudfront ?
  Negative
Will I be able to stream my audio only from Amazon S3 ?
  Neutral
563e0f762d1761a701f0f8db	X	So, as long as the file is on the same server as the web application, there shouldn't be any issues.
  Negative
But once I move the files to Amazon, the app shall start to download the files to the VPS and then play ??
  Negative
563e0f762d1761a701f0f8dc	X	Not only Amazon, be it any remote server or storage service.
  Negative
.
  Neutral
Streaming of multimedia objects are done, just not to keep the end user waiting for a long time for the object to download.
  Negative
Take Youtube videos as example.
  Neutral
563e0f762d1761a701f0f8dd	X	@Nannakuhtum - This is not correct.
  Negative
If your point your player at a url on S3, it will be served directly to the browser and will not go via the web server.
  Negative
Also, Youtube uses progressive download for its video service and not streaming.
  Negative
See blog.mydeo.com/2009/01/12/… for more info on streaming vs progressive download
563e0f762d1761a701f0f8de	X	@GeoffAppleford That is not fully incorrect.
  Negative
.
  Neutral
The mention of Application server should be replaced with the client where the video is playing.
  Negative
.
  Neutral
The blog that u referred was really helpful.
  Negative
An extract from the blog: "A temporary copy of the video file is then stored on the local computer so that the viewer can watch the file over and over without having to download the file each time."
  Negative
About youtube videos, it is not using progressive download, If it is so one cannot seek to the preferred position in the video and not a copy of the video is saved in the client.
  Negative
563e0f762d1761a701f0f8df	X	@Nannakuhtum - youtube absolutely does use progressive download.
  Negative
Google it.
  Neutral
And, you can seek within progressive download videos as long as the server supports byte-range requests.
  Negative
563e0f762d1761a701f0f8e0	X	Hmm.
  Neutral
I got your point, but you would suggest me to use cloudfront, won't you ?
  Negative
Because without using streaming distribution, it is going to be a download of the media rather than stream of the media, isn't it ?
  Negative
563e0f762d1761a701f0f8e1	X	Cloudfront can be setup as a streaming distribution or as a standard CDN using progressive download.
  Negative
Unless you have a specific reason to use a streaming distribution, I suggest you stick to progressive download.
  Negative
While it will still be a download, rather than pure streaming, the media will start playing as soon as enough has downloaded - eg you dont need to download the entire file before playing it.
  Negative
563e0f762d1761a701f0f8e2	X	I am new to Streaming Media.
  Negative
I am working on a web application built using Symfony 1.4.
  Negative
It features audio players and I am using jPlayer for the player.
  Negative
I use ffmpeg for encoding the audio files.
  Negative
Currently, I am storing my audio files on the development server.
  Negative
However, I would want to use Amazon S3 Storage Service for storing my audio files.
  Neutral
While going through the information available over the WWW and Amzon's site, I came to realize that it would require a streaming distribution service viz.
  Negative
Amazon Cloudfront.
  Neutral
At present, I am not using any streaming distribution while I play audio from my server.
  Negative
Is it necessary to use Amazon Cloudfront ?
  Neutral
Can't I directly serve my audio files from Amazon S3 by providing a URL as http://s3.mybucket.com/XXX ?
  Negative
What are the consequences of serving files directly from Amazon S3 and not using Cloudfront ?
  Neutral
A demo of the player that I am using can be seen here: http://audiodip.org/project/detailsProject/id/5
563e0f762d1761a701f0f8e3	X	If the audio file has to be retrieved from a remote server(in your case S3) you should read and play it as a streaming audio.
  Negative
Otherwise it is like downloading the full audio file to the application server and then playing it.
  Negative
Cloudfront is a content delivery service with which streaming comes of no extra cost.
  Positive
So it is better use the streaming service from cloudfront.
  Positive
and you will have to enable your audio player to play streaming data.
  Negative
563e0f762d1761a701f0f8e4	X	Can't I directly serve my audio files from Amazon S3 by providing a URL as http://s3.mybucket.com/XXX Yes, you can.
  Negative
If your files on S3 are public then your url will be in the format http://bucket.s3.amazonaws.com/filename, otherwise you need to create a pre-signed url using a GUI tool or one of the AWS S3 API.
  Negative
Just use that url as the audio source in your player.
  Negative
It will be served directly to the browser and will not go via your web server.
  Negative
You could put Cloudfront in front of S3 and it may improve performance but it is not necessary.
  Negative
You certainly don't need to use a streaming distribution.
  Neutral
563e0f762d1761a701f0f8e5	X	Ok thanks, the documentation was indeed confusing.
  Negative
Hope they change it :)
563e0f762d1761a701f0f8e6	X	I receive the same response from Amazon S3 with nil values and my object IS NOT removed
563e0f762d1761a701f0f8e7	X	Im trying to delete an object on S3 using the ruby aws-sdk (version 2).
  Very negative
It works fine, but it returns this Which doesnt make sense because in the documentation it says the response should be of the type: resp.delete_marker #=> true/false resp.version_id #=> String resp.request_charged #=> String, one of "requester" Why am I becoming nil?
  Negative
I want to know if the object was deleted or not.
  Negative
I am getting that response both when i succeed in deleting the object and when I dont.
  Negative
This is the code Im using to delete the object:
563e0f762d1761a701f0f8e8	X	Your delete object was successful.
  Negative
The Amazon S3 API only returns those values under certain circumstances.
  Negative
In this case, your object was not in a versioned bucket (no version id or delete marker boolean), and is not configured for request-payer.
  Negative
As a general rule, if the SDK does no raise an error from the response, it is successful.
  Positive
In this case, the API reference documentation may be confusing as it does not clearly indicate that these values may be nil.
  Negative
563e0f762d1761a701f0f8e9	X	Are your S3 permissions on the bucket/files set appropriately?
  Negative
It sounds like you might have it restricted in a way that allows you to see it and not whomever else you have viewing (e.g. by IP address).
  Negative
563e0f772d1761a701f0f8ea	X	There are only four options: list, upload/delete, view permissions, edit permissions.
  Negative
I'm not sure which of these will allow anyone to see the images on the website.
  Negative
I'm still very new to S3.
  Positive
563e0f772d1761a701f0f8eb	X	I'm using carrierwave and fog, and in the initializer I have config.root = Rails.root.join('tmp') config.cache_dir = 'carrierwave' config.fog_credentials = { :provider => 'AWS', :aws_access_key_id => 'ACCESSKEY', :aws_secret_access_key => 'SECRETKEY' } config.fog_directory = 'BUCKETNAME' config.fog_public = true
563e0f772d1761a701f0f8ec	X	When I view the source of the page and click on the image link, firefox brings up a page saying this connection is untrusted and that the certificate is invalid.
  Very negative
How do I fix this?
  Neutral
563e0f772d1761a701f0f8ed	X	@Kyle: if you view in non-SSL mode, does it show the image?
  Negative
563e0f772d1761a701f0f8ee	X	I figured it out, I wasn't storing the correct url, I thought it was the carrierwave uploader's .
  Negative
url method, but it actually was something different when I looked on S3.
  Negative
I changed it to the format of S3 and it worked perfectly!
  Positive
563e0f772d1761a701f0f8ef	X	CarrierWave was doing the bucketname followed by the standard s3 stuff, but when I looked on S3 it said it was the reverse.
  Negative
563e0f772d1761a701f0f8f0	X	I recently added photo uploading to a web application that I'm hosting on heroku.
  Negative
I'm using S3 for storage and it's working great, however when other users go onto the site, they see a photo missing icon rather than the photo.
  Negative
On my machine however, I see the photos on the site.
  Negative
Any clues as to what is happening?
  Neutral
Thanks!
  Positive
563e0f772d1761a701f0f8f1	X	When you upload your photos to s3 you need to set their access level to public using whatever s3 lib your using.
  Neutral
Here's snippet from s3 api docs: x-amz-acl The canned ACL to apply, to the object that is created after completing multipart upload.
  Negative
For more information, go to REST Access Policy in the Amazon S3 Developer Guide.
  Negative
Type: String Default: private Valid Values: private | public-read | public-read-write | authenticated-read | bucket-owner-read | bucket-owner-full-control http://docs.amazonwebservices.com/AmazonS3/latest/API/mpUploadInitiate.html
563e0f782d1761a701f0f8f2	X	Hello, I appreciate your help, but my intention is not for private use (or have to pay for it).
  Negative
Sorry but that can not solve my problem
563e0f782d1761a701f0f8f3	X	I'm searching along months (maybe 1 year) a way to upload a file to mediafire.
  Negative
I know mediafire use flash, y know mediafire API is so private... But i know too a program called "File&Image uploader" that can upload files to mediafire (HOW?!)
  Negative
my knowledge about networks are very, VERY basics, but i could make a Ruby script to upload images to Imageshack using the API and the "rest_client" gem, so maybe... i can do the same for mediafire... (With help) I tried all... ALL!
  Positive
: curl, wget, wput, ssl, ruby example scripts, python example scripts, perl example scripts, and a lot of unuseful CLI apps for linux and windows thati don't remember the name now... After all, at the moment i don't know the method to upload a file to mediafire.
  Negative
Well, at this point, My questions are: 1ª - Is it possible to upload a file (no matther if in my account or in "free" mode) to mediafire, using ruby or python?
  Negative
And anyone could give me a brief example or tell me the easiest way?
  Neutral
2º - If not possible to mediafire, could you tell me a FREE and GOOD server (I mean without recaptcha) with ability to upload a file using the rest_client gem ?
  Negative
(or another easy way) Thankyou for read.
  Neutral
563e0f782d1761a701f0f8f4	X	So, I'm sure this answer won't be optimal to what the writer desires, but here goes anyhow.
  Negative
If you want to upload files with media content you should really be taking a look at Amazon's S3 API.
  Positive
http://aws.amazon.com/s3/ Yes, you may have to pay at a certain point, but the free point on S3 will equal or exceed the free point on mediafire.
  Negative
The difference is, that on every level S3 is a better delivery method for your media files, and if you're attempting to abuse free offers you will have an equally frustrating time everywhere
563e0f782d1761a701f0f8f5	X	"something wrong with Amazon S3" seems unlikely.
  Very negative
How many uploads are you doing, per second, approximately?
  Negative
563e0f792d1761a701f0f8f6	X	@Michael-sqlbot Its max of 5 per second
563e0f792d1761a701f0f8f7	X	I'm getting the same error at the moment.
  Negative
I'm talking to one of their support people.
  Neutral
Will comment/answer back when I have an answer.
  Positive
563e0f792d1761a701f0f8f8	X	The reply, after a looooong wait, was that it's our S3 account that's at fault.
  Negative
The error is from Amazon but I think FP send back their own string, else I'd expect more than 10 results from Google!
  Negative
563e0f792d1761a701f0f8f9	X	Yeh, I guess this is kind of limitation from S3, I found it working very next day.
  Negative
563e0f792d1761a701f0f8fa	X	I'm using filepicker's rest API to upload images, https://developers.filepicker.io/docs/web/rest/#blob-store It was working fine for few uploads, then it started throwing the error - RateLimit Exception for Amazon IO Error I think something wrong with amazon S3, any idea please ?
  Negative
563e0f792d1761a701f0f8fb	X	not ideal, but unfortunately it doesn't appear that the rightscale or aws ruby sdks support this without dropping to the http level.
  Negative
563e0f792d1761a701f0f8fc	X	I think it is possible to add this feature within aws-sdk.
  Negative
I didn't have time to complete it yet, but take a look at this gist.
  Negative
It raises an authentication error on s3.client.set_bucket_website.
  Negative
563e0f792d1761a701f0f8fd	X	hjblok - great thinking!
  Positive
Using the appfog ruby sdk totally solved it!!!
  Negative
Thank you!!
  Neutral
563e0f7a2d1761a701f0f8fe	X	Unfortunately you seem to have misunderstood the question.
  Negative
I want to turn a bucket into a website, using the above described Amazon API.
  Negative
I am looking for a ruby library that supports this operation on a bucket object.
  Negative
The code is obviously not required to run "on S3", as it is a storage solution only, just as you correctly remarked.
  Negative
563e0f7a2d1761a701f0f8ff	X	Hey Jeevan, Alojscha is trying to make the equivalent of this http request with a ruby sdk wrapper: docs.amazonwebservices.com/AmazonS3/latest/API/…
563e0f7a2d1761a701f0f900	X	I want to set up an Amazon S3 bucket as a website as described here: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTBucketPUTwebsite.html?r=5271 but using an ruby API, preferably the aws-sdk for ruby.
  Very negative
Is there a possibility to do that / a library that already supports that?
  Neutral
Could not find anything in aws-sdk and right-aws, but maybe I was just blind?
  Negative
563e0f7a2d1761a701f0f901	X	It is possible to configure your bucket as website using an ruby API.
  Negative
I did found a solution, but this uses the aws-s3 gem, not the aws-sdk gem.
  Positive
I found this solution in the ponyhost gem: EDIT you could also use the fog gem to accomplish this.
  Very positive
563e0f7a2d1761a701f0f902	X	Current version of AWS SDK for Ruby has the method #configure_website for bucket objects.
  Negative
So something like this would work: (the block to configure_website may be omitted if you don't need to set non-default options)
563e0f7a2d1761a701f0f903	X	@Aljoscha AWS S3 is just a storage solution, to store all your files.
  Negative
It doesnt provide any kind of run time solution.
  Negative
You need to have a Ec2 instance to host your ruby based application or either to use ruby API.
  Neutral
You can just host a static web site on S3 but cant run any kind of app.
  Negative
563e0f7b2d1761a701f0f904	X	I have been tasked with copying s3 objects from one bucket to another.
  Negative
The bucket contains millions of objects.
  Positive
The object should only be physically copied if either of two conditions are met.
  Negative
The object does not exist in the target bucket or; The object in the source bucket has changed and no longer is identical to its counterpart in the destination bucket.
  Negative
I am using the 1.5.3 version of the AWS SDK for .
  Negative
NET and I cannot change versions.
  Negative
563e0f7b2d1761a701f0f905	X	The AWS Command Line Interface (CLI) has an in-built sync operation that will copy files if they do not exist or if they have changed.
  Negative
If you wanted to write your own version of this sync functionality you would have to list objects from both locations, look for differences and then use the Amazon S3 copy API call to copy files between buckets.
  Negative
Using the CLI is a lot simpler!
  Positive
563e0f7b2d1761a701f0f906	X	This has just saved me a lot of time.
  Negative
Thanks.
  Neutral
563e0f7b2d1761a701f0f907	X	This is not actually the case.
  Negative
It is possible to pipe/stream to s3!
  Neutral
you just need to know the size of the upload.
  Neutral
If your client can provide that then you can indeed use pipe to upload to s3 without a nasty hard drive write.
  Negative
I'm writing a cli and intermediary server that will upload to s3.
  Negative
Because I control both the client and server I can determine the file size before upload.
  Positive
I think there may be other edge cases like mine that should not be dismissed.
  Negative
I use knox to stream to s3 with a put request.
  Negative
563e0f7b2d1761a701f0f908	X	@CharlesTWall3 This is a very valid comment, I didn't think about that at the time - I was thinking about a server-side only solution.
  Negative
Feel free to post an answer if you manage to get something working, I'll happily vote for your solution.
  Negative
You may also want to edit this answer.
  Negative
Thanks!
  Positive
563e0f7b2d1761a701f0f909	X	Will do.
  Neutral
Cheerio.
  Neutral
563e0f7b2d1761a701f0f90a	X	@gulthor - thanks for your input.
  Negative
For my situation, i was interested in streaming from mongodb via node app (no browser).
  Negative
Found solution by using "s3-upload-stream" NPM module.
  Neutral
It uses S3 multipart API and therefore does NOT require the overall filesize up front.
  Negative
It works in chunks and passes their size automatically.
  Positive
Only took a few mins to copy the example code off the readme and plugin to my app.
  Negative
Gotta love the convenience of NodeJS modules community.
  Positive
There are quite few old SOF posts still out there trying to do this using bespoke solutions that are not ideal.
  Negative
Thanks again for suggestion.
  Neutral
563e0f7b2d1761a701f0f90b	X	I'm trying to stream upload a file submitted via a form directly to an Amazon S3 bucket, using aws-sdk or knox.
  Negative
Form handling is done with formidable.
  Positive
My question is: how do I properly use formidable with aws-sdk (or knox) using each of these libraries' latest features for handling streams?
  Negative
I'm aware that this topic has already been asked here in different flavors, ie: However, I believe the answers are a bit outdated and/or off topic (ie.
  Negative
CORS support, which I don't wish to use for now for various reasons) and/or, most importantly, make no reference to the latest features from either aws-sdk (see: https://github.com/aws/aws-sdk-js/issues/13#issuecomment-16085442) or knox (notably putStream() or its readableStream.pipe(req) variant, both explained in the doc).
  Negative
After hours of struggling, I came to the conclusion that I needed some help (disclaimer: I'm quite a newbie with streams).
  Negative
HTML form: Express bodyParser middleware is configured this way: POST request handler: However, I'm getting the following error: { [RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period.
  Negative
Idle connections will be closed.]
  Neutral
message: 'Your socket connection to the server was not read from or written to within the timeout period.
  Neutral
Idle connections will be closed.'
  Neutral
, code: 'RequestTimeout', name: 'RequestTimeout', statusCode: 400, retryable: false } A knox version of handlePart() function tailored this way also miserably fails: I also get a big res object with a 400 statusCode somewhere.
  Very negative
Region is configured to eu-west-1 in both case.
  Negative
Additional notes: node 0.10.12 latest formidable from npm (1.0.14) latest aws-sdk from npm (1.3.1) latest knox from npm (0.8.3)
563e0f7c2d1761a701f0f90c	X	Well, according to the creator of Formidable, direct streaming to Amazon S3 is impossible : The S3 API requires you to provide the size of new files when creating them.
  Very negative
This information is not available for multipart/form-data files until they have been fully received.
  Negative
This means streaming is impossible.
  Neutral
Indeed, form.bytesExpected refers to the size of the whole form, and not the size of the single file.
  Negative
The data must therefore either hit the memory or the disk on the server first before being uploaded to S3.
  Negative
563e0f7c2d1761a701f0f90d	X	Any possibility of providing the actual code, that compiles?
  Neutral
all of the examples have errors...
563e0f7c2d1761a701f0f90e	X	What do you mean the actual code?
  Negative
The entire file?
  Neutral
563e0f7c2d1761a701f0f90f	X	For example, your samples are missing a required semicolon.
  Negative
563e0f7c2d1761a701f0f910	X	I am on a phone.
  Negative
I was expecting something to be missing.
  Negative
Sorry.
  Positive
On my actual code there are no 'compiling' errors.
  Negative
563e0f7c2d1761a701f0f911	X	Notice: $filename = "${file_name}"; and $filename = '${file_name}'; is completely different, as the second does not evaluate $file_name as a variable!
  Very negative
563e0f7c2d1761a701f0f912	X	Where do I set that piece of code?
  Negative
Just above $params->key = ?
  Neutral
.
  Neutral
According to your the variable $filanem, it is a scope because if I do $params->key = rand_string(5).'
  Negative
${filename}'; it returns the Dogfilename.png.
  Negative
Correct me if I misunderstood 'Scope'.
  Negative
563e0f7c2d1761a701f0f913	X	Result: i.imgur.com/fmGoD.jpg
563e0f7c2d1761a701f0f914	X	Okay now I get your point.
  Negative
I'll try and do that.
  Neutral
I need to find exactly where the putObject is
563e0f7c2d1761a701f0f915	X	Just another point, you can upload normally, and then rename when uploaded?
  Negative
563e0f7c2d1761a701f0f916	X	As the question states, it is uploaded onto Amazon S3 and thus I am not able to do that.
  Negative
I can upload, copy, replace, delete previous (that will cost me 4 connections instead of 1).
  Positive
563e0f7c2d1761a701f0f917	X	I get this: 3Sf5f.
  Negative
563e0f7c2d1761a701f0f918	X	I changed the $filename to '${filename}' and I get this: 3Sf5f.myimage.png
563e0f7c2d1761a701f0f919	X	This means that somewhere down the line, your $filename is losing its structure.
  Very negative
Look further up in your code, are you changing $filename anywhere?
  Neutral
563e0f7c2d1761a701f0f91a	X	Notice I'm appending $extension to the params key, not $filename.
  Negative
If you want to override filename after you've got the extension, just do $filename = $extension.
  Negative
563e0f7c2d1761a701f0f91b	X	No filename exists in the entire file for some reason.
  Negative
The entire app is based on 1 file.
  Neutral
563e0f7c2d1761a701f0f91c	X	Returns nothing sir
563e0f7d2d1761a701f0f91d	X	do you have access to change the html form file?
  Negative
563e0f7d2d1761a701f0f91e	X	I do have access and your answer returns nothing sir.
  Negative
563e0f7d2d1761a701f0f91f	X	the variable $file must contain the temp location of the uploaded file.
  Negative
563e0f7d2d1761a701f0f920	X	$file in this case is the contents of $_FILES['form_field_name']['tmp_name']
563e0f7d2d1761a701f0f921	X	$file for me returns nothing
563e0f7d2d1761a701f0f922	X	It echo's empty so the 2nd part of your answer doesn't work
563e0f7d2d1761a701f0f923	X	I played around with some variables and $filename = "${filename}"; is the equivalent of saying $filename = $filename; It doesn't do anything at all, you are setting a variable equal to itself.
  Very negative
The variable $filename never even gets set in that function (getHttpUploadPostParams()).
  Negative
I'm guess from the comments on the function that the filename is in an element of $headers.
  Negative
Try doing a print_r($headers); die; before $filename = "${filename}"; and see what you get;
563e0f7d2d1761a701f0f924	X	Wrong.
  Negative
$filename = "${filename}"; is not the same as $filename = $filename.
  Negative
Because I am using $filename further down and the first way i get the actual filename, the second way I dont.
  Negative
563e0f7d2d1761a701f0f925	X	It is the same thing create a php file and just put this in it: $filename = 'test'; $filename2 = "${filename}"; echo "filename2: $filename2"; You will get "filename2: test" as the output.
  Very negative
I understand you are not getting the same thing but there is some other problem in your code.
  Negative
Did you do a print_r($headers); if so what did you get?
  Negative
563e0f7d2d1761a701f0f926	X	Returns nothing.
  Negative
563e0f7d2d1761a701f0f927	X	@jQuerybeast Sorry.
  Negative
I changed that.
  Neutral
I wrote the code too fast.
  Negative
:/
563e0f7d2d1761a701f0f928	X	This works but this is not the case.
  Negative
If I change the image.png to "$filename" (which returns the actual filename), return nothing.
  Negative
563e0f7d2d1761a701f0f929	X	@jQuerybeast Nop.
  Negative
You rename the image.png to $new_name, so it is a random string with extension appended.
  Negative
563e0f7d2d1761a701f0f92a	X	How do you know it will be .
  Neutral
png?
  Neutral
.
  Neutral
Obviously my question is for all sort of extensions.
  Neutral
563e0f7d2d1761a701f0f92b	X	So on $params->key = $filename; what will I call alongside?
  Negative
563e0f7d2d1761a701f0f92c	X	This is my entire HTML file: pastebin.com/qXCVr3eD
563e0f7d2d1761a701f0f92d	X	Invalid according to Policy: Extra input fields: ext
563e0f7d2d1761a701f0f92e	X	you could use this directly $params->key = $filename.
  Negative
$_POST['ext']; but I see you can't add an extra field.
  Negative
in that case this solution is not possible.
  Negative
here the extension is evaluated and sent as a part of form data.
  Negative
563e0f7d2d1761a701f0f92f	X	Short or longer number dont bother that much.
  Negative
Again, your answer wont work if you've read all the answer etc.
  Neutral
What if I upload a file with no extension?
  Neutral
563e0f7d2d1761a701f0f930	X	so the problem is in the s3 upload class, i never used this class , but i used this one: the Zend Framework s3 class (you could use it in standalone mode) give a try if you are desperate... framework.zend.com/manual/en/zend.service.amazon.s3.html
563e0f7e2d1761a701f0f931	X	Will that class allow me to reset to anything at any time?
  Negative
563e0f7e2d1761a701f0f932	X	what do yo mean about "reset"?
  Negative
563e0f7e2d1761a701f0f933	X	Im really sorry I was probably thinking of something else.
  Negative
I meant, will that class allow me to change the filename, into any random screen and the file extension?
  Neutral
Because it seems my problem comes from Amazon's end.
  Negative
563e0f7e2d1761a701f0f934	X	Doesn't work.
  Negative
Was one of my many tests.
  Neutral
Returns nothing
563e0f7e2d1761a701f0f935	X	Have you confirmed if your $filename variable is a valid string?
  Negative
563e0f7e2d1761a701f0f936	X	I did.
  Neutral
I am not sure if my confirmation was correct.
  Negative
If I do $filename = 'someimage.png', I get its file extension
563e0f7e2d1761a701f0f937	X	AFAICT, it appears your issue has more to do with this variable then getting the file extension part.
  Negative
563e0f7e2d1761a701f0f938	X	This is the entire file: pastebin.com/EXfJwpny It is the only class that exists in my app.
  Negative
Search for $filename.
  Negative
Only one exists in the class
563e0f7e2d1761a701f0f939	X	syntax error, unexpected T_UNSET
563e0f7e2d1761a701f0f93a	X	Missing Semicolon.
  Negative
563e0f7e2d1761a701f0f93b	X	The entire file: pastebin.com/EXfJwpny
563e0f7e2d1761a701f0f93c	X	Warning: implode() [function.implode]: Invalid arguments passed on your edit
563e0f7e2d1761a701f0f93d	X	Wow.
  Negative
I get this: gVcwVcfcd208495d565ef66e7dff9f98764da.
  Neutral
Note the file extension still missing.
  Negative
This is frustrating
563e0f7e2d1761a701f0f93e	X	Doesn't work.
  Very negative
Only the first one.
  Neutral
563e0f7e2d1761a701f0f93f	X	After the first line try vardump($temp).
  Negative
Maybe there is something in your string that doesn't normally show up.
  Negative
563e0f7e2d1761a701f0f940	X	I am using the Amazon S3 API to upload files and I am changing the name of the file each time I upload.
  Negative
So for example: Dog.png > 3Sf5f.png Now I got the random part working as such: So I set the random_string to the name parameter as such: Now my problem is that this wont show any extension.
  Negative
So the file will upload as 3Sf5f instead of 3Sf5f.png.
  Negative
The variable $filename gives me the full name of the file with its extension.
  Neutral
If I use $params->key = rand_string(5).'
  Negative
${filename}'; I get: So I tried to retrieve the $filename extension and apply it.
  Negative
I tried more than 30 methods without any positive one.
  Negative
For example I tried the $path_info(), I tried substr(strrchr($file_name,'.')
  Negative
,1); any many more.
  Negative
All of them give me either 3Sf5fDog.png or just 3Sf5f.
  Negative
An example of what I tried: .
  Positive
.
  Neutral
The entire class file: http://pastebin.com/QAwJphmW (there are no other files for the entire script).
  Negative
What I'm I doing wrong?
  Neutral
This is really frustrating.
  Negative
563e0f7e2d1761a701f0f941	X	The variable $filename (and thus "${filename}") is NOT IN SCOPE at line 1053 of your code (line numbering based on raw paste from pastebin).
  Negative
So, no matter what you do, you'll never find the extension of a variable that does not exist.
  Negative
And I've finally worked out what you're doing.
  Positive
I presume this is an extension of PHP: Rename file before upload Simple answer: you can't do it as you envisage.Why - the '$filename' is not parsed at the time that URL is created, but the variable is passed to Amazon S3 and handled there.
  Negative
The solution So, the only option I can think of is to have use the "successRedirect" parameter to point to another URL.
  Negative
That URL will receive the "bucket" and "key" as query parameters from Amazon (http://doc.s3.amazonaws.com/proposals/post.html#Dealing_with_Success).
  Positive
Point that to a PHP script that renames the file on Amazon S3 (copy + delete), then redirects the user to another success screen.
  Negative
So, in your code, line 34, That will do exactly what you want.
  Negative
In response to your comments "Is this the only way - what about the costs as Amazon charge per request?"
  Negative
Delete requests are free.
  Neutral
No data transfer costs when moving on the same bucket (or even in the same region).
  Negative
So this solution (which is the only way without you transferring to an intermediate server, renaming and uploading) it doubles the cost of upload a from 1c per 1000 uploads to 2c per 1000 uploads.
  Negative
It's taken me 10 minutes @ $200/hour to find that out and respond = $33 = 1,666,666 uploads!
  Negative
Costs pale a bit when you do the maths :) Compare with the other solution: do a post to an webserver, rename the file and then upload from the webserver: you move all the bandwidth from the clinet tdirectly to yourself - twice.
  Negative
And this also introduces risk and increased possible failure points.
  Negative
In response to "Doesn't work.
  Negative
I you upload a file then the old one gets deleted" I would assusme this is not a problem as you upload a file and then rename it within a second or two.
  Negative
But if you want ot gurantee each file gets uploaded, then you need to do a lot more than create a random filename anyway:
563e0f7e2d1761a701f0f942	X	This explode() divides up the file name into an array with periods as delimiters, and then grabs the last piece of the array (incase a file name is foo.bar.jpg), and puts a period in front of it.
  Negative
This should get you the desired extension to append it to the rand_string(5).
  Negative
563e0f7e2d1761a701f0f943	X	I think something as simple as below should work to extract file extension from the file-name:
563e0f7e2d1761a701f0f944	X	if you're uploading images try this
563e0f7e2d1761a701f0f945	X	What happends if you: Do you get something like 'Dog.png'?
  Negative
If you don't there is something wrong in the way you are getting the filename.
  Negative
If you do get something like 'Dog.png', here is what I use to get the file extension.
  Negative
Then you should be able to do this:
563e0f7e2d1761a701f0f946	X	You need to first find out what the original extension is and not rename the entire file.
  Negative
So keep the extension and rename de file name.
  Positive
Assuming you have image name in $image_name:
563e0f7e2d1761a701f0f947	X	ok here's another try that I used when I had trouble getting the extension on the server side.
  Negative
what I did was, I used javascript to extract the file extension and the send it via post.
  Negative
in the next php file you can directly use $_POST['ext'] as extension.
  Negative
hope that helped.
  Positive
let me know if you have any trouble implementing this
563e0f7f2d1761a701f0f948	X	i am using this in my websites (and works fine for years): your function generates too short filenames (5 characters), this way creates longer filenames, avoiding to collide the file names.
  Very negative
example output: aff5a25e84311485d4eedea7e5f24a4f.png
563e0f7f2d1761a701f0f949	X	It appears what's actually going on is rather than fully producing the filename right now, you're in effect passing a very small 'program' through the interface so it can then produce the filename later (when the variable $filename exists and is in scope).
  Negative
The other side of the interface eventually executes that 'program' you pass in, which produces the modified filename.
  Negative
(Of course passing a 'program' to something else to execute later doesn't tend to make debugging real easy:-) (It's of course up to you whether you want to "make this work" or "do it a different way".
  Negative
"Different ways" typically involve renaming or copying the file yourself before you even try to invoke the upload interface, and are described in other answers.)
  Negative
If you decide you want to "make it work", then the entire filename parameter needs to be a program, rather than just part of it.
  Negative
This somewhat uncommon functionality typically involves enclosing the entire string in single quotes.
  Negative
(You also need to do something about existing single quote marks inside the string so they don't terminate the quoted string too soon.
  Negative
One way is to quote each of them with a backslash.
  Positive
Another way that may look cleaner and usually works is to replace them with double quotes.)
  Positive
In other words, I believe the code below will work for you (I haven't got the right environment to test it, so I'm not sure).
  Negative
(Once you get it working, you might want to revisit your naming scheme.
  Negative
Perhaps the name needs to be a little longer.
  Neutral
Or perhaps it should include some identifiable information {like today's date, or the original name of the file}.
  Neutral
You may hit on something like $file_base.
  Neutral
rand_string(7).
  Neutral
$file_extension.
  Neutral
563e0f7f2d1761a701f0f94a	X	Untested, but simple enough to work: will return the extension part (without the '.')
  Neutral
563e0f7f2d1761a701f0f94b	X	A simple solution to re-name a file and compute the extension: Note, that md5() is always 32 bytes long and non unique regarding the computed value.
  Negative
For for many practical instances, it's unique enough.
  Positive
Addendum Additionally, you may use this solution to trace variable changes: A sample use case:
563e0f7f2d1761a701f0f94c	X	How about this?
  Negative
563e0f802d1761a701f0f94d	X	No exception is thrown?
  Negative
What is the status code of the request?
  Neutral
563e0f802d1761a701f0f94e	X	No exception, the image is uploaded and response is 200 OK.
  Negative
563e0f812d1761a701f0f94f	X	When you say the image was uploaded successfully, did you also compare the md5sum?
  Negative
After uploading successfully, try comparing the md5sum of the object you uploaded.
  Negative
This way you can be damn sure that the object was not changed at all.
  Neutral
Also you say when I try open the image but it is damaged.
  Negative
.
  Neutral
are you trying to view it on desktop?
  Negative
563e0f812d1761a701f0f950	X	I try to open the browser and on the desktop too!
  Negative
563e0f812d1761a701f0f951	X	@A.
  Neutral
Anderson Do you have a sample (any public link?)
  Neutral
of the image/object you are trying to upload?
  Neutral
563e0f812d1761a701f0f952	X	Thanks for answer, I believe that be some wrong with my request body, because the request is performed ok, but I'm using Retrofit 2, TypedFile was removed from lib, and I cant make downgrade, I need use the version 2.
  Negative
563e0f812d1761a701f0f953	X	Have you considered leaving off the explicit Content-Type and Content-Length headers, and letting Retrofit obtain them from the RequestBody ?
  Negative
563e0f812d1761a701f0f954	X	ok, I'll try this!
  Negative
563e0f812d1761a701f0f955	X	This does not work AWS api rest requires that this header is sent.
  Negative
563e0f812d1761a701f0f956	X	It make sense, I'll trying this approach later!
  Negative
563e0f812d1761a701f0f957	X	So I tried, but does not work well.
  Negative
The image is not sent right!
  Negative
563e0f812d1761a701f0f958	X	their approach seems ok, I figured out that the s3 AWS need that content is to be sent in the request body and not as multipart.
  Negative
563e0f812d1761a701f0f959	X	I'm trying upload a Image from my Android APP to Amazon AWS S3 and I need use AWS Restful API.
  Negative
I'm using Retrofit 2 to make to the request.
  Neutral
My application is connecting successfully with Amazon S3 and performing the request as expected, but when I try to view the Image from the Bucket, the picture does not open.
  Negative
I downloaded the Image to my pc and tried to open but keep getting the message that the image is corrupted.
  Negative
Lets see my complete code bellow.
  Negative
My Gradle dependencies Here is created a File and starts the request Retrofit Interface Utils class to the mount the credentials Lastly the method to make a request I appreciate any helps, thanks in advance!
  Positive
563e0f812d1761a701f0f95a	X	I haven't used Retrofit 2, just Retrofit 1, so YMMV, but I believe that the typical way to do what you're trying to do is to use TypedFile where you are attempting to use RequestBody.
  Very negative
I'm guessing that Retrofit uses RequestBody internally.
  Positive
You would create the TypedFile something like: and your interface would be: There's a decent example at https://futurestud.io/blog/retrofit-how-to-upload-files/
563e0f812d1761a701f0f95b	X	You are sending a multipart payload, but forcing the Content-type to be image/jpeg.
  Negative
Your jpg is corrupt because S3 probably saved the multipart headers into your jpg file since you told it the whole message was a JPG.
  Negative
Since you do not actually have multiple parts to send, you can drop the Multipart annotation and use Body instead of Part for your RequestBody You should also be able to remove explicitly setting the Content-type and Content-length headers.
  Negative
563e0f812d1761a701f0f95c	X	I have the same problem, and as I use Fiddler checked the HTTP request content, I found retrofit 2.0.0 beta1 has a different with 1.9.0.
  Very negative
In my problem, the different of HTTP request content prevent server get the correct data.
  Negative
In order to make a same HTTP request content, i do next steps using retrofit 2.0.0 deta1.
  Negative
In the retrofit service, add a form-data header for the http request; int retrofit 2.0.0 deta1, the header using @Multipart will get a data like this: Content-Type: multipart/mixed as the deafult value is mixed, and has no boundary title.
  Negative
Do not using @Multipart to upload file, just using @Body RequestBody if you using @Multipart to request Server, you have to pass param(file) through @Part(key), then a new problem you will get.
  Very negative
May be retrofit 2.0.0beta1 has a BUG ..., @Multipart generate a bad http request compile with 1.9.0.
  Negative
When you call the method, you need pass MultipartRequestBody to @Body RequestBody Using MultipartBuilder to create a MultipartRequestBody, when you new MultipartBuilder, call this consturt: the param is you set int @headers(boundary=) This method will help form a data like below int HTTP request content: Content-Disposition: form-data; name="imgFile"; filename="IMG_20150911_113029.
  Negative
jpg" Content-Type: image/jpg Content-Length: 1179469 RequestBody value is what you has generate in your code.
  Negative
I just resolve this problem temporary.
  Negative
Hope can help you!
  Neutral
563e0f852d1761a701f0f95d	X	Do you control the file and folder names?
  Neutral
If so, you could have your script probe how many images there are (i.e. check if 012/99.
  Negative
jpg exists etc) to avoid having a list of files.
  Negative
563e0f852d1761a701f0f95e	X	it was my first choice (yes, I can choose the file name I prefer), and I still love it very much, but I'd like to show always the last modified files first AND I can't load all files data since they are too many and it would be slow.
  Negative
I though to choose consecutive numbers as filenames, but it would be difficult to handle in case of modification/erase of a file (last modified should always be the first in the slideshow)
563e0f852d1761a701f0f95f	X	I don't see a way to order the results with that SOAP API call.
  Negative
So it still doesn't seem to satisfy your need.
  Negative
563e0f852d1761a701f0f960	X	From my test, it seems that the xml is ordered by last modification, even if I didn't found a clear indication about that.
  Negative
563e0f852d1761a701f0f961	X	If that's the case then I'd say this is your best bet as much as I dislike SOAP.
  Negative
563e0f852d1761a701f0f962	X	Eventually I will use the REST API GET to create list.html file, however, thank you.
  Negative
563e0f852d1761a701f0f963	X	It could be THE solution, but I'd like to get an ordered list with newest (and last modified)files first and it should not deliver the entire list, since it could be very large.
  Negative
I have to think how to choose the filename to make it possible through the "prefix" parameter
563e0f852d1761a701f0f964	X	Hypothesis: I have thousands of images into different folders in an amazon S3 bucket.
  Negative
I'd like to make them accessibile to unlogged users as slideshow, but I don't want to deal with db and server poor performance (in case of too many users at the same time) , so I'd like to use only javascript.
  Negative
The problem is that I should however deliver to the client the file list, since I can't use XMLHttpRequest to fetch and parse the xml file that Amazon provides when you try to browse a bucket because (I expect) the browsing page should be located on my webserver.
  Negative
I think I should write some server-side code to create,after every upload/modification, an updated filelist to share with users, but I'm not sure it's a good idea.
  Negative
Can anybody suggest me the best way to proceed?
  Neutral
Happy New Year!
  Positive
563e0f852d1761a701f0f965	X	Possible answer, tell me what do you think about: Amazon provides ListBucket operation http://docs.amazonwebservices.com/AmazonS3/latest/API/SOAPListBucket.html I can choose how many results to get at once using max-keys and marker (for pagination) parameters (example: http://download.terracotta.org/?max-keys=5).
  Negative
I will obtain a xml file (as smallas I want) that I can parse locally with js in a "list.html" file, for example.
  Negative
I could then include this list.html file (that should print just the definition of an array of images) in a iframe included in my slideshow.html file on my webserver.
  Negative
Too dirty?
  Neutral
563e0f852d1761a701f0f966	X	The Amazon S3 JavaScript API has a method, bucket.list() that will list the contents of a bucket.
  Negative
563e0f862d1761a701f0f967	X	Thanks for the answer Geoff, unfortunately it doesn't seem to work either.
  Negative
I'll do a bit more fiddling around and see if I've overlooked something somewhere.
  Positive
563e0f862d1761a701f0f968	X	@davee - Well in that case, I suspect its because that library hasn't been updated to support the new server side encryption.
  Negative
Why not use the official PHP SDK?
  Negative
563e0f862d1761a701f0f969	X	I thought that it could have been the class too, but then I looked at the source and see the headers are set like so: if (is_array($requestHeaders)) foreach ($requestHeaders as $h => $v) $rest->setHeader($h, $v); so in theory adding the server-side encryption should work.
  Negative
This leads me to think that perhaps amazon hasn't fully implemented the feature across all its systems yet.
  Negative
Also, unfortunately the code I inherited heavily relies on this class and I don't have the time to update all the code.
  Negative
I'll see if I can get a response from someone from amazon and post my findings here.
  Positive
Cheers.
  Neutral
563e0f862d1761a701f0f96a	X	I should note that the above works perfectly now.
  Positive
563e0f862d1761a701f0f96b	X	I have decide to avail of amazons new server-side encryption with s3, however, I have run into a problem which I am unable to resolve.
  Very negative
I am using the s3 PHP class found here : https://github.com/tpyo/amazon-s3-php-class I had been using this code to put objects originally (and it was working) : I then did as instructed here : http://docs.amazonwebservices.com/AmazonS3/latest/API/index.html?RESTObjectPUT.html and added the 'x-amz-server-side​-encryption' request header.
  Very negative
But now when I try to put an object it fails without error.
  Negative
My new code is : ); Has anybody experimented with this new feature or can anyone see an error in the code.
  Positive
Cheers.
  Neutral
563e0f862d1761a701f0f96c	X	That header should be part of the $metaHeaders array and not $requestHeaders array.
  Negative
Here's the method definition from the docs: You might also consider using the SDK for PHP?
  Negative
563e0f862d1761a701f0f96d	X	We can upload files with encryption using the code following $s3->create_object($bucket_name,$destination,array( 'acl'=>AmazonS3::ACL_PUBLIC, 'fileUpload' => $file_local, 'encryption'=>"AES256")); And you can download latest sdk from here
563e0f862d1761a701f0f96e	X	With the official SDK: Source: http://docs.aws.amazon.com/AmazonS3/latest/dev/SSEUsingPHPSDK.html
563e0f862d1761a701f0f96f	X	Why don't you just implement an ordered dictionary?
  Very negative
563e0f862d1761a701f0f970	X	OK, but then how would I parse a String representing a JSON object into an OrderedDictionary?
  Negative
Why not just use an Array of tuples instead of an OrderedDictionary?
  Negative
I've updated my question.
  Neutral
563e0f862d1761a701f0f971	X	In Swift, is it possible to parse a String representing a JSON object that only contains strings into an Array of tuples [(String, String)] (not a Dictionary<String, String>)?
  Negative
I'm programming my iPhone app to forward the presigned post response from my server to Amazon S3, which requires the clients to preserve the ordering of the fields.
  Negative
This is an example JSON string, with whitespace added for presentation: According to Amazon S3: API Reference: Authenticating Requests in Browser-Based Uploads Using POST, I think it's safe to assume that none of the strings will contain ", :, or whitespace characters.
  Negative
So, I guess my question is: How do I parse a String like the one above into an Array of tuples [(String, String)]?
  Negative
563e0f862d1761a701f0f972	X	it looks quite simple to try and find out.
  Negative
.
  Neutral
563e0f872d1761a701f0f973	X	If i create a temporary url to an s3 object, can I then change the domain to my cloudfront distribution and have the content still be available from cloudfront?
  Negative
563e0f872d1761a701f0f974	X	It won't work if you simply change domain to CloudFront distribution in your S3 temporary url.
  Negative
The approach is different with CloudFront.
  Positive
See the Serving Private Content chapter of CloudFront Developer Guide.
  Negative
.
  Neutral
The most relevant part: Use the CloudFront control API to create a CloudFront origin access identity.
  Positive
For more information, see Creating a CloudFront Origin Access Identity.
  Negative
Use the Amazon S3 API (or your favorite Amazon S3 tool) to update the ACL on your private objects to give read permission to the CloudFront origin access identity you just created.
  Negative
For a list of Amazon S3 tools you can use, go to Amazon CloudFront Developer Tools.
  Negative
For more information about setting the ACL, see Modifying the ACL on Your Private Content Objects.
  Negative
Set up a private content distribution or streaming distribution (either create a new one or update an existing distribution).
  Negative
For more information, see Setting Up a Private Content Distribution and Streaming Distribution.
  Negative
Use the Amazon S3 API (or your favorite Amazon S3 tool) to update the ACL on your private objects to remove any read permission grants for the public, leaving the read permission for the CloudFront origin access identity.
  Negative
For more information, see Modifying the ACL on Your Private Content Objects.
  Negative
You can stop here if you simply want to serve private content with basic URLs.
  Negative
Continue if you want to use signed URLs.
  Neutral
Use the AWS web site to create a key pair and download the private key, which you'll use to sign the URLs.
  Negative
For more information about creating your key pair, see Creating a Key Pair.
  Negative
Update your private content distribution or streaming distribution to specify that the distribution's URLs must be signed, and who can sign them.
  Negative
For more information, see Requiring Signed URLs.
  Negative
Create a signed URL to give the end user.
  Neutral
For more information, see Creating a Signed URL.
  Negative
563e0f872d1761a701f0f975	X	Just tried it out, they do work.
  Negative
Note this is not for signed urls.
  Negative
just the general url to the object.
  Neutral
563e0f872d1761a701f0f976	X	I'm developing a Cordova application to interact with Amazon S3.
  Negative
I have a png image stored in S3.
  Neutral
I'm using getObject function from AWS Javascript SDK to retrieve that image.
  Negative
I have an array of uint8array in the data.body property of the retrieved object.
  Negative
Is there any way to save this array as an image file?
  Neutral
maybe with File System plugin from Cordova?
  Negative
or maybe something like a FileWriter( as FileReader from Mozilla Web API Interfaces)
563e0f872d1761a701f0f977	X	what is the error message?
  Negative
563e0f872d1761a701f0f978	X	It throws an error on the require keyword.
  Negative
563e0f872d1761a701f0f979	X	I'm trying to call a method listObjects on my Amazon S3 Bucket using the Node.js SDK.
  Negative
However I am getting an error on the keyword require.
  Neutral
The API I am using is this and this.
  Negative
I also have installed AWS-SDK via NPM.
  Positive
The version of node I have is v0.10.18.
  Negative
Assistance is appreciated.
  Positive
563e0f872d1761a701f0f97a	X	You first have to install the aws sdk by calling "npm install aws-sdk" in a command-line, from your script directory.
  Negative
563e0f872d1761a701f0f97b	X	I have uploaded several files to Amazon S3 using boto.
  Negative
However, I failed to set a lifecycle using statement (I know this can be done using the AWS Management Console, but I need to allow each user to decide how long want to keep the file).
  Neutral
The boto API reference for S3 properly documents configure_lifecycle(lifecycle_config, headers=None) as the solution, but I'm unable to configure this.
  Negative
Can anyone correct my code?
  Neutral
Thanks!
  Positive
563e0f872d1761a701f0f97c	X	You aren't showing where "lifecycle_config" comes from in this example.
  Negative
However, what you should do is create a Lifecycle object, like this: See class boto.s3.lifecycle for details about the Lifecycle object and what the above parameters mean.
  Neutral
Once you have a Lifecycle object, you can then use that in the call to configure_lifecycle(), like this:
563e0f872d1761a701f0f97d	X	I am trying this code to upload file of size 107MB.
  Negative
but file does not get upload on s3, when I check log on s3, it shows me for some parts of this file, "IncompleteBody".
  Negative
How do I deal with that??
  Neutral
563e0f872d1761a701f0f97e	X	This is what it shows in log : REST.PUT.PART 6/4/5_34fa2f47f8f2e3d/645_d48b407b9d44efc.mp4 "PUT /my.bucket/6/4/5_34fa2f47f8f2e3d/645_d48b407b9d44efc.mp4?partNumber=5&uploadId=C‌​9utxlgdMqWCKrYuOEWyE2TOANRjx9r8YnDL3YV53kHVjEAGQ55U3IjqxodIAJRlbdQS8Fd5kWhHdlL_8k‌​g-- HTTP/1.1" 400 IncompleteBody 260 5242880 3356 - "-" "aws-sdk-php2/2.8.20 Guzzle/3.9.3 curl/7.30.0 PHP/5.3.28" I have also tried this solution " docs.aws.amazon.com/AmazonS3/latest/dev/usingHLmpuPHP.html"; but its also giving same problem
563e0f882d1761a701f0f97f	X	I tried to upload large file into Amazon s3 using PHP.
  Very negative
I have found nice solutions on various forums but these solutions are for SDK version 1 .
  Neutral
http://docs.aws.amazon.com/AmazonS3/latest/dev/LLuploadFilePHP.html Of course, I have found examples on Amazon API documentation.
  Negative
This example expects file on local disk and can not handle with input stream.
  Negative
I couldn't find similar examples for the SDK for PHPv2 as shown in first link.
  Negative
Did someone solved similar problem successfully?
  Negative
563e0f882d1761a701f0f980	X	I recently just prepared a code sample for this.
  Negative
In this example I am using a file, but you can use a stream as well.
  Positive
563e0f882d1761a701f0f981	X	Thank you for your answer and explanation!
  Positive
563e0f882d1761a701f0f982	X	You're welcome @osc!
  Positive
Welcome to SO and don't forget to upvote & accept answers if we've helped you out!
  Neutral
563e0f882d1761a701f0f983	X	I tried to upvote but it says that I dont have enough points!
  Negative
563e0f882d1761a701f0f984	X	My doubt is the title of my question: I'm studying AWS, and I'm not understanding if when we use the S3 Glacier Storage, are we then using the Amazon Glacier Service or is the Glacier Storage Service just a property of Amazon S3?
  Very negative
563e0f882d1761a701f0f985	X	Yes.
  Neutral
Data from an S3 bucket can copy data into Glacier archive storage.
  Negative
This is exactly the same as using a Glacier storage archive directly behind the scenes.
  Neutral
When using the lifecycle properties of an S3 bucket and creating an archive, you are using both services together.
  Neutral
They have different use-cases, speed & access capabilities, and pricing.
  Negative
They work together very seamlessly, but are in fact separate services.
  Neutral
You can take a look at the Amazon S3 FAQ's Amazon Glacier section to get some additional info.
  Neutral
With that being said: be aware that if you archive FROM S3 to Glacier then you'll be using only the S3 APIs to access the Glacier archive.
  Negative
They are still in fact different services, but when you perform an archive operation from an S3 bucket it creates a mapping for you.
  Positive
However, if you create an archive directly in Glacier bypassing S3, then you can use the separate Glacier API.
  Neutral
563e0f882d1761a701f0f986	X	My site is using photos stored on Amazon s3, but I have the following problem; Chrome sometimes loads the image, and sometimes doesn-t load the same image.
  Negative
When it doesn-t load I get this in the console... Firefox also sometimes loads the image and sometimes doesn't load the same image.
  Negative
I haven't checked other browsers.
  Negative
my s3 bucket metadata is set to my upload php code is as follows NB all photos have been reworked as jpg's (extension .
  Negative
jpg) I have looked at this SO question, but I have the API call set properly (right?)
  Negative
Amazon S3 is not serving files correctly NB2 because amazon saves the link to the file as HTTPS this I have gets blocked, so I call the link using HTTP only.
  Negative
563e0f882d1761a701f0f987	X	Thanks!
  Positive
It works.
  Positive
I did it in some other (wierd) way:
563e0f882d1761a701f0f988	X	<code> s3Client.putObject(targetBucketName, key ,s3Client.getObject(sourceBucketName, key).
  Negative
getObjectContent() ,s3Client.getObjectMetadata(sourceBucketName, key));</code>
563e0f882d1761a701f0f989	X	@aviad - I'm not sure, but that looks like you are downloading the file to your computer before uploading again to the new bucket, rather than just copying it within S3 itself.
  Very negative
563e0f882d1761a701f0f98a	X	Does anybody know is it possible to programatically transfer files stored on amazon s3 from one region to another?
  Negative
563e0f882d1761a701f0f98b	X	This is easily accomplished using the Amazon S3 API to copy the object from one bucket to another.
  Positive
It doesn't matter that the buckets are in different regions.
  Negative
Here's an example using the Rest API.
  Neutral
Or if you prefer, the SDKs can do the same thing.
  Neutral
Here's a .
  Neutral
Net SDK example.
  Neutral
If you mean that you want to change a buckets region, you would have to: Of course most of the major S3 GUI tools can also copy objects between buckets and regions too.
  Negative
563e0f892d1761a701f0f98c	X	What about setting version to earlier than latest?
  Neutral
Didn't try it, just a shot in the dark.
  Negative
563e0f892d1761a701f0f98d	X	@zaak Yes, I tried that as well.
  Negative
No luck.
  Neutral
563e0f892d1761a701f0f98e	X	Can you compare how the gsutil auth header (viewable with "gsutil -D ...") differs from the one sent by the PHP SDK?
  Negative
563e0f892d1761a701f0f98f	X	Same problem here, eventually introduced my own GoogleStorageServiceProvider which uses the league/flysystem-aws-s3-v2 library instead of v3
563e0f892d1761a701f0f990	X	@JorisBlaak Thanks.
  Negative
I'm going to avoid Google Storage for now until they either provide better documentation or update their support for the lastest stable Amazon SDK.
  Negative
563e0f8a2d1761a701f0f991	X	I feel pretty odd posting this here but since SO is the only official channel for Google API support, I guess I need to ask it here.
  Negative
If you have API, tool usage, or other software development-related questions, search for and post questions on Stack Overflow, using the official google-cloud-storage tag.
  Negative
Ok, so here goes.
  Negative
I've spent the better part of two days working on trying to get Google Storage to work on the v3 version (latest) of Amazon's PHP SDK.
  Negative
I can't use an older version of the SDK because I'm trying to stick to Laravel 5.1's filesystem without having to write a brand new driver for Google Storage.
  Negative
I believe this is within the spirit of what Google advertises for Google Storage: https://cloud.google.com/storage/docs/migrating In a simple migration from Amazon S3 to Google Cloud Storage, you can use your existing tools and libraries for generating authenticated REST requests to Amazon S3, to also send authenticated requests to Google Cloud Storage.
  Negative
The changes you need to make to your existing tools and libraries are described in this section.
  Positive
To get set up for a simple migration do the following: Set a default Google project.
  Negative
Get a developer key.
  Neutral
In your existing tools or libraries, make the following changes: Change the request endpoint to use the Google Cloud Storage request endpoint.
  Negative
Replace the Amazon Web Services (AWS) access and secret key with the corresponding Google Cloud Storage access key and secret key (collectively called your Google developer key).
  Negative
That's it!
  Neutral
At this point you can start using your existing tools and libraries to send keyed-hash message authentication code (HMAC) requests to Google Cloud Storage.
  Negative
What a pitch!
  Neutral
Let's give it a try using Interoperability credentials that work using gsutil.
  Neutral
Doesn't work.
  Negative
You get an "Incorrect Authentication Header".
  Negative
Let's take a look at that header.
  Neutral
AWS4-HMAC-SHA256 Credential=GOOGGUxxxxxxxxxxx/20150611/US/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=9c7de4xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx I created a SO post about this at this point, and someone suggested that I add 'signature' => 'v2'.
  Negative
Google Storage Incorrect Authorization Header with Amazon S3 PHP SDK v3 Let's try that: No luck.
  Negative
Same error.
  Negative
The authorization header hasn't changed.
  Negative
Let's look at S3Client's code and see how 'signature' gets used: It doesn't.
  Negative
So now we're deviating from S3's official documentation because they say the same thing: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/configuration.html It's not 'signature', it's 'signature_version'.
  Negative
Let's change that to v2.
  Neutral
At least we get a different error this time!
  Negative
UnresolvedSignatureException in SignatureProvider.php line 61: Unable to resolve a signature for v2/s3/US.
  Negative
Valid signature versions include v4 and anonymous.
  Negative
So, after toying with this for two days it looks like it isn't possible, at least not with the ease that Google wants you to believe in their pitch.
  Negative
I can't get this to work at all, so I'm hoping someone here can shed some light on this.
  Negative
Either I missed something important, or Google is falsely advertising that Google Storage works using Amazon's S3 SDK and wasting developers' time.
  Negative
I'm thinking that maybe we have to manually hijack the authorization header, but that's outside of my expertise.
  Negative
Any help would be greatly appreciated.
  Positive
563e0f8a2d1761a701f0f992	X	So I'm going to post later than never and say that I'm using laravel 5.1, you have to make a choice on whether you want to go with AWS or GCS at the time of configuration as you can't install two versions of the aws php client (which limits your choice of flysystem to v2 or v3.
  Negative
GCS requires v2 for the time being while the s3 implementation in laravel requires v3.
  Negative
I spent hours trying to get v3 to work with GCS but the authentication headers are different so I didn't bother.
  Negative
You do have to provide your own provider, but it's not that difficult to setup.
  Negative
Just create GcsAppsServiceProvider.php in app/Providers Essentially in your config you're just duplicating the s3 config in filesystem.php and changing region -> base_url.
  Negative
You're also free to change the default and cloud to gcs to support google cloud storage.
  Negative
Last but not least you also need to add the provider to the provider list in app.php The AWS S3 adapter also mentions that GCS supports it in their documentation.
  Negative
563e0f8a2d1761a701f0f993	X	I could use the system.getInfo("deviceID") for local as well, which I think is the better approach.
  Negative
563e0f8a2d1761a701f0f994	X	I am currently developing a mobile app using the Corona SDK and Lua.
  Negative
I need to store information about each user and load the information for the current user when they load the app.
  Negative
What is the best method to store this information for each user, and how would I get this data on app load.
  Positive
I was thinking about using sqlLite and having a single row for each user.
  Neutral
However when the user re loads the app I would have no way of accessing the data for the current user because when the app loads I would need something to index the database.
  Negative
Is there any way I can get some information from the mobile device on app load to index the database?
  Negative
Any ideas or suggestions?
  Neutral
563e0f8a2d1761a701f0f995	X	I'm not clear on where the database is stored.
  Negative
If it's remote on a server just use the device ID system.getInfo( "deviceID") If it's local and you have multiple local users then use a login.
  Negative
You could use a registration process for either instance and store The registration keys for automatic access when launching the app.
  Negative
563e0f8a2d1761a701f0f996	X	take a look at amazon's services.
  Neutral
like S3,simpleDB, dynamoDB.
  Negative
.
  Neutral
Some implementations are already available in Code Exchange http://developer.anscamobile.com/code/amazon-simpledb-api http://developer.anscamobile.com/code/amazon-s3-rest-api-implementation-corona
563e0f8a2d1761a701f0f997	X	Is there a way to create a CloudFront signed url that limits the number of times that a file can be downloaded?
  Negative
According to this post Controlling number of downloads on Amazon S3, you can get the number of file downloads via the cloudfront api (but it cant find any reference to this on the amazon site) Has anyone managed to achieve this via CloudFront?
  Negative
563e0f8a2d1761a701f0f998	X	Yes, with CloudFront you can serve Private Content.
  Negative
Basically you can protect your content in two ways: Require that your users use special CloudFront signed URLs to access your content, not the standard CloudFront public URLs.
  Negative
Require that your users access your Amazon S3 content using CloudFront URLs, not Amazon S3 URLs.
  Negative
When you create signed URLs for your objects, you can specify:
563e0f8a2d1761a701f0f999	X	i've used s3fuse in the past to 'mount' s3 as a filesystem.
  Negative
I thought that when you did a mkdir /var/mys3mount/newfolder/ it only created the folder in the local file system, and not in S3.
  Negative
It was giving the appearance of a folder existing but s3 wouldn't see it until you put an object there.
  Negative
Then giving you the full path in the folder concept.
  Neutral
563e0f8a2d1761a701f0f99a	X	Hi Greg, I've seen the same behavior with riofs when using mkdir locally, but I've also seen that when a prefix object exists on the S3 bucket, riofs will create a local directory to mirror this.
  Negative
This can be useful when automating things around these "mounts".
  Neutral
563e0f8b2d1761a701f0f99b	X	It is usually explained that there are no folders/directories on S3, which is true, however there are PREFIX objects.
  Negative
In some cases - e.g. using riofs to "mount" a filesystem to S3 - it could actually be useful to know how to create one of these.
  Negative
Does anyone know if there is a "correct" way to do this with the AWS CLI?
  Neutral
It's likely possible using the low-level API aws s3api ... Related SO posts: amazon S3 boto - how to create folder?
  Negative
Creating a folder via s3cmd (Amazon S3) P.S. I also want to point out that in the AWS console this action is actually named "Create Folder...", so it's not really fair to tell people that there is no "concept" of a folder on S3.
  Very negative
Many thanks!
  Positive
563e0f8b2d1761a701f0f99c	X	There really, really aren't directories in S3.
  Negative
However, as you point out, there are prefixes and delimiters and the default delimiter is / which allows you to get a pretty convincing simulation of a hierarchical directory structure in an S3 bucket.
  Neutral
But the bucket is still just a flat space containing objects with key names.
  Negative
If you want to create a directory you have to create an object with a key whose name includes or ends with a delimiter character (/ by default).
  Negative
So your technique described above may not feel right but it is the only way.
  Negative
And whoever came up with the Create Folder idea in the console should be ashamed of themselves.
  Negative
It causes a lot of confusion.
  Negative
563e0f8b2d1761a701f0f99d	X	After some quick fiddling around I found this seems to work: aws s3api put-object --bucket test --key dir-test/ But it only works if you include the "/" at the end of the argument to --key.
  Negative
That part just didn't feel right... surely there's a better way?
  Negative
563e0f8b2d1761a701f0f99e	X	Thanks David.
  Neutral
I've spent some time with the getting started guide and the docs and samples but so far it's been a slow process to get to a point of seeing suitable sample code.
  Negative
Amazon seem to have written the docs for people who are going to learn everything and spend a lot of time with it - I don't have that luxury, I just want to shove a file in, and do other stuff.
  Very negative
Looks like I need to put a lot more time in which is a shame.
  Negative
563e0f8b2d1761a701f0f99f	X	I know what you mean.
  Negative
I'm going on three days now trying to find an easy to understand example for uploading images securely to s3.
  Negative
563e0f8b2d1761a701f0f9a0	X	To be honest I think they should improve the documentation!
  Positive
563e0f8b2d1761a701f0f9a1	X	I need to upload a bitmap to Amazon S3.
  Negative
I have never used S3, and the docs are proving less than helpful as I can't see anything to cover this specific requirement.
  Negative
Unfortunately I'm struggling to find time on this project to spend a whole day learning how it all hangs together so hoping one of you kind people can give me some pointers.
  Negative
Can you point to me to a source of reference that explains how to push a file to S3, and get a URL reference in return?
  Negative
More specifically: - Where do the credentials go when using the S3 Android SDK?
  Neutral
- Do I need to create a bucket before uploading a file, or can they exist outside buckets?
  Neutral
- Which SDK method do I use to push a bitmap up to S3?
  Neutral
- Am I right in thinking I need the CORE and S3 libs to do what I need, and no others?
  Neutral
563e0f8b2d1761a701f0f9a2	X	Take a look at the Amazon S3 API documentation to get a feel for what can and can't be done with Amazon S3.
  Negative
Note that there are two APIs, a simpler REST API and a more-involved SOAP API.
  Negative
You can write your own code to make HTTP requests to interact with the REST API, or use a SOAP library to consume the SOAP API.
  Negative
All of the Amazon services have these standard API endpoints (REST, SOAP) and in theory you can write a client in any programming language!
  Negative
Fortunately for Android developers, Amazon have released a (Beta) SDK that does all of this work for you.
  Negative
There's a Getting Started guide and Javadocs too.
  Negative
With this SDK you should be able to integrate S3 with your application in a matter of hours.
  Negative
The Getting Started guide comes with a full sample and shows how to supply the required credentials.
  Positive
Conceptually, Amazon S3 stores data in Buckets where a bucket contains Objects.
  Negative
Generally you'll use one bucket per application, and add as many objects as you like.
  Negative
S3 doesn't support or have any concept of folders, but you can put slashes (/) in your object names.
  Negative
563e0f8b2d1761a701f0f9a3	X	
563e0f8b2d1761a701f0f9a4	X	Thanks, I did it.
  Negative
563e0f8b2d1761a701f0f9a5	X	I have a PHP REST API that hosts all images in the Amazon S3.
  Negative
I'm looking for a plugin, or trick, to resize the images using GET params.
  Negative
For example: I found this plugin, but a member of my team said it is ASP.NET based and doesn't fit to my PHP API project.
  Negative
Should I use a script hosted in EC2 to resize those images?
  Negative
Is there other way?
  Neutral
Ideas are welcome.
  Positive
Thanks!
  Positive
563e0f8b2d1761a701f0f9a6	X	I suggest setting up your own PHP service to resize images based on the query string values, as you describe.
  Negative
Yes, the PHP service could be hosted on AWS EC2 or another hosting platform.
  Negative
The service would need to receive the query string such as: http://bodruk.com/images/image.jpg?width=300&height=300 This would need to be configured (perhaps using mod_rewrite [1]) to receive the name of the image (example: 'image.jpg') and pass the query string size values into your PHP script.
  Negative
The script would then find your image on S3, resize it using an image library (such as ImageMagick / PHP GD or PHPThumb [2]) save it (or not) back to S3 and also pass the image data back through on the original request.
  Negative
I wish you good fortune!
  Positive
[1] https://httpd.apache.org/docs/current/mod/mod_rewrite.html [2] http://phpthumb.sourceforge.net/
563e0f8c2d1761a701f0f9a7	X	Could be a memory issue... how much are you giving php?
  Negative
563e0f8c2d1761a701f0f9a8	X	The weird thing is that it doesn't seem to be producing an error in the log....The only error it generates is that its missing the favicon.
  Negative
.
  Neutral
563e0f8c2d1761a701f0f9a9	X	I'm configuring a new Drupal installation, and I installed the MediaMover module so that I could take media and put it on Amazon S3.
  Very negative
However, when I try to enable the S3 module within Media Mover and hit Save Settings, it results in a Server 500 Error every time.
  Neutral
Is there something I might be missing that would cause this?
  Negative
It says its only dependency is the MediaMover api, which is installed and eneabled.
  Negative
Or maybe some configuration that is needed that I have missed...
563e0f8c2d1761a701f0f9aa	X	The details of a 500 error can be found in your server's logs: it can be any number of issues, ranging from a server misconfiguration to permissions to bugs with the module itself.
  Neutral
Once you identify what is actually happening, and if you deterimine it's not your server's configuration, you're going to want to file an issue on Media Mover's issue queue.
  Negative
563e0f8c2d1761a701f0f9ab	X	your answer is Exceptions :) Look at PHP docs about them ;)
563e0f8c2d1761a701f0f9ac	X	just so you do not have to google it: php.net/manual/en/language.exceptions.php
563e0f8c2d1761a701f0f9ad	X	@Floris consider accepting my answer to prevent this question from getting more unneeded attention, or if my answer didn't solve your problem, use the comments section to ask for further detail.
  Negative
563e0f8c2d1761a701f0f9ae	X	I have a long running script which gets a (long) array of folders (with subarray of files in that folder) where I have to do several actions on each file.
  Negative
What is the best way to make sure I make all actions successful?
  Positive
And how to handle unsuccessful actions?
  Negative
Lets say what will happen if my mysql server is unavailable or like the Amazon S3 API is not working correctly.
  Negative
pseudocode of my script:
563e0f8c2d1761a701f0f9af	X	As mentioned, what you could do is throw and catch Exceptions.
  Negative
So for instance, if you iterate over files in a folder using a foreach, doing something with those files, on an error, you can throw an Exception and it will stop code execution till it is catched.
  Negative
So maybe you want to use a logger instead.
  Negative
Since it is 2014, you probably want to use a DIC to inject a logger service or otherwise, you can just use a singleton (only considering the great flaws that brings) that stores your errors.
  Negative
So either way you have this service that stores every error.
  Negative
At the end you just check if it has any errors and then act accordingly.
  Negative
563e0f8c2d1761a701f0f9b0	X	I have implemented Amazon API to upload and download data from amazon server.
  Negative
It does not use any specific URL like traditional web service call but instead it used bucket and secret keys.
  Negative
With this keys I assume that it protects unauthorised access to amazon web service.
  Negative
But my question is when we try to upload data to s3 server or download data from it by using amazon API... will the data transfer to/from server securely?
  Negative
The code for downloading the data is like this: I tried to look into amazon documentation and forums but there is no clear thing mentioned.
  Negative
I have found that we can encrypt the data with amazon API and can store it to server but I haven't found how exactly amazon's API transfer the data.
  Negative
563e0f8d2d1761a701f0f9b1	X	Please post detailed questions here
563e0f8d2d1761a701f0f9b2	X	I am building gallery app that get image url from api via php (laravel5) server.
  Negative
But the image url is a link from S3 Amazon.
  Positive
Should I set cache header on the server or S3?
  Negative
563e0f8d2d1761a701f0f9b3	X	thanks for this - ok so I see i am confused.
  Negative
.
  Neutral
you are right that the data is in ebs.
  Positive
I guess i want to get data from ebs out of Amazon and down to my local machine.
  Negative
To do this I have to create an s3 bucket, mount it to the ec2 instance then copy the data from ebs into the bucket and from there down to my local machine?
  Negative
Holy moly that's complicated is it not?
  Negative
563e0f8d2d1761a701f0f9b4	X	@utunga - alternatively you might want to reuse your SSH credentials to access the instance via SSH File Transfer Protocol (SFTP); most FTP programs support this these days, good ones are e.g. WinSCP, Cyberduck or Filezilla.
  Negative
563e0f8d2d1761a701f0f9b5	X	again, thanks for the comments Stefan.
  Positive
I have found out earlier on that the same credentials file approach that works just fine with ssh is not working with scp.
  Negative
when i try to transfer even the smallest file I get a 'Permission Denied' error.
  Negative
From what i've read elsewhere amazon doesn't allow scp - hence the existence of things like s3cmd ?
  Negative
But perhaps I'm still confused.
  Negative
have you or has anyone successfully used scp or sftp with amazon s3 accounts connecting offsite ?
  Negative
563e0f8d2d1761a701f0f9b6	X	@utunga - SCP/SFTP support has nothing to do with AWS, the earlier SCP and its successor SFTP differ significantly though; each can be disabled/enabled separately within the typical SSH daemon, but I haven't encountered a situation where SFTP didn't just work out of the box for quite a while (mostly Ubuntu 12.04, see also my related answer to Uploading files on Amazon EC2) - given you inherited the instance, it might just be disabled there?
  Negative
563e0f8d2d1761a701f0f9b7	X	@utunga - You are indeed still messing services up a bit btw., I suggest to make yourself familiar with these: Amazon S3 is storage for the Internet.
  Negative
You need a dedicated S3 client to interact with this, this has nothing to do with (S)FTP - nowadays many former (S)FTP only programs have support for the S3 API build in as well though.
  Negative
Amazon EC2 is a web service that provides resizable compute capacity in the cloud, i.e. it provides virtual machines with your choice of OS (Windows/Unix/Linux/...).
  Negative
563e0f8d2d1761a701f0f9b8	X	I've inherited an already configured ec2 instance and am trying to download data from it.
  Negative
I have set up S3Browser with relevant credentials but just need the name of the external bucket to connect to.
  Neutral
I can ssh to the machine and see that the bucket with the data is already mounted - thusly *some numbers changed to protect the innocent.
  Negative
.
  Neutral
But what I need is the name of the bucket for - say the /ebs mount point - to enter into s3browser.
  Negative
I realise this is kind of going backwards... but there must be a way.
  Positive
If not where can I find information on available s3 buckets?
  Neutral
563e0f8d2d1761a701f0f9b9	X	You might eventually be confusing a few AWS concepts, at least the information you provided seems to be inconsistent with your question at first sight.
  Negative
While it is indeed possible to mount an Amazon S3 bucket on an Amazon EC2 instance (see e.g s3fs, which is a FUSE-based file system backed by Amazon S3), the name of the mount point in question suggests that this is an Amazon Elastic Block Store (EBS) volume instead.
  Negative
If that would be the case, you can only access the data via the EC2 instance where the volume is attached to and not via external tools.
  Negative
No, snapshots are only available through the Amazon EC2 APIs.
  Neutral
This is most easily done via the AWS Management Console, which allows you to Access and manage Amazon Web Services through a simple and intuitive web-based user interface.
  Negative
563e0f8e2d1761a701f0f9ba	X	We all know you can do direct uploads to amazon S3 using a form.
  Negative
See: http://aws.amazon.com/articles/1434/ I would love to replicate this functionality using an API, but without storing the uploaded file on our webserver.
  Positive
I know if you would stream the file through - for example - PHP, your file is stored in the TMP directory before it is uploaded to S3.
  Negative
But I want to avoid that.
  Negative
Isn't there a way that could work like this: I know this might sound far-fetched, and possibly not possible at all.
  Negative
But I wanted to see if someone thinks there might be a remote possibility that it could work in a way we haven't though about yet.
  Neutral
563e0f8e2d1761a701f0f9bb	X	Have a look at S3-Curl.
  Neutral
It's a python wrapper that handles AWS keys and headers to properly generate the write CURL commands for the REST API for various amazon services (S3 included).
  Negative
You could look inside the source of the .
  Neutral
pl file to get an idea of how to create the curl requests yourself (only if you don't want to use s3-curl and have a restriction that you can only use curl directly).
  Negative
You could use this in combination with Amazon's STS to generate a temporary token granting access for that particular upload.
  Negative
In this case, your modified flow would be:
563e0f8e2d1761a701f0f9bc	X	In my case I cannot use Amazon's SDK as my host is refusing to install it for me.
  Negative
563e0f8e2d1761a701f0f9bd	X	You don't need to have them install it.
  Negative
Just put it on your PHP include path (or, if you're really desperate, just in the domain directory).
  Negative
563e0f8e2d1761a701f0f9be	X	Hi, do you have a link to a tutoria to learn how to go about it please?
  Negative
Thanks
563e0f8e2d1761a701f0f9bf	X	I am trying to copy a 1TB file from one bucket to another.
  Negative
I know that this can be done easily if I log into the AWS S3 panel but I would like to do it using PHP.
  Negative
I am using the following AWS S3 class from github I am using it in my PHP code as follows: I'm getting no error_log.
  Very negative
What am I doing wrong that I am missing, please?
  Negative
563e0f8e2d1761a701f0f9c0	X	At 1 TB, the object is too large to copy in a single operation.
  Negative
To quote from the S3 REST API documentation: You can store individual objects of up to 5 TB in Amazon S3.
  Negative
You create a copy of your object up to 5 GB in size in a single atomic operation using this API.
  Negative
However, for copying an object greater than 5 GB, you must use the multipart upload API.
  Neutral
Unfortunately, it doesn't appear that the S3 class you're using supports multipart uploads, so you'll need to use something else.
  Negative
I'd strongly recommend that you use Amazon's AWS SDK for PHP — it's a bit bigger and more complex than the one you're using right now, but it supports the entirety of the S3 API (as well as other AWS services!)
  Negative
, so it'll be able to handle this operation.
  Neutral
563e0f8e2d1761a701f0f9c1	X	I need to create a HMAC-SHA1 signature to send my policy document to amazon web services s3 API.
  Negative
Is there a way of producing this using codenameone's API or do I need to use native java to accomplish this?
  Negative
563e0f8e2d1761a701f0f9c2	X	Did you try the bouncy castle cn1lib?
  Negative
It should offer support for HMAC-SHA1.
  Neutral
563e0f8f2d1761a701f0f9c3	X	We are doing a COPY Object request with Directive Replace.
  Negative
It works for some files but we found that some files are still not encrypted.
  Neutral
I think I will need to try your first solution to run a script every day.
  Negative
563e0f8f2d1761a701f0f9c4	X	Thanks Julio, this makes sense, seems like the only way is to change the headers in the request.
  Negative
I was hoping there would be some way to do this through a system wide custom boto configuration or something, having a proxy server seems overengineered.
  Negative
Anyway I would think this is a pretty common use case though, people want a totally secure bucket.
  Negative
Hopefully this will be an option in future releases.
  Neutral
563e0f8f2d1761a701f0f9c5	X	I want to set an S3 bucket policy so that all requests to upload to that bucket will use server side encryption, even if it is not specified in the request header.
  Negative
I have seen this post (Amazon S3 Server Side Encryption Bucket Policy problems) where someone has managed to set a bucket policy that denies all put requests that don't specify server side encryption, but I don't want to deny, I want the puts to succeed but use server side encryption.
  Very negative
My issue is with streaming the output from EMR to my S3 bucket, I don't control the code that is making the requests, and it seems to me that server side encryption must be specified on a per request basis.
  Very negative
563e0f8f2d1761a701f0f9c6	X	IMHO There is no way to automatically tell Amazon S3 to turn on SSE for every PUT requests.
  Negative
So, what I would investigate is the following : write a script that list your bucket for each object, get the meta data if SSE is not enabled, use the PUT COPY API (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) to add SSE "(...) When copying an object, you can preserve most of the metadata (default) or specify new metadata (...)" If the PUT operation succeeded, use the DELETE object API to delete the original object Then run that script on an hourly or daily basis, depending on your business requirements.
  Negative
You can use S3 API in Python (http://boto.readthedocs.org/en/latest/ref/s3.html) to make it easier to write the script.
  Negative
If this "change-after-write" solution is not valid for you business wise, you can work at different level (aligned with Julio's answer above) use a proxy between your API client and S3 API (like a reverse proxy on your site), and configure it to add the SSE HTTP header for every PUT / POST requests.
  Negative
Developer must go through the proxy and not be authorised to issue requests against S3 API endpoints write a wrapper library to add the SSE meta data automatically and oblige developer to use your library on top of the SDK.
  Negative
The later today are a matter of discipline in the organisation, as it is not easy to enforce them at a technical level.
  Negative
Seb
563e0f8f2d1761a701f0f9c7	X	S3 will not perform this automatically, you would have to create a workaround.
  Negative
I would suggest passing requests thru a proxy that would "enrich" them adding the proper header.
  Negative
To do that i'd try (in order): 1- Apache Camel Content Enrich 2- NGXINX / HTTPD mod_proxy 3- Custom code I bet there is also a very smart ruby http lib for that too :)
563e0f8f2d1761a701f0f9c8	X	I'm working on an automated mechanism for our EBS volumes to be backed up on a daily basis.
  Negative
Regarding that can you please tell me how to take backup of snapshots, how to move it to s3 and then how to take incremental back up?
  Positive
563e0f8f2d1761a701f0f9c9	X	Your apparently haven't yet realized the full potential of Amazon EBS, insofar your requirements are mostly build in already, see sections Features of Amazon EBS volumes as well as Amazon EBS Snapshots: Amazon EBS also provides the ability to create point-in-time snapshots of volumes, which are persisted to Amazon S3.
  Negative
These snapshots can be used as the starting point for new Amazon EBS volumes, and protect data for long-term durability.
  Negative
[...] [emphasis mine] and Amazon EBS provides the ability to back up point-in-time snapshots of your data to Amazon S3 for durable recovery.
  Negative
Amazon EBS snapshots are incremental backups, meaning that only the blocks on the device that have changed since your last snapshot will be saved.
  Negative
[...] [emphasis mine] So you neither need to move EBS snapshots to S3 nor handle their incremental nature yourself and the only thing missing is the scheduled usage of the respective APIs, which can be achieved in one of the following two ways: Good luck!
  Negative
In fact it isn't even possible to access EBS snapshots in S3 outside of the aforementioned API, see the FAQ Will I be able to access my snapshots using the regular Amazon S3 APIs?
  Negative
: No, snapshots are only available through the Amazon EC2 APIs.
  Negative
You might want to review the other EBS related FAQs in section Amazon Elastic Block Storage (EBS) within the Amazon EC2 FAQs as well.
  Negative
563e0f8f2d1761a701f0f9ca	X	Check out http://skeddly.com - they have a feature for doing automated backups.
  Negative
563e0f8f2d1761a701f0f9cb	X	I am one of the developer of Bucket Explorer, it is amazon s3 tool.
  Negative
Bucket Explorer upload only updated or new files on Amazon s3 bucket.
  Negative
It does not upload identical files on s3.
  Negative
So when whenever you upload/download it will upload/download only new or updated data.
  Negative
Bucket Explorer
563e0f8f2d1761a701f0f9cc	X	Thanks, I did not realize that you could append to the blob after the initial write.
  Negative
I think this largely solves my problem, although since yesterday I've also been looking at the Page mode.
  Negative
Specifically just appending pages to the BLOB.
  Negative
Do you see any issues with that approach?
  Neutral
563e0f8f2d1761a701f0f9cd	X	I believe page blobs need to have predetermined (fixed) length, so you'll need to create a very large page blob and then gradually fill it.
  Negative
Block blobs can instead just get longer and longer (but you have to commit the entire list of blocks each time, so that list may get really long).
  Negative
563e0f902d1761a701f0f9ce	X	You only pay for the pages you have written though so there is no issue in just making a page blob that is a couple of TB and then just appending to it.
  Negative
You might need to keep the length somewhere though, perhaps at the beginning of the file.
  Neutral
563e0f902d1761a701f0f9cf	X	What are the cloud data storage APIs that accomodate streaming data well?
  Negative
Specifically, a constant data stream that: 1) has no known end and is continually appended to and 2) can be read from at any time.
  Negative
Due to the nature of distributed access, the big cloud storage options like Amazon S3, Google Storage for Developers, and Windows Azure Blobs do not seem to support streaming data.
  Negative
Current beliefs: 1) Amazon S3 does not allow append operations to objects (only replace).
  Negative
The multipart upload API allows a "streaming" upload, but it requires to be "finalized" once completely written.
  Neutral
2) Google Storage objects are immutable, so same thing.
  Negative
3) Windows Azure blog storage has has block storage, but like Amazon S3 multipart upload, requires the blocks to be "finalized" so an open-ended stream is not possible.
  Very negative
Any ideas?
  Neutral
563e0f902d1761a701f0f9d0	X	With Windows Azure blob storage, you can keep appending to the same blob (and committing the block list after each write) as long as you want, and you can request any byte range when reading.
  Negative
However, you still wouldn't get the behavior of a single HTTP request with data continually streaming down.
  Neutral
(You'd have to request a range and then make another request for the next range, etc.
  Negative
In other words, at any given time, the blob has finite length.)
  Negative
Building your own code to front-end the data (socket-based or maybe a chunked HTTP response) may be your only option, if I'm understanding the requirements correctly.
  Negative
563e0f902d1761a701f0f9d1	X	What you want is a Windows Azure Page Blob, rather than a Block Blob.
  Negative
For info about page blobs see: http://msdn.microsoft.com/en-us/library/windowsazure/ee691964.aspx.
  Positive
With a Page Blob you will be able to append to an existing blob, the main consideration is that you have to write whole 512 byte pages, so if you appending to an existing file you may have to also send up to 511 bytes of existing data from the end of your file.
  Negative
563e0f902d1761a701f0f9d2	X	Why do you think that Amazon SDK does not work on GAE?
  Negative
563e0f902d1761a701f0f9d3	X	Thank you for your answer.
  Positive
Fortunately there is an experimental feature that allows creating files programatically in the Blobstore (code.google.com/appengine/docs/java/blobstore/…) - so that part will work.
  Neutral
But what is the file download limit in your scenarion?
  Neutral
1.
  Neutral
create file programatically in Blobstore, 2.
  Positive
create entry in BigTable for key, 3.
  Positive
user calls servlet which will serve file from Blobstore.
  Neutral
Is the file size dowload limit 32MB in this case?
  Neutral
563e0f902d1761a701f0f9d4	X	You can get maximum of 32MB with one api call to the blobstore service, if that's not enough for you consider to use google storage option.
  Negative
563e0f902d1761a701f0f9d5	X	There's no limit on the size of programmatically created blobs, and users can download them all at once when you serve them using the blob serving API.
  Negative
563e0f902d1761a701f0f9d6	X	I am creating XML files in my GAE web application and I would like to host them somewhere.
  Negative
The link has to be consistent HOST_URL + filename.
  Positive
So Amazon S3 looks like it would work - I can upload a file and the URL is pointing directly to the file.
  Negative
Now my question is - how can I upload files from GAE to S3?
  Neutral
The Amazon SDK does not work on GAE.
  Negative
What is the upload limit from GAE?
  Neutral
Is it 1MB or 32MB?
  Neutral
Can you provide maybe a sample HTTP request for uploading data directly to S3?
  Negative
Would be using the Blobstore API easier?
  Neutral
What is the file-size limit for uploading a file that is created in GAE and need to uploaded to Blobstore API directly?
  Negative
Thanks.
  Neutral
563e0f902d1761a701f0f9d7	X	As you have noticed already S3 api's don't work well with GAE.
  Negative
For storing files on GAE you can use either BigTable, Blobstore or google storage so pick the option which best suits your needs.
  Negative
There is a nice article describing them all with code samples here You can save a filename and the blob location in a bigtable.
  Negative
Once you have your data stored on GAE you can create a special servlet which would take the file name, find the correspoding data informatio in bigtable, retrive it from blobstore for example and return it to the user (just an idea).
  Negative
563e0f912d1761a701f0f9d8	X	Try this.
  Neutral
S3::deleteObject('wecombinate','products/images/image1.
  Negative
png');
563e0f912d1761a701f0f9d9	X	@IqbalMalik yes, that's exactly how I am doing it...
563e0f912d1761a701f0f9da	X	This link might have info you need.
  Negative
docs.aws.amazon.com/AmazonS3/latest/dev/…
563e0f912d1761a701f0f9db	X	@IqbalMalik Thanks for the link.
  Positive
I have read the Amazon docs and have found nothing related to my problem unfortunately.
  Negative
563e0f912d1761a701f0f9dc	X	Try it with a leading slash: $s3->deleteObject('wecombinate', '/products/images/image1.
  Negative
png') and see if that changes anything.
  Positive
563e0f912d1761a701f0f9dd	X	I'm having trouble deleting an object in one of my buckets which uses slashes in the object name to help with organization.
  Negative
For example, my bucket name is wecombinate and my object name is products/images/image1.
  Negative
png When I try to delete, I get "[BucketNotEmpty] The bucket you tried to delete is not empty" as if I'm trying to delete the whole bucket, which I'm not, I am using the DELETE object REST API request to delete the single item products/images/image1.
  Negative
png.
  Neutral
I'm using the popular https://github.com/tpyo/amazon-s3-php-class PHP class to manage S3 and the code seems fine, plus no issues reported on GitHub.
  Positive
The code to do the delete: Is there a known problem with using slashes in the object name?
  Negative
Any other things I might be missing?
  Negative
563e0f912d1761a701f0f9de	X	You need to add a leading slash to the key to get it working:
563e0f912d1761a701f0f9df	X	How can I set the S3 bucket file header to be the following: content-encoding: gzip content-type: text/css I am using Amazon API using the SDK for .
  Negative
NET.
  Neutral
I am uploading a file to S3 using the PutObjectRequest.
  Negative
The problem that when the file is uploaded, the content type and content encoding headers aren't beign updated (I've checked via the files properties on Amazon Console).
  Negative
example: Also tried: What I'm doing wrong?
  Negative
563e0f912d1761a701f0f9e0	X	you add the content-type to the contentType property in the api like the following //for the content-encoding //add the following header hope this could help
563e0f922d1761a701f0f9e1	X	A brief look at the class makes me think that the library is expecting the first param to be a file path, not file data.
  Negative
If it's not a file or not accessible, it runs trigger_error('S3::inputFile(): Unable to open input file: '.
  Negative
$file, E_USER_WARNING); and returns false.
  Negative
563e0f922d1761a701f0f9e2	X	@JonStirling - Thank you, you're right.
  Positive
I've been googling around and can't even make out IF there is a way to upload file data to S3.
  Negative
.
  Neutral
563e0f922d1761a701f0f9e3	X	@Mortimer check my answer then.
  Negative
563e0f922d1761a701f0f9e4	X	i highly recommend you to not use that class because it too old 2011 and not updated than since
563e0f922d1761a701f0f9e5	X	I am using PHP CURL to generate a customized PNG image from a REST API.
  Negative
Once this image has loaded I would like to upload it into an AWS S3 Bucket and show the link to it.
  Negative
Here's my script so far: It keeps failing.
  Negative
Now, I think the problem is with where I use putObjectFile - the $data variable represents the image, but maybe it has to be passed in another way?
  Negative
I am using a common PHP Class for S3: http://undesigned.org.za/2007/10/22/amazon-s3-php-class
563e0f922d1761a701f0f9e6	X	Use PHP memory wrapper to store the contents of the image, and use $s3->putObject() method: Proven method (you may need to alter the code a bit) with PHP 5.5 and latest AWS libraries.
  Negative
http://php.net/manual/en/wrappers.php.php
563e0f922d1761a701f0f9e7	X	can you help me with stackoverflow.com/questions/22505525/… ?
  Negative
563e0f922d1761a701f0f9e8	X	Is anybody using this API?
  Neutral
I am trying to connect to Amazon S3 and EC2, following this paper here, but I get stuck on that line: Don't know what to put inside forName function or how to implement or get CloudProvider instance.
  Negative
Thanks.
  Neutral
563e0f922d1761a701f0f9e9	X	It should be like this:
563e0f932d1761a701f0f9ea	X	hi, thanks for the answer.
  Negative
yes, i included the signature on my authorization header.
  Neutral
I generated my signature using this: var policyBase64 = Base64.encode(JSON.stringify(POLICY_JSON)); var signature = b64_hmac_sha1(secret, policyBase64);
563e0f932d1761a701f0f9eb	X	Is it working now?
  Negative
563e0f932d1761a701f0f9ec	X	nope, it still saying SignatureDoesNotMatch.
  Positive
Could you take a look at my policy json?
  Neutral
Is there anything missing on my policy.
  Negative
I'm using temporary credentials so I added x-amz-security-token on my header.
  Negative
563e0f932d1761a701f0f9ed	X	Why are you encoding and signing policy?
  Negative
Did you try these steps http://docs.aws.amazon.com/general/latest/gr/sigv4_signing.html
563e0f932d1761a701f0f9ee	X	im sorry, I'm reading it now.
  Negative
I'm having difficulty understanding a canonical request.
  Negative
Right now I'm using Chrome advance rest client to perform http request to amazon.
  Negative
How do I perform canonical req there?
  Neutral
563e0f932d1761a701f0f9ef	X	I'm trying to implement an unploading of image to amazon s3 using only rest api.
  Negative
I've seen their docs but the problem is I'm only using temporary credential which will expire for about an hour.
  Negative
Below is the response My policy Here what I tried so far: Using chrome advance rest client I entered this url: https://mybucket.s3.amazonaws.com/avatars/test@domain.com with headers of The result was: 403 Forbidden and its saying SignatureDoesNotMatch.
  Negative
Does anyone able to accomplish uploading of object using only Rest Api of s3 (not using of SDK's).
  Neutral
The client asked me if its possible to build it using only javascript.
  Negative
Is this possible?
  Neutral
563e0f932d1761a701f0f9f0	X	Should you not sign the content?
  Negative
Check this how to sign.
  Neutral
After signing you have to pass the signature value in the Authorization header.
  Negative
Authorization: AWS AWSAccessKeyId:Signature
563e0f932d1761a701f0f9f1	X	I have download files from Google drive and save into my local system by using google drive api with java.My aim is to make a copy of documents from gdrive to amazon s3.
  Very negative
I can achieve this by download the Gdrive documents into my local directory and upload into amazon-s3 by using the s3Utility's public void uploadToBucket(int userId, String bucketName, String fileName, File fileData) method.
  Negative
Is there any direct way to achieve this?
  Neutral
that is i want to reduce one step.
  Negative
i don't like to download documents into my local.Instead of this i would like to give the gdrive document's downloadurl into s3 method,it will need to save the document into s3.
  Negative
Is it possible?
  Neutral
Any Suggestions?
  Neutral
sorry the essay type of question
563e0f932d1761a701f0f9f2	X	are you looking for Route 53 (aws.amazon.com/route53)?
  Negative
563e0f932d1761a701f0f9f3	X	I have the basic knowledge of route53,but I want to be able to create domains.Does Route 53 allow me to create domians?I did not find any API associated with it.
  Positive
563e0f932d1761a701f0f9f4	X	maybe this: forums.aws.amazon.com/… ?
  Negative
563e0f932d1761a701f0f9f5	X	the PHP2 SDK reference for Route 53 is here: docs.aws.amazon.com/aws-sdk-php-2/latest/…
563e0f932d1761a701f0f9f6	X	btw, you do not have to use AWS for DNS things.
  Negative
If your host allows you to change DNS settings for your domains then you can use that instead.
  Negative
all you need to do it seems is to point your domain to the bucket.
  Negative
563e0f932d1761a701f0f9f7	X	Thank you for pointing out the limitation!As you said I need to do a proxy,but can you please elaborate on how exactly I can achieve it?I am still a newbie working on the cloud and aws stuff!
  Positive
563e0f942d1761a701f0f9f8	X	I updated my original answer to include links to the nginx docs as you could use this server to proxy the requests.
  Negative
This setup is not specific to the cloud or aws, it would be the same to proxy and rewrite requests to any backend server.
  Negative
Those docs will help you get started but you will probably need to do more research on this and post other specific questions, either here or on serverfault if you get stuck on a specific point.
  Negative
563e0f942d1761a701f0f9f9	X	Just to make sure if I am going the right way,earlier I used cpanel and its services(API's) to create subdomains(on the fly) and transfer files via FTP.Is the approach highlighted by you better or I can go with this one?Offcourse I don't yet have a cpanel on Amzon,bt thats a different problem probably!
  Negative
563e0f942d1761a701f0f9fa	X	The approach I suggested is not necessarily better in your case.
  Negative
It really depends on your needs and experience.
  Negative
Each method has advantages and disadvantages.
  Positive
As advantages in storing the data directly to S3 and using this proxy method I would mention that your EC2 instance will not need a persistent storage attached to it (EBS), S3 has great data durability and reliability, the load on your server will probably be smaller than when using cPanel so you might use a cheaper instance and it is easier to load balance the load between multiple servers than it is with cPanel.
  Negative
563e0f942d1761a701f0f9fb	X	I am going through your suggestions,but would the functionality work fast in terms of speed?As subdomain creation will take time and I don't want the user to wait for the results to propagate!!
  Negative
Do you say its the best way to go while using AWS and amazon EC2?
  Neutral
I have accepted your answer and will post any specific questions when help needed(I believe I will definitely need it)
563e0f942d1761a701f0f9fc	X	INFO: I am working on an app built in php which facilitates users to create HTML templates and publish them on web.As the templates would be static I thought of using amazon S3 for storing them as it can host static websites and has good infrastructure overall for the application.br/> QUERY: I am facing an issue while publishing it to the web.I want the template to be published as a subdomain on my domain,for eg: I own www.ABC.com,I ask the user to name the template,if he names it as mysite,I publish his template as mysite.ABC.com(similar for all users).
  Very negative
Now,I can store the template in the S3 bucket using putObjectFile in the aws s3 api,but I am not sure how can I create a subdomain(on the fly) and publish it on that domain.
  Negative
(I want to automate the process for the user).
  Negative
Also,can I make the bucket as hosting static website using the API?
  Negative
Earlier,I worked with cpanel and cpanel API's allow us to create domains and do a FTP to the domain with the content,I am not clear how can I achieve it here.
  Negative
RESEARCH: The success till now I have achieved is,I have hosted a site using the S3 console.Using the AWS services I have moved the files to a bucket with the name same as the subdomain of the user.I want now to have the bucket endpoint changed to the subdomain.
  Negative
REFERENCE: This website does the same,they create a directory like structure and publish the website on web.I don't know if they host it on Amazon,but I want to achieve something similar.Hope I am clear and get some guidance.
  Negative
Thank you for the attention
563e0f942d1761a701f0f9fd	X	I researched a similar scenario some time ago but I was unable to use S3 for this because of a S3 limitation.
  Negative
To host a static website on S3 on a custom domain or subdomain, you need to create a bucket with a name that matches that domain.
  Negative
And because each S3 account is limited to 100 buckets, you will only be able to host those many domains or subdomains on a single account.
  Negative
Based on the use case you described, I suspect this S3 limitation will also force you to find another solution.
  Negative
In my case, the solution was to set up an EC2 instance that proxies requests to S3 after rewriting them.
  Negative
For example if someone requests: http://mysite.abc.com/file.html that goes through our EC2 server where the request is rewritten and forwarded to S3 as something like: http://ourbucket.s3.amazonaws.com/mysite.abc.com/file.html UPDATE: There are several proxy servers that you could use for this but I would recommend nginx as it worked great in our case.
  Negative
To get started, check out the following nginx docs: http://wiki.nginx.org/HttpRewriteModule http://wiki.nginx.org/HttpProxyModule
563e0f942d1761a701f0f9fe	X	When use scan for 1), it means I have to spend a lot of read capacity units to get all items then extract the result of just two hash primary key?
  Negative
wow ~ it's very expensive!
  Positive
any other choice?
  Neutral
563e0f942d1761a701f0f9ff	X	You can choose which attributes to fetch - meaning that your read throuout won't go crazy if you have big attributes for each item.
  Negative
You can also add conditions to minimize returned results which reduces network IO but doesn't save throughout
563e0f942d1761a701f0fa00	X	it's useful info for me, but I found no description on AWS about the spent read capacity unit of scan depending on the attributes retrieved (it has a similar description on query but not for scan).
  Negative
any reference to prompt?
  Neutral
thanks a lot
563e0f942d1761a701f0fa01	X	I would write a post on the AWS DynamoDB forum asking for documentation clarifications.
  Positive
563e0f942d1761a701f0fa02	X	Could I expect that if I just want the scan to return hash primary key and no any other attributes then each return item size should be smaller than 100 byte.
  Negative
And I need only single read capacity unit for more than 40 items?
  Negative
563e0f942d1761a701f0fa03	X	I would like to extract the primary key of table to a list , but find no api to do that.
  Negative
for example, as the amazon example thread table, I want to ask how to : 1) get the hash primary key list, in the amazon example thread table it would be an array of ["Amazon DynamoDB", "Amazon S3"] 2) with assigning the hash primary key to "Amazon DynamoDB", I want to get the range primary key list and it would be an array of ["Amazon DynamoDB Thread 1", "Amazon DynamoDB Thread 2"]
563e0f942d1761a701f0fa04	X	For 1 what you want is to run a Scan operation on a table.
  Negative
Scan Gets all the items of the list.
  Negative
Depends on the API you are using, you can get only the hash key or any attributes you want.
  Negative
For 2 what you want is Query - which gets a hash attribute and returns all rows that have the hash attribute (can be more than one).
  Negative
Overview - Query and Scan operations Java mapper reference - Scan and Query
563e0f942d1761a701f0fa05	X	Why the downvote?
  Negative
Do I need to be more/less specific?
  Neutral
563e0f952d1761a701f0fa06	X	Why aren't you using the S3 API instead of trying to use it as a filesystem?
  Negative
563e0f952d1761a701f0fa07	X	Not the downvoter, but I wonder if he/she was looking for a chunk of code you are having trouble with.
  Negative
Whilst we do have a policy here against discursive questions, the question seems specific enough to me, so +1.
  Negative
563e0f952d1761a701f0fa08	X	@StevenVondruska I'll look into that.
  Positive
Thanks.
  Neutral
563e0f952d1761a701f0fa09	X	@halfer I figured people were more reacting to the title of the post (I definitely am frustrated!)
  Negative
rather than the content, which is why I tried again.
  Negative
Thanks!
  Positive
563e0f952d1761a701f0fa0a	X	I don't know why I even tried to mount the S3 bucket on the local filesystem...probably because it was somebody else's idea first.
  Negative
563e0f952d1761a701f0fa0b	X	+1 for the leaky abstraction article!
  Negative
563e0f952d1761a701f0fa0c	X	I checked out Gaufrette, thinking it would make s3 integration easier, but really, amazon's php sdk is quite easy to use by itself.
  Negative
563e0f952d1761a701f0fa0d	X	I'm working on a project that is being hosted on Amazon Web Services.
  Negative
The server setup consists of two EC2 instances, one Elastic Load Balancer and an extra Elastic Block Store on which the web application resides.
  Negative
The project is supposed to use S3 for storage of files that users upload.
  Negative
For the sake of this question, I'll call the S3 bucket static.example.com I have tried using s3fs (https://code.google.com/p/s3fs/wiki/FuseOverAmazon), RioFS (https://github.com/skoobe/riofs) and s3ql (https://code.google.com/p/s3ql/).
  Negative
s3fs will mount the filesystem but won't let me write to the bucket (I asked this question on SO: How can I mount an S3 volume with proper permissions using FUSE).
  Negative
RioFS will mount the filesystem and will let me write to the bucket from the shell, but files that are saved using PHP don't appear in the bucket (I opened an issue with the project on GitHub).
  Negative
s3ql will mount the bucket, but none of the files that are already in the bucket appear in the filesystem.
  Negative
These are the mount commands I used: I've also tried using this S3 class: https://github.com/tpyo/amazon-s3-php-class/ and this FuelPHP specific S3 package: https://github.com/tomschlick/fuel-s3.
  Negative
I was able to get the FuelPHP package to list the available buckets and files, but saving files to the bucket failed (but did not error).
  Negative
Have you ever mounted an S3 bucket on a local linux filesystem and used PHP to write a file to the bucket successfully?
  Negative
What tool(s) did you use?
  Negative
If you used one of the above mentioned tools, what version did you use?
  Neutral
EDIT I have been informed that the issue I opened with RioFS on GitHub has been resolved.
  Negative
Although I decided to use the S3 REST API rather than attempting to mount a bucket as a volume, it seems that RioFS may be a viable option these days.
  Negative
563e0f952d1761a701f0fa0e	X	Have you ever mounted an S3 bucket on a local linux filesystem?
  Negative
No.
  Neutral
It's fun for testing, but I wouldn't let it near a production system.
  Positive
It's much better to use a library to communicate with S3.
  Negative
Here's why: The bottom line is that S3 under FUSE is a leaky abstraction.
  Negative
S3 doesn't have (or need) directories.
  Negative
Filesystems weren't built for billions of files.
  Negative
Their permissions models are incompatible.
  Negative
You are wasting a lot of the power of S3 by trying to shoehorn it into a filesystem.
  Negative
Two random PHP libraries for talking to S3: https://github.com/KnpLabs/Gaufrette https://aws.amazon.com/sdkforphp/ - this one is useful if you expand beyond just using S3, or if you need to do any of the fancy requests mentioned above.
  Negative
563e0f952d1761a701f0fa0f	X	Quite often, it is advantageous to write files to the EBS volume, then force subsequent public requests for the file(s) to route through CloudFront CDN.
  Negative
In that way, if the app must do any transformations to the file, it's much easier to do on the local drive & system, then force requests for the transformed files to pull from the origin via CloudFront.
  Negative
e.g. if your user is uploading an image for an avatar, and the avatar image needs several iterations for size & crop, your app can create these on the local volume, but all public requests for the file will take place through a cloudfront origin-pull request.
  Negative
In that way, you have maximum flexibility to keep the original file (or an optimized version of the file), and any subsequent user requests can either pull an existing version from cloud front edge, or cloud front will route the request back to the app and create any necessary iterations.
  Negative
An elementary example of the above would be WordPress, which creates multiple sized/cropped versions of any graphic image uploaded, in addition to keeping the original (subject to file size restrictions, and/or plugin transformations).
  Negative
CDN-capable WordPress plugins such as W3 Total Cache rewrite requests to pull through CDN, so the app only needs to create unique first-request iterations.
  Negative
Adding browser caching URL versioning (http://domain.tld/file.php?x123) further refines and leverages CDN functionality.
  Negative
If you are concerned about rapid expansion of EBS volume file size or inodes, you can automate a pruning process for seldom-requested files, or aged files.
  Negative
563e0f952d1761a701f0fa10	X	As per the Amazon multipart upload documentation the Each part must be at least 5 MB in size, except the last part.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html Question is how do I upload a file less than 5MB through multipart upload api to AWS S3 bucket.
  Negative
The reason I am asking this that I want to use multipart upload API for all files when uploading to S3
563e0f962d1761a701f0fa11	X	You can still upload it using multipart upload, the same as you would a larger file... but you have to upload it with only one part.
  Negative
The rule enforced by S3 is that all parts except the last part must be >= 5MB.
  Negative
If the first part is also the last part, this rule isn't violated and S3 accepts the small file as a multipart upload.
  Negative
563e0f962d1761a701f0fa12	X	I have a lot of subdirectories containing a lot of images (millions) on S3.
  Negative
Having the files in these subdirectories has turned out to be a lot of trouble, and since all file names are actually unique, there is no reason why they should reside in subdirectories.
  Negative
So I need to find a fast and scalable way to move all files from the subdirectories into one common directory or alternatively delete the sub directories without deleting the files.
  Negative
Is there a way to do this?
  Neutral
I'm on ruby, but open to almost anything
563e0f962d1761a701f0fa13	X	I have added a comment to your other question, explaining why S3 does not have folders, but file name prefixes instead (See Amazon AWS IOS SDK: How to list ALL file names in a FOLDER).
  Negative
With that in mind, you will probably need to use a combination of two S3 API calls in order to achieve what you want: copy a file to a new one (removing the prefix from the file name) and deleting the original.
  Negative
Maybe there is a Ruby S3 SDK or framework out there exposing a rename feature, but under the hood it will likely be a copy/delete.
  Neutral
Related question: Amazon S3 boto: how to rename a file in a bucket?
  Negative
563e0f972d1761a701f0fa14	X	this question might be more suitable for webmasters.stackexchange.com
563e0f972d1761a701f0fa15	X	ok, can you provide recommendation, that would be more useful for thanks?
  Negative
563e0f972d1761a701f0fa16	X	THANKS Alfasin - I didnt know about that site.
  Negative
Im only new to SO.
  Negative
563e0f972d1761a701f0fa17	X	I really would recommend that you not ask this question on webmasters.
  Negative
You'll receive a poor reception there as well, for the same reason.
  Negative
Stack Exchange sites, as a whole, aren't suited for this kind of A-B question.
  Negative
563e0f972d1761a701f0fa18	X	Then could you recommend a website that does, Im guessing QnA websites are all about sharing of Knowledge ?
  Negative
563e0f972d1761a701f0fa19	X	If anyone else has other info, pros and cons I will vote that to be the correct answer and pass on the points
563e0f972d1761a701f0fa1a	X	For web Sites/Applications and eCommerce, which storage solution is more desirable and Why ?
  Negative
Im very new to Amazon Cloud services I need some direction here.
  Negative
563e0f972d1761a701f0fa1b	X	S3 likes network storage, you can access it with web api ebs likes hard disk, you can access it in your ec2 you can also see, http://www.differencebetween.net/technology/internet/difference-between-amazon-s3-and-amazon-ebs/
563e0f972d1761a701f0fa1c	X	I found another SO question here Should I persist images on EBS or S3?
  Negative
This pretty much answers my question.
  Positive
563e0f972d1761a701f0fa1d	X	EBS is meant for transactional data and S3 for everything else.
  Negative
In EBS you can have static IP which is required when we use database access etc.
563e0f972d1761a701f0fa1e	X	I bet creating a custom storage class is the best way to go.
  Negative
The custom class can re-use code from FileSystemStorage and S3BotoStorage.
  Positive
563e0f972d1761a701f0fa1f	X	This may help you: djangosnippets.org/snippets/1976
563e0f972d1761a701f0fa20	X	I have django application that was using S3BotoStorage backend to store uploaded files on Amazon s3.
  Negative
But in web api services(using django-tastypie) it was taking long time to upload file on s3.
  Negative
As there were request passes through web server and then to amazon s3 storage backend.
  Negative
So, we come with solution to let them upload first on Web server and implement django-celery tasks through which files get uploaded to amazon s3.
  Negative
It is finished and working.
  Neutral
But after that we want to modify the url of files to amazon s3 storage location urls.
  Negative
But when we try to modify file_field_obj.
  Neutral
storage to s3botostroage.
  Neutral
This gets revert it back to Default File Storage as expected.
  Negative
So is there any option we can modify Django Models FileField storage field after uploading files on s3.
  Negative
So, in settings there will be DefaultFileStorage pointing to FileSystemStorage.
  Negative
But if files are on s3 then, they will point to s3 storage locations.
  Negative
563e0f982d1761a701f0fa21	X	The solution already exists in an app: django-queued-storage It should handle creating celery tasks that uploads between the storage backends.
  Negative
563e0f982d1761a701f0fa22	X	That's not really an answer.
  Negative
A comment would suffice.
  Negative
563e0f982d1761a701f0fa23	X	Sorry, but I respectfully disagree - you asked: [...] is there any way to send the notification directly to the SWF, without having a service consuming them and starting the workflow?
  Negative
- I provided an answer stating that there is no such way, including an explanation and a proper reference - I've added that quote to make this more obvious now.
  Negative
Though unfortunate, sometimes a negative answer can also be an answer (depending on the question asked).
  Negative
563e0f982d1761a701f0fa24	X	Makes sense.
  Negative
Thanks.
  Neutral
563e0f982d1761a701f0fa25	X	I have a workflow which takes a file in an S3 bucket and does a lot of processing and further requests based on the file contents.
  Negative
Currently, clients have to trigger the workflow manually after uploading the file.
  Negative
This seems to be a pretty common use case for me, so is there any way to trigger the workflow as soon as the file is uploaded?
  Negative
I imagine there should be an SNS notification in between, but is there any way to send the notification directly to the SWF, without having a service consuming them and starting the workflow?
  Negative
563e0f982d1761a701f0fa26	X	AWS has finally launched New Event Notifications for Amazon S3 today, which indeed simply extend the long available PUT Bucket notification API with additional event types for object creation via the S3 APIs such as PUT, POST, and COPY: [...] is there any way to send the notification directly to the SWF, without having a service consuming them and starting the workflow?
  Very negative
Unfortunately there is no such way, you indeed need a mediating service - while the PUT Bucket notification has obviously been designed to allow for other types of events too, Amazon S3 doesn't support Amazon SNS notifications for anything but Enabling RRS Lost Object Notifications as of today: This implementation of the PUT operation uses the notification subresource to enable notifications of specified events for a bucket.
  Very negative
Currently, the s3:ReducedRedundancyLostObject event is the only event supported for notifications.
  Negative
The s3:ReducedRedundancyLostObject event is triggered when Amazon S3 detects that it has lost all replicas of an object and can no longer service requests for that object.
  Negative
[emphasis mine]
563e0f982d1761a701f0fa27	X	As Steffen Opel said, there is no way to do this right now.
  Negative
However, an alternate route to what his updated answer provided would be to use AWS's new event processing service Lambda (which ATM is in preview).
  Neutral
The documentation that shows you how to configure it for S3 is here, but at a high level:
563e0f982d1761a701f0fa28	X	Is there a way to use Fine Uploader to upload to an Amazon S3 bucket by providing the already signed policy document along with the key and the other credentials all at once by overriding the policy post request with our own XML api call?
  Negative
Our company API returns all the credentials including signed policy for the file in one response and is already well established so setting up a signing page is not an option.
  Negative
563e0f982d1761a701f0fa29	X	This may work for non-chunked uploads since Fine Uploader will only make one request for the signed policy document.
  Negative
However, when uploading files in chunks, the S3 REST API must be used.
  Neutral
In that case, a policy document is not used.
  Negative
Instead, a long string of relevant headers for each request must be signed.
  Negative
This signature is then included with the REST call.
  Negative
The headers change with each request, therefore requiring a new signature.
  Negative
If you want to support chunking and concurrent chunking to S3, you'll need to ensure each request is signed separately via a signature server, or make use of an identity provider to handle this client-side, as demonstrated in our documentation at http://docs.fineuploader.com/branch/master/features/no-server-uploads.html.
  Negative
563e0f982d1761a701f0fa2a	X	Thanks for your answer, I already setup S3FS unfortunately.
  Negative
My folder is read/write enabled, but not allowed for public access, for that you need to set a bucket policy.
  Negative
I managed to solve this by using s3curl.
  Positive
;)
563e0f982d1761a701f0fa2b	X	I've managed to use S3FS to mount an Amazon S3 folder into my Wordpress site.
  Negative
Basically, my gallery folder for NextGEN gallery is a symlink to a mounted S3FS folder of the bucket, so when I upload an image, the file is automatically added to the S3 bucket.
  Negative
I'm busy writing an Apache rewrite rule to replace the links, to fetch gallery images from S3 instead, without having to hack or change anything with NextGEN, but one problem I'm finding, is that images are not public by default on S3.
  Negative
Is there a way to change a parent folder, to make its children always be public, including new files as they are generated?
  Neutral
Is it possible or advisable to use a cron task to manually make a folder public using the S3 command line API?
  Negative
563e0f982d1761a701f0fa2c	X	I'm the lead developer and maintainer of Open source project RioFS: a userspace filesystem to mount Amazon S3 buckets.
  Negative
Our project is an alternative to “s3fs” project, main advantage comparing to “s3fs” are: simplicity, the speed of operations and bugs-free code.
  Negative
Currently the project is in the “beta” state, but it's been running on several high-loaded fileservers for quite some time.
  Negative
We are seeking for more people to join our project and help with the testing.
  Negative
From our side we offer quick bugs fix and will listen to your requests to add new features.
  Negative
Regarding your issue: if'd you use RioFS, you could mount a bucket and have a write / read access to it using the following command (assuming you have installed RioFS and have exported AWSACCESSKEYID and AWSSECRETACCESSKEY environment variables): (please refer to project description for command line arguments) Please note that the project is still in the development, there are could be still a number of bugs left.
  Negative
If you find that something doesn't work as expected: please fill a issue report on the project's GitHub page.
  Negative
Hope it helps and we are looking forward to seeing you joined our community !
  Positive
563e0f982d1761a701f0fa2d	X	I downloaded s3curl and used that to add the bucket policy to S3.
  Negative
See this link: http://blog.travelmarx.com/2012/09/working-with-s3curl-and-amazon-s3.html You can generate your bucket policies using the Amazon Policy Generator: http://awspolicygen.s3.amazonaws.com/policygen.html
563e0f992d1761a701f0fa2e	X	Do you have reason to believe that the method can fail without throwing an exception?
  Negative
563e0f992d1761a701f0fa2f	X	This is pretty similar to what I recently did with the PHP API.
  Negative
I'd capture the originals etag, copy, compare original etag to copied etag, if match, delete original.
  Negative
563e0f992d1761a701f0fa30	X	Historical note, following up on Dan's comment: It appears that the system for generating etags changes when a multipart upload is used as opposed to a regular upload, so comparison by MD5 might be the better approach.
  Negative
563e0f992d1761a701f0fa31	X	Since S3 doesn't have immediate consistency, wouldn't this method occasionally show that the object hasn't been copied even though it has?
  Negative
563e0f992d1761a701f0fa32	X	This is probably just me being unfamiliar with S3, but what explicitly would I check for on the etag string?
  Negative
563e0f992d1761a701f0fa33	X	If you set it when you made your CopyObjectRequest, you should be able to compare the two for equality.
  Positive
Hope that helps.
  Neutral
563e0f992d1761a701f0fa34	X	Ultimately, if you're unsure, just test it.
  Negative
Use an file you don't care about.
  Negative
563e0f992d1761a701f0fa35	X	I feel like I am missing some larger context, what is an ETag?
  Negative
563e0f992d1761a701f0fa36	X	They explain it well in this thread: stackoverflow.com/questions/6591047/…
563e0f992d1761a701f0fa37	X	I'm trying to implement a move operation using the Amazon S3 Java API.
  Negative
The problem I am having is that the CopyObjectResult object returned by the AmazonS3Client.copyObject method doesn't seem to have a clear indicator about wiether the operation was successful or not.
  Negative
Given that after this operation I am going to be deleting a file, I'd want to make sure that the operation was successful.
  Negative
Any suggestions?
  Neutral
563e0f992d1761a701f0fa38	X	This is what I ended up doing, Note that this is Groovy code, but it is extremely similar to how the Java code would work.
  Negative
I don't like having to make two additional operations to check the metadata, so if there is anyway to do this more efficiently let me know.
  Negative
563e0f992d1761a701f0fa39	X	I'm pretty sure you can just use CopyObjectResult object's getETag method to confirm that, after created, it has a valid ETag, as was made in the CopyObjectRequest setETag method.
  Negative
getETag public String getETag() Gets the ETag value for the new object that was created in the associated CopyObjectRequest.
  Negative
Returns: The ETag value for the new object.
  Positive
See Also: setETag(String) setETag public void setETag(String etag) Sets the ETag value for the new object that was created from the associated copy object request.
  Negative
Parameters: etag - The ETag value for the new object.
  Positive
See Also: getETag() Always trust the data.
  Positive
Been a year since I did a similar function with the Amazon PhP SDK a couple years ago, but it should work.
  Negative
563e0f9a2d1761a701f0fa3a	X	possible duplicate of Amazon - EC2 cost?
  Negative
563e0f9a2d1761a701f0fa3b	X	I have gone through AWS docs but they were not much clear, Again I am not much clear.
  Negative
.
  Neutral
why is EBS required then if everything is to stored on S3?
  Negative
Also for daily users of about 500 which CPU instance should I purchase?
  Negative
563e0f9a2d1761a701f0fa3c	X	You can store files on S3 without needing servers.
  Negative
S3 cannot run server-side code like PHP, but it can serve static websites, assets like images and videos, etc.
  Negative
It is an additional offering of AWS that has no direct comparison on a host like Hostgator.
  Negative
EBS is, again, like a traditional hard drive like you'd have on Hostgator.
  Negative
563e0f9a2d1761a701f0fa3d	X	Sorry to bother you but again why should I host videos on S3 rather then EBS.
  Negative
I get that S3 will be faster and it doesn't run PHP but if I host videos on EBS then it will work just fine right?
  Negative
Also just last thing should I buy small or medium CPU for average of 500 users per day
563e0f9a2d1761a701f0fa3e	X	Re: "should I buy": serverfault.com/questions/384686/…
563e0f9a2d1761a701f0fa3f	X	Re: videos, EBS can handle videos, but S3 is generally a better place for large static files.
  Negative
For one, hosting them off S3 means your server isn't handling that bandwidth - it'll have more bandwidth available for the rest of what it has to do, and S3 has effectively infinite bandwidth for your level of usage.
  Negative
EBS can have disk failures just like any other hard disk, whereas S3 is engineered to be highly redundant.
  Negative
563e0f9a2d1761a701f0fa40	X	I have PHP based website hosted on hostgator VPS, I have also subscribed to free tier of amazon.
  Negative
Now I am planning to shift website on amazon but I am not quiet sure how much is the capacity of the micro CPU of EC2 and also I have 5 GB of Amazon S3 storage plus 30 GB of amazon EBS Storage.
  Very negative
I have data of almost 20 GB and I have video streaming site.
  Negative
Now I am having trouble figuring this out.
  Neutral
I know if I run only one instance then it will cost me nothing for whole month.
  Negative
Thanks
563e0f9a2d1761a701f0fa41	X	Not much.
  Negative
Micro instances have severely restricted CPU if you use it much.
  Negative
It means any data your instances send to the outside Internet over 15 GB/month will cost you money.
  Negative
S3 is a storage system.
  Neutral
EBS is more like a traditional hard disk on a server.
  Negative
You'd generally host video content off S3 rather than on an instance's EBS hard drive.
  Negative
Up to you.
  Positive
RDS is just an EC2 instance with MySQL installed that Amazon manages for you.
  Positive
If you'd prefer to manage it yourself, install MySQL on an EC2 instance.
  Negative
The AWS docs answer all of this and are well worth perusing.
  Positive
563e0f9a2d1761a701f0fa42	X	I think you should be able to achive this by using curl and s3cmd.
  Negative
E.g. pipe the output of curl into s3cmd
563e0f9a2d1761a701f0fa43	X	Since my all data is in FTP server, how can i achieve with curl?
  Negative
563e0f9a2d1761a701f0fa44	X	I would like to copy all my folders and files available from FTP server to Amazon S3 bucket.
  Negative
Tried to find the information on web to find the tools or AWS S3 provides any APIS for copy folders and files from FTP server.
  Negative
Any tools or pointers to links would be helpful
563e0f9a2d1761a701f0fa45	X	The AWS Command Line Interface (CLI) can be used to copy multiple files and folders to Amazon S3.
  Negative
It also has a sync command that can intelligently copy on new or modified files.
  Negative
This has nothing to do with your origin being an FTP server.
  Negative
Rather, it's a means of copying from some source (Windows, Linux, Mac) to Amazon S3.
  Negative
You can call the command from outside of your FTP software (eg as a script triggered hourly).
  Negative
563e0f9a2d1761a701f0fa46	X	when i put wrong keys it returns output:S3 Object ( ) and put right key and print_r($s3); its again returns S3 Object ( ) then how trace key validation with this.
  Negative
563e0f9b2d1761a701f0fa47	X	its working for me.
  Neutral
563e0f9b2d1761a701f0fa48	X	the official PHP API functions are little different: create_bucket() and delete_bucket() and to test for true if($return->status == 200).
  Negative
Otherwise works for me +1
563e0f9b2d1761a701f0fa49	X	yep: undesigned.org.za/2007/10/22/amazon-s3-php-class
563e0f9b2d1761a701f0fa4a	X	I'm learning the Amazon S3 PHP API, and I want to save my Amazon keys if these are valid for login into an Amazon account or alert the user if he puts wrong keys via the following code: Now, how can I check whether theses keys are valid or invalid and save them in a database if correct, e.g.: Does the object return any error if keys is valid or invalid?
  Very negative
I don't know how get an authentication response, could someone please suggest a solution for it?
  Negative
Thank You.
  Positive
563e0f9b2d1761a701f0fa4b	X	you may try this
563e0f9b2d1761a701f0fa4c	X	Amazon limits its response to max 1000 objects, as stated in the documentation.
  Negative
Higher values for $maxkey will not work here.
  Negative
563e0f9b2d1761a701f0fa4d	X	Thanks for clarify.
  Neutral
I edited an answer.
  Positive
563e0f9b2d1761a701f0fa4e	X	This is the method in my S3 library file.
  Negative
[public static function getBucket($bucket, $prefix = null, $marker = null, $maxKeys = null) ].
  Negative
What change should I make?
  Neutral
563e0f9b2d1761a701f0fa4f	X	With your first request you get 1000 object-keys.
  Negative
Take the last one and pass this as parameter for $marker on your second request to retrieve the next 1000 (max) objects
563e0f9b2d1761a701f0fa50	X	I'm totally new to this.
  Negative
Can you please edit the above code @ itinance
563e0f9b2d1761a701f0fa51	X	To be honestly, as a developer you should be able to call a method with a second parameter and also to read and understand documentation :) Stackoverflow is a platform to give you hints and answer your questions, not to do your work in general
563e0f9b2d1761a701f0fa52	X	I got it.
  Negative
I used iterator method.
  Neutral
563e0f9b2d1761a701f0fa53	X	I started listing the contents of a S3 bucket using foreach loop.
  Negative
But the problem is that it is not listing above 1000 index.
  Negative
The last few files are not listing.
  Negative
I tried getting the length of the array and I found that last few files are not even getting stored in that array.
  Negative
What should I do to list them all?
  Neutral
563e0f9b2d1761a701f0fa54	X	The default number of listed files is set to 1000, so thats why you cannot get more than 1000 files: Here you have a description of a function: Use $marker to set where to start getting files and $maxKeys as a limit.
  Very negative
WWW: http://www.drupalcontrib.org/api/drupal/contributions!media_mover!contrib!mm_s3!S3.php/function/S3%3A%3AgetBucket/6 If you use the same library.
  Negative
563e0f9b2d1761a701f0fa55	X	As stated in the S3 developer documentation: As buckets can contain a virtually unlimited number of keys, the complete results of a list query can be extremely large.
  Negative
To manage large result sets, Amazon S3 API support pagination to split them into multiple responses.
  Negative
Each list keys response returns a page of up to 1,000 keys with an indicator indicating if the response is truncated.
  Negative
You send a series of list keys requests until you have received all the keys.
  Negative
AWS SDK wrapper libraries provide the same pagination.
  Negative
That means you need multi page requests to iterate over multiple "pages".
  Neutral
This will depend on the implementation of your "s3"-Class.
  Neutral
Which library are you using for s3?
  Negative
If the implementation you are using offers a "getBucket"-Method, there will be an optional parameter $marker.
  Negative
Place here the key of the last object to retrieve the next 1000 objects after that object.
  Neutral
Documentation: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e0f9b2d1761a701f0fa56	X	thanks Nicola!!!
  Negative
563e0f9c2d1761a701f0fa57	X	@JohnZ if you come up with a S3 MediaStorage class and are interested in sharing it please get in touch with me, we could add it to the Eve repo or, at the very least, make an extension out of it.
  Negative
563e0f9c2d1761a701f0fa58	X	indeed a good idea!
  Positive
will get you posted
563e0f9c2d1761a701f0fa59	X	I know we now can upload/post a media type field to the python-eve REST-API framework and it will be stored in mongodb.
  Negative
Is there a way we can change the storage of those media?
  Neutral
E.g. a remote storage server e.g. amazon S3?
  Negative
So that we only store the URL of the image in mongodb instead of the whole image?
  Neutral
563e0f9c2d1761a701f0fa5a	X	While the standard behaviour is to store on GridFS, you can also provide your own MediaStorage subclass which can store wherever you want to (file system, S3, etc.) Something like this would do the trick: Check out the actual MediaStorage class for implementation details and/or see the actual GridFSMediaStorage class for reference.
  Negative
563e0f9c2d1761a701f0fa5b	X	Is this possible?
  Neutral
Is there a programme that will allow this to happen?
  Neutral
I have a program that needs to access a lot of data from a central storage, but the likes of Amazon S3 only allows access via RESTful API which is no good for this program.
  Negative
Only UNC or drive letters are acceptable... Help!
  Neutral
Bernard
563e0f9c2d1761a701f0fa5c	X	I believe that you can map a Windows drive letter to a WebDav store.
  Negative
There are plenty of online Webdav storage providers.
  Negative
563e0f9c2d1761a701f0fa5d	X	downvoters please state teh reason, if I violate anything.
  Negative
.
  Neutral
563e0f9c2d1761a701f0fa5e	X	which S3 class exactly?
  Negative
563e0f9c2d1761a701f0fa5f	X	I have seen many posts through which we can upload image on S3, but what if we want to save image from google/facebook , that is something like graph.facebook.com/abc.img (suppose it is a valid image) I have seen a similar post but that uses Node.js but I simply want to use php to save image on s3, any help in this matter is appreciated Thanks
563e0f9c2d1761a701f0fa60	X	You could use this Facebook SDK PHP Class: https://developers.facebook.com/docs/reference/php/4.0.0 And this one for S3: https://github.com/tpyo/amazon-s3-php-class Should be pretty easy ;)
563e0f9d2d1761a701f0fa61	X	Could you check if this works <img ng-src="yanpy.dev.s3.amazonaws.com/img/boats/1/1.jpg">;
563e0f9d2d1761a701f0fa62	X	Configure S3 to allow access.
  Very negative
If it is for a public website you might want to enable public access in S3.
  Negative
563e0f9d2d1761a701f0fa63	X	@Sebastian I made the image public in S3 and it works.
  Positive
However, from the security point of view, is this right?
  Neutral
Should it be public?
  Neutral
563e0f9d2d1761a701f0fa64	X	if you have a website which should be publicly accessible you must make it public (read-only of course)
563e0f9d2d1761a701f0fa65	X	Im developing a website with AngularJS in frontend that sends requests to a Rails 4 API backend.
  Negative
I have to manage quite images, so I would like to use Amazon S3 (but Im newbie with this and Im a bit lost).
  Negative
I have uploaded an image to folder in a new bucket (yanpy.dev) I have created in path: How can I get this image to display it in a view in my AngularJS front-end?
  Neutral
Im trying something like: But its not working.
  Neutral
563e0f9d2d1761a701f0fa66	X	See @Sebastian comment above.
  Negative
Point for him.
  Neutral
563e0f9d2d1761a701f0fa67	X	I am writing script that connects to amazon S3 storage.
  Negative
The script is supposed to create 2 buckets: Bucket is for data Bucket is for logs I successfully created both buckets but I can't set up logging.
  Negative
Below is shown code I use for enabling bucket logging In amazon AWS API for PHP version 2 is written that Bucket, LoggingEnabled and Type are mandatory.
  Negative
But the documentation does not say how to exactly implement there parameters.
  Negative
Could you please help me with structure of config array for putBucketLogging method?
  Negative
563e0f9d2d1761a701f0fa68	X	You can also use the service's API documents as a reference, which sometimes contain more details about how to specifically structure some of the data types for requests.
  Negative
The S3 API docs for PUT Bucket Logging have more details about how to specify the grantee.
  Negative
Also, you should not use capital letters in bucket names (See Rules for Bucket Naming).
  Negative
563e0f9d2d1761a701f0fa69	X	After searching in manuals the php array for method putBucketLogging is However enabling logging fails with exception that tells me I have to set permissions on log bucket...
563e0f9d2d1761a701f0fa6a	X	Thanks bro, you really help me on this one
563e0f9d2d1761a701f0fa6b	X	This will upload the file both times in your S3 bucket, once during pick, once during store, the callback on store is called slowly, so if your user changes pages in between your file will only exist in filepicker's records and not your database.
  Negative
You could call it after pick, but then you will point to the wrong file.
  Negative
563e0f9d2d1761a701f0fa6c	X	I am trying to use this technique, but the onSuccess callback seems to be returning an intermediate promise value of "temp.txt".
  Negative
How do I save a file without having to dramatically alter the filename to a non-human readable format using filepicker.js ?
  Neutral
563e0f9d2d1761a701f0fa6d	X	I've been uploading files to the amazon s3 using the javascript library filepicker.io and the implementation works ok, the problem I'm facing now is that when users upload files with white spaces or fancy characters those files are not accessible through http, i was wondering if there is any way to apply some kind of renaming to the file prior the uploading face.
  Very negative
I'm using the pickAndStore method from the filepicker.io api Thanks.
  Neutral
563e0f9d2d1761a701f0fa6e	X	You can specify a "path" parameter in both the filepicker.store() call and filepicker.pickAndStore(), so if you want to specifically strip the whitespace from the filenames but keep the rest the same, the structure would be:
563e0f9e2d1761a701f0fa6f	X	You can also use path on pickAndStore but you won't be able to preprocess the inkblob.
  Negative
563e0f9e2d1761a701f0fa70	X	Thanks Ryan, I was reading over the documents last night and what you are saying is correct.
  Negative
I have contacted their team after signing up to the affiliates programme.
  Negative
I shall update this question and links to relevant resources as I progress with this issue.
  Negative
563e0f9e2d1761a701f0fa71	X	I would like to make requests for ItemSearch using Amazon Product Advertising API with meteor.
  Negative
http://s3.amazonaws.com/awsdocs/Associates/latest/prod-adv-api-dg.pdf https://images-na.ssl-images-amazon.com/images/G/01/webstore_t_d/API/WebstoreAPI_SearchProductUsersGuide.pdf Essentially, I would like the users on my web application to search and select books that they have read which will then be displayed in their profile.
  Negative
As the user types in the field, I would like the api to return a limited number of suggestions.
  Negative
When one item is selected, I would like to store the title and author of the book and url of the books' advertisement page on amazons website.
  Negative
I have been sourcing the documents and branching out from the following two links.
  Negative
I am beginning to understand in an abstract way of how the error and data callbacks work.
  Negative
Everything I am reading is abstract.
  Neutral
I need to help in setting up the searchItem feature which falls under Amazon Product Advertising API .
  Negative
I will limit the search index to the 'books' product category.
  Negative
According to amazon, I am effectively advertising for amazon in my use case so I joined their affiliates program https://affiliate-program.amazon.com/ But really, my use case intensions are for my users to list books they have read on their profile page.
  Negative
I need to capture 3 data points (title, author, and, url of the advertising page for the book on amazon.com).
  Negative
Has anybody attempted to use this API?
  Negative
If so, please can you shed light on how you set up to make requests to the API in meteor.
  Positive
563e0f9e2d1761a701f0fa72	X	The Amazon Product Advertising API does not fall under Amazon Web Services, but instead, Amazon Associates.
  Negative
The AWS SDK does not support non-AWS services (including other Amazon services), and likely never will.
  Negative
You'll need to find an entirely different package for hitting the Amazon Product Advertising API.
  Negative
563e0f9e2d1761a701f0fa73	X	I should clarify, that the problem with Marcos is that the upload from Filepicker.io when using a service other than local computer upload (ie: Dropbox, File URL, Box, Webcam, etc) is not sending the file to the configured S3 bucket.
  Negative
That's why he is not getting the S3 file key, because Filepicker.io is not uploading the file to S3 when using other options besides local upload.
  Negative
What we need to know is why this is happening and how to fix it.
  Negative
563e0f9e2d1761a701f0fa74	X	Hey thanks this has solved my problem.
  Negative
563e0f9e2d1761a701f0fa75	X	In case anyone hits this, you will not receive the key if you are on a free plan.
  Negative
563e0f9e2d1761a701f0fa76	X	I've been using Filepicker.IO in order to upload files directly from the browser to the amazon s3 and most things are working fine, the only problem i'm facing now is that after the upload is done, i'm not getting the name of the file in the s3.
  Very negative
Filepicker js api is returning this object: Usually this object comes with a property named 'key' which has the name of the file in the S3.
  Negative
This happens when the upload is not done from the local computer, if i pick a local file everything works ok, but if i pick a file from any of the providers (e.g Dropbox, Google Drive), i can't get the filename in the S3 server.
  Negative
Thanks.
  Neutral
563e0f9e2d1761a701f0fa77	X	You should make sure that you are using a function that is explicitly storing to S3, for instance filepicker.pickAndStore or filepicker.store.
  Negative
As noted in the filepicker.io pick API documentation, the "key" parameter on fpfiles returned specifically from the .
  Negative
pick() call are deprecated and not meant to be used.
  Negative
563e0f9e2d1761a701f0fa78	X	I should have explained better that we are using CopyObjectRequest to do this now.
  Negative
I've updated the question to reflect this.
  Neutral
563e0f9e2d1761a701f0fa79	X	The ObjectMetadata.setHeader is marked as internal use only.
  Negative
Have you used this successfully?
  Negative
Our code is only going to be run once, so we won't need to worry about it later if Amazon makes changes.
  Negative
But YMMV.
  Neutral
563e0f9e2d1761a701f0fa7a	X	Hmmh, I wasn't aware of that restriction indeed, but recall having used Expires: at some point a while back; I might be wrong though, insofar I often use other SDKs for interacting with S3 (e.g. C#/Python, which do definitely support this) and could have mixed that up - the code itself doesn't differ from the other setXYZHeader() methods currently (see ObjectMetadata.java), so the restriction would be based on a non visible side effect, if any.
  Negative
563e0f9e2d1761a701f0fa7b	X	It's probably used by the other header methods internally and internal only because only certain headers will work.
  Negative
They should add a setExpires method on ObjectMetadata as it's the only header that doesn't have it's own method.
  Negative
563e0f9e2d1761a701f0fa7c	X	The setHeader method solved this for us, thanks!
  Positive
563e0f9f2d1761a701f0fa7d	X	Thanks.
  Neutral
You are absolutely correct Cache-Control supersedes the Expires header.
  Positive
Still, we'd like to include it for HTTP/1.0 clients that do not respect Cache-Control.
  Negative
563e0f9f2d1761a701f0fa7e	X	I'm updating existing objects in an Amazon S3 bucket to set some metadata.
  Negative
I'd like to set the HTTP Expires header for each object to better handle HTTP/1.0 clients.
  Negative
We're using the AWS Java SDK, which allows for metadata changes to an object without re-uploading the object content.
  Negative
We do this using CopyObjectRequest to copy an object to itself.
  Negative
The ObjectMetadata class allows us to set the Cache-Control, Content-Type and several other headers.
  Positive
But not the Expires header.
  Negative
I know that S3 stores and serves the Expires header for objects PUT using the REST API.
  Negative
Is there a way to do this from the Java SDK?
  Neutral
Updated to indicate that we are using CopyObjectRequest
563e0f9f2d1761a701f0fa7f	X	To change the metadata of an existing Amazon S3 object, you need to copy the object to itself and provide the desired new metadata on the fly, see copyObject(): By default, all object metadata for the source object are copied to the new destination object, unless new object metadata in the specified CopyObjectRequest is provided.
  Negative
This can be achieved like so approximately (fragment from the top of my head, so beware): Please be aware of the following easy to miss, but important copyObject() constraint: The Amazon S3 Acccess Control List (ACL) is not copied to the new object.
  Very negative
The new object will have the default Amazon S3 ACL, CannedAccessControlList.Private, unless one is explicitly provided in the specified CopyObjectRequest.
  Negative
This is not accounted for in my code fragment yet!
  Negative
Good luck!
  Positive
563e0f9f2d1761a701f0fa80	X	We were looking for a similar solution and eventually settled for max-age cache-control directive.
  Negative
And we eventually realized that hte cache-control overrides the Expires even if expires is more restrictive.
  Negative
And anyways cache-control met our requirement as well.
  Positive
563e0f9f2d1761a701f0fa81	X	What is your actual question?
  Neutral
563e0f9f2d1761a701f0fa82	X	1-) What is wrong with my amazon uploading code?
  Negative
2-) How to implement uploading in background which I mentioned above.
  Positive
563e0f9f2d1761a701f0fa83	X	Wire up the NSURLSession Progress delegate then you would know- If the upload hangs after say 128 KB transfer then there is a problem in request.
  Negative
563e0f9f2d1761a701f0fa84	X	I have a generic question, experienced developers may laugh at me when they read that but I couldn't manage it somehow.
  Negative
Basically, I want to upload images from filesystem to my amazon s3 bucket and after that upload links of these images to my server.
  Negative
After I upload images to s3, I got links of them and store them in an array.
  Negative
Then I want to upload those links to my server.
  Negative
But, in order to cover uploading against application suspending I want to make this process run in background.
  Negative
The schema is below.
  Negative
Upload images to s3 -> get s3 links of these images -> upload links to server I'm trying to use presignedURL utility of Amazon SDK since Amazon recommends to use it for background uploading.
  Negative
iOS API docs says NSURLSessionUploadTask does what I'm trying to do.
  Negative
Firstly, I couldn't manage to upload images to s3 with presignedURL (I managed to upload with TransferManagerUploadRequest).
  Negative
Secondly, this process should be running even if application is suspended.
  Negative
But I don't know how to do that actually.
  Negative
I put my "upload-to-s3" code below which is not working right now.
  Negative
Sorry for the complicated question, it indeed shows my mind right now.
  Positive
563e0f9f2d1761a701f0fa85	X	I think it's cool if it can.
  Negative
Maybe you can send e-mail to support@github.com
563e0f9f2d1761a701f0fa86	X	Is there a way to override the file in the Downloads section on GitHub when uploading a file with the same filename?
  Neutral
(e.g. via developer API or the ruby script, etc) The reason is that I want to keep track of the number of downloads.
  Negative
Thanks!
  Positive
563e0f9f2d1761a701f0fa87	X	I havn't tried this but it's possible that you could replace the file on Amazon S3.
  Negative
I don't know if it will work or if it's a one-time upload token you get without the posibility to delete the file.
  Negative
See the API documentation for uploading a file on Github (which includes using the amazon s3 rest api to upload the file): http://developer.github.com/v3/repos/downloads/ And API documentation for deleing a file on amazon s3: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectDELETE.html And API documentation for putting a file on amazon s3: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectPUT.html
563e0fa02d1761a701f0fa88	X	I have an app using RestKit successfully.
  Very negative
I am building an IAP in the app and for iOS5 I needed a place to host the app files for the IAP.
  Negative
I decided using Amazon S3 for that.
  Neutral
Now I now that I can integrate amazon API but I wish to keep my app simple, and since I am using RestKit I just want to use it to download the files.
  Negative
Is there a guide or explanation on how to generate a bucket url with expiration and secrets ?
  Negative
Thanks Shani
563e0fa02d1761a701f0fa89	X	Sure: all the information you need is in the Authenticating REST Requests documentation page.
  Negative
Also, it's not entirely clear from your question, but I hope you're putting the URL generation in some web app somewhere that you control, rather than directly embedding it in the IOS app.
  Negative
I also hope you're using IAM to restrict that key to the appropriate permissions level regardless.
  Positive
563e0fa02d1761a701f0fa8a	X	I'm currently serving up static images to my site via Amazon Cloudfront and for some reason my images won't update when I try to overwrite them with an updated image.
  Negative
The old image continues to display.
  Positive
I've even tried deleting the entire images folder and uploading the newest ones without success.
  Negative
The only thing that works is renaming the image.
  Neutral
Anyone else experience this?
  Neutral
563e0fa02d1761a701f0fa8b	X	recently amazon s3 announced new feature called content invalidation.
  Negative
it allows you to invalidate some file just with a one call.
  Positive
check cloudfront api references for more details.
  Negative
563e0fa02d1761a701f0fa8c	X	Actually I think these are too many questions for one question.
  Negative
But some of them are really interesting.
  Positive
563e0fa02d1761a701f0fa8d	X	Yeah, actually the is only one question: Where we can find materials about Amazon MapReduce best practices for logs analysis?
  Negative
Updated the description.
  Neutral
563e0fa02d1761a701f0fa8e	X	I'm parsing access logs generated by Apache, Nginx, Darwin (video streaming server) and aggregating statistics for each delivered file by date / referrer / useragent.
  Negative
Tons of logs generated every hour and that number likely to be increased dramatically in near future - so processing that kind of data in distributed manner via Amazon Elastic MapReduce sounds reasonable.
  Negative
Right now I'm ready with mappers and reducers to process my data and tested the whole process with the following flow: I've done that manually according to thousands of tutorials that are googlable on the Internet about Amazon ERM.
  Very negative
What should I do next?
  Neutral
What is a best approach to automate this process?
  Positive
What are common practices for: Sure in most cases it depends on your infrastructure and application architecture.
  Negative
Sure I can implement that everything with my custom solution, possibly re-investing a lot of thing that are already used by others somewhere.
  Negative
But there should be some kind of common practices that I would like to become familiar with.
  Negative
I think that this topic can be useful for many people who trying to process access logs with Amazon Elastic MapReduce, but wasn't able to find good materials about best practices to handle that.
  Negative
UPD: Just to clarify here is the single final question: What are best practices for logs processing powered by Amazon Elastic MapReduce?
  Negative
Related posts: Getting data in and out of Elastic MapReduce HDFS
563e0fa22d1761a701f0fa8f	X	That's a very very wide open question, but here are some thoughts you could consider: Hope that gives you some clues.
  Negative
563e0fa32d1761a701f0fa90	X	I have made a ruby on rails application with amw S3.
  Negative
I can upload the photo, but I try to delete the photo, even though the photo reference is deleted in database, the photo is still in S3.
  Negative
How do I actually delete files from S3?
  Negative
563e0fa32d1761a701f0fa91	X	Your question isn't correctly specified.
  Negative
If you could provide more information, it would be great.
  Negative
But here are some options how to handle files on Amazon S3.
  Negative
According to this documentation you can use method delete(see the implementation) from the Ruby Library for Amazon's Simple Storage Service's (S3) REST API.
  Negative
So the should look like this: Very helpful could be also this question.
  Positive
Anyway I highly recommend you to use Paperclip or Carrierwave when uploading files to Amazon S3.
  Negative
Some helpful articles:
563e0fb32d1761a701f0fa92	X	I've been trying to get Authorization for Amazon's s3 rest api going.
  Negative
It's pretty damn complicated.
  Positive
Because I'm trying to make a simple GET request from an admin page on my website, I'm just trying to do this through Javascript.
  Negative
Here are the instructions for constructing the Signature for the Authorization header: To keep us sane, they give us a few examples, with the following givens: The output for this in their docs is bWq2s1WEIj+Ydj0vQ697zp+IXMU=.
  Negative
Based on the following I am getting ZGVjNzNmNTE0MGU4OWQxYTg3NTg0M2MxZDM5NjIyZDI0MGQxZGY0ZQ==: I used code.google.com's CryptoJS.HmacSHA1 function for the SHA1 hashing.
  Negative
My final Signature function looks like this: What is going wrong here???
  Negative
563e0fb32d1761a701f0fa93	X	I actually found the answer from an SO question with reference to google's older (2.0) CrytpoJs library.
  Negative
You need: Then you create your signature as so: I couldn't find a way to to get Strings instead of Bits in the new version.
  Negative
563e0fb32d1761a701f0fa94	X	EC2 instances are persistent if you create them as EBS backed volumes, which you should almost always do.
  Negative
stackoverflow.com/a/3630707/141172
563e0fb32d1761a701f0fa95	X	Hi, Michael - Are you saying my app should directly call S3 iOS API to save photo in S3?
  Negative
I saw a post saying S3 is "eventual consistency" which means after a successful photo upload, the photo may not be immediately available for read.
  Positive
But the iPhone app does need read the photo from server to display on the screen right away.
  Negative
This way the iPhone app will interpret the upload as a "failure" and will try to upload again.
  Negative
563e0fb32d1761a701f0fa96	X	Eric - please check the following stackoverflow.com/questions/2288402/…
563e0fb32d1761a701f0fa97	X	Hi @100calorie, S3 provides read-after-write consistency in most of the regions (see here), so bear this in mind while picking the region for your buckets.
  Negative
563e0fb52d1761a701f0fa98	X	I am creating an iPhone app to allow users upload & share photos.
  Negative
Currently the photos uploaded are stored in my 1and1 cloud server I subscribed.
  Negative
Now I want to try AWS.
  Negative
I have subscribed a free tier AWS Linux EC2 and set up php/mysql.
  Negative
My question is, for scalability purpose, where should I store user pictures: EC2 or S3?
  Negative
And how to connect EC2 with S3 so user uploaded photos will be stored in S3?
  Negative
My understanding is that when user upload a photo to my EC2 instance, it is stored in EC2 and it will fill the space soon since I have only 5GB space.
  Negative
With limited knowledge of AWS, my question may sound st**d but any help and advice will be appreciated!
  Negative
563e0fb52d1761a701f0fa99	X	You should store your pictures in S3, data stored within your EC2 instances are not persistent.
  Negative
Use AWS SDK to upload data to S3.
  Negative
563e2e4161a801306526704e	X	For this use case I would use S3.
  Negative
The advantage of using S3 backing for your pictures is that you can easily use Amazon's Cloud Front CDN with S3 as the origin (you can also use your EC2 instance, but that involves more work).
  Negative
And how to connect EC2 with S3 so user uploaded photos will be stored in S3 There is an S3 API for PHP http://aws.amazon.com/sdkforphp/
563e2e4161a801306526704f	X	I think this might be what you are looking for stackoverflow.com/questions/8310462/…
563e2e4161a8013065267050	X	HI, thanks for the links.
  Negative
can you suggest some video tutorials to get me started?
  Negative
I am totally new to this and find the AWS forums a little overwhelming.
  Negative
563e2e4161a8013065267051	X	@user1652930 Afraid I can't.
  Negative
There might be some out there but I can't stand video tutorials personally and thus never remember them when I see them.
  Negative
563e2e4161a8013065267052	X	I trying to explore AWS S3 and I found out that we can store data and have a URL for a file which can be used on a website, but my intention is to store files on S3 and have users of my website post and retrieve files to/from S3 without my intervention.
  Negative
I am trying to have my server and JSP/Servlets pages on EC2 on which Tomcat (and MySQL server) will be running.
  Negative
Is this possible and if yes, how can i achieve this.
  Neutral
Thanks, SD
563e2e4161a8013065267053	X	Yes, it's possible.
  Positive
A full answer to this question is tantamount to a consulting gig, but some resources that should get you started:
563e2e4161a8013065267054	X	Also, check-out this article on using SimpleDB with Ruby geekin.gs/using-amazon-aws-simpledb-with-ruby-roundup
563e2e4161a8013065267055	X	Are there any online services/servers that could store information like: So that it could be retrieved by Ruby script?
  Very negative
563e2e4261a8013065267056	X	For a simple key/value store in the cloud check out Amazon SimpleDB For complex relational data use a database.
  Negative
If you want a database in the cloud check-out Amazon RDS.
  Negative
563e2e4261a8013065267057	X	Yes, they are also known as databases.
  Neutral
You can set up your own db if you have a server, or you can try and find someone who will host databases for you (try Googling "Free MySQL" for example)
563e2e4261a8013065267058	X	Amazon S3 is an online server that stores your variables, properties, files online and allows you to retrieve them via kind of API.
  Negative
563e2e4261a8013065267059	X	With a small amount of knowledge about how git works, you could easily set up a 1-file rack or Sinatra application on heroku to do this.
  Negative
563e2e4261a801306526705a	X	OpenKeyval Simple Key/Value storage.
  Negative
And it's Free.
  Positive
Not secure though.
  Negative
563e2e4261a801306526705b	X	I need to find a storage service that can programatically (REST API) send (FTP) a file to a third party service.
  Negative
I was thinking of using Amazon S3, but I found a previous similar question here: Sending file from S3 to third party FTP server using CloudFront and apparently it cant be done.
  Negative
What I want to avoid is downloading the file to my app server and then upload it to the third party server.
  Negative
I want to send a command to the storage server to send it directly to the third party server.
  Negative
If I can't find a storage service that delivers this functionality, pricing is my second priority.
  Negative
Right now I'm thinking either box.net, sugarsync or dropbox, as well as amazon S3 since all of these provide a REST API I can use.
  Negative
Thank you for your help!
  Positive
563e2e4261a801306526705c	X	Thanks will give it a go but it looks like what i need
563e2e4261a801306526705d	X	I am now at the point where i cant get any further with out some help.I am trying to host files on the cloud and then access those files via code (C#).
  Negative
So far i have tried Rapidshare and Skydrive and have been unable to get either working at all.
  Negative
Below is a few things that i am trying to do or rather must be able to do with the cloud storage.
  Negative
I don't really mind having to pay as long as the price is not ridiculous.Any help at all will be much appreciated.
  Negative
Thanks Stalkerh
563e2e4261a801306526705e	X	Why don't you look at Amazon S3 they do what you want, are cheap and have a C# API wrapping their web service (But ThreeSharp is better).
  Negative
563e2e4361a801306526705f	X	Yes you're right.
  Positive
I think it was the backslashes that caused the issue.
  Neutral
563e2e4361a8013065267060	X	I'm using the .
  Negative
NET API straight from Amazon to upload some files to S3.
  Negative
However, I'm getting the exception message: The request signature we calculated does not match the signature you provided.
  Negative
Check your key and signing method.
  Neutral
My code is as follows: Is there anything immediately obvious I'm doing wrong?
  Negative
The ProcessFiles method just returns a list of filenames in that directory.
  Negative
563e2e4361a8013065267061	X	Does ProcessFiles return just the filename or the complete path?
  Negative
Regardless, it's unlikely that FilePath and Key should be set to the same thing.
  Negative
FilePath should be set to the full path of the local file to upload.
  Neutral
eg c:\Dev\pktest\myfile.txt Key is the name of the file to store on S3.
  Negative
eg myfile.txt.
  Neutral
Or if you want keep the same path structure: Dev/pktest/myfile.
  Neutral
txt (note the forward slashes)
563e2e4361a8013065267062	X	Amazon supports a single byte range request Response header from the above request correctly shows Content-Length: 500 However if you add another range It ignores the range request and gives the content length of the whole file Content-Length: 1274819234 Does anyone know if amazon s3 supports multiple byte range requests?
  Negative
Or a workaround?
  Neutral
563e2e4361a8013065267063	X	According to the doc as well as api still Amazon S3 does not supports multiple byte range requests.
  Negative
Can you tell us the use-case ?
  Neutral
563e2e4361a8013065267064	X	(1) I can list the files on a folder this way: However, this: or this: does not work.
  Negative
How can I query the files on the root folder?
  Neutral
(2) The code above returns the list of files in a folder.
  Positive
How can I get the list of folders within a folder?
  Neutral
I there any parameter I can set to get this list?
  Negative
Note: I know that this is a 'flat' file system, similar to Amazon S3.
  Negative
However, both (cloudfiles and S3) provides a way to work with 'folder'.
  Neutral
In S3 is easy.
  Positive
In cloudfiles (with the .
  Neutral
net API) I could not find how to do this.
  Negative
Any hint will be highly appreciated.
  Positive
563e2e4361a8013065267065	X	This has just been fixed with the latest push and closes issue #51 on github Link to downloadable package Hope this helps.
  Negative
563e2e4461a8013065267066	X	Great Chris, i didn't know that.
  Negative
Just 15cents per Gig/month?
  Negative
Are you sure ?
  Neutral
Is sooo good to be true =)
563e2e4461a8013065267067	X	I bought this service Chris, great tip you gave me.
  Positive
I already learn how upload files using API, now i'm trying to figure out how to retrieve the url from the object i store there.
  Negative
Thanks dude.
  Positive
563e2e4461a8013065267068	X	Can i upload some image in Smug Mug ?
  Negative
563e2e4461a8013065267069	X	Yes, it is similar to Flickr and Picasa.
  Negative
The API would let you upload.
  Neutral
But, truly, it is more for photo sharing, not sure if there are access restrictions, bandwidth limits or ??
  Negative
.
  Neutral
You should also check out DropBox (I highly recommend it), it will let you have a Public section for files.
  Positive
563e2e4461a801306526706a	X	That's ok Harscware i bought Amazon S3 service, great service dude.
  Positive
Thanks by the help.
  Positive
563e2e4461a801306526706b	X	i'm looking for some api that i can upload an image in their server and then retrieve the url to store in my MySQL.
  Negative
Any advice ?
  Neutral
Best regards, Valter Henrique.
  Positive
563e2e4461a801306526706c	X	Amazon S3 has a nice API, does exactly what you need and is very cheap.
  Negative
15 cents per Gig/month.
  Negative
http://aws.amazon.com
563e2e4461a801306526706d	X	I wonder if the Smug Mug Java API might do the trick.
  Negative
Surely, you should check out the DropBox API (with DropBox you get 2GB storage free, files are sync'd to your desktop).
  Negative
563e2e4461a801306526706e	X	Google App Engine is the first that comes to mind.
  Negative
You have to enable billing, but it's free if you stay under a certain quota.
  Neutral
You can use their Blobstore to serve files up to 2 gigabytes, and it's very fast.
  Negative
A quick search found this overview and guide, but Google's own documentation is excellent.
  Positive
563e2e4561a801306526706f	X	I'm designing a RESTful API that should handle binary file uploads.
  Negative
Should this be done the same way as html form upload or is there a better way?
  Negative
563e2e4561a8013065267070	X	Take a look at the Amazon api for an idea.
  Neutral
It uses a PUT query and then through sendREST it sends the content.
  Neutral
Uploading files to Amazon S3 with REST API
563e2e4561a8013065267071	X	A good way is to upload the binary information using streams.
  Negative
You could have a look at the JeCARS client project.
  Neutral
To be exact the JC_RESTComm.java class performs the upload.
  Neutral
563e2e4561a8013065267072	X	"Taking an input and applying a one-way function is called "hashing" the input, and what Amazon stores on their system is a "hash" of your secret key" - If Amazon stores a HASH of your secret key, how is it possible for Amazon to HASH the text sent to them ?
  Negative
563e2e4561a8013065267073	X	First you say "what Amazon stores on their system is a "hash" of your secret key" and then later "Amazon looks up their copy of the secret key".
  Negative
These seem to contradict each other.
  Negative
I believe the first statement is wrong.
  Negative
563e2e4561a8013065267074	X	foursquare for example have a client ID and a client secret you get when you sign up.
  Negative
And I need to send both to get a response.
  Neutral
So isnt this just going to be able to be seen and intercepted on the 'wire'.
  Positive
They dont make mention of making or forcing https.
  Negative
There is also a common scenario of a serverless App these days so it was a) send the request to FS via the server (still able to see intercepted) b) send via client browser to FS (very easy to spot the client key and secret.)
  Negative
What am I missing...the secret is surely like a password you must never SEND it http always https.
  Negative
563e2e4561a8013065267075	X	This url tells more details of Amazon S3 Auth implementation - docs.aws.amazon.com/AmazonS3/latest/dev/S3_Authentication2.html
563e2e4561a8013065267076	X	"Theoretically, any mathematical functions that maps one thing to another can be reversed" - Thats not true, hash functions are the example.
  Negative
it is very easy to show.
  Positive
lets say we have a function that turns words into numbers, based on sum of values(a=1, b=2, c=3 etc).
  Negative
Eg "SO" would be 18 + 14 = 32.
  Negative
So we have changed SO into 32 but if i reveal this function to somebody, and give him number 32, there is no way he can know if our basic word was "SO" or "ZF"(26+6) or one of dozens other possibilities
563e2e4661a8013065267077	X	I am just starting to think about how api keys and secret keys work.
  Negative
Just 2 days ago I signed up for Amazon S3 and installed the S3Fox Plugin.
  Negative
They asked me for both my Access Key and Secret Access Key, both of which require me to login to access.
  Negative
So I'm wondering, if they're asking me for my secret key, they must be storing it somewhere right?
  Negative
Isn't that basically the same thing as asking me for my credit card numbers or password and storing that in their own database?
  Negative
How are secret keys and api keys supposed to work?
  Neutral
How secret do they need to be?
  Neutral
Are these applications that use the secret keys storing it somehow?
  Negative
Thanks for the insight.
  Positive
563e2e4661a8013065267078	X	Basically elaborating on what's outlined here.
  Negative
Here's how it works: let's say we have a function that takes a number from zero through nine, adds three and, if the result is greater than ten, subtracts ten.
  Negative
So f(2) = 5, f(8) = 1, etc.
  Negative
Now, we can make another function, call it f', that goes backwards, by adding seven instead of three.
  Negative
f'(5) = 2, f'(1) = 8, etc.
  Negative
That's an example of a two-way function and its inverse.
  Positive
Theoretically, any mathematical functions that maps one thing to another can be reversed.
  Negative
In practice, though, you can make a function that scrambles its input so well that it's incredibly difficult to reverse.
  Negative
Taking an input and applying a one-way function is called "hashing" the input, and what Amazon stores on their system is a "hash" of your secret key.
  Negative
SHA1 is an example of this kind of "one-way" function, it's also hardened against attacks.
  Positive
The HMAC function builds on established hash functions to use a known key to authenticate a string of text.
  Positive
It works like this: The difference between this and PKI is that this method is RESTful, allowing a minimum number of exchanges between your system and Amazon's servers.
  Positive
Isn't that basically the same thing as asking me for my credit card numbers or password and storing that in their own database?
  Negative
Yes, though the damage someone can do with S3 seems to be limited to draining your account.
  Negative
How secret do they need to be?
  Neutral
Are these applications that use the secret keys storing it somehow?
  Negative
At some point, you're going to have to load the secret key, and with most Unix based systems, if an attacker can get root access they can get the key.
  Negative
If you encrypt the key, you have to have code to decrypt it, and at some point the decryption code has to be plain text so it can be executed.
  Negative
This is the same problem DRM has, except that you own the computer.
  Negative
In many cases, I just put secret keys in a file with limited permissions, and take the usual precautions to prevent my system from being rooted.
  Negative
There are a few tricks to make it work properly with a multiuser system, such as avoiding temporary files and such.
  Negative
563e2e4661a8013065267079	X	Public Key Cryptography is used to defend against very specific attacks, some of which are common.
  Negative
In short this is complex math that allows one to verify that at individual has both the Public and Private Key pair while only knowing the public key.
  Positive
This is very different from a credit card or static password.
  Neutral
As an example if you are authenticating with an OpenSSH server then the server doesn't need the private key.
  Negative
Ideally if Amazon's API database where to be compromised the attacker would have a list of public keys and would be unable to access the user's API using this information.
  Negative
However ideal systems are not always put into practice and i don't know for sure if Amazon is protecting against this attack vector, but they should be.
  Negative
In public key authentication is statistically immune to brute force.
  Positive
Passwords are often dictionary words which can be broken relativity fast.
  Negative
However a private key is a massive number that isn't easy to guess.
  Neutral
If the attacker had the public key then they could perform many guesses "offline" on a super computer, but even then it would take a lot of time and money to break the key.
  Negative
563e2e4661a801306526707a	X	Pause upload?
  Neutral
There isn't a way to do that in PHP.
  Negative
PHP just uses commands to upload a file and they are just executed, you cannot alter them.
  Positive
563e2e4661a801306526707b	X	Is any other way to do the pause and resume functionality ??
  Neutral
but that should work with the help of PHP langauage.
  Neutral
.
  Neutral
Because I have a php web application , from there I need to give the upload data , pause and resume option...
563e2e4761a801306526707c	X	Like I said.
  Negative
I never heard about pause upload in PHP.
  Neutral
PHP is a server side scripting.
  Positive
As soon as you start a process you cannot pause it from client side: eg.
  Negative
Browser
563e2e4761a801306526707d	X	Hi Lorga, Thanks for reply.
  Negative
.
  Neutral
Do u know how the amazon is using the pause and resume functionality inside the console.
  Neutral
.
  Neutral
(Means when we login into Amazon console panel, they have direct upload option for any data.)
  Negative
????
  Neutral
563e2e4761a801306526707e	X	Iorga ... with i :).
  Negative
Don't know ... but I'm sure it's not PHP :)
563e2e4761a801306526707f	X	I am trying to upload the file to amazon s3 bucket using the PHP-REST API and my code is working fine.
  Negative
.
  Neutral
but I want to know how can I implement the pause and resume functionality by using the Rest API.
  Negative
currently I am following one blog post for uploading the file:- http://www.anyexample.com/programming/php/uploading_files_to_amazon_s3_with_rest_api.xml So, I need to implement the reusme and pause fucntionality.
  Negative
.
  Neutral
Plz help me on this.
  Neutral
.
  Neutral
563e2e4761a8013065267080	X	I think your answer is better, supposing the apache commons library is available.
  Negative
563e2e4761a8013065267081	X	Trying to pull an image off of Amazon S3 (returns S3ObjectInputStream) and send it to the mandrill email api (takes a base64-encoded string).
  Negative
How can this be done in Scala?
  Neutral
563e2e4761a8013065267082	X	Here is one solution, there are probably others more efficient.
  Negative
You could (and should) also replace the sun.misc encoder by the apache commons Base64 for a better compatibility.
  Negative
563e2e4761a8013065267083	X	I also managed to do it just using the Apache commons; not sure which approach is better, but figured I'd leave this answer for the record:
563e2e4761a8013065267084	X	Here's a simple encoder/decoder I wrote that you can include as source.
  Negative
So, no external dependencies.
  Negative
The interface is a bit more scala-esque: import io.github.marklister.base64.Base64.
  Negative
_ // Same as Lomig Mégard's answer val b64 = bytes.toBase64
563e2e4761a8013065267085	X	did you make any progress with this?
  Negative
I have a different scenario whereby knox appears to do all the setup (checking debug etc) but the registered callback functions never get invoked from req response or error events.
  Negative
Spent 2 hours double checking all settings.
  Neutral
The URL generated by knox from input configuration is all correct.
  Negative
s3cmd works fine with same settings.
  Positive
563e2e4761a8013065267086	X	To the best of my knowledge, I'm not behind a proxy here at work.
  Negative
As with any corporate connection, I'm sure there are a handful of devices between my desktop and AWS, however I haven't had to worry about configuring proxy servers for any other connections from my desktop.
  Negative
The region parameter is an interesting idea.
  Positive
While we are hosting out of us-east, I wonder if it would help to make that explicit.
  Negative
I only see 'us-standard' as a possible value in the link you shared, however, so it seems unlikely that this will help.
  Negative
I'll give it a try!
  Positive
563e2e4861a8013065267087	X	us-standard is the "default".
  Negative
i see, "us-east-1" is missing in that list.
  Negative
set the region option to "us-east-1", that should fix your problem!!!
  Negative
563e2e4861a8013065267088	X	As the knox docs say, see Amazon's endpoint documentation for the up–to–date list (it seems us-standard is no longer a region name; use us-east-1).
  Negative
563e2e4861a8013065267089	X	I'm trying to integrate an S3 deployment step into my Grunt toolchain to upload the newly built file out to AWS.
  Negative
However, the step always fails silently (claims to succeed but doesn't do anything), and while debugging the results I've found a few different points along the way that are getting hung up.
  Neutral
I'm using grunt-s3 as the package that handles the grunt commands, which in turn calls the knox package which wraps Amazon's S3 API.
  Negative
Here's where things are falling apart: 1) There's a point in the logic where knox uses the fs package to try to get the size of the file it's about to upload via fs.stat(file, callback).
  Very negative
Near as I can tell, the process dies somewhere under the node.js layer between the fs.stat call and the callback getting invoked.
  Negative
I have set breakpoints and 'debugger' statements all over the place in the callback logic and neither node-inspector nor the IntelliJ debugger can seem to catch the process after fs.stat() is called.
  Negative
2) If I hack the knox plugin and change the fs.stat call to fs.statSync(), the process successfully moves forward.
  Negative
However, later in the process I can see knox set up the expected PUT URL with S3 to upload the file and then call stream.pipe() to upload the file.
  Neutral
Nothing seems to happen as a result of the stream.pipe() call, and I can't see any activity on WireShark that indicates an upload between my computer and AWS taking place.
  Negative
However, if I use the command line tool s3cmd to do the upload, the file uploads fine.
  Neutral
I'm about ready to ditch grunt for this step and move to directly invoking s3cmd, but I'd love to do it the grunt way if possible.
  Positive
Anyone have any suggestions as to what might be happening during these two steps?
  Negative
Thanks!
  Positive
563e2e4861a801306526708a	X	are you sitting behind i proxy?
  Negative
if so, knox will not work.
  Negative
if not, how does your s3-config look like?
  Negative
another important thing to notice is the location of your bucket.
  Positive
manually setting the region (in my example "eu-west-1") helped for me, because knox sets region to "us-standard" per default.
  Negative
see a list of possible values here, check in your bucket-properties where yours is located, and set that value manually!
  Negative
here a (for me) working config: }
563e2e4861a801306526708b	X	Thank you for your such a good explanation!
  Positive
Probably we will go with CORS approach and create workaround for IE.
  Positive
563e2e4861a801306526708c	X	Glad to.
  Neutral
If this solves your problem, please mark it as answered.
  Positive
563e2e4861a801306526708d	X	With AWS services we have the Web application running from the S3 bucket and accessing the data through the REST API from Load Balancer (which is set of Node.js applications running on EC2 instance).
  Negative
Currently we have specified URL's as following: But having this setup brought us a set of problems since requests are CORS with this setup.
  Negative
We could workaround CORS with special headers, but that doesn't work with all browsers.
  Negative
What we want to achieve is running API on the same domain but with different path: One of the ideas was to attach the API Load Balancer to the CDN and forward all request to Load Balancer if query is coming on the "/api/*" path.
  Negative
But that doesn't work since our API is using not only HEAD and GET requests, but also POST, PUT, DELETE.
  Negative
Another idea is using second EC2 instance instead of S3 bucket to host website (using some web server like nginx or apache).
  Negative
But that gives too much overhead when everything is in place already (S3 static content hosting).
  Negative
Also if using this scenario we wouldn't get all the benefits of Amazon CloudFront performance.
  Negative
So, could your recommend how to combine Load Balancer and S3, so they would run on same domain, but with different paths?
  Negative
(API on somedomain.com/api and Web App on somedomain.com) Thank you!
  Positive
563e2e4861a801306526708e	X	You can't have an EC2 instance and an S3 bucket with the same host name.
  Negative
Consider what happens when a web browser makes a request to that host name.
  Negative
DNS resolves it to an IP address (or addresses) and the packets of the request are delivered to that address.
  Negative
The address either terminates at the EC2 instance or the S3 bucket, not both.
  Negative
As I understand your situation, you have static web pages hosted on S3 that include JavaScript code that makes various HTTP requests to the EC2 instance.
  Negative
If the S3 web pages are on a different host than the EC2 instance then the same origin policy will prevent the browser from even attempting some of the requests.
  Negative
The only solutions I can see are: The first method is no good, because you almost might as well not use S3 at all in that case.
  Negative
The second case should be okay for you.
  Positive
It should work in any browser, because it's not really CORS.
  Negative
So no CORS headers are needed.
  Negative
But it's tricky.
  Neutral
The third, CORS, approach should be just fine.
  Negative
Your EC2 instance just has to return the proper headers telling web pages from the S3 bucket that it's safe for them to talk to the EC2 instance.
  Negative
563e2e4861a801306526708f	X	I need help with aws s3 Rest auth.
  Negative
I have the next code: I need help i'm geeting the next error: (403) Forbiden.
  Negative
The authentication is done following the steps on this link: http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html.
  Negative
The credentials are: PublicKey: 25496a25b6554f54b5e6 PrivateKey: 551a656b548e8466f555d540156b5a You just need to use the date for the SHA1, there is no need to use the entire the request string.
  Negative
Example headers: Date: Mon, 26 Mar 2007 19:37:58 +0000 Authorization: FXST AKIAIOSFODNN7EXAMPLE:frJIUN8DYpKDtOLCwo//yllqDzg=
563e2e4861a8013065267090	X	Your problem seems to be your canonicalString and your HttpWebRequest , I had the same problem, please see the solution in this post: Amazon S3 REST API 403 error c# That's sad how Amazon API is badly documented
563e2e4861a8013065267091	X	Be careful how you use http statuses like 401, 409 and 412 - they have particular meaning in the HTTP protocol and aren't codes you can just decide to use in some generalised error scenario because you like the way the wording sounds :) '422 unprocessable entity' is probably what you're looking for as a general-purpose "while it was syntactically valid for its media type, we were unable to accommodate the semantics of the submitted request entity" tools.ietf.org/html/rfc4918#section-11.2
563e2e4961a8013065267092	X	Note that 422 is a WebDAV specific extension.
  Very negative
The 400 status code would be more appropriate.
  Negative
563e2e4961a8013065267093	X	It's an extension defined in WebDAV, but still a generic HTTP status code.
  Negative
563e2e4961a8013065267094	X	If you 200 and an extra header for the success, you are tunneling a protocol over HTTP, instead of using HTTP properly.
  Negative
Don't do that.
  Negative
563e2e4961a8013065267095	X	Among the data my application sends to a third-party SOA server are complex XMLs.
  Negative
The server owner does provide the XML schemas (.
  Negative
xsd) and, since the server rejects invalid XMLs with a meaningless message, I need to validate them locally before sending.
  Negative
I could use a stand-alone XML schema validator but they are slow, mainly because of the time required to parse the schema files.
  Negative
So I wrote my own schema validator (in Java, if that matters) in the form of an HTTP Server which caches the already parsed schemas.
  Negative
The problem is: many things can go wrong in the course of the validation process.
  Negative
Other than unexpected exceptions and successful validation: Since it's an HTTP Server I'd like to provide the client with meaningful status codes.
  Negative
Should the server answer with a 400 error (Bad request) for all the above cases?
  Negative
Or they have nothing to do with HTTP and it should answer 200 with a message in the body?
  Neutral
Any other suggestion?
  Neutral
Update: the main application is written in Ruby, which doesn't have a good xml schema validation library, so a separate validation server is not over-engineering.
  Negative
563e2e4a61a8013065267096	X	It's a perfectly valid thinking to map error situations in the validation process to meaningful HTTP status codes.
  Negative
I suppose you send the XML file to your validation server as a POST content using the URI to determine a specific schema for validation.
  Negative
So here are some suggestions for error mappings:
563e2e4a61a8013065267097	X	Status code 422 ("Unprocessable Entity") sounds close enough: "The 422 (Unprocessable Entity) status code means the server understands the content type of the request entity (hence a 415(Unsupported Media Type) status code is inappropriate), and the syntax of the request entity is correct (thus a 400 (Bad Request) status code is inappropriate) but was unable to process the contained instructions.
  Very negative
For example, this error condition may occur if an XML request body contains well-formed (i.e., syntactically correct), but semantically erroneous, XML instructions."
  Negative
563e2e4a61a8013065267098	X	Amazon could be used as a model for how to map http status codes to real application level conditions: http://docs.amazonwebservices.com/AWSImportExport/latest/API/index.html?Errors.html (see Amazon S3 Status Codes heading)
563e2e4a61a8013065267099	X	Say you're posting XML files to a resource, eg like so: POST /validator Content-type: application/xml If the request entity fails to parse as the media type it was submitted as (ie as application/xml), 400 Bad Request is the right status.
  Very negative
If it parses syntactically as the media type it was submitted as, but it doesn't validate against some desired schema, or otherwise has semantics which make it unprocessable by the resource it's submitted to - then 422 Unprocessable Entity is the best status (although you should probably accompany it by some more specific error information in the error response; also note it's technically defined in an extension to HTTP, WebDAV, although is quite widely used in HTTP APIs and more appropriate than any of the other HTTP error statuses when there's a semantic error with a submitted entity).
  Negative
If it's being submitted as a media type which implies a particular schema on top of xml (eg as application/xhtml+xml) then you can use 400 Bad Request if it fails to validate against that schema.
  Negative
But if its media type is plain XML then I'd argue that the schema isn't part of the media type, although it's a bit of a grey area; if the xml file specifies its schema you could maybe interpret validation as being part of the syntactic requirements for application/xml.
  Negative
If you're submitting the XML files via a multipart/form or application/x-www-form-urlencoded form submissions, then you'd have to use 422 Unprocessable Entity for all problems with the XML file; 400 would only be appropriate if there's a syntactic problem with the basic form upload.
  Very negative
563e2e4a61a801306526709a	X	From w3c: 400 = The request could not be understood by the server due to malformed syntax.
  Negative
I wouldn't serve that up unless it was actually the case that the server could not understand the request.
  Negative
If you're just getting invalid xml, serve a 200 and explain why things are not working.
  Negative
Regards Fake
563e2e4a61a801306526709b	X	I'd go with 400 Bad request and a more specific message in the body (possibly with a secondary error code in a header, like X-Parse-Error: 10451 for easier processing)
563e2e4a61a801306526709c	X	That sounds like a neat idea, but the HTTP status codes don't really provide an "operation failed" case.
  Very negative
I would return HTTP 200 with an X-Validation-Result: true/false header, using the body for any text or "reason" as necessary.
  Negative
Save the HTTP 4xx for HTTP-level errors, not application-level errors.
  Negative
It's kind of a shame and a double-standard, though.
  Negative
Many applications use HTTP authentication, and they're able to return HTTP 401 Not Authorized or 403 Forbidden from the application level.
  Negative
It would be convenient and sensible to have some sort of blanket HTTP 4xx Request Rejected that you could use.
  Negative
563e2e4b61a801306526709d	X	I am using the store function from filepicker's JavaScript API to upload files to Amazon S3, like so: The files are fairly big (several megabytes).
  Negative
Therefore, the users should have the option to suspend the upload after it has been started.
  Negative
Here's the question: How would you go about stopping the upload?
  Neutral
Looking at the API documentation, the only thing I can think of is finishing the upload and deleting the file from the server afterwards.
  Negative
That feels wrong, though.
  Negative
Any suggestions?
  Neutral
563e2e4b61a801306526709e	X	There is currently no way of suspending or canceling an upload once it's in progress.
  Negative
563e2e4b61a801306526709f	X	Please provide some sample code so that we can see what your PHP script is doing.
  Negative
563e2e4c61a80130652670a0	X	How can you be sure that the delay is caused by your PHP script and not your browser/location?
  Negative
You might need to run a few benchmarks before you can be 100% sure that PHP is to blame.
  Negative
It might be a server hardware issue (out of your hands) or a local software issue (out of your hands for other users).
  Negative
563e2e4c61a80130652670a1	X	the readfile() function takes 3 seconds to completely read the .
  Negative
mp3 file, tested using microtime() functions
563e2e4c61a80130652670a2	X	I want to allow only registered user's to download a .
  Negative
mp3 file.
  Neutral
So, I decided to hide the actual location of the .
  Neutral
mp3 file and allow downloads using http://example.com/donwload.php?mp3_name=1 The File donwload.php checks weather the user is logged in and then uses readfile() to read the file location in a folder that is not shown the the user.
  Negative
The problem here is that, accessing http://example.com/donwload.php?mp3_name=1 for a .
  Negative
mp3 file of 500kb takes 3 seconds to load (the save dialog box to appear) Is there any other way to do so or read files quickly in PHP?
  Negative
Thanks Akash
563e2e4c61a80130652670a3	X	Really short answer, as I'm running out.
  Negative
(Will provide more information later).
  Neutral
This answer also takes an alterntive approach to what you are trying to do.
  Negative
If possible you should use a system that has a dedicated ACL backing all of the files which are stored on it.
  Positive
For instance, if you go with Amazon S3, then you can provide your own ACL for each object that is stored within a bucket, and you can also generate links on the fly that are valid and signed for only X number of minutes.
  Negative
Given your scenario, what you could do is store every MP3 file that you have on something like Amazon S3 (There are others out there so don't feel like you have to use S3), and then when a user makes a purchase and the transaction is confirmed, you can use the S3 API to generate a link for the image.
  Negative
It would be something like : get_object_url( 'my-mp3s.
  Negative
com', 'albums/Foo/bar.
  Negative
mpg') You will then get a URL which you can provide to the customer.
  Negative
Alternatively, you can ask Amazon to generate a URL that expired within 15 minutes.
  Negative
563e2e4d61a80130652670a4	X	You could use file_get_contents instead of readfile() http://php.net/manual/en/function.file-get-contents.php or the X-Sendfile apache module.
  Negative
563e2e4d61a80130652670a5	X	If you have your mp3 on a different server you could install lighttpd with modsecdownload or run lighttpd on a different port if you have only one machine only http://redmine.lighttpd.net/wiki/1/Docs:ModSecDownload so you than you could generate only for logged in users a download link, which is valid for few mins - hours, and they can download it.
  Negative
I think this is better, cause it will use less memory than reading the file and sending it to the user, if you have much traffic.
  Negative
Hope this helps!
  Positive
563e2e4d61a80130652670a6	X	Not adding an answer unfortunately: I see the same pros and cons of each route that you do.
  Negative
I can help with the S3 REST APIs, though; ASIHTTPRequest has slick S3 support: allseeing-i.
  Positive
com/ASIHTTPRequest/S3
563e2e4d61a80130652670a7	X	Matthew, thanks for the tip on ASIHTTPRequest.
  Negative
Is there any reason it's preferred over the iOS Beta SDK that Amazon provides for AWS?
  Negative
563e2e4d61a80130652670a8	X	No reason that I know of.
  Negative
I'm just had good success with the ASI library in a wide variety of data transfer situations, so haven't bothered with anything else.
  Negative
563e2e4d61a80130652670a9	X	Joe, I don't think I can use iOS Keychain because it's meant to store sensitive data in a secure way so it can't be extracted by other users or malicious apps.
  Negative
However, the user themselves can extract items from it.
  Neutral
See this: blog.crackpassword.com/2010/08/peeking-inside-keychain-secrets
563e2e4e61a80130652670aa	X	Update: Looks like there are ultimately 2 ways to do this.
  Negative
First, it can be proxied through my server which has downsides of placing my server in the middle of every transaction.
  Neutral
Advantage of this is there are fewer points of error with multiple legs of communication.
  Negative
Second approach is to use "pre-signed URLs" with AWS that Adrian Petrescu pointed out.
  Negative
563e2e4e61a80130652670ab	X	Thanks for the tip on the AWS IAM.
  Negative
Too bad it's still in beta.
  Neutral
Can you elaborate on the "presigned URLs" solution you mention?
  Positive
Is this just using HTTP POST with json policy doc?
  Negative
563e2e4e61a80130652670ac	X	Hey TMC, I added more details (and two links) about presigned URLs to my answer.
  Negative
Hope that helps :)
563e2e4e61a80130652670ad	X	In my quick read, the client would have to request the pre-signed URL from my server since it's based on the AWS secret key.
  Negative
Then it would use that pre-signed URL to do it's file upload.
  Neutral
So essentially, this is no different than the HTTP POST method mentioned earlier, correct?
  Negative
563e2e4e61a80130652670ae	X	You would still need a server of your own, but by using presigned URLs this server's job is much easier -- all he has to do is return a URL, not do the upload himself as hipplar is suggesting.
  Negative
That's a huge difference.
  Positive
563e2e4e61a80130652670af	X	Presumably after the upload is completed to S3, the client should tell the server it was successful?
  Negative
Additionally, why in the world does the AWS iOS SDK have "S3GetPreSignedURLRequest" which requires the access key to be on the client?
  Negative
563e2e4e61a80130652670b0	X	Shadowmatter is right.
  Positive
Uploading directly to S3 is the better option.
  Negative
His code worked great.
  Positive
I've placed my fork into a gist that contains some example Python code in addition to the Objective-C code.
  Negative
This is also a great way to get around Heroku's 30 second Request Timeout issue.
  Positive
563e2e4e61a80130652670b1	X	I'm trying to implement the above but keep getting 405 error - method not allowed.
  Negative
Do I need to modify the bucket's policy to enable the above?
  Neutral
563e2e4e61a80130652670b2	X	@maethorr Although I'm no longer on this project, I would expect a bad policy to generate a 401 or 403 error code.
  Negative
My best guess is that you're accidentally using POST and not PUT?
  Negative
563e2e4e61a80130652670b3	X	@shadowmatter found out the issue.
  Negative
I PUT onto the wrong end-point (the website endpoint <bucketname>.
  Negative
s3-website-ap-southeast-2.
  Neutral
amazonaws.com instead of just <bucketname>.
  Negative
s3.amazonaws.com).
  Neutral
Thank you for your reply.
  Positive
563e2e4e61a80130652670b4	X	Major +1 for the EC2<->S3 info.
  Negative
563e2e4e61a80130652670b5	X	We run in Azure so moving to EC2 not an option.
  Negative
I mentioned in my original post that my server being proxy doesn't appear to be the only way since amazon supports uploads by HTTP POST with json policy files.
  Negative
If there is a way to get away from my server being the middleman that is the ideal approach for obvious reasons.
  Positive
563e2e4e61a80130652670b6	X	I prefer to upload the image directly but still store a reference and meta data on my own web server db.
  Negative
563e2e4f61a80130652670b7	X	Joe, amazon provides an iPhone SDK which explains Dat's confusion.
  Negative
563e2e4f61a80130652670b8	X	I want to allow users of an iPhone app to upload photos and use Amazon S3.
  Negative
There are 2 ways I see going about this: For option 1, the security is straightforward.
  Positive
I don't ever have to tell the iPhone my S3 secret.
  Negative
Downside is that everything is proxied through our server for uploads which sort of defeats the purpose of going to S3.
  Negative
For option 2, in theory it's better but the issue is how do you enable the iPhone (or any mobile app on a different platform) to write into my S3 bucket without giving it my secret?
  Negative
Additionally, I'm not sure if this is a good design or not because the flow would be: iphone uploads to S3, gets the URL, then tells the server what the URL is so it can add it to our database to reference in the future.
  Very negative
However, since the communication is separated into 2 legs (iphone->S3 vs iPhone->My-Server) it leaves it fragile as a non-atomic operation.
  Neutral
I've found some older info that references using Browser-based Uploads using POST but unsure if that is still the recommended approach.
  Negative
I'm hoping for a better solution where we can just use the REST APIs rather than relying on POST.
  Negative
I've also see the AWS iOS Beta SDK, but their docs didn't help much and I found an Amazon article that was equally unhelpful as it cautions you on what not to do, but doesn't tell you an alternative approach: The mobile AWS SDKs sign the API requests sent to Amazon Web Services (AWS) in order to validate the identity of the AWS account making the request.
  Negative
Otherwise, a malicious developer could easily make requests to another developer's infrastructure.
  Negative
The requests are signed using an AWS Access Key ID and a Secret Access Key that AWS provides.
  Negative
The Secret Access Key is similar to a password, and it is extremely important to keep secret.
  Positive
Tip: You can view all your AWS security credentials, including Access Key ID and Secret Access Key, on the AWS web site at http://aws.amazon.com/security-credentials.
  Negative
Embedding credentials in source code is problematic for software, including mobile applications, because malicious users can de-compile the software or view the source code to retrieve the Secret Access Key.
  Negative
Does anyone have any advice on the best architectural design and flow for this?
  Neutral
Update: The more I dig into this, it seems that a bunch of pople recommend using the HTTP POST method with the json policy file as described here: http://docs.amazonwebservices.com/AmazonS3/2006-03-01/dev/index.html?UsingHTTPPOST.html.
  Negative
With this, the flow would be something like (1) iPhone makes request to my server, asking for policy file (2) server generates json policy file and gives back to client (3) iPhone does HTTP POST of photo + json policy to S3.
  Negative
I hate that I'm using HTTP POST in an apparently kludgy way but it appears to be better because it removes the need for my server to store the photo at all.
  Negative
563e2e4f61a80130652670b9	X	I've discussed this issue on the AWS forums before.
  Negative
As I say there, the proper solution for accessing AWS from a mobile device is to use the AWS Identity and Access Management service to generate temporary, limited-privilege access keys for each user.
  Negative
The service is great, but it's still in beta for now and it's not part of the mobile SDK yet.
  Positive
I have a feeling once this thing is released for good, you'll see it out on the mobile SDK immediately afterwards.
  Negative
Until then, generate presigned URLs for your users, or proxy through your own server like some others have suggested.
  Negative
The presigned URL will allow your users to temporarily GET or PUT to an S3 object in one of your buckets without actually having your credentials (they are hashed into the signature).
  Negative
You can read about the details here.
  Neutral
EDIT: I've implemented a proper solution for this problem, using the preview beta of IAM.
  Negative
It's available on GitHub, and you can read about it here.
  Positive
563e2e4f61a80130652670ba	X	You can upload directly from your iPhone to S3 using the REST API, and having the server be responsible for generating the part of the Authorization header value that requires the secret key.
  Negative
This way, you don't risk exposing the access key to anyone with a jailbroken iPhone, while you don't put the burden of uploading the file on the server.
  Negative
The exact details of the request to make can be found under "Example Object PUT" of "Signing and Authenticating REST Requests".
  Negative
I strongly recommend reading that document before proceeding any further.
  Positive
The following code, written in Python, generates the part of the Authorization header value that is derived from your S3 secret access key.
  Negative
You should substitute your own secret access key and bucket name in virtual host form for _S3_SECRET and _S3_BUCKET_NAME below, respectively: Calling this with the filename foo.txt and content-type text/plain yields: Note that if you run this code, the time returned will be different, and so the encoded HMAC digest will be different.
  Negative
Now the iPhone client just has to issue a PUT request to S3 using the returned date and HMAC digest.
  Negative
Assuming that Then you can do the following to create the NSURLRequest: Next you can issue the request.
  Neutral
If you're using the excellent AFNetworking library, then you can wrap request in an AFXMLRequestOperation object using XMLDocumentRequestOperationWithRequest:success:failure:, and then invoking its start method.
  Negative
Don't forget to release headers and request when done.
  Neutral
Note that the client got the value of the Date header from the server.
  Neutral
This is because, as Amazon describes under "Time Stamp Requirement": "A valid time stamp (using either the HTTP Date header or an x-amz-date alternative) is mandatory for authenticated requests.
  Negative
Furthermore, the client time-stamp included with an authenticated request must be within 15 minutes of the Amazon S3 system time when the request is received.
  Negative
If not, the request will fail with the RequestTimeTooSkewed error status code. "
  Negative
So instead of relying on the client having the correct time in order for the request to succeed, rely on the server, which should be using NTP (and a daemon like ntpd).
  Negative
563e2e4f61a80130652670bb	X	Upload to your server and then post to S3.
  Negative
From an architecture standpoint you will want to do this from your server.
  Neutral
There are many things that could go wrong during the data transfer you can handle better on the server and if you want to store any data about the image you are sending to S3 you are probably going to have a server side call anyway.
  Negative
Plus, your Secret Access Key is stored in a more secure environment.
  Negative
Your own.
  Neutral
If you are worried about scalability and you are going to be doing a considerable number of S3 transfers I would consider hosting your server on and EC2 instance.
  Negative
Transferring data between the two is free (given you store you data in following data centers).
  Negative
There is no Data Transfer charge for data transferred between Amazon EC2 and Amazon S3 within the same Region or for data transferred between the Amazon EC2 Northern Virginia Region and the Amazon S3 US Standard Region."
  Negative
Amazon Simple Storage Service (Amazon S3) There are many post here on SO Amazon - EC2 cost?
  Negative
(example) about the pros and cons of using EC2.
  Neutral
563e2e4f61a80130652670bc	X	I m confused.
  Negative
Why would amazon come up w/ the ios sdk to upload data to s3 then tell us not to use it (Embedding credentials in source code is problematic for software, including mobile applications, because malicious users can de-compile the software or view the source code to retrieve the Secret Access Key)???
  Negative
563e2e4f61a80130652670bd	X	they might have provided the sdk for the purpose that maybe the application could permit authentication to individual s3 accounts?
  Negative
e.g an app that lets users store files in their own (user's) bucket instead of provider?
  Negative
i feel a security flaw in merging the keys with application and distributing it.
  Negative
anyone can (mis)use them once the keys are revealed anyhow (its never secure when you're giving it out).
  Negative
on the other hand, keeping the functionality reserved to server will keep your keys transparent to user,isn't it?
  Negative
563e2e5061a80130652670be	X	possible duplicate of Retrieve bucket's objects without knowing bucket's region with AWS S3 REST API
563e2e5061a80130652670bf	X	I want to obtain the location of a bucket in Amazon S3.
  Negative
According to the documentation I should use the API GET location (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html).
  Negative
My problem is that, again according to the API, I have to include an authorization header which in the latest version (v4) needs the region where the bucket is located (http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html).
  Negative
So, how can I sign the authorization, giving that I don't know the region where the bucket is located?
  Negative
563e2e5061a80130652670c0	X	Could you give some sample code of such thin S3 wrapper?
  Negative
563e2e5061a80130652670c1	X	I'm writing an application in Java that will upload a file up to AWS S3.
  Negative
The file will be given to the application in an argument, not hardcoded.
  Negative
I'd like to write tests to ensure that the file actually gets uploaded to S3.
  Negative
The test will be written before the code for TDD.
  Neutral
(I have actually already written the code, but I'd like to ingrain TDD practices into all of my work as a habit) How exactly would I go about doing this?
  Negative
I will be using JUnit as that's what I'm most familiar with.
  Negative
Thanks in advance for any help.
  Neutral
563e2e5061a80130652670c2	X	The actual uploading and the tests that are doing it are part of your integration testing, not the unit testing.
  Negative
If you wrap the S3 API in a very thin class, you will mock that class for unit testing of your business classes, and you will use the real implementation for integration testing.
  Negative
If you have decided, your business classes to take directly the AmazonS3 interface, then for unit testing you have to mock that one.
  Negative
The actual exploratory testing (learning and verifying) if and how amazon s3 works is what you actually do in separate experimental setup.
  Positive
P.S. I do not recommend using the AmazonS3 interface directly in your business classes, rather, wrap it in a thin interface of yours, so that if you decide to change the 'back-end storage' you can easily change it.
  Very negative
563e2e5061a80130652670c3	X	I'm not a Java programmer but you probably want to look into mocking.
  Negative
There is a SoapUI tool called MockService that appears to allow mocking of an external service like those provided by AWS.
  Negative
563e2e5061a80130652670c4	X	Op De Cirkel answer is good in unit testing scope but if you are writing framework support or simply need to run the AWS S3 calls during your tests, you can run any service that offer AWS compatible APIs.
  Neutral
OpenStack is one of them and can be run in a virtual machine (see DevStack).
  Negative
Or you can choose from a variety of test-oriented tools that provide AWS compatible APIs.
  Neutral
Here are some that expose S3 service:
563e2e5161a80130652670c5	X	Map a directory that points to your desired root folder under your applications root
563e2e5161a80130652670c6	X	How do I do that?
  Negative
Any examples?
  Neutral
563e2e5161a80130652670c7	X	Tried this and it didn't seem to do anything.
  Negative
Anyway, it looks like there already is a virtual directory for my site in my applicationHost.config file that is pointing to the correct directory.
  Negative
It still seems to be looking for the file in the inetsrv directory I mentioned.
  Negative
563e2e5161a80130652670c8	X	Can you post some code?
  Neutral
563e2e5161a80130652670c9	X	K, edited the question with my controller code
563e2e5161a80130652670ca	X	is request.videopath an absolute path or is it a relative path?
  Negative
an example?
  Neutral
563e2e5161a80130652670cb	X	Relative.
  Neutral
It would be a string like 'myVideo.flv' or 'files/myVideo.
  Neutral
flv'.
  Neutral
563e2e5161a80130652670cc	X	I have a Web Api Application that performs a file upload to Amazon S3 when I POST a file path to it.
  Negative
How do I change the root directory this file path is relative to?
  Neutral
Right now, if I send myVideo.flv as the file path, my app tries to find the file to upload at c:\windows\system32\inetsrv\myVideo.flv.
  Negative
I'd like it to look for the file at c:\MyApp\files\myVideo.flv.
  Negative
Is this something I change in the app config or iis?
  Neutral
Here is my controller method:
563e2e5161a80130652670cd	X	Open IIS, Expand Sites and find your web site, Right click on the web site and select "Add Virtual Directory".
  Negative
This directory can be pointed to anywhere on the machine.
  Positive
In your app, you would read/write to this virtual directory under the root of the application.
  Negative
Also be mindful of permissions, to make sure the users accessing the site have proper permissions to that virtual directory.
  Negative
563e2e5161a80130652670ce	X	Uploading to S3 is a pain in the ass.
  Negative
.
  Neutral
I would not recommend that to anyone.
  Negative
563e2e5161a80130652670cf	X	after creating a instance in amazon cloud using webservice in java i need to transfer a executable file or war file via program from my local machine to the newly created instance in amazon and i want to execute that excetuable,i tried and found that there is something called createbucket in ec2 api and using that we can upload the file to that and we can transfer that reference using PutObjectRequest i can transfer the reference to a remote computer in amazon do it is possible or if it is wrong please suggest me the correct way to proceed for file transfer from my local machine to the amazon ec2.
  Very negative
563e2e5161a80130652670d0	X	The basic suggestion is, you shouldn't transfer the file(s) with CreateBucket, which is actually an S3 API.
  Negative
Use scp may be a better solution.
  Negative
Amazon S3, which you are trying to use with CreateBucket, is a data storage service mainly for flexible, public (with authentication) file sharing.
  Negative
You can use REST or SOAP APIs to access the data, but cannot really read/write it in EC2 instances as if it's in local harddisk.
  Negative
To access file system in EC2 instances, that really depends on your operating system (on EC2).
  Negative
If it's running Linux, scp is a mature choice.
  Positive
You can use Java to directly invoke scp, if you are using Linux locally, or pscp if you are using Windows.
  Negative
If the EC2 instance is running Windows, one choice is to host an SSH/SFTP environment with FreeSSHD, and then proceed like Linux.
  Negative
Another option is use Shared Folder and regular file copy.
  Negative
563e2e5261a80130652670d1	X	Is your code running behind some kind of firewall?
  Neutral
563e2e5261a80130652670d2	X	@Rohit: Yes, the code is running with firewall, is firewall related to this?
  Negative
563e2e5261a80130652670d3	X	I'm not an expert on firewall settings.
  Negative
But it looks like your firewall is signing the incoming data again with ssl certificates hosted on firewall server.
  Negative
That causes the host name mismatch on your code.
  Neutral
Check for it with your systems engineer
563e2e5261a80130652670d4	X	I'contact our system engineer and try to figure this out, thank you @Rohit!
  Negative
563e2e5261a80130652670d5	X	When using JAVA API of SQS (sdk version: 1.9.30), sometimes I got following exceptions: There are some question like mine (i.e SSL problems with S3/AWS using the Java API: "hostname in certificate didn't match"), but seems not the same one.
  Negative
Because in my situation the exception is like: [sqs url] !
  Negative
= [my company's url] looks like when I try to connect to AWS server, the sdk connect to my company's server for some reason, and make the SSL handshake fail.
  Negative
There are other one got the same question (https://github.com/Upplication/Amazon-S3-FileSystem-NIO2/issues/40), but still have no answer.
  Negative
Is this a bug of SDK and any solution for this?
  Neutral
563e2e5261a80130652670d6	X	yes thats what the backticks are for
563e2e5261a80130652670d7	X	I'll recommend not to use reserved words, but if you must use them, then always use backticks.
  Negative
563e2e5261a80130652670d8	X	It's safe - it wouldn't work otherwise.
  Negative
But it's a good idea not to use reserved words if you can, because it makes the code that bit harder to follow.
  Negative
563e2e5261a80130652670d9	X	I recount instances wherein a SQL Function returned results with pure numeric column-names.
  Negative
The situation aggravates with the wrappers of various scripting environment such as PHP.
  Negative
Consider using aliases through AS aliasname in those cases.
  Negative
563e2e5261a80130652670da	X	This question already has an answer here: I need to return a multidimensional array from a query with the id keys named key.
  Negative
(needed for object keys in amazon S3 API) example: The problem: key is a reserved name in MySQL.
  Negative
I have to use the name key.
  Neutral
The following query gives an error but adding backticks around key doesn't give errors Is it safe to continue like this or is there a better way?
  Negative
I was thinking to rename the id key in the array in PHP but doing it in the query seems faster
563e2e5261a80130652670db	X	As stated, use backticks.
  Negative
From the MYSQL Docs Reserved words are permitted as identifiers if you quote them as described in Section 9.2, “Schema Object Names”:
563e2e5261a80130652670dc	X	To be away from reserved keyword around table field in query always considered as the best way...If you are using in reserved keyword in query, then backtick allow you to use reserved keyword... As backtick is not defined in ANSI SQL standard, it'll probably create problem when you migrate from MySQL environment...
563e2e5361a80130652670dd	X	I am using Microsoft MVC I have written a view that uploads files to the Amazon S3 API.
  Very negative
I would like a progress bar in my view to show the progress of the processing of that action in my controller not the uploading of the file in particular.
  Negative
I have tried a few JQUERY/AJAX uploaders but every time I loose the value of The HttpPostedFileBase and the value of the file is null.
  Negative
I basically need to find a progress bar function that supports Post and multipart/form-data.
  Neutral
Code is below ASPX Controller action
563e2e5361a80130652670de	X	Here is an example that I think will help you: Asynchronous Process With Progress Bar
563e2e5361a80130652670df	X	To clarify, I have one folder in my bucket that's available to public where the text file is located at.
  Negative
563e2e5361a80130652670e0	X	The second link is something that looks like it's going to work out great for me.
  Positive
Thanks again
563e2e5361a80130652670e1	X	yeah.
  Positive
IIRC that's sigv3.
  Neutral
you can find sigv4 examples but the signing is more complicated
563e2e5361a80130652670e2	X	here is also a sigv4 example: gist.github.com/vszakats/2917d28a951844ab80b1
563e2e5361a80130652670e3	X	As the title says, is it possible to upload to S3 via shell script without aws-cli-tools?
  Negative
If so, how?
  Neutral
What I'm trying to do is read from a txt file on S3 (which is public, so no authentication is required).
  Negative
But I want to be able to overwrite whatever is in the file (which is just a number).
  Negative
Thanks in advance, Fadi
563e2e5361a80130652670e4	X	Yes you can!
  Positive
You basically emulate the api calls the SDK would do for you through standard linux cmd utils.
  Neutral
Look at: https://aws.amazon.com/code/Amazon-S3/943 and/or http://tmont.com/blargh/2014/1/uploading-to-s3-in-bash
563e2e5361a80130652670e5	X	I use s3cmd which is a command line tool written in Python.
  Negative
It uses the (restful) web APIs.
  Neutral
would be the interesting bits: Synchronize a directory tree to S3 s3cmd sync LOCAL_DIR s3://BUCKET[/PREFIX] or s3://BUCKET[/PREFIX] LOCAL_DIR
563e2e5461a80130652670e6	X	It's also worth noting a caveat with reading spot price history from the API - it will only show changes between your start+end time.
  Negative
Some instance types aren't supported in certain availability zones (and detecting this is quite hard since if you ask for the last 24 hours of spot price data an unchanged price would look like an unsupported instance type in that region (or vice versa)
563e2e5461a80130652670e7	X	Note that as of April 2014, those URLs are deprecated and the pricing info is outdated.
  Negative
They're now at a0.awsstatic.com/pricing/1/ec2/linux-od.
  Neutral
min.js and a0.awsstatic.com/pricing/1/s3/pricing-storage-s3.
  Negative
min.js.
  Neutral
If you look in the source of any pricing page and search for "json" you'll find the appropriate links.
  Positive
563e2e5461a80130652670e8	X	@TimDorr can you edit the post, or post your answer if the above is now outdated?
  Negative
563e2e5461a80130652670e9	X	Nice!
  Positive
I was going to get around to this eventually myself, but you've saved me the trouble.
  Negative
563e2e5461a80130652670ea	X	Very nice but i noted it doesnt include the previous generation instances like t1.micro - is there another .
  Negative
js file that contains those prices ?
  Neutral
563e2e5461a80130652670eb	X	i wasn't able to find anything for base pricing, however deltacloud is a very interesting project for cross-cloud information and management deltacloud.org
563e2e5461a80130652670ec	X	I also did not see a general API for pricing.
  Negative
The closest I found was the spot price history that brokenbeatnik described.
  Positive
563e2e5461a80130652670ed	X	Are there any API's that have up-to-date pricing on Amazon Web Services?
  Negative
Something that can be queried, for example, for the latest price S3 for a given region, or EC2, etc. thanks
563e2e5561a80130652670ee	X	This is something I have asked for (via AWS evangelists and surveys) previously, but hasn't been forthcoming.
  Negative
I guess the AWS folks have more interesting innovations on their horizon.
  Negative
As pointed out by @brokenbeatnik, there is an API for spot-price history.
  Negative
API docs here: http://docs.amazonwebservices.com/AWSEC2/latest/APIReference/ApiReference-query-DescribeSpotPriceHistory.html I find it odd that the spot-price history has an official API, but that they didn't do this for on-demand services at the same time.
  Negative
Anyway, to answer the question, yes you can query the advertised AWS pricing... The best I can come up with is from examining the (client-side) source of the various services' pricing pages.
  Positive
Therein you'll find that the tables are built in JS and populated with JSON data, data that you can GET yourself.
  Negative
E.g.: That's only half the battle though, next you have to pick apart the object format to get at the values you want, e.g., in Python this gets the Hi-CPU On-Demand Extra-Large Linux Instance pricing for Virginia: Disclaimer: Obviously this is not an AWS sanctioned API and as such I wouldn't recommend expecting stability of the data format or even continued existence of the source.
  Very negative
But it's there, and it beats transcribing the pricing data into static config/source files!
  Negative
563e2e5561a80130652670ef	X	For the people who wanted to use the data from the amazon api who uses things like "t1.micro" here is a translation array
563e2e5561a80130652670f0	X	I've create a quick & dirty API in Python for accessing the pricing data in those JSON files and converting it to the relevant values (the right translations and the right instance types).
  Negative
You can get the code here: https://github.com/erans/ec2instancespricing And read a bit more about it here: http://forecastcloudy.net/2012/04/03/quick-dirty-api-for-accessing-amazon-web-services-aws-ec2-pricing-data/ You can use this file as a module and call the functions to get a Python dictionary with the results, or you can use it as a command line tool to get the output is a human readable table, JSON or CSV to use in combination with other command line tools.
  Negative
563e2e5561a80130652670f1	X	There is a nice API available via the link below which you can query for AWS pricing.
  Positive
http://info.awsstream.com If you play around a bit with the filters, you can see how to construct a query to return the specific information you are after e.g. region, instance type etc.
  Negative
For example, to return a json containing the EC2 pricing for the eu-west-1 region linux instances, you can format your query as per below.
  Negative
http://info.awsstream.com/instances.json?region=eu-west-1&os=linux Just replace json with xml in the query above to return the information in an xml format.
  Negative
Note - similar to the URL's posted by other contributors above, I don't believe this is an officially sanctioned AWS API.
  Negative
However, based on a number of spot checks I've made over the last couple of days I can confirm that at time of posting the pricing information seems to be correct.
  Neutral
563e2e5561a80130652670f2	X	I don't believe there's an API that covers general current prices for the standard services.
  Negative
However, for EC2 in particular, you can see spot price history so that you don't have to guess what the market price for a spot instance is.
  Neutral
More on this is available at: http://docs.amazonwebservices.com/AWSEC2/latest/DeveloperGuide/using-spot-instances-history.html
563e2e5561a80130652670f3	X	I too needed an API to retrieve AWS pricing.
  Negative
I was surprised to find nothing especially given the large number of APIs available for AWS resources.
  Negative
My preferred language is Ruby so I wrote a Gem to called AWSCosts that provides programmatic access to AWS pricing.
  Negative
Here is an example of how to find the on demand price for a m1.medium Linux instance.
  Neutral
AWSCosts.region('us-east-1').
  Neutral
ec2.on_demand(:linux).
  Neutral
price('m1.
  Positive
medium')
563e2e5561a80130652670f4	X	I made a Gist of forward and reverse names in Yaml should anyone need them for Rails, etc.
563e2e5561a80130652670f5	X	Another quick & dirty, but with a conversion to a more convenient final data format
563e2e5561a80130652670f6	X	Here is another unsanctioned "api" which covers reserved instances: http://aws.amazon.com/ec2/pricing/pricing-reserved-instances.json
563e2e5661a80130652670f7	X	There is no pricing api, but there are very nice price mentioned above.
  Negative
In the addition to the ec2 price ripper I'd like to share my rds and elasticache price rippers: https://github.com/evgeny-gridasov/rdsinstancespricing https://github.com/evgeny-gridasov/elasticachepricing
563e2e5661a80130652670f8	X	There is a reply to a similar question which lists all the .
  Negative
js files containing the prices, which are barely JSON files (with only a callback(...); statement to remove).
  Negative
Here is an exemple for Linux On Demand prices : http://aws-assets-pricing-prod.s3.amazonaws.com/pricing/ec2/linux-od.js (Get the full list directly on that reply)
563e2e5661a80130652670f9	X	For those who need the comprehensive AWS instance pricing data (EC2, RDS, ElastiCache and Redshift), here is the Python module grown from the one suggested above by Eran Sandler: https://github.com/ilia-semenov/awspricingfull It contains previous generation instances as well as current generation ones (including newest d2 family), reserved and on-demand pricing.
  Negative
JSON, Table and CSV formats available.
  Negative
563e2e5661a80130652670fa	X	Some quick questions: Thanks
563e2e5661a80130652670fb	X	Amazon S3 is an object store, not a filesystem.
  Negative
It has a specific set of APIs for uploading, listing, downloading, etc but it does not behave like a normal filesystem.
  Negative
There are some utilities that can mount S3 as a filesystem (eg Expandrive, Cloudberry Drive, s3fs), but in the background these utilities are actually translating requests into API calls.
  Negative
This can cause some issues -- for example, you can modify a 100MB file on a local disk by just writing one by to disk.
  Negative
If you wish to modify one byte on S3, you must upload the whole object again.
  Negative
This can cause synchronization problems between your computer and S3, so such methods are not recommended for production situations.
  Negative
(However, they're a great way of uploading/downloading initial data.)
  Neutral
A good in-between option is to use the AWS Command-Line Interface (CLI), which has commands such as aws s3 cp and aws s3 sync, which are reliable ways to upload/download/sync files with Amazon S3.
  Negative
To answer your questions... Amazon S3 does not support a "soft link" (symbolic link).
  Negative
Amazon S3 is an object store, not a file system, so it only contains objects.
  Negative
Objects can also have meta-data that is often for cache control, redirection, classification, etc.
  Negative
Amazon S3 does not support directories (sort of).
  Negative
Amazon S3 objects are kept within buckets, and the buckets are 'flat' -- they do not contains directories/sub-folders.
  Negative
However, it does maintain the illusion of directories.
  Neutral
For example, if file bar.jpg is stored in the foo directory, then the Key (filename) of the object is foo/bar.
  Negative
jpg.
  Neutral
This makes the object 'appear' to be in the foo directory, but that's not how it is stored.
  Negative
The AWS Management Console maintains this illusion by allowing users to create and open Folders, but the actual data is stored 'flat'.
  Negative
This leads to some interesting behaviours: The above-mentioned utilities take all this into account when allowing an Amazon S3 bucket to be mounted.
  Positive
They translate 'normal' filesystem commands into Amazon S3 API calls, but they can't do everything (eg they might emulate renaming a file but they typically won't let you rename a directory).
  Negative
563e2e5861a80130652670fc	X	Yes, that is an interesting alternative.
  Positive
For the OP's statistical analysis this might be less useful -- all strings would be have the same length.
  Negative
563e2e5961a80130652670fd	X	Ugh, that sounds like a great way to annoy users.
  Negative
563e2e5a61a80130652670fe	X	Yeah, that's the idea.
  Neutral
It's basically to do the job for the leechers a bit harder.
  Neutral
563e2e5a61a80130652670ff	X	Thank you for your answer.
  Positive
Indeed it sounds like exactly what I need.
  Neutral
I think I gotta do a lot of research, hiring someone very experienced to do this, or simply move onto using Rapidshare.
  Positive
On the other hand, I know Amazon has vast amounts of bandwith, but what I want to achieve with this is stopping leechers from downloading hundreds of files non-stop.
  Negative
We are not gonna charge for the files, therefore this is very likely to happen.
  Negative
563e2e5a61a8013065267100	X	Do you feel comfortable downloading the AWS SDK for PHP and putting it on your web server?
  Negative
And do you have a working MySQL database on that server?
  Neutral
If you can do that, then turning $user_can_download = true into a real check wouldn't be much work.
  Negative
.
  Neutral
don't have time to do that now but it may be a nice Breakfast Coding Exercise tomorrow morning :) Of course, making it foolproof and integrating it with Joomla would take quite a bit longer, and you'd really have to hire someone to do that...but a rudimentary implentation of what @Pushpesh described is easy enough to put together
563e2e5a61a8013065267101	X	Yeah, I've thought about hiring someone to do this actually, but before that I wanted to check how easy/difficult is this to do, and if there are already solutions created that I could implement.
  Negative
Regarding the query string authentication, I have read about it, and I have a component for Joomla that it says it uses that for serving the files.
  Negative
However, this component falls short on doing all the rest.
  Neutral
563e2e5a61a8013065267102	X	You can 'easily' generate time limited URLs for your files, and make them expire in about 5 mins or so.
  Negative
Then only make a time limited URL for a user if they have not recently downloaded.
  Neutral
Which means cookies and IP addresses, and other tricks - several people usually share an IP, and cookies can be cleared, and people can try another browser, etc.
  Negative
So the unique thing is perhaps hard... You will need a ruby or PHP programmer, etc to do this.
  Positive
563e2e5a61a8013065267103	X	I must say this is the first time I ask anything here, and I'm not a developer, so please be patient with my lack of knownledge.
  Very negative
This requirement is for a website I am creating with some friends, so it's not that I'm making money with this.
  Negative
This is the problem: I want to implement some kind of restriction to downloads, very much in the same way Rapidshare or any other file sharing service does: The user should be able to download only 1 file simultaneously The user should wait before being able to download another file, let's say 2 hours.
  Very negative
However, I am not trying to create a file sharing website.
  Neutral
I am going to upload all the files to Amazon S3, and the only thing I need is to be able to restrict the downloads.
  Negative
I will create the links to the files.
  Positive
I don't care if users are registered or not, they should be able to download anyway.
  Negative
The website is built in Joomla!
  Negative
, which uses Apache + MySQL.
  Negative
The files would be located at Amazon's servers.
  Negative
My question is the following.
  Neutral
Is there any way to implement this in a not-so-extremely-complicated way?
  Neutral
Do you know some script or web-service that could help me get this done?
  Negative
I have looked around, but the only thing I've found are Payment gateways, and we don't plan to charge for downloads.
  Negative
Any help will be much appreciated.
  Neutral
Thanks!
  Positive
UPDATE: I solved this problem using this script: http://www.vibralogix.com/linklokurl/features.php
563e2e5a61a8013065267104	X	As far as I know, there is no way to check on the current status of a download from S3.
  Negative
Having said that, S3 really does have plenty of bandwidth available, so I wouldn't worry too much about overloading their servers :) Just last week, Amazon announced that S3 is now serving an average of 650,000 objects / second.
  Negative
If you want to implement something like @Pushpesh's solution in PHP, one solution would be to use the Amazon SDK for PHP and do something like this: This uses the get_object_url function, which generates pre-signed URLs that allow you to let others download files you've set to private in S3 without making these files publicly available.
  Negative
As you can see, the link this generates will only be valid for 10 minutes, and it's a unique link.
  Positive
So you can safely let people download from this link without having to worry about people spreading the link: the link will have expired.
  Negative
The only way people can get a new, valid link is to go through your download script, which will refuse to generate a new link if the IP/user that is trying to initiate a download has already exceeded their usage limit.
  Negative
It's important that you set these files to private in S3, though: if you make them publicly available, this won't do much good.
  Negative
You probably also want to take a look at the docs for the S3 API that generates these pre-signed URLs.
  Negative
563e2e5b61a8013065267105	X	Only 2 ways comes to my mind - you either copy a file with unique hash and let apache serve it.
  Negative
.
  Neutral
then you don't have any control over when user actually ends his download (or starts).
  Negative
Useful for big files.
  Neutral
Another way is to pass it through php.
  Negative
Still, you would need to kill download session in case user stops download.
  Neutral
563e2e5b61a8013065267106	X	If there is no plugin for that you won't be able to do it the easy way by adding a script or copy & paste "some" code.
  Negative
So either hire somebody or you'll need to learn what you need to approach the task an your own, in other words learn how to program.
  Negative
Your question already contains the steps you need to implement: Record who is downloading what and when and keep track of the status of the download.
  Neutral
I have not tried to track a download status before but I'm not sure if it is possible to get the status somehow directly from the server that is sending the file.
  Negative
But you can get it from the client: Download Status with PHP and JavaScript I'm further not sure if this will work properly in your scenario because the file will come from S3.
  Negative
S3 itself has a so called feature "query string protection": With Query string authentication, you have the ability to share Amazon S3 objects through URLs that are valid for a predefined expiration time.
  Negative
So you need to lookup the S3 API to figure out how to implement this.
  Negative
What you could try is to send an ajax request to your server when the user clicked the download link, send the amazon s3 link your server generated back as a response and have the client side javascript somehow trigger that file download then.
  Negative
563e2e5b61a8013065267107	X	You can monitor user downloads by their ip address, store it in a database along with the time at which the user downloaded and the session id (with hashes of course) and check this before each download request.
  Negative
If the current time is less than 2 hours within the same session, block requests, else give them the download.
  Negative
Table Structure: This is a very basic implementation.
  Negative
I'm sure more robust methods exist.
  Negative
Hope this helps.
  Positive
563e2e5b61a8013065267108	X	Please edit the question and demostrate the the effort you have tried this far: your source code, exceptions you are getting, etc.
563e2e5b61a8013065267109	X	Hello and welcome to StackOverflow.
  Negative
Please take some time to read the help page, especially the sections named "What topics can I ask about here?"
  Neutral
and "What types of questions should I avoid asking?"
  Neutral
.
  Neutral
And more importantly, please read the Stack Overflow question checklist.
  Positive
You might also want to learn about Minimal, Complete, and Verifiable Examples.
  Neutral
563e2e5b61a801306526710a	X	Thank you!
  Positive
I am doing exactly that but I'm gonna host my pictures at Amazon S3.
  Negative
563e2e5c61a801306526710b	X	I am using Github API to create an issue (specifically, with requests module in python).
  Negative
I need to include the picture in the issue content, but I could't find any way that worked for me.
  Negative
Could you please suggest something that works from python?
  Positive
Thanks Edit: It could be really easily done by Github markdowns for linking the pictures that I stored at Amazon S3.
  Negative
563e2e5c61a801306526710c	X	Assuming you are already able to post things to the Issues Page of a given repository then follow these steps to post the image.
  Negative
I too at first found it annoying to post pictures to github... Github uses markdown and with markdown you can theoretically use an image sourced on your computer (this I believe only works on markdown that you host) but you can also provide a link or image source which is on the internet... particularly convenient for posting pictures to github... is github or you can use dropbox, picasa, photobucket, instagram, or flixster or 20 other image hosting services on the internet, but the point is you then once the image has an href="http://photobucket.com/myimage/3549604690" you can now host that into your markdown file.
  Very negative
Like so: Meaning in my case: Notice that it works on stackOverflow as well.
  Positive
For guides on markdown syntax: http://daringfireball.net/projects/markdown/syntax#img https://guides.github.com/features/mastering-markdown/ So STEP BY STEP (if you want to do it all through Github) and not apply any other APIs then have the user: Create a new Github Repo or use their existing repo.
  Negative
(Your choice).
  Neutral
Make a post request to insert the image inside the repository's files.
  Neutral
Post the issue to the repository and now use the url of the image where it was hosted.
  Negative
From what I've seen when images are added to a repository they go to the path: https://github.com/myUser/PictureHoster/blob/master/myscreenshot.jpg.
  Neutral
Python should brush my teeth!
  Neutral
I tried to get it to work and here: !
  Neutral
[Screen shot of Python stuff](https://github.com/myUser/PictureHoster/blob/master/myscreenshot.jpg "Screen Shot") it is not working.
  Negative
.
  Neutral
(Or something like that to find out I would go to your github page click on repositories click on the image inside your repo then right click on it once it's open and do copy Image URL).
  Negative
Basically to use a different picture hosting service you do the same right click on it inside your flixster account and copy/paste that image url.
  Negative
563e2e5c61a801306526710d	X	I'm building a business solution that has an ios app backed up with Web API 2.
  Negative
I'm using AWS (Amazon Web Services) to host the API.
  Negative
The ios app can take pictures.
  Neutral
Pictures will be associated with a user in SQL Server.
  Neutral
So I have a User table and Pictures table in my DB.
  Negative
I was thinking of using S3 to save the images.
  Negative
So how can I go about doing this?
  Neutral
the ios app can upload the image to S3 but how can i relate the image to a user?
  Neutral
563e2e5c61a801306526710e	X	You can either store the full s3 image url in your database and read the url from the db to then find the image on s3, or else use some sort of folder naming convention on s3 that would let you determine who's image file it is based on the filename along, i.e.: etc.
563e2e5c61a801306526710f	X	When using plain auth credentials I can do: ... to access BlobStoreContext for S3.
  Negative
In native Amazon java api I can use Security Token Service (STS) to assume role and use temporary credentials to access S3 or any other AWS service.
  Negative
How do I do this in jclouds?
  Neutral
563e2e5d61a8013065267110	X	I figured it out.
  Neutral
This code snippet allows to assume role and use temp credentials to access S3:
563e2e5d61a8013065267111	X	+1.
  Negative
Good Job.
  Positive
You can accept your own answer.
  Neutral
563e2e5d61a8013065267112	X	It says I have to wait a day.
  Negative
:) Which I usually do anyway in case someone has a better answer.
  Negative
563e2e5d61a8013065267113	X	Small correction, you're missing a comma after the "NotResource":[...] property to be valid JSON
563e2e5d61a8013065267114	X	Edited that part of the code block.
  Negative
Thank you.
  Positive
563e2e5d61a8013065267115	X	I have a bucket filled with contents that need to be mostly public.
  Positive
However, there is one folder (aka "prefix") that should only be accessible by an authenticated IAM user.
  Neutral
When I try to save this policy I get the following error messages from AWS: Obviously this error applies specifically to the second statement.
  Very negative
Is it not possible to use the "s3:prefix" condition with the "s3:GetObject" action?
  Negative
Is it possible to take one portion of a public bucket and make it accessible only to authenticated users?
  Neutral
In case it matters, this bucket will only be accessed read-only via api.
  Negative
This question is similar to Amazon S3 bucket policy for public restrictions only, except I am trying to solve the problem by taking a different approach.
  Negative
563e2e5d61a8013065267116	X	After much digging through AWS documentation, as well as many trial and error permutations in the policy editor, I think I have found an adequate solution.
  Negative
Apparently, AWS provides an option called NotResource (not found in the Policy Generator currently).
  Negative
With this, I do not even need to play around with conditions.
  Negative
This means that the following statement will work in a bucket policy:
563e2e5d61a8013065267117	X	i've tried decoding the data with Javascript's atob function, but i'll try it server side as well.
  Negative
563e2e5d61a8013065267118	X	wow... that actually did it.
  Positive
thanks!
  Positive
563e2e5e61a8013065267119	X	I've got a drag and drop function which takes the file that's been dropped on it and converts it to Base64 data.
  Negative
Before, it was uploading to Imgur, whose API supports Base64 uploads, and now I'm working on moving to Amazon S3.
  Negative
I've seen examples of people using XMLHTTP requests and CORS to upload data to S3, I'm using Amazon's AWS S3 SDK gem to avoid having to sign policies and other things, as the gem does that for me.
  Negative
So what I've done is send the Base64 data to a local controller metod which uses the gem to upload to S3.
  Negative
The other posts using Ajax i've seen show that S3 supports raw data uploads, but the gem doesn't seem to, as whenever I view the uploads i get broken images.
  Negative
Am I uploading it incorrectly?
  Negative
Is the data in the wrong format?
  Neutral
I've tried the basic Base64, atob Base64, and blob urls, but nothing works so far.
  Negative
JS: Controller method: Edit: To be clear, I've tried a couple of different formats, the one displayed above is decoded base64.
  Negative
Regular Base64 looks like this: and a blob url looks like this:
563e2e5e61a801306526711a	X	Am I reading this right that you are: If that's the case, you need to decode the data in step 2 before sending it on to S3.
  Very negative
Something like this might work:
563e2e5e61a801306526711b	X	I want to create an application that needs to store xml and jpg files.
  Negative
Do you know any service on the internet that allows me to store files(jpg most important) and retrieve the files from the service when I need them?
  Negative
I'm looking for something like flickr but with the option of manipulating files trough webservices.
  Negative
563e2e5e61a801306526711c	X	You could maybe use Amazon S3: http://aws.amazon.com/s3/ You can use their API to manipulate your files from different environments:
563e2e5e61a801306526711d	X	Please post your code
563e2e5e61a801306526711e	X	Do you have permission to rename the file?
  Negative
563e2e5f61a801306526711f	X	Define "not working".
  Negative
563e2e5f61a8013065267120	X	Are you sure the first argument exists?
  Neutral
file_exists($path) == TRUE ?
  Negative
563e2e5f61a8013065267121	X	After update: s3:// is not really a normal file.
  Negative
I suppose you're using Services_Amazon_S3?
  Neutral
563e2e5f61a8013065267122	X	Yes.i am using stream wrapper.
  Negative
can u please give me the code(how to rename the files in bucket)
563e2e6061a8013065267123	X	@user, I will not "give you the code" -- you are either discarding the return value or otherwise have an error in your own rename call.
  Very negative
Is the return value true, or false?
  Negative
If it's returning false, then the rename failed at the S3 level.
  Negative
If it's returning true, then S3 says the rename worked.
  Negative
563e2e6061a8013065267124	X	@Charles the link you provided is broken
563e2e6061a8013065267125	X	@tq, fixed.
  Negative
It's irksome that they did not redirect their svn repo viewer to the new canonical file locations on github.
  Negative
563e2e6061a8013065267126	X	How to rename the file using PHP(in linux).
  Negative
I am using rename(oldfile,newfile), but not working.
  Negative
563e2e6061a8013065267127	X	It looks like you're using a stream wrapper for Amazon S3.
  Negative
It's up to the individual, custom wrapper code to implement rename functionality.
  Negative
If your wrapper isn't doing the rename, then either the code it's using is buggy, or it doesn't implement that functionality and either isn't reporting it, or you aren't checking the return code from the rename function call and it's returning false to signify failure.
  Negative
You will probably need to actually use the normal S3 API to perform your file rename.
  Negative
If the function is actually returning true, you should file a bug with the people that provided the stream wrapper library.
  Negative
Edit: If you are using PEAR's Services_Amazon_S3 as suggested in the comments, then the stream wrapper it provides does do rename using the rename method starting at about line 570.
  Negative
Edit2: After examining the code further, if there are failures, you will see warnings emitted from the stream wrapper.
  Negative
Perhaps you don't have error_reporting cranked up all the way to -1?
  Neutral
563e2e6061a8013065267128	X	I think this question would be best posed to Amazon Customer / Technical Support.
  Negative
But if you do get an answer there, please do post the solution here as well :)
563e2e6061a8013065267129	X	I already asked this question on AWS forum, however still waiting for an answer.
  Negative
Thought of checking out the bigger community here.
  Positive
:) Either ways, I will keep posted if I have it working.
  Positive
563e2e6061a801306526712a	X	This looks like a bug in the SDK generation.
  Negative
Also, friendly reminder, never use your root credentials anywhere, create admin IAM user instead.
  Negative
563e2e6061a801306526712b	X	I have created sample GET and POST APIs on Amazon API Gateway following their official documentation.
  Negative
I have generated JS SDK for these APIs, which I am using to call these APIs from a client-side JS file hosted on S3.
  Negative
This works flawlessly without any 'Authorization Type'.
  Positive
Now, when I set 'Authorization Type' for GET method as 'IAM', I am required to pass IAM credentials in order for it to work.
  Negative
In spite of passing my AWS account's root credentials, I am getting this in the response headers: And finally it returns a 403 error code.
  Negative
My question is: Has anyone successfully attempted to use generated javascript SDK from Amazon API Gateway with IAM authentication?
  Negative
Can you point where I might be going wrong?
  Negative
Thanks.
  Neutral
563e2e6061a801306526712c	X	I was able to resolve this with the help of few folks on AWS Forum.
  Positive
It appears that the API Gateway GET method expects an empty body.
  Negative
By default, if you are following the README sample that comes with generated JS SDK, passing 'undefined' or just '{}' inside the body to GET causes a non-matching payload and this results in an incorrect signature being calculated.
  Negative
As of now, I just made a small tweak in the /lib/apiGatewayCore/sigV4Client.
  Negative
js by hardcoding the body = ''.
  Neutral
This should be a temporary workout as this may affect your other API Gateway methods that require a filled 'body'.
  Negative
In my case, I only had GET methods.
  Negative
563e2e6161a801306526712d	X	I'm posting an image to Amazon S3 using mattt's AFAmazonS3Manager library, basically by following the example there.
  Negative
The upload works fine, but when I download the file from S3, it has those headers that make the file invalid.
  Neutral
When I remove the headers, the file becomes valid and I can open it with Preview or Photoshop.
  Negative
The code to create the image is as follow: I'm using a PUT request, as per Amazon S3's REST API reference.
  Negative
Any idea what I could do to store a valid image on S3?
  Neutral
563e2e6161a801306526712e	X	Simple i want to apply image compression using PNG/JPEG/Bitmap file.
  Negative
Android we have Bitmap.CompressFormat to compressed our bitmap file and use for further operation.
  Negative
Bitmap.CompressFormat class allow to compress in 3 format as below : My query is i want to compress file in any on of below format : I have found some image compression library like ImageIo & ImageMagick but didn't get any success.
  Negative
I want to use this file to upload on AmazonServer.
  Negative
Please guide me how to achieve this or is there any other option to upload image on amazon server.
  Negative
Thanks for your time.
  Positive
563e2e6161a801306526712f	X	I don't know about those file's compression but i created this class to upload files programatically into an Amazon s3 bucket that uses the Amazon SDK api: And for usage with your file:
563e2e6161a8013065267130	X	Ok no one seems to care about this...with SO that is hardly ever the case :).
  Very negative
Needed to access my server-side presigned post credentials from a mobile client and was a bit confused till I came across this.
  Negative
Thanks.
  Neutral
563e2e6161a8013065267131	X	I am creating an API for a backend service with Rails 4.
  Negative
The service needs to upload an image file to an amazon s3 bucket.
  Negative
I'd like to use a direct upload url, so that the clients manage the uploads to s3 and the server is not kept busy.
  Negative
Currently I have the following prototypical rails action This generates such an url: Now I try to post the test.png to this url with the following: curl -v -T test.png "url" and I get the following error response: I believe the problem comes from the fact, that the specified X-Amz-SignedHeaders Header is wrong.
  Very negative
I am not sure which headers are used by default from the amazon rails sdk gem.
  Negative
How should I change my url generation, so that a mobile client can just take the url and post a file to it?
  Negative
563e2e6161a8013065267132	X	Ok no one seems to care about this, but if by any chance someone stumbles upon this, here is a solution: in config/initializers/aws.
  Negative
rb in your model/controller/concern/or whatever Then you can use a mobile client to upload or via curl Note that you will have to add the x-amz-acl: public-read header if you used the public-read cal option.
  Negative
563e2e6261a8013065267133	X	Duplicate of this question, see it for answer.
  Negative
LINK
563e2e6261a8013065267134	X	this is incorrect.
  Negative
You can set the 'download filename' by using the headers in the original question.
  Neutral
See my answer.
  Positive
563e2e6261a8013065267135	X	@Geoff; Thank you for the info.
  Negative
As I wrote on my comment, I never tried it tho and of course their classes / APIs could bring solution as in your answer.
  Negative
Now how do we contact him?
  Neutral
He accepted my answer as a solution while there is a way to do that.
  Positive
563e2e6261a8013065267136	X	I am using amazon S3 service with PHP by using this API https://github.com/tpyo/amazon-s3-php-class I am passing the url to client like this So when the client clicks or paste the URL into browser , the file downloaded with the name of filename_11052011111924.zip.
  Negative
But I stored my original filename in DB.
  Negative
So is it possible to download when passing the URL alone to the client and download with original file name.I am not sure whether this will help me.
  Negative
563e2e6261a8013065267137	X	I don't think that will work (I never tried it though).
  Negative
You might need to download the file to your server first, later use headers, once it is completed (or after sometime later with some bot or cron) you can delete the file(s).
  Negative
This approach will be using your bandwidth.
  Negative
563e2e6261a8013065267138	X	If you set the headers that you listed on your file when you upload it to S3, you will be able to download the file with the original filename.
  Negative
(you can also set these on existing files in S3 - see the AWS docs) I'm not sure if your library supports this but you can do it with the AWS S3 SDK.
  Negative
Something like (I don't know php so check the syntax): Update You can also adjust certain headers each time you generate a new url.
  Negative
See http://docs.amazonwebservices.com/AWSSDKforPHP/latest/#m=AmazonS3/get_object_url
563e2e6261a8013065267139	X	Yes, you can tell to AWS how output file must be named: Note: we encode file name!
  Negative
563e2e6261a801306526713a	X	I don't see how deploying a Django app would be any different just because it uses Angular.
  Negative
Angular is exclusively static files from the point of view of Django.
  Negative
563e2e6261a801306526713b	X	It's better to deploy static files to S3 and access them through CloudFront.
  Negative
Read about why CDN is useful.
  Negative
563e2e6261a801306526713c	X	I am able to deploy (It is not different than deploying any django app) I wanted to know about the more Proper way like compressors and things like that to make loading faster (which aren't always obvious for beginners ) and whether to use docker containers or not.
  Negative
563e2e6361a801306526713d	X	I have an app similar to this with django REST for API backend and AngularJS as client.
  Negative
The angularJS app resides in the static folder.
  Negative
I can't wrap my head around how to properly deploy this to beanstalk.
  Negative
This answer gives a good idea on how to do so.
  Positive
Also others suggest using Docker like in this tutorial So is it best to just have the angularJS under the django static folder deployed to amazon S3, or should I have a docker container and deploy this as a whole to AWS beanstalk?
  Negative
563e2e6361a801306526713e	X	I am working on rails 4 application where I can upload images with description.
  Negative
I can upload images on web application but I want to test the json API.
  Negative
I want to upload image from HTTP POST on postman Rest client of chrome browser, But I cant seem to make it to work.
  Negative
Here's my complete Pins controller, here I am using paperclip 4.2 gem for image upload and amazon s3 storage
563e2e6361a801306526713f	X	I actually tried this method -- I've just come to the conclusion that S3 can not support deleting buckets right now, and that with it's horrendous access speed leaves an extremely bitter taste in my mouth for S3.
  Very negative
563e2e6361a8013065267140	X	s3cmd del s3cmd ls s3://Mybigbucket/somepattern | awk '{print $4}' .
  Negative
Yeah this is painful
563e2e6361a8013065267141	X	even listing the keys at 1000 time or whatever the number was -- that took forever -- more than an afternoon and I finally killed it after I got bored and noticing that my heap was way too overfilled.
  Very negative
563e2e6361a8013065267142	X	I don't think there is an API call to just get the number of items.
  Negative
Probably you've used a tool that also gets the contents of the files - that's why it took so long.
  Negative
Just use Fiddler or some other tool to send the GET bucket request (see the REST API link in my answer).
  Negative
It shouldn't take long to get the xml back.
  Negative
I am afraid that I don't have such a big bucket to test it myself.
  Negative
563e2e6461a8013065267143	X	Where is the second way to count?
  Negative
563e2e6461a8013065267144	X	So I know this is a common question but there just doesn't seem to be any good answers for it.
  Negative
I have a bucket with gobs (I have no clue how many) number of files in them.
  Negative
They are all within 2k a piece.
  Neutral
1) How do I figure out how many of these files I have WITHOUT listing them?
  Negative
I've used the s3cmd.rb, aws/s3, and jets3t stuff and the best I can find is a command to count the first 1000 records (really performing GETS on them).
  Very negative
I've been using jets3t's applet as well cause it's really nice to work with but even that I can't list all my objects cause I run out of heap space.
  Negative
(presumably cause it is peforming GETS on all of them and keeping them in memory) 2) How can I just delete a bucket?
  Negative
The best thing I've seen is a paralleized delete loop and that has problems cause sometimes it tries to delete the same file.
  Negative
This is what all the 'deleteall' commands that I've ran across do.
  Positive
What do you guys do who have boasted about hosting millions of images/txts??
  Neutral
What happens when you want to remove it?
  Neutral
3) Lastly, are there alternate answers to this?
  Negative
All of these files are txt/xml files so I'm not even sure S3 is such a concern -- maybe I should move this to a document database of sorts??
  Negative
What it boils down to is that the amazon S3 API is just straight out missing 2 very important operations -- COUNT and DEL_BUCKET.
  Negative
(actually there is a delete bucket command but it only works when the bucket is empty) If someone comes up with a method that does not suck to do these two operations I'd gladly give up lots of bounty.
  Negative
UPDATE Just to answer a few questions.
  Neutral
The reason I ask this was I have been for the past year or so been storing hundreds of thousands, more like millions of 2k txt and xml documents.
  Negative
The last time, a couple of months ago, I wished to delete the bucket it literally took DAYS to do so because the bucket has to be empty before you can delete it.
  Negative
This was such a pain in the ass I am fearing ever having to do this again without API support for it.
  Negative
UPDATE this rocks the house!
  Positive
http://github.com/SFEley/s3nuke/ I rm'd a good couple gigs worth of 1-2k files within minutes.
  Positive
563e2e6461a8013065267145	X	I am most certainly not one of those 'guys do who have boasted about hosting millions of images/txts', as I only have a few thousand, and this may not be the answer you are looking for, but I looked at this a while back.
  Very negative
From what I remember, there is an API command called HEAD which gets information about an object rather than retrieving the complete object which is what GET does, which may help in counting the objects.
  Negative
As far as deleting Buckets, at the time I was looking, the API definitely stated that the bucket had to be empty, so you need to delete all the objects first.
  Negative
But, I never used either of these commands, because I was using S3 as a backup and in the end I wrote a few routines that uploaded the files I wanted to S3 (so that part was automated), but never bothered with the restore/delete/file management side of the equation.
  Negative
For that use Bucket Explorer which did all I need.
  Negative
In my case, it wasn't worth spending time when for $50 I can get a program that does all I need.
  Negative
There are probably others that do the same (eg CloudBerry) In your case, with Bucket Explorer, you can right click on a bucket and select delete or right click and select properties and it will count the number of objects and the size they take up.
  Negative
It certainly does not download the whole object.
  Negative
(Eg the last bucket I looked it was 12Gb and around 500 files and it would take hours to download 12GB whereas the size and count is returned in a second or two).
  Negative
And if there is a limit, then it certainly isn't 1000.
  Negative
Hope this helps.
  Positive
563e2e6761a8013065267146	X	"List" won't retrieve the data.
  Negative
I use s3cmd (a python script) and I would have done something like this: But first check how many bucketfiles_ files you get.
  Negative
There will be one s3cmd running per file.
  Negative
It will take a while, but not days.
  Negative
563e2e6761a8013065267147	X	1) Regarding your first question, you can list the items on a bucket without actually retrieving them.
  Negative
You can do that both with the SOAP and the REST API.
  Neutral
As you can see, you can define the maximum number of items to list and the position to start the listing from (the marker).
  Negative
Read more about it here.
  Positive
I do not know of any implementation of the paging, but especially for the REST interface it would be very easy to implement it in any language.
  Negative
2) I believe the only way to delete a bucket is to first empty it from all items.
  Negative
See alse this question.
  Positive
3) I would say that S3 is very well suited for storing a large number of files.
  Negative
It depends however on what you want to do.
  Neutral
Do you plan to also store binary files?
  Neutral
Do you need to perform any queries or just listing the files is enough?
  Neutral
563e2e6761a8013065267148	X	I've had the same problem with deleting hundreds of thousands of files from a bucket.
  Negative
It may be worthwhile to fire up an EC2 instance to run the parallel delete because the latency to S3 is low.
  Negative
I think there's some money to be made hosting a bunch of EC2 servers and charging people to delete buckets quickly.
  Negative
(At least until Amazon gets around to changing the API)
563e2e6761a8013065267149	X	Old thread, but still relevant as I was looking for the answer until I just figured this out.
  Negative
I wanted a file count using a GUI-based tool (i.e. no code).
  Negative
I happen to already use a tool called 3Hub for drag & drop transfers to and from S3.
  Negative
I wanted to know how many files I had in a particular bucket (I don't think billing breaks it down by buckets).
  Neutral
I had 20521 files in the bucket and did the file count in less than a minute.
  Negative
I'd like to know if anyone's found a better way since this would take some time on hundreds of thousands of files.
  Negative
563e2e6761a801306526714a	X	To count objects in an S3 bucket: Go to AWS Billing, then reports, then AWS Usage reports.
  Negative
Select Amazon Simple Storage Service, then Operation StandardStorage.
  Negative
Download a CSV file that includes a UsageType of StorageObjectCount that lists the item count for each bucket.
  Negative
563e2e6761a801306526714b	X	Can I make a PHP script, using Youtube API or any other legal way, to directly copy mp4 files from my own youtube channel to Amazon S3 or other media host?
  Negative
I need this because my client wants me to make native iPhone / Android apps that will stream the videos from his channel.
  Negative
I know I can do it manually from Youtube web, but the problem is that I can only download vids locally and then again upload to Amazon which seems like a waste of time and assets.
  Very negative
563e2e6861a801306526714c	X	This is really stating the obvious but, I really hope you're not actually using the private key posted here.
  Negative
563e2e6861a801306526714d	X	I'm trying to serve time-limited links to private content on a Cloudfront-enabled Amazon S3 bucket.
  Negative
I would like to be able to use the AWS PHP API But I keep getting this access denied message The cdn_private_key is a string containing the RSA private key which looks sort of like this: I may be doing something wrong there, but I would've expected to get an error about the key or signature instead of an access denied message.
  Very negative
I have also tried manually signing using the following method, but get the same error, only with different HostId and RequestId:
563e2e6861a801306526714e	X	I've eventually solved the problem.
  Very negative
It seems that Amazon aren't very clear about this ... hidden deep in the bowels of AWS documentation you are instructed to set the bucket permissions on the S3 bucket to allow CloudFront access to it.
  Negative
Further confusion ensues when you have to set the Principal property on the policy, as it suggests you need to get the Canonical User ID.
  Negative
However, this is NOT the Canonical User ID for your AWS account found on the Security Credentials page .
  Neutral
.
  Neutral
it is instead found in the "Origin Access Identity" link on the CloudFront console.
  Negative
Here is how to do it .... First create/obtain the CloudFront Origin Access Identity like this:- Now apply the policy to the bucket to allow CloudFront access:-
563e2e6961a801306526714f	X	Assume 200,000 images in a flat Amazon S3 bucket.
  Negative
The bucket looks something like this: (a 6 digit hash followed by a count, followed by the extension) If I need all files matching 000001-*.
  Negative
jpg, what's the most efficient way to get that?
  Neutral
In PHP I'd use rglob($path,'{000001-*.
  Negative
jpg}',GLOB_BRACE) to get an array of matches, but I don't think that works remotely.
  Negative
I can get a list of all files in the bucket, then find matches in the array, but that seems like an expensive request.
  Negative
What do you recommend?
  Positive
563e2e6961a8013065267150	X	Amazon provides a way to do this directly using the S3 api.
  Positive
You can use the prefix option when calling listing S3 objects to only return objects that begin with the prefix.
  Negative
eg using the AWS SDK for PHP: You might also find the delimiter option useful - you could use that to get a list of all the unique 6 digit hashes.
  Negative
563e2e6961a8013065267151	X	I have an application to upload files to Amazon S3 using AWS .
  Negative
net API.
  Positive
My query is can we use throttling while uploading the data to S3?
  Negative
If I upload too many files from a single machine of small size say < 5MB then my entire bandwidth is choked up.
  Negative
So is there a way to manage the bandwidth while uploading the data.
  Neutral
We also tried with Low level .
  Negative
net API and high level API but there is no Attribute or API to set throttling.
  Negative
Then I tried using Throttled Stream class of .
  Negative
net but this works smooth if I keep chunk size of 512KB and my current Network is of 2MBPS, I'm still not sure is it because of throttled stream class or because of low chunk size that it works fine.
  Positive
Below is my code: I followed the below link http://www.codeproject.com/Articles/18243/Bandwidth-throttling
563e2e6a61a8013065267152	X	Why don't you just run on heroku or something?
  Negative
563e2e6a61a8013065267153	X	See I don't know what that is, thats the problem, theres so many different ways to do these types of things, its hard to find / know the best efficient path to take
563e2e6a61a8013065267154	X	Well, now you know heroku :)
563e2e6a61a8013065267155	X	Dang, expensive huh?
  Negative
lol
563e2e6a61a8013065267156	X	So what should I be looking into to do what I need?
  Negative
I'm new to aws btw, I know nothing yet.
  Negative
563e2e6a61a8013065267157	X	You can use the EC2 free tier to get free EC2 hosting within limits.
  Negative
Added a link.
  Neutral
563e2e6a61a8013065267158	X	Would my videos be stored on EC2 now as well?
  Negative
Would I need RDS anymore
563e2e6b61a8013065267159	X	@EricJ.
  Neutral
I believe that's free for one year though.
  Positive
563e2e6b61a801306526715a	X	The best place to store videos is probably in S3, because you can optionally use CloudFront, Amazon's Content Delivery Network, with S3 to deliver the videos to your customers quickly no matter where in the world they may be.
  Negative
You can though store them in an EBS backed file system in EC2.
  Negative
See stackoverflow.com/a/3630707/141172
563e2e6b61a801306526715b	X	So what should I be looking into?
  Negative
563e2e6b61a801306526715c	X	Personally, I have been using AWS services for years and honestly wouldn't think of hosting anything other than a trivial website or application on anything else.
  Negative
563e2e6b61a801306526715d	X	I know in amazon's documentation it says S3 is not really made for server side scripting but to rather use EC2 instead.
  Negative
I don't need and will not use a operating system that EC2 provides to handle my server calls, it just seems pricey and seems to be an overkill.
  Negative
Basically, I have a couple php files that handle writing data to a RDS database and uploading videos to S3.
  Negative
Is it wrong for me to have my php files in S3 and allow static web hosting so that my iphone api can call the php scripts?
  Negative
563e2e6b61a801306526715e	X	Static web hosting means just that... no server-side script execution.
  Negative
You cannot run PHP scripts on S3.
  Negative
You can host a static website on Amazon S3.
  Neutral
On a static website, individual web pages include static content.
  Negative
They may also contain client-side scripts.
  Neutral
By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET.
  Negative
Amazon S3 does not support server-side scripting.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html Check out the EC2 free tier.
  Negative
563e2e6b61a801306526715f	X	S3 has no way of actually executing your PHP files.
  Negative
It is just file storage that just happens to also be able to also serve up files in response to basic HTTP requests.
  Negative
But it can only serve up static content in this manner.
  Negative
There is however nothing that says you need to use EC2 for your web application.
  Negative
You can use whatever you want for that, though you will likely see bandwidth cost penalties around S3 that you may be able to avoid using EC2.
  Negative
563e2e6c61a8013065267160	X	I use larvel 4 with AWS sdk.
  Negative
I want to add another AWS service with different key and region.
  Positive
when I used only one AWS service for example SES I put my key in /app/config/aws/aws-sdk-php-laravel just in the array 'secret' => 'xxxxxxxxxxxxx', all works well now I want add another service for example S3.
  Positive
so I create custom config file in the same directory: configaws.php (with chmod 777): and use this line in my config.php file 'config_file' => 'configaws.php', the problem is that Laravel can't find this file, I got exception 'Unable to open configaws.php for reading'.
  Very negative
my simple question is where I should put my configaws.php file?
  Negative
because I try a lot of options and nothing works for me.
  Negative
is there another option to use 2 different keys for amazon services?
  Negative
563e2e6c61a8013065267161	X	if you are using Laravel 4.1+, The best way store sensitive information using dot files.
  Negative
For "real" applications, it is advisable to keep all of your sensitive configuration out of your configuration files.
  Negative
Things such as database passwords, Stripe API keys, and encryption keys should be kept out of your configuration files whenever possible.
  Very negative
So, where should we place them?
  Negative
Thankfully, Laravel provides a very simple solution to protecting these types of configuration items using "dot" files.
  Positive
Assume your environment is production or you can check your environment by reading the following doc.
  Neutral
http://laravel.com/docs/configuration#environment-configuration For production environment, create a .
  Positive
env.php in the project root of your application and add the AWS information like so.
  Negative
Now, when you need the api information, use the PHP's super global variables $_ENV or $_SERVER to retrieve the information.
  Neutral
For example, to retrieve the Amazon s3 api info, you can do the following: For more information please check the Laravel documentation: http://laravel.com/docs/configuration#protecting-sensitive-configuration
563e2e6c61a8013065267162	X	Thanks for the reply.
  Negative
I keep getting Unknown error 500 from Facebook which most probably caused by the URL from Paperclip has extension like .../my-image.jpg?
  Negative
140583423.
  Neutral
I tried hard-coding the URL without that extension and works.
  Negative
Any way to disable that extension?
  Neutral
563e2e6c61a8013065267163	X	If I understood your comment, @user.
  Negative
photo.url(:origin).
  Neutral
split('?')
  Neutral
[0] can work for you
563e2e6c61a8013065267164	X	Thanks!
  Positive
the traditional split didn't even cross my mind
563e2e6c61a8013065267165	X	I'm using Koala for Facebook API and Paperclip for Amazon S3.
  Negative
I already finished the code for S3 Upload, but having problem for Facebook's upload.
  Neutral
Here's my simplified code: I keep getting this error on last line: I think the way I set file is wrong but I can't find other way to do it.
  Very negative
Thanks before.
  Neutral
563e2e6c61a8013065267166	X	While uploading picture on facebook using URL, you just need to send picture url directly (you don't need to send binary data).
  Negative
Update API call as: Some other ways to use put_picture method: Source: Koala gem.
  Positive
563e2e6d61a8013065267167	X	You may want to try CloudBuddy Personal m1.mycloudbuddy.com/index.html.
  Negative
It runs only on windows though.
  Negative
563e2e6d61a8013065267168	X	tl;dr - Is there a robust S3 ACL management tool, possibly for use with CloudFront?
  Negative
I'm working on a personal private content distribution (via CloudFront) but obviously the AWS Console is severely lacking in this regard.
  Negative
I know there are a handful of S3 clients out there, but none of them really do much for advanced ACL.
  Negative
To avoid having to use the AWS cli tools or to write wrappers for the API for everything (this is for configuring long-term systems, not for anything that would need to be done programmatically), I'm looking for one that has the best ACL support.
  Neutral
OR, if anyone has suggestions for managing CloudFront and custom ACLs (specifically for adding canonical user IDs/OriginAccessIdentities to buckets), I'm totally open to that too.
  Negative
On a side note, the AWS docs mention the following: Once you have a private content distribution, you must grant your CloudFront origin access identity read access to the private content.
  Negative
You do this by modifying the Amazon S3 ACL on each of the objects (not on the bucket).
  Negative
which seems, er, exceptionally hard to maintain for a system that could potentially be used as swap (sic) storage for protected assets and modified on a regular basis (tens+ of times per day).
  Negative
Am I misreading that, or is it really intended to be that static and explicit?
  Negative
563e2e6d61a8013065267169	X	Thanks for the suggestions, but I can't use those (Mac - didn't mention, not your fault).
  Negative
I ended up going with Bucket Explorer, FWIW.
  Negative
563e2e6d61a801306526716a	X	Cyberduck for Mac & Windows supports ACL editing.
  Negative
Refer to http://trac.cyberduck.ch/wiki/help/en/howto/s3.
  Neutral
563e2e6d61a801306526716b	X	Thanks Nick - let's see if I can get the multipart upload API to play nicely with app-engine :)
563e2e6d61a801306526716c	X	@JohnIdol Have you considered just writing the code yourself?
  Negative
The RESTful API is pretty straightforward - the only complexity is computing the header signature, and there's probably a library that will do that for you.
  Negative
563e2e6d61a801306526716d	X	I am keeping that for the all-else-fails scenario :) Now I am giving it another shot with the 'low-level' aws API approach (the 'high level' approach fails with all sorts of 'access denied' issues).
  Negative
563e2e6d61a801306526716e	X	thanks man -- much appreciated :)
563e2e6d61a801306526716f	X	I am trying to upload a file to amazon S3 from GAE.
  Negative
I tried the official amazon sdk (jetS3t, built on top of the lower-level sdk), just to find out that even if you can get it to work locally by setting permissions on the local JVM it is not supported for GAE crypto-related reasons once you deploy it.
  Negative
Then out of desperation I found that some good soul forked the official low-level amazon sdk so that it would work with GAE.
  Neutral
This kind of works (even though I can see some strage NullPointer exceptions being thrown here and there) and the file gets uploaded ... but if the file size exceeds 5MB I am getting a error from within the API: I don't fully understand this as the current GAE limitations seem to be 32MB on file size upload and 1MB on request/response while my problem is occurring only when the file is around 5MB or bigger.
  Very negative
I think my only alternative left is jclouds, but I am having trouble finding examples of uploading files to S3 using the BlobStore library.
  Negative
Does anyone have experience/examples to share of S3 file upload with jClouds?
  Neutral
And am I likely to incur in the same urlfetch.Fetch() was too large error?
  Negative
Any help appreciated.
  Positive
563e2e6e61a8013065267170	X	URLFetch requests are limited to 5MB, as documented here.
  Negative
The only solutions that will work are those that involve breaking up a large payload into smaller chunks.
  Negative
Fortunately, S3 provides a multipart upload API.
  Neutral
563e2e6e61a8013065267171	X	On the release notes for 1.5.0, I read In response to popular demand, the HTTP request and response sizes have been increased to 32 MB.
  Negative
So, request and response, URL fetch is not mentioned.
  Negative
Indeed, looking at URL Fetch documentation, it says it's max 5 Mb.
  Negative
563e2e6e61a8013065267172	X	thanks Michael, but when i changed to https, the code in first line to "POST s3.amazonaws.com HTTP/1.1".
  Negative
the AWS server still returns Method not Allowed.
  Negative
563e2e6e61a8013065267173	X	i want to use the AWS S3 by soap, i checked the aws website, there is an example to list all the bucket.http://docs.aws.amazon.com/AmazonS3/latest/API/SOAPListAllMyBuckets.html.
  Negative
so when i use the soap template.
  Neutral
it returns "405 Method Not Allowed".
  Negative
any help?
  Neutral
thanks this is what is sent to aws s3 server.
  Neutral
563e2e6e61a8013065267174	X	You need to use HTTPS, not HTTP.
  Negative
Note that SOAP requests, both authenticated and anonymous, must be sent to Amazon S3 using SSL.
  Negative
Amazon S3 returns an error when you send a SOAP request over HTTP.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/API/APISoap.html Note also that the SOAP interface is deprecated.
  Negative
563e2e6e61a8013065267175	X	Sony, thanks for your help.
  Positive
Especialy Amazon Elastic Transcoder.
  Neutral
I will have to process many videos, so i think thats an interesting alternative.
  Positive
563e2e6e61a8013065267176	X	users will load up videos from ios app, android app and vía webupload to my server.
  Negative
so i get a lot of different video formats, which i need to encode for showing them in the website.
  Negative
As we now, video encoding is not a simple thing.
  Negative
I´m doing it up to now on the webserver itself, using ffmpeg with php.
  Negative
That was ok for the Beta Version, but now i need a professional service.
  Neutral
The encoding takes a lot of server cpu time and trying to cover all the video formats is practically imposible for me.
  Negative
From what i found out up to now, the most profesional solution seems to be a online/cloud service like like transloadit.com, or zencoder.com or encoding.com.
  Negative
There are more, but that's what o foung up to now.
  Neutral
Does anyone have experience with these or other similar services and can tell something about the advantages and disadvantages of each of them?
  Neutral
I also consider using Amazon S3 bucket to save the uploaded videos.
  Negative
Some (or all) of the named services, deliver the encoded videos also to a S3 bucket if wanted.
  Positive
And last but not least i'd like to get also some help in which player is recommended to use.
  Positive
The videos will be called from desktop, tablets and smartphones vía a web app.
  Negative
The alternatives i see up to now are jwplayer.com, flowplayer.org and vid.ly player.
  Positive
563e2e6e61a8013065267177	X	One very highlevel third party service which I used in the past if Wistia.com.
  Negative
They have APIs to upload videos (most format, I guess) to them.
  Neutral
Once you upload, you need to wait for them to be transcoded and ready to be streamed - You can check this status via API again.
  Negative
Then you will get an HTML embed code (as from youtube), using which you can stream the videos on your website.
  Neutral
This basically abstracts lot of things for you (no need to deal with jwplayer, transcoder etc) Hoeever, You can use AWS elastic transcoder for transcoding if you want to do it the developer way.
  Negative
You should be able to call the APIs from anywhere (even from your web-server).
  Neutral
You can configure S3 such that your users can directly upload to S3, Transcoder picks it from S3, converts and puts it back in S3 again.
  Negative
API Reference - Amazon Elastic Transcoder This example helps you setup a video trans-coding pipeline.
  Negative
563e2e6e61a8013065267178	X	There is also http://www.bitcodin.com as cloud-based transcoding service, which can be used via an API to automate processes.
  Negative
There is a full tutorial on the usage together with Amazon S3 and CloudFront: http://www.bitcodin.com/blog/2015/02/create-mpeg-dash-hls-content-for-amazon-s3-and-cloudfront/
563e2e6e61a8013065267179	X	I have used both amazon's elastic_encoder and zencoder.com both if them are great I personally would prefer zencoder.com which is efficient and much faster but its cons are that it is costlier.
  Negative
As you have mentioned that you are using amazons s3 bucket for uploading video then it is worth mentioning that amazons elastic transcoder works smoothly with s3 and is lot cheaper then zencoder.
  Positive
Moreover zencoder supports more video formats than elasctic trancoder.
  Negative
At work I am using elastic transcoder as primary trascoding but when it fails I user zencoder.com for most part elastic transcoder should do fine.
  Negative
For implementation of aws elastic encoder (django and python stack) and this post which explains implementation for zencoder.Hope it helps.
  Negative
563e2e6e61a801306526717a	X	I am new bee to AWS Bucket.
  Negative
Please consider my content in the post.
  Neutral
I have more than 100 GB of files stored on a Windows Azure.
  Negative
It has some what folowing structure: As now we are migrating another server.
  Negative
We need to copy these all files and its contents on AWS Bucket using C#.
  Negative
Also, i have a all the URLS of these links to a text file.
  Negative
So, can we copy the files directly from AZURE to AWS Bucket?
  Negative
or need to Download it on local and then need to Upload to AWS Bucket?
  Neutral
Any suggestions?
  Neutral
Help Appreciated!
  Neutral
563e2e6f61a801306526717b	X	Looking at Amazon S3's Put Object REST API documentation, I believe it doesn't support fetching a URL's contents and saving that as an object (which Azure Blob Service does support BTW).
  Negative
So your only option would be to download blobs from Azure Storage to local computer and then upload them into Amazon S3.
  Negative
563e2e6f61a801306526717c	X	Besides HTTP 503, you can also use HTTP 421 - There are too many connections from your internet address.
  Negative
(But my personal favorite is HTTP 418 - I'm a teapot.
  Negative
:-))
563e2e6f61a801306526717d	X	Twitter is returning a non standard HTTP 420
563e2e6f61a801306526717e	X	RFC 2616 defines none of these 4xx's and says "The 4xx class of status code is intended for cases in which the client seems to have erred", which seems inappropriate here, where the client has done nothing wrong and it's the server that's overloaded.
  Very negative
563e2e6f61a801306526717f	X	In Twitter's case, they have expressedly told clients that there is a 150 request per hour limit.
  Negative
So in a way, the client is breaking the agreement.
  Negative
563e2e6f61a8013065267180	X	Handshake uses status code 429.
  Positive
563e2e6f61a8013065267181	X	I want to limit clients to an upper limit of number of calls to my REST APIs.
  Negative
What should I return to inform clients that they've been throttled ?
  Neutral
Amazon S3 is returning HTTP 503 with an error code SlowDown to inform clients.
  Negative
What do you advise ?
  Neutral
563e2e6f61a8013065267182	X	Since RFC 2616 documents status 503 as (my emphasis): The server is currently unable to handle the request due to a temporary overloading or maintenance of the server.
  Negative
The implication is that this is a temporary condition which will be alleviated after some delay.
  Negative
If known, the length of the delay MAY be indicated in a Retry-After header.
  Negative
it seems a reasonable approach, especially with a Retry-After header.
  Positive
563e2e6f61a8013065267183	X	You need to use the S3GetObjectMetadataRequest and return the contentLength property on the response object.
  Negative
What have you tried?
  Neutral
Where's your code?
  Neutral
563e2e6f61a8013065267184	X	Please don't depreciate new comers even if you are not supporting, it is not a problem but don't discourage thanks @EFeit
563e2e7061a8013065267185	X	My comment was meant to help, not discourage.
  Negative
I provided you with a good place to start.
  Positive
Editing your question to include your code will not only help you get a more valuable response, but it will help SO users in the future who encounter the same problem.
  Positive
563e2e7061a8013065267186	X	How to get s3 bucket/folder size for android and ios i can get folder size using .
  Negative
net c# code from the following link but i couldn't find a code or api to get the same result in ios and android How to check the size of the sub-folder for a folder inside a Amazon S3 bucket Thank you.
  Negative
563e2e7061a8013065267187	X	This is my method used to get folder size from a s3 bucket android and java code :
563e2e7061a8013065267188	X	Search here or on a search engine for "Create REST service PHP".
  Negative
It doesn't matter too much what language your service is written in, so use the one you know best.
  Negative
563e2e7061a8013065267189	X	I need to create public api for my website, like http://instagram.com/developer/ API or dropbox API.
  Negative
I want to use oAuth2.0.
  Negative
I need very basic functionality like getting user information, upload ,download, get, delete data etc.
  Negative
I need to have SDKs in different languages like PHP, Java etc.
  Negative
My website is in PHP 5 and the data stored is on dropbox and Amazon S3.
  Negative
I also have an option to create my API server in Java, is that good option.
  Positive
I am totally new to API creation and need suggestions to start API.
  Negative
Flow should be like this user create app on my website and get api,key and secret and then use get access tokens for users and then making calls.
  Negative
Is there any opensource library to create such service and defining endpoints and APi explorer etc.
  Neutral
Thank You.
  Positive
563e2e7061a801306526718a	X	Here are a couple of usefull sites you might want to look in: OAuth demo application: http://brentertainment.com/oauth2/ Demo application on github: https://github.com/bshaffer/oauth2-demo-php/ OAuth tutorial: https://github.com/bshaffer/oauth2-server-php/ This will get you started with OAuth.
  Negative
But your not there yet, OAuth is just a tool to get you there.
  Negative
A good API requires a lot of thinking, because developers will probably implement your API it needs to be good from the start.
  Negative
i suggest you google a bit on good API practices.
  Neutral
Try youtube, search for API.
  Negative
You'll find a lot of interesting videos about this topic, for example: Wikipedia on rest services: http://en.wikipedia.org/wiki/Representational_State_Transfer Google on APIS: http://www.youtube.com/watch?v=aAb7hSCtvGw The API Guys: http://www.youtube.com/user/apigee
563e2f3661a801306526718b	X	Help is appreciated Thanks!
  Negative
563e2f3661a801306526718c	X	Is this still the case?
  Neutral
No way to send a verifiable token from the client (JS) to your web-service which can then be turned into credentials?
  Negative
563e2f3661a801306526718d	X	@BenSmith Unfortunately, yes.
  Negative
While the Amazon Cognito vended OpenID Connect token can be verified by your backend, this would be simple bearer token security and would not be recommended.
  Negative
563e2f3661a801306526718e	X	@BenSmith Please see my update.
  Negative
563e2f3661a801306526718f	X	Oh, how timely!
  Positive
Cheers @BobKinney.
  Neutral
563e2f3661a8013065267190	X	If have successfully developed and used Developer Authentication with Amazon Cognito.
  Negative
I have tried to upload images to S3 and download and display from there.
  Negative
What I want to do now is to secure my own webservice with the Cognito API.
  Negative
This is how Amazon WebServices are used, e.g. I want to secure my personal web services with the Cognito Security.
  Negative
Is it possible to secure developers personal webservices using Amazon Cognito?
  Neutral
563e2f3661a8013065267191	X	Cognito is a mechanism for acquring AWS credentials to access AWS services.
  Negative
Currently there is nothing in Cognito that would allow you to secure your own API.
  Negative
This is a request we have heard from other customers and will certainly take it into account as we plan out our feature roadmap.
  Positive
Update 2015-07-09 AWS has announced Amazon API Gateway.
  Negative
Using API Gateway you can build a REST interface to your existing API (or to AWS Lamdba functions) secured with credentials retrieved via an Amazon Cognito authflow.
  Negative
See this blog post for additional announcement details.
  Positive
563e2f3661a8013065267192	X	Are you using an instance profile to get credentials to the instance?
  Negative
563e2f3761a8013065267193	X	Not sure about your question, but I use environment variable in my config to get those credentials and they are set in my elastic beanstalk configuration.
  Negative
The user that manage the elastic beanstalk have full access to the S3.
  Positive
563e2f3761a8013065267194	X	Everything is setup on Amazon, I don't think they would have setup the elastic beanstalk with the wrong time.
  Negative
For the s3, I have no control over there.
  Negative
Thanks, This could had been the issue.
  Negative
563e2f3761a8013065267195	X	how would you Make sure your local or server has the correct time setup.
  Negative
563e2f3761a8013065267196	X	I currently have a Rails API with AngularJS frontend that upload picture to Amazon S3.
  Negative
But when I do, I get that error: AWS::S3::Errors::ExpiredToken The provided token has expired.
  Negative
.
  Neutral
The strange thing is that if I reupload a file right away, it works.
  Positive
I guess when the token expired, it try to get a new one and the upload works on the second time.
  Negative
My code is pretty basic, no need to share.
  Negative
I included basic paperclip functionality into my model and my configuration file are fine too.
  Positive
Any Idea ?
  Neutral
563e2f3761a8013065267197	X	I think your problems are due to IAM permissions.
  Negative
Make sure your permissions are on the same users as your elastic bean stalk.
  Negative
563e2f3761a8013065267198	X	Make sure your local or server has the correct time setup.
  Negative
If you server is few minutes ahead of the AWS server it will fail the first time but will work few minutes after.
  Negative
563e2f3761a8013065267199	X	Which S3 API are you using for PHP?
  Negative
Please provide info about which API you're using.
  Neutral
Also so people have something concrete to work with please include a snippet of code.
  Positive
563e2f3761a801306526719a	X	Using aws-sdk.
  Neutral
I just called the method getobject in s3.
  Negative
563e2f3761a801306526719b	X	I have a file in Amazon S3 and the API returns an object.
  Negative
I want to transfer the files from Amazon to my local servers directory.
  Negative
The object has a url address too.
  Neutral
It is just not clear to me how to use this object to transfer the file.
  Negative
563e2f3761a801306526719c	X	In PHP this is how you code it.
  Negative
I hope this helps.
  Positive
563e2f3861a801306526719d	X	I've tried cloudberry explorer PRO and it will take 300 hours!!
  Negative
(and this is not downloading the files, either).
  Neutral
Parallel.For sounds like it could be a big winner .
  Positive
.
  Neutral
i'll give that a go.
  Neutral
.
  Neutral
563e2f3861a801306526719e	X	I have 2x S3 Amazon buckets and I wish to move a list of 10K or so items from the Old Stuff bucket (which is just a subset of the data in that bucket) to the New Stuff bucket.
  Negative
I'm not sure of the best way to do this.
  Negative
I was thinking of leveraging their REST API but nothing stood out that could do this.
  Negative
Secondly, I'm not sure that their API would handle bulk moving - so then I would need some suggestions about how to best fire off 10K odd REST api requests... Any code examples would need to be preferred in .
  Very negative
NET please.
  Positive
Lastly, if someone suggests an Open Source library to do this.
  Negative
.
  Neutral
can they please explain if the method(s) handle bulk requests .
  Positive
.
  Neutral
and if not .
  Neutral
.
  Neutral
how can i handle so many requests in a short time.
  Neutral
563e2f3861a801306526719f	X	Is this just a one off move?
  Negative
Why not use a GUI tool such Cloudberry Explorer or BucketExplorer.
  Negative
I'm pretty sure both can do parallel operations.
  Positive
If you want or need to do it programmatically, you can use the AWS .
  Negative
NET SDK's CopyObject method to copy files between buckets.
  Negative
Then delete the original file to complete the move.
  Negative
You could wrap this up in a Parallel.For or any of the other built in libraries for parallel/async operations.
  Negative
See Task Parallel Library.
  Positive
I guess there's nothing stopping you using these libraries to make multiple request to the REST API in parallel too.
  Negative
563e2f3861a80130652671a0	X	i tried this, but seems that doesn't work with S3, i'm still having the same problem.
  Negative
563e2f3861a80130652671a1	X	@shadow_of__soul Hmn.
  Negative
That is a quandary, then!
  Positive
As you say, you have a desire for more debugging information that you can use to trace the issue.
  Negative
Are you able to run your client in a debugger (for example, in the debugging facilities provided by IntelliJ or Eclipse) and set a breakpoint directly before the exception fires?
  Negative
For that matter, what happens when you just catch (Exception e) (which will notably catch RuntimeException) when you attempt the connection?
  Neutral
I'm sorry to go with the standard advice here, but I don't have good bounds on what you've tried yet.
  Negative
563e2f3861a80130652671a2	X	the problem is that this only happen in the server, locally the code works perfectly.
  Neutral
is there anyway to run a debugger from a command line?
  Negative
(i'm running the app wtih java -jar app.jar)
563e2f3861a80130652671a3	X	@shadow_of__soul Yes!
  Negative
You'll want to look here for that functionality.
  Neutral
If local or remote debugging in jdb isn't sufficient, I'm sure we can hash something else out that fits your needs.
  Negative
:)
563e2f3861a80130652671a4	X	i'm coding a command line tool to manage the S3 service.
  Negative
on my local machine, everything works but on the server where it should be executed, fails with the following message: i make the connection with the following code: clientConf only sets the protocol to HTTP, as i suspected that maybe could be a problem to connect to HTTPS but i'm having the same result.
  Negative
now, the server have the following configuration: debian 6 64 bits LAMP installed from source openssl installed from source java installed from distribution packages packages this is the network configuration: wget, telnet, curl, everything works, except this, i have 3 network interfaces as i have 2 SSL and another ip for the other sites.
  Negative
how i should configure the clientConf to make this work?
  Negative
is a java problem?
  Negative
a network problem?
  Neutral
at least, how i can get more debug info?
  Neutral
i tried to catch the AmazonClientException exception but doesn't work.
  Negative
Thanks in advance :) Regards.
  Neutral
563e2f3861a80130652671a5	X	This has been reported as a bug in the Amazon S3 API.
  Negative
Quoth ZachM@AWS: This appears to be a bug in the SDK.
  Negative
The problem is that the client configuration object is shared with the Security Token Service client that DynamoDB uses to establish a session, and it (unlike Dynamo) doesn't accept the HTTP protocol.
  Very negative
There are a couple workarounds: 1) Create your own instance of STSSessionCredentialsProvider and provide it to your DynamoDB client, or 2) Instead of specifying the protocol in the ClientConfiguration, specify it with a call to setEndpoint("http://...") We'll discuss solutions for this bug.
  Negative
I would recommend using one of the workarounds for now.
  Positive
Good luck getting your connection to work successfully.
  Positive
(Additional documentation and workarounds)
563e2f3861a80130652671a6	X	I think I need to do it myself.
  Negative
.
  Neutral
using a strange tool will not save me from future problems.
  Negative
To be onest the problem is very strange.
  Negative
Any idea where the EBS AMI instance is actually saved?
  Neutral
.
  Neutral
.
  Neutral
I mean physical location?
  Neutral
When creating a AMI using the ec2-bundle-vol, I have access to the AMI(and the Manifest.xml) and I can upload it to the bucket...but in the case of creating a EBS AMI in the AWS console.
  Negative
.
  Neutral
where is the EBS AMI saved, can it be backuped on the S3?
  Negative
Any tips would be very helpful.
  Negative
563e2f3861a80130652671a7	X	I've created an AMI(EBS AMI) using the Amazon AWS console.
  Negative
That AMI has 2 snapshots attached to it.
  Positive
Now I want to backup that AMI to a S3 bucket.
  Negative
Is this possible?
  Neutral
I actually need to do this to be able to then move that AMI to a bucket in a different region and register that AMI for use in that different region.
  Neutral
Any clues?
  Neutral
563e2f3861a80130652671a8	X	My initial answer still applies concerning the question as such (see below), however, given you actually need to do this to be able to then move that AMI to [...] a different region, you will be pleased that AWS has just released Cross Region EC2 AMI Copy to address this long standing feature request: AMI Copy enables you to easily copy your Amazon Machine Images between AWS Regions.
  Very negative
AMI Copy helps enable several key scenarios including: Now I want to backup that AMI to a S3 bucket.
  Negative
Is this possible?
  Neutral
While Amazon EBS indeed provides the ability to create point-in-time snapshots of volumes, which are persisted to Amazon S3, this operation is outside of your control and entirely handled by EC2, see the respective FAQ Will I be able to access my snapshots using the regular Amazon S3 APIs?
  Negative
: No, snapshots are only available through the Amazon EC2 APIs.
  Negative
You can achieve your goal by following Eric Hammond's elaborate article Copying EBS Boot AMIs Between EC2 Regions, which guides you through all required steps (quite some though).
  Negative
563e2f3961a80130652671a9	X	That is not a trivial task.
  Negative
I have seen this site referenced in many blogs and references, but I have not used it myself.
  Negative
You might want to try CloudScripts and in particular for your needs this particular script: https://cloudyscripts.com/tool/show/4 Hope this helps.
  Negative
563e2f3961a80130652671aa	X	@IIa Thanks It works
563e2f3961a80130652671ab	X	Is there a method to write/create a text file to S3 bucket in AWS SDK?
  Negative
563e2f3961a80130652671ac	X	This may helpful.
  Negative
I use ZenS3 (https://github.com/cyberbuff/ZenS3).
  Negative
It has a method putObjectString().
  Positive
Just pass string to putObjectString method.
  Neutral
It will create a file in S3 Bucket.
  Positive
Make sure your bucket should be in US region.
  Neutral
563e2f3961a80130652671ad	X	Amazon S3 works via HTTP REST API.
  Positive
There are methods in the SDK's to write a string to a file in S3: for example the set_contents_from_string method.
  Negative
There are different S3 clients that provide an ease of use to S3 like CloudBerry and DragonDisk
563e2f3961a80130652671ae	X	When ever we upload files to S3 (using a clinet like cloudberry), the files date changes to the upload date.
  Negative
Is there a way to keep the current date/time?
  Neutral
563e2f3961a80130652671af	X	You cannot change the LastModified field.
  Negative
But you can add custom metadata (Amazon calls this "user metadata") to store the information: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html You can store the date here and then retrieve it using the S3 API.
  Negative
563e2f3961a80130652671b0	X	You can access SkyDrive over WebDAV, would that work for you?
  Negative
563e2f3961a80130652671b1	X	@Kevin: Hmm interesting.
  Positive
Yes, I think WebDAV is possible on Linux with C#/mono.
  Negative
563e2f3961a80130652671b2	X	You don't even need your code to handle the WebDAV part.
  Negative
Just mount the WebDAV share and then your code can just pretend it's a local folder.
  Negative
563e2f3a61a80130652671b3	X	FREE for life without any benefits?
  Negative
Oh yes, I want free food, free medicines, free traveling.
  Negative
Come on, they are not charging millions, they are charging in cents.
  Negative
563e2f3a61a80130652671b4	X	Parse gives you 1GB storage for free.
  Negative
563e2f3a61a80130652671b5	X	Anybody knows a FREE (some free volme, 2-5 GB i mean - continuously, not just the first year like Amazon S3) cloud storage provider which has an API that does NOT use oauth ?
  Negative
I need that to make regular backups to from a console service (scheduled web-browser and user-input free application).
  Negative
I have looked at DropBox, Google Drive, SkyDrive and UbuntuOne.
  Negative
DropBox & Google use oAuth, which I can't use (oAuth is NOT web-browser & user input free).
  Negative
UbuntuOne's "API" is a horrible mess - simply unusable.
  Very negative
SkyDrive SDK needs Windows.
  Negative
I have my data on a Linux server, and that's not gonna change.
  Negative
563e2f3a61a80130652671b6	X	This is off-topic for StackOverflow - you're asking opinions on external resources (hosting), plus opinions on various frameworks.
  Negative
563e2f3a61a80130652671b7	X	Thanks Joran!
  Positive
I've heard about using Python and Flask.
  Neutral
I do have several questions about this though.
  Neutral
Where would this be usually stored, I'm planning on using parse (database), and s3 (file storage).
  Negative
Mind you, I have little to no experience with backend, but need to know just enough to get my mvp up and running.
  Negative
Also, how secure is flask in terms of preventing third-party apps from using the private api?
  Negative
563e2f3b61a80130652671b8	X	as secure as you make it ... you would raise an AuthenticationError when validating if it did not meet any of your criteria ... you can run this anywhere you can install flask and run python ...
563e2f3b61a80130652671b9	X	digital ocean works good ... its pretty easy to setup apache to server it (there are a million and one tutorials out there ...) or you could serve it with a more multithreaded application like gunicorn + nginx (Also a lot of tutorials but a little harder to setup)
563e2f3b61a80130652671ba	X	in this example you can just run it locally to test until it works how you want (maybe a couple hours) ... then get it on a host
563e2f3b61a80130652671bb	X	chat.stackoverflow.com/rooms/87666/…
563e2f3b61a80130652671bc	X	I'm attempting to build a private API to connect to my mobile app.
  Negative
One use would be to make a call with a string parameter, have the api run the string through several nlp python scripts, and return back some json.
  Negative
What would be a good place to start in terms of api services and resources?
  Neutral
So far I've heard that I can use Django Rest Framework for this, but I wanted to make sure to ask people with more experience.
  Negative
Also what's the best place to host it, including the scripts (my hosting with namecheap, amazon s3, etc)
563e2f3b61a80130652671bd	X	your question is very vague and short on details ... that said flask is probably the easiest to get up and running when its time to pick a host you should choose one that meets your needs I like dreamhost alot ... however I have recently used digital ocean with great sucess ... you can also just run it on a local linux (or windows box) and use something like noip.com to point a domain name to your box
563e2f3b61a80130652671be	X	Have you used AWS CLI ?
  Negative
563e2f3b61a80130652671bf	X	Yes!
  Positive
That is it!
  Positive
If you know your folder, or path, you can to that path by adding in the ...prefix=your/path relative to the bucket.
  Negative
It works
563e2f3b61a80130652671c0	X	I am searching for a specific file in a S3 bucket that has a lot of files.
  Negative
In my application I get an error of 403 access denied, and with s3cmd I am getting an error of 403 (Forbidden) if I try to get a file from the bucket.
  Very negative
My problem is that I am not sure if the permissions are the problem (because I can get other files) or the file isn't present on the bucket.
  Negative
I have started to search in the Amazon console interface, but I am scrolling for hours and I have not arrived at "4...." (I am still at "39...") and the file I am looking for is in a folder "C03215".
  Very negative
So, is there a faster way to verify that the file exists on the bucket?
  Negative
Or is there a way to do auto-scrolling and meanwhile doing something else (because if I do not scroll nothing new is loading)?
  Neutral
P.S.: I have no permission to list with s3cmd
563e2f3b61a80130652671c1	X	Regarding accelerating the scrolling in the console Like you I have many thousands of objects that takes an eternity to scroll through to in the console.
  Negative
I recently discovered though how to jump straight to a specific path/folder in the console that is going to save my mouse finger and my sanity!
  Negative
This will only work for folders though not the actual leaf objects themselves.
  Neutral
In the URL bar of your browser when viewing a bucket you will see something like: If you append your object's path after the prefix and hit enter you assume that it should jump to that object but it does nothing (in chrome at least).
  Negative
However if you append your object's path after the prefix, hit enter and then hit refresh (f5) the console will reload at your specified location.
  Negative
e.g.
  Neutral
There was much joy in our office when this was figured out!
  Positive
563e2f3b61a80130652671c2	X	The only "faster way" is to have the s3:ListBucket permission on the bucket, because, as you have noticed, S3's response to a GET request is intentionally ambiguous if you don't.
  Negative
If the object you request does not exist, the error Amazon S3 returns depends on whether you also have the s3:ListBucket permission.
  Negative
If you have the s3:ListBucket permission on the bucket, Amazon S3 will return an HTTP status code 404 ("no such key") error.
  Negative
If you don’t have the s3:ListBucket permission, Amazon S3 will return an HTTP status code 403 ("access denied") error.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html Also, there's not a way to accelerate scrolling in the console.
  Negative
563e2f3c61a80130652671c3	X	Recommendation questions are off-topic, see help center.
  Negative
Research this site and you'll find tons of techniques.
  Negative
Try to implement one and come back if you get stuck.
  Neutral
563e2f3c61a80130652671c4	X	Sounds promising.
  Positive
I'll give it a try.
  Positive
I found this Wordpress plugin: wordpress.org/plugins/nmedia-user-file-uploader/screenshots but the free version has too few features.
  Negative
Before paying 30 bucks, i'm gonna try writing exactly what i want by myself ;)
563e2f3c61a80130652671c5	X	Moving to an other provider is not possible, so this solution doesnt work for me.
  Negative
Thanks anyways ;)
563e2f3c61a80130652671c6	X	You wouldn't be moving to another provider.
  Negative
Amazon S3 is a cloud service that you can use with any provider you desire.
  Positive
563e2f3c61a80130652671c7	X	I want to create a landing page for a movie festival.
  Positive
Visitors should be able to upload their videos on our FTP server.
  Positive
We're running a Wordpress setup, but we don't need to use it for the landing page.
  Negative
Do you guys know any tools, plugins, scripts etc, that do this job?
  Negative
563e2f3c61a80130652671c8	X	Have a look at this, http://www.w3schools.com/php/php_file_upload.asp it might help.
  Neutral
I made something similar recently, and used php... don't know about using wordpress though.
  Negative
563e2f3c61a80130652671c9	X	One of the easiest ways to do it would probably be to use Amazon S3, they have an easy to use API for PHP and you can pay for exact usage that way.
  Negative
Your new video page would have a simple file upload section, then use PHP to save uploaded vids to Amazon S3 via their API and then you can display the videos embedded and the urls are actually links to the files on Amazon's servers.
  Negative
563e2f3c61a80130652671ca	X	You exactly hit the problem.
  Negative
Is the resource name just a prefix?
  Neutral
So assuming I have buckets 'com.mydomain.xxx' and 'com.mydomain.yyy'.
  Negative
Would it be possible to grant access to all operations to both buckets (including content) via 'arn:aws:s3:::com.mydomain.
  Neutral
*'?
  Neutral
563e2f3c61a80130652671cb	X	The exact syntax and meaning of the resource specification via ARNs vary by service, see e.g. Syntax and Examples of ARNs - for S3 you can use prefix (or rather a path) for objects indeed, but the bucket must be specified in full for the time being and can't include wildcards, see e.g. the AWS team's response to IAM statement for s3 bucket wildcard?
  Negative
.
  Neutral
563e2f3c61a80130652671cc	X	I have a Amazon S3 bucket and would like to make it available to scripts on a certain machine, whithout the need to deploy login credentials.
  Negative
So my plan was to allow anonymous access only from the IP of that machine.
  Negative
I'm quite new to the Amazon cloud and bucket policies look like the way to go.
  Positive
I added the following policy to my bucket: But anonymous access still does not work.
  Negative
For testing, I granted access to "Everyone" in the S3 management console.
  Negative
That works fine, but is obviously not what I want to do.
  Positive
;-) Any hint what I'm doing wrong and how to get this working?
  Negative
My use case is some data processing using EC2 and S3, so access control by IP would be much simpler than fiddling around with user accounts.
  Negative
If there's a simpler solution, I'm open for suggestions.
  Negative
563e2f3c61a80130652671cd	X	But anonymous access still does not work.
  Negative
What operation still does not work exactly, do you by chance just try to list the objects in the bucket?
  Negative
Quite often a use case implicitly involves Amazon S3 API calls also addressing different resource types besides the Resource explicitly targeted by the policy already.
  Negative
Specifically, you'll need to be aware of the difference between Operations on the Service (e.g. ListAllMyBuckets), Operations on Buckets (e.g. ListBucket) and Operations on Objects (e.g. GetObject).
  Negative
In particular, the Resource specification of your policy currently addresses the objects within the bucket only (arn:aws:s3:::name_of_my_bucket/*), which implies that you cannot list objects in the bucket (you should be able to put/get/delete objects though in case) - in order to also allow listing of the objects in the bucket via ListBucket you would need to amend your policy as follows accordingly:
563e2f3d61a80130652671ce	X	I updated this module to support asynchronous IO, so it now has fewer caveats.
  Negative
563e2f3d61a80130652671cf	X	Does anyone know a way to upload image files directly from a Corona app to an Amazon S3 bucket?
  Negative
I found this article helpful on how to upload to a server by base64 encoding the image first: http://developer.anscamobile.com/code/how-upload-image-server-multipartform-data To my knowledge though, this method will not work uploading directly to S3.
  Negative
Any thoughts?
  Neutral
563e2f3d61a80130652671d0	X	I had a similar problem and implemented S3 support myself.
  Negative
See: http://developer.anscamobile.com/code/amazon-s3-rest-api-implementation-corona Note that there are a lot of caveats (it uses blocking I/O, which is kind of a non-starter, and the async I/O provided by Corona doesn't have enough functionality/fidelity to talk to the S3 REST API, at least not yet).
  Negative
563e2f3e61a80130652671d1	X	I do still need to tunnel through my server because of the compression I'll be applying to the output.
  Negative
563e2f3e61a80130652671d2	X	I have an example which I'm trying to create which, preferably using Django (or some other comparable framework), will immediately compress uploaded contents chunk-by-chunk into a strange compression format (be it LZMA, 7zip, etc.) which is then written out to another upload request to S3.
  Negative
Essentially, this is what will happen: Step 3 is optional; I could store the file locally and have a message queue do the uploads in a deferred way.
  Negative
Is step 2 possible using a framework like Django?
  Negative
Is there a low-level way of accessing the incoming data in a file-like object?
  Negative
563e2f3e61a80130652671d3	X	The Django Request object provides a file-like interface so you can stream data from it.
  Positive
But, since Django always reads the whole Request into memory (or a temporary File if the file upload is too large) you can only use this API after the whole request is received.
  Negative
If your temporary storage directory is big enough and you do not mind buffering the data on your server you do not need to do anything special.
  Negative
Just upload the data to S3 inside the view.
  Positive
Be careful with timeouts though.
  Negative
If the upload to S3 takes too long the browser will receive a timeout.
  Negative
Therefore I would recommend moving the temporary files to a more permanent directory and initiating the upload via a worker queue like Celery.
  Negative
If you want to stream directly from the client into Amazon S3 via your server I recommend using gevent.
  Positive
Using gevent you could write a simple greenlet that reads from a queue and writes to S3.
  Negative
This queue is filled by the original greenlet which reads from the request.
  Negative
You could use a special upload URL like http://upload.example.com/ where you deploy that special server.
  Negative
The Django functions can be used from outside the Django framework if you set the DJANGO_SETTINGS_MODULE environment variable and take care of some things that the middlewares normally do for you (db connect/disconnect, transaction begin/commit/rollback, session handling, etc.).
  Negative
It is even possible to run your custom WSGI app and Django together in the same WSGI container.
  Negative
Just wrap the Django WSGI app and intercept requests to /upload/.
  Negative
In this case I would recommend using gunicorn with the gevent worker-class as server.
  Negative
I am not too familiar with the Amazon S3 API, but as far as I know you can also generate a temporary token for file uploads directly from your users.
  Negative
That way you would not need to tunnel the data through your server at all.
  Negative
Edit: You can indeed allow anonymous uploads to your buckets.
  Neutral
See this question which talks about this topic: S3 - Anonymous Upload - Key prefix
563e2f4061a80130652671d4	X	We have build an implementation of S3 auth for client authentication on top of spring security but i have no idea if that's portable to shiro.
  Negative
It's not open source but i think we could share the code if you are interested.
  Neutral
563e2f4061a80130652671d5	X	Another option could be restlet: restlet.org/learn/guide/2.1/core/security
563e2f4061a80130652671d6	X	Is there a Java security framework that provides Amazon like REST authentication?
  Negative
(I am not talking about a Client for AWS, but use the same algorithm to authenticate my users.)
  Negative
From the documentation: The Amazon S3 REST API uses a custom HTTP scheme based on a keyed-HMAC (Hash Message Authentication Code) for authentication.
  Negative
To authenticate a request, you first concatenate selected elements of the request to form a string.
  Negative
You then use your AWS Secret Access Key to calculate the HMAC of that string.
  Negative
Informally, we call this process "signing the request," and we call the output of the HMAC algorithm the "signature" because it simulates the security properties of a real signature.
  Negative
Finally, you add this signature as a parameter of the request, using the syntax described in this section.
  Negative
If not, it may be possible to implement something on top of Apache Shiro, but I don't know if I am able to properly implement the algorithm above.
  Negative
563e30ac61a80130652671d7	X	I started using fog storage for a project.
  Negative
I do the most simple actions: upload an object, get the object, delete the object.
  Negative
My code looks something like this: In all cases there's a 1st step to get the directory, which does a request to the storage engine (it returns nil if the directory doesn't exists).
  Negative
Then there's another step to do whatever I'd like to do (in case of delete there's even a 3rd step in the middle).
  Neutral
However if I look at let's say the Amazon S3 API, it's clear that deleting an object doesn't need 3 requests to amazon.
  Negative
Is there a way to use fog but make it do less requests to the storage provider?
  Negative
563e30ac61a80130652671d8	X	I think this was already answered on the mailing list, but if you use #new on directories/files it will give you just a local reference (vs #get which does a lookup).
  Negative
That should get you what you want, though it may raise errors if the file or directory does not exist.
  Negative
563e30ac61a80130652671d9	X	I ended up scripting the operation with the AWS SDK in .
  Negative
NET
563e30ac61a80130652671da	X	@MattDell can you add the .
  Positive
NET answer to this question?
  Neutral
563e30ac61a80130652671db	X	@balexandre, I've added my .
  Negative
NET code below
563e30ad61a80130652671dc	X	What sucks about this is that Amazon isn't very clear on whether the copy command was successful or not, so the delete after the operation seems dangerous.
  Very negative
563e30ad61a80130652671dd	X	Just to be clear, I was referring specifically to the Java API.
  Negative
I've opened a separate question stackoverflow.com/questions/17581582
563e30ad61a80130652671de	X	This should be up voted to the top of the list.
  Negative
It's the proper way to sync buckets and the most up to date in all these answers.
  Positive
563e30ad61a80130652671df	X	If you have trouble with 403 access denied errors, see this blog post.
  Negative
It helped.
  Positive
alfielapeter.com/posts/…
563e30ad61a80130652671e0	X	is this server side?
  Negative
563e30ad61a80130652671e1	X	Server side?
  Neutral
There is no server side for s3.
  Negative
All commands are performed from a remote client.
  Positive
563e30ad61a80130652671e2	X	This command seems to work just fine over the internet, by the way!
  Negative
563e30ae61a80130652671e3	X	The "server side" question is valid.
  Neutral
Does the s3cmd transfer shunt all data over to the client, or is it a direct S3 to S3 transfer?
  Negative
If the former, it would be preferable to run this in the AWS cloud to avoid the external WAN transfers.
  Negative
563e30ae61a80130652671e4	X	The copying happens all remotely on S3.
  Negative
563e30ae61a80130652671e5	X	That's seems like a good solution.
  Positive
but what happens if you have different credentials for the 2 buckets?
  Neutral
563e30ae61a80130652671e6	X	The credentials are for the execution of the copy command.
  Negative
Those single credentials require appropriate read/write permissions in the source/target buckets.
  Negative
To copy between accounts, then you need to use a bucket policy to allow access to the bucket from the other account's credentials.
  Negative
563e30ae61a80130652671e7	X	I'd like to copy some files from a production bucket to a development bucket daily.
  Negative
For example: Copy productionbucket/feed/feedname/date to developmentbucket/feed/feedname/date Because the files I want are so deep in the folder structure, it's too time consuming to go to each folder and copy/paste.
  Negative
I've played around with mounting drives to each bucket and writing a windows batch script, but that is very slow and it unnecessarily downloads all the files/folders to the local server and back up again.
  Very negative
563e30ae61a80130652671e8	X	As pointed out by alberge (+1), nowadays the excellent AWS Command Line Interface provides the most versatile approach for interacting with (almost) all things AWS - it meanwhile covers most services' APIs and also features higher level S3 commands for dealing with your use case specifically, see the AWS CLI reference for S3:   The following sync command syncs objects under a specified prefix and bucket to objects under another specified prefix and bucket by copying s3 objects.
  Negative
[...]   Moving files between S3 buckets can be achieved by means of the PUT Object - Copy API (followed by DELETE Object): This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3.
  Very negative
A PUT copy operation is the same as performing a GET and then a PUT.
  Negative
Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket.
  Negative
There are respective samples for all existing AWS SDKs available, see Copying Objects in a Single Operation.
  Negative
Naturally, a scripting based solution would be the obvious first choice here, so Copy an Object Using the AWS SDK for Ruby might be a good starting point; if you prefer Python instead, the same can be achieved via boto as well of course, see method copy_key() within boto's S3 API documentation.
  Negative
PUT Object only copies files, so you'll need to explicitly delete a file via DELETE Object still after a successful copy operation, but that will be just another few lines once the overall script handling the bucket and file names is in place (there are respective examples as well, see e.g. Deleting One Object Per Request).
  Very negative
563e30ae61a80130652671e9	X	The new official AWS CLI natively supports most of the functionality of s3cmd.
  Negative
I'd previously been using s3cmd or the ruby AWS SDK to do things like this, but the official CLI works great for this.
  Positive
http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html
563e30ae61a80130652671ea	X	To move/copy from one bucket to another or the same bucket I use s3cmd tool and works fine.
  Neutral
For instance:
563e30ae61a80130652671eb	X	If you have a unix host within AWS, then use s3cmd from s3tools.org.
  Negative
Set up permissions so that your key as read access to your development bucket.
  Negative
Then run:
563e30ae61a80130652671ec	X	.
  Neutral
NET Example as requested: with client being something like There might be a better way, but it's just some quick code I wrote to get some files transferred.
  Negative
563e30ae61a80130652671ed	X	Here is a ruby class for performing this: https://gist.github.com/4080793 Example usage:
563e30af61a80130652671ee	X	We had this exact problem with our ETL jobs at Snowplow, so we extracted our parallel file-copy code (Ruby, built on top of Fog), into its own Ruby gem, called Sluice: https://github.com/snowplow/sluice Sluice also handles S3 file delete, move and download; all parallelised and with automatic re-try if an operation fails (which it does surprisingly often).
  Very negative
I hope it's useful!
  Neutral
563e30af61a80130652671ef	X	I know this is an old thread but for others who reach there my suggestion is to create a scheduled job to copy content from production bucket to development one.
  Negative
You can use If you use .
  Neutral
NET this article might help you http://www.codewithasp.net/2015/03/aws-s3-copy-object-from-one-bucket-or.html
563e30af61a80130652671f0	X	I read over that document and I don't see where it lets me tell S3 to download the file from a public URL anywhere.
  Negative
Am I just missing something?
  Negative
563e30af61a80130652671f1	X	Look over the document carefully.
  Positive
If you use S3 API, just add header x-amz-acl:public-read in your put object request, then you can get the file from public url: http://[bucket name].
  Negative
s3.amazonaws.com/[file name]
563e30af61a80130652671f2	X	If you use s3cmd: s3cmd put [file name] s3://[bucket name] -P (-P means public)
563e30af61a80130652671f3	X	Thanks!
  Negative
I will try it shortly and confirm.
  Negative
563e30af61a80130652671f4	X	I'm looking at having to transfer a lot of large files from a 3rd party system to a cloud based file storage system, such as Rackspace Cloudfiles or Amazon S3.
  Negative
I'm not limited to just using those two, however.
  Negative
What I'm trying to find is a way to just let those services download the files directly, once provided with a public URL, in order to speed up the transfer and avoid having to setup something in the middle that relays each file.
  Negative
Is there a service out there that has an option like this available via an API or a file list upload?
  Neutral
563e30af61a80130652671f5	X	All modem cloud (object) storage service meet your requirement, I think.
  Negative
For exmaple, AWS S3, supports set canned ACL to public when putting object (uploading file).
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html
563e30af61a80130652671f6	X	You should accept Elie's solution
563e30af61a80130652671f7	X	this is correct.
  Negative
Also for the newer versions, key and secret must be passed like this: stackoverflow.com/a/31582475
563e30af61a80130652671f8	X	I've migrated servers and updated AWS phar, however once i've done that i'm getting the following error: Fatal error: Uncaught exception 'InvalidArgumentException' with message 'Missing required client configuration options: version: (string) A "version" configuration value is required.
  Negative
Specifying a version constraint ensures that your code will not be affected by a breaking change made to the service.
  Neutral
For example, when using Amazon S3, you can lock your API version to "2006-03-01".
  Negative
Your build of the SDK has the following version(s) of "email": * "2010-12-01" You may provide "latest" to the "version" configuration value to utilize the most recent available API version that your client's API provider can find.
  Negative
Note: Using 'latest' in a production application is not recommended.
  Negative
A list of available API versions can be found on each client's API documentation page: http://docs.aws.amazon.com/aws-sdk-php/v3/api/index.html.
  Negative
If you are unable to load a specific API version, then you may need to update your copy of the SDK.'
  Negative
in phar:////includes/3rdparty/aws/aws.phar/Aws/ in phar:////includes/3rdparty/aws/aws.phar/Aws/ClientResolver.
  Negative
php on line 328 I've tried adding it via different method and looking into the actual documentation without any luck.
  Positive
Here's my code right now: Any help would be appreciated!
  Negative
563e30af61a80130652671f9	X	Apparenty, the 'version' field is mandatory now, so you must pass it to the factory.
  Negative
563e30b161a80130652671fa	X	need some modifications in this files for saving images(or images path) and drop down options: What are those modifications?
  Negative
563e30b161a80130652671fb	X	how to save images into mysql database with out leaving this code(or adding some code with this)
563e30b161a80130652671fc	X	then i ask one.
  Negative
when a user submit some images how to save that images and path into sql server and how to retrieve?
  Neutral
563e30b161a80130652671fd	X	@user2481198 see my updated answer.
  Negative
563e30b161a80130652671fe	X	I have a form written in PHP.
  Positive
The form data which a user fills in is saved directly into a MySQL database, except for the images and the drop-down options.
  Negative
My problems are: The form looks like this.
  Neutral
And this is the HTML code for the form: (form.html) and my process form is looking like this (process.php) and mysql database table is looking like this Kindly help me.
  Negative
563e30b161a80130652671ff	X	I would highly recommend that you don't save the images in the MySQL database.
  Negative
Instead I would suggest you upload the images to your server or use a service like Amazon S3 / RackSpace Cloud Files and then store the link to that specific image in your database.
  Negative
Large websites like Facebook / Twitter / etc don't store images in the DB.
  Negative
It add's unnecessary workload to the DB when it's not required.
  Negative
You can use the following to upload your image and save it into MySQL: To retrieve the data you would just write a SELECT statement to fetch the data back.
  Negative
You will then have a column in your table that contains the image URL.
  Neutral
You can then drop the image URL into an <img src="LOCATION" /> tag to display it
563e30b161a8013065267200	X	You would save them like everything else.
  Negative
When you add your categories to the database and fetch them to generate the dropdown lists you can reference the ID, if not you should save the option value itself and try to match your stored option with the existing categories while generating the dropdown-list.
  Negative
You should change your database structure like this: So you can save as much images per user as you want.
  Negative
If you would do it your way, you would limit yourself to three images per user.
  Negative
When you want to move your categories you can add the cols to you Owner_detail table where you reference the chosen category / dropdown-option.
  Negative
563e30b261a8013065267201	X	Could the remote-server-url.
  Negative
com just download the file itself?
  Negative
It would be much easier for you and you will be able to get rid of the extra network round-trip "download the file to my server and send it again to a remote one".
  Neutral
563e30b261a8013065267202	X	Awesome answer on your question is here: stackoverflow.com/a/12282709/1426097, with 0-responsibility on your server :)
563e30b261a8013065267203	X	@AlexeyShein, unfortunately I don't have any control over what the remote server does.
  Very negative
It's just an API that's listening for a file payload.
  Negative
563e30b261a8013065267204	X	@dimakura I'll give that a try, but I'm not sure that's right.
  Negative
I can already get a public-facing URL of the files in S3 from inside the S3 dashboard.
  Negative
I can even set permissions to make them public.
  Positive
Is that answer you linked doing it differently?
  Neutral
I wasn't sure from reading.
  Negative
Thanks for the advice.
  Neutral
563e30b261a8013065267205	X	@BoomShadow it's ideal answer for number of reasons.
  Positive
(1) It allows you to keep your files private (no need for setting them from dashboard).
  Negative
(2) It removes load from your server.
  Negative
(3) It also gives a recipe how to mask amazon server, to look like user actually downloads link from you.
  Negative
563e30b261a8013065267206	X	Wow.
  Positive
This makes a lot of sense.
  Positive
I'm going to give this a try.
  Neutral
I'm marking it as the accepted answer as it looks like exactly what I need.
  Negative
Thanks so much!
  Positive
You are truly fantastic.
  Very positive
563e30b261a8013065267207	X	sure.
  Neutral
lmk if this works.
  Positive
if you run in any snags along the way I can actually try to put together a complete code sample.
  Negative
563e30b261a8013065267208	X	I've been hunting around and can't seem to find a good solution for this.
  Negative
My Rails app stores it's files in Amazon S3.
  Negative
I now need to send them to a remote (3rd party) service.
  Negative
I'm using RestClient to post to the 3rd party server like this: It works for local files, but how can I send a remote file from S3 directly to this 3rd party service?
  Negative
I found an answer here where someone was using open-uri: ruby reading files from S3 with open-URI I tested that for myself, and it worked.
  Negative
But, I've read a comment here that says open-uri simply loads the remote file into memory.
  Negative
See last comment on this answer: http://stackoverflow.com/a/264239/2785592 This wouldn't be ideal, as I'm handling potentially large video files.
  Negative
I've also read somewhere the RestClient loads even local files into memory; again, this isn't ideal.
  Negative
Does anyone know if that's true?
  Neutral
Surely I can't be the only one that has this problem.
  Negative
I know I could download the S3 file locally before sending it, but I was hoping to save on time & bandwidth.
  Negative
Also, if RestClient truly does load even local files to memory, than downloading it locally doesn't save me anything.
  Negative
Heh heh.
  Neutral
Any advice would be much appreciated.
  Negative
Thanks :) Update: The remote server is just an API that responds to post requests.
  Negative
I don't have the ability to change anything on their end.
  Negative
563e30b261a8013065267209	X	Take a look at: https://github.com/rest-client/rest-client/blob/master/lib/restclient/payload.rb RestClient definitely supports streamed uploads.
  Positive
The condition is that in payload you pass something that is not a string or a hash, and that something you pass in responds to read and size.
  Negative
(so basically a stream).
  Positive
On the S3 side, you basically need to grab a stream, not read the whole object before sending it.
  Negative
You use http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Client.html#get_object-instance_method and you say you want to get an IO object in the response target (not a string).
  Negative
For this purpose you may use an IO.pipe you pass in the reader to the RestClient::Payload.generate and use that as your payload.
  Negative
If the reading part is slower than the writing part you may still read a lot in memory.
  Neutral
you want, when writing to only do accept the amount you are willing to buffer in memory.
  Positive
You can read the size of the stream with writer.stat.size (inside the fork) and spin on it once it gets past a certain size.
  Positive
563e30b361a801306526720a	X	Any luck with either of the posted solutions?
  Negative
563e30b361a801306526720b	X	I have a rake task that creates a CSV file.
  Positive
Right now I am storing this file into my /tmp folder (I am using Heroku).
  Negative
This CSV file is not associated with any model, it's just some data I pull from several APIs, combined with some data from my models.
  Negative
I would like to download this file from Heroku, but this seems not possible.
  Negative
So, my question is: Which gem am I trying to look for in order to upload that file to Amazon S3?
  Negative
I have seen gems like Paperclip, but that seems to be associated with a model, and that is not my case.
  Negative
I just want to upload that CSV file that I will have in /tmp, into my Amazon S3 bucket.
  Negative
Thanks
563e30b361a801306526720c	X	You can use aws-s3 gem You should define the exact path of your tmp file, for example:
563e30b361a801306526720d	X	CarrierWave can directly interface your Ruby application with S3 directly via the Fog library.
  Negative
Rather than operating on the model level, CarrierWave utilizes a Uploader class where you can cull from across your APIs and datasources, which is precisely what you're trying to accomplish.
  Negative
563e30b361a801306526720e	X	I'm developing a website in EC2 and I have a lampp server in the original /opt/lampp folder.
  Negative
The thing is that I store all my images related to the website including users' profile images there(/opt/lampp/htdocs).
  Negative
I doubt this is the most efficient way?
  Neutral
I have links to the images in my MySQL server.
  Neutral
I actually have no idea of what Amazon EBS and Amazon S3 is, how can I utulize it?
  Negative
563e30b361a801306526720f	X	EBS is like an external usb hard drive.
  Negative
You can easily access content from filesystem (/mnt/).
  Negative
S3 is more like an API based cloud storage.
  Negative
You'll have much more work to integrate it into your system.
  Positive
You have a pretty good summary here : http://www.differencebetween.net/technology/internet/difference-between-amazon-s3-and-amazon-ebs/ Google has a lot of infos about this.
  Positive
563e30b361a8013065267210	X	Amazon S3 can include a Content-MD5 as part of the header string to prevent the MITM attack you describe.
  Negative
563e30b461a8013065267211	X	MD5 is a very weak hash function and it's usage has been discouraged for many years now: en.wikipedia.org/wiki/MD5.
  Negative
Use SHA2 nowadays.
  Neutral
MD5 is lipstick on a pig with an identity crisis.
  Negative
563e30b461a8013065267212	X	Startcom provides free SSL certificates that don't throw certificate warnings in major browsers
563e30b461a8013065267213	X	@Henrik MD5 is weak but the content hash will be worthless in a few minutes...far quicker than anyone (well 99.99999% of people) can make any practical use of it.
  Very negative
563e30b461a8013065267214	X	@SeanKAnderson (rant: I find it absurd how people talk about 99.99999%s when the internet is under siege by spy agencies which have automated A LOT of attacks already at 2008 -- it's such a strange way to deal with a real issue -- "Naaah, won't be a problem; for my grandma to wouldn't be able to hack it"
563e30b461a8013065267215	X	Exactly.
  Very negative
The only thing SSL verifies as far as your API is concerned is that the call it's dealing with hasn't been messed with en route.
  Negative
The API still has no idea who's talking to it or whether or not they should have access at all.
  Negative
563e30b461a8013065267216	X	Good answer.
  Neutral
I would also recommend having a look at these excellent resources .
  Positive
.
  Neutral
owasp.org/index.php/Web_Service_Security_Cheat_Sheet and owasp.org/index.php/REST_Security_Cheat_Sheet (DRAFT)
563e30b461a8013065267217	X	Just a minor point, but using SSL also has the additional benefit of preventing eavesdropping and man in the middle attacks.
  Negative
563e30b461a8013065267218	X	@Les Hazlewood Could you explain how HTTP Basic authentication over Https can help to determine server knows whom its talking to?
  Negative
563e30b461a8013065267219	X	@Les Hazlewood here I asked it in a question; tnx stackoverflow.com/questions/14043397/…
563e30b561a801306526721a	X	You could get a GoDaddy ssl certificate for like $30 a year I think.
  Very negative
I was shocked to see how much the Verisign SSL certs go for ($600 a year or something if I remember correctly?)
  Negative
But the GoDaddy option is perfectly feasible.
  Positive
563e30b561a801306526721b	X	Unless you are using SSL/TLS mutual authentication, and the cert used by the user/client is trusted by the server, then you have not authenticated the user to the server/application.
  Negative
You would need to do something more to authenticate the user to the server/application.
  Negative
563e30b561a801306526721c	X	Ryan: SSL encryption these days takes a pretty tiny amount of processing power compared to what you'd use to generate a response with a web app framework like Django or Rails etc.
563e30b561a801306526721d	X	certs from startcom are free and widely recognized.
  Negative
cacert.org is an open alternative with less recognition
563e30b561a801306526721e	X	This doesn't address the question, which is about authentication.
  Negative
563e30b561a801306526721f	X	Self-signed certs are free, but AFAIK you still need a static IP.
  Negative
563e30b661a8013065267220	X	@dF there is no requirement of having a static IP except for certain licensing requirements of commercial paid for certificates.
  Negative
563e30b661a8013065267221	X	If you have control of the certificate stores on both ends (clientes & server) this may be a viable option but... certificate management and distribution is probably much more complex in production than in a development environment.
  Negative
Be sure to understand the complexities to this alternative before commiting to it.
  Negative
563e30b661a8013065267222	X	Coming up with a string that will generate the same md5 hash as the valid content may be much easier than it should be, but coming up with an evil version of valid content that hashes to the same value is still prohibitively difficult.
  Negative
This is why md5 isn't used for password hashes anymore, but is still used to verify downloads.
  Negative
563e30b661a8013065267223	X	Background: I'm designing the authentication scheme for a REST web service.
  Negative
This doesn't "really" need to be secure (it's more of a personal project) but I want to make it as secure as possible as an exercise/learning experience.
  Negative
I don't want to use SSL since I don't want the hassle and, mostly, the expense of setting it up.
  Negative
These SO questions were especially useful to get me started: I'm thinking of using a simplified version of Amazon S3's authentication (I like OAuth but it seems too complicated for my needs).
  Negative
I'm adding a randomly generated nonce, supplied by the server, to the request, to prevent replay attacks.
  Negative
To get to the question: Both S3 and OAuth rely on signing the request URL along with a few selected headers.
  Negative
Neither of them sign the request body for POST or PUT requests.
  Neutral
Isn't this vulnerable to a man-in-the-middle attack, which keeps the url and headers and replaces the request body with any data the attacker wants?
  Negative
It seems like I can guard against this by including a hash of the request body in the string that gets signed.
  Negative
Is this secure?
  Neutral
563e30b661a8013065267224	X	A previous answer only mentioned SSL in the context of data transfer and didn't actually cover authentication.
  Negative
You're really asking about securely authenticating REST API clients.
  Neutral
Unless you're using TLS client authentication, SSL alone is NOT a viable authentication mechanism for a REST API.
  Negative
SSL without client authc only authenticates the server, which is irrelevant for most REST APIs because you really want to authenticate the client.
  Negative
If you don't use TLS client authentication, you'll need to use something like a digest-based authentication scheme (like Amazon Web Service's custom scheme) or OAuth 1.0a or even HTTP Basic authentication (but over SSL only).
  Negative
These schemes authenticate that the request was sent by someone expected.
  Negative
TLS (SSL) (without client authentication) ensures that the data sent over the wire remains untampered.
  Negative
They are separate - but complementary - concerns.
  Neutral
For those interested, I've expanded on an SO question about HTTP Authentication Schemes and how they work.
  Positive
563e30b661a8013065267225	X	REST means working with the standards of the web, and the standard for "secure" transfer on the web is SSL.
  Negative
Anything else is going to be kind of funky and require extra deployment effort for clients, which will have to have encryption libraries available.
  Negative
Once you commit to SSL, there's really nothing fancy required for authentication in principle.
  Negative
You can again go with web standards and use HTTP Basic auth (username and secret token sent along with each request) as it's much simpler than an elaborate signing protocol, and still effective in the context of a secure connection.
  Negative
You just need to be sure the password never goes over plain text; so if the password is ever received over a plain text connection, you might even disable the password and mail the developer.
  Very negative
You should also ensure the credentials aren't logged anywhere upon receipt, just as you wouldn't log a regular password.
  Negative
HTTP Digest is a safer approach as it prevents the secret token being passed along; instead, it's a hash the server can verify on the other end.
  Negative
Though it may be overkill for less sensitive applications if you've taken the precautions mentioned above.
  Negative
After all, the user's password is already transmitted in plain-text when they log in (unless you're doing some fancy JavaScript encryption in the browser), and likewise their cookies on each request.
  Negative
Note that with APIs, it's better for the client to be passing tokens - randomly generated strings - instead of the password the developer logs into the website with.
  Negative
So the developer should be able to log into your site and generate new tokens that can be used for API verification.
  Negative
The main reason to use a token is that it can be replaced if it's compromised, whereas if the password is compromised, the owner could log into the developer's account and do anything they want with it.
  Negative
A further advantage of tokens is you can issue multiple tokens to the same developers.
  Negative
Perhaps because they have multiple apps or because they want tokens with different access levels.
  Neutral
(Updated to cover implications of making the connection SSL-only.)
  Negative
563e30b661a8013065267226	X	Or you could use the known solution to this problem and use SSL.
  Negative
Self-signed certs are free and its a personal project right?
  Neutral
563e30b761a8013065267227	X	If you require the hash of the body as one of the parameters in the URL and that URL is signed via a private key, then a man-in-the-middle attack would only be able to replace the body with content that would generate the same hash.
  Negative
Easy to do with MD5 hash values now at least and when SHA-1 is broken, well, you get the picture.
  Positive
To secure the body from tampering, you would need to require a signature of the body, which a man-in-the-middle attack would be less likely to be able to break since they wouldn't know the private key that generates the signature.
  Negative
563e30b761a8013065267228	X	In fact, the original S3 auth does allow for the content to be signed, albeit with a weak MD5 signature.
  Negative
You can simply enforce their optional practice of including a Content-MD5 header in the HMAC (string to be signed).
  Negative
http://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html Their new v4 authentication scheme is more secure.
  Negative
http://docs.aws.amazon.com/general/latest/gr/signature-version-4.html
563e30b761a8013065267229	X	Remember that your suggestions makes it difficult for clients to communicate with the server.
  Negative
They need to understand your innovative solution and encrypt the data accordingly, this model is not so good for public API (unless you are amazon\yahoo\google.
  Negative
.)
  Neutral
.
  Neutral
Anyways, if you must encrypt the body content I would suggest you to check out existing standards and solutions like: XML encryption (W3C standard) XML Security
563e30b861a801306526722a	X	i'm looking for an API on Amazon (not S3!)
  Negative
that will give me notifications about certain events.
  Neutral
For example: I know there is the Event Notification Service API which really seems to be out of date.
  Negative
Further, there seems to be the the Instant Order Processing Notification for Amazon Checkout, which has an empty documentation!?
  Negative
(click link) I'm looking for something like the eBay Platform Notification API which gives me all these events and sends data to an URL i define on those occuring events.
  Negative
563e30b861a801306526722b	X	You can get the 'Instant Order Processing Notification' document here It only provides provides three messages though:
563e30b861a801306526722c	X	There is none.
  Negative
There is only a payment gateway IPN, if you use their payment processing.
  Negative
This is unbelievable?
  Negative
that a company so large, cannot send a simple notice of a sale to us via IPN, like paypal and Ebay.
  Negative
563e30b961a801306526722d	X	Related: stackoverflow.com/questions/6669109
563e30b961a801306526722e	X	Is anyone aware of a tool that will automatically deploy a Rails app static assets to Rackspace Cloud Files or Amazon Cloud Front?
  Negative
In my perfect world capistrano would automatically upload everything in javascripts, stylesheets, and images then override the default image_tag and script_tags to route to the appropriate CDN path.
  Negative
It would be great if the deploy task created a new container with each deploy like cap creates a new release directory, or maybe it should use the same containers and keep a cached file with the hashes of all the deployed assets and only deploy new assets to take advantage of long CDN TTLs.
  Negative
563e30b961a801306526722f	X	I'm not aware of anything, but you could probably script something to do this without too much work.
  Negative
The Fog gem provides an agnostic API for pushing files to Amazon S3 and Rackspace Cloud files, among others.
  Negative
563e30b961a8013065267230	X	I haven't done it myself yet, but I think it can be done with rsync as a capistrano task.
  Negative
Have look at this.
  Neutral
http://railscasts.com/episodes/133-capistrano-tasks
563e30b961a8013065267231	X	I have previously used Rackspace Cloud Files CloudFuse for Linux http://www.rackspace.com/knowledge_center/article/mounting-rackspace-cloud-files-to-linux-using-cloudfuse.
  Negative
It allows you to mount your cloud files containers so that they can be written using standard file system operations, which makes for simple scripting in your deploy scripts.
  Negative
You'll obviously want to take care of keeping the machine that does this secure.
  Neutral
563e30b961a8013065267232	X	...I'm highly doubtful of this (a connection requires 2 parties) my only advice would be to look to see if a site like flickr has an api you could tap into to do this ... unless someone else on S.O has a hack to do so
563e30b961a8013065267233	X	How to incorporate an api into the above code?
  Negative
Any clue on that?
  Neutral
563e30b961a8013065267234	X	I am a newbie in the programming world,It could be of great help if you provide me with some more details.
  Negative
I have edited the question by inclusion of some code.
  Positive
All I need to ensure is that android doesn't give an out of memory error.
  Negative
Is there any ready made website and port number that accepts the file?
  Neutral
563e30b961a8013065267235	X	if it is just about memory issues, you could direct your stream into a file on the sd-card.
  Negative
563e30b961a8013065267236	X	I haven't wrote any server side code till now.
  Negative
Is there any way I can try writing an android program where I can write the file being uploaded from the mobile to a remote server?
  Negative
Additional details:- My code - All I need to make sure is that android doesn't run out of memory exception.
  Negative
563e30b961a8013065267237	X	There are several options.
  Negative
One is to use webDAV or FTP on server.
  Negative
But nowadays there is a lot of file storage services which you can access with RESTful API like Amazon S3
563e30ba61a8013065267238	X	It seems you want to mock a Server.
  Negative
Mocking HTTP Server is a good point to start from.
  Positive
563e30ba61a8013065267239	X	If your using php to do it (ie server side script) your server must relay the file.
  Negative
check the api for a client side implementation (ie javascript).
  Neutral
563e30ba61a801306526723a	X	That is a way to redirect the request, but all the headers that he outputs before 'Location' are meaningless since after the location header a new HTTP request is being generated that has no relation with the previous one (as HTTP is a stateless protocol).
  Negative
563e30ba61a801306526723b	X	When you put a file into S3 you tell it the content-type, it will work out transfer-encoding by itself, and will use the filename exactly as-is, so it will work fine.
  Negative
If your files aren't public on S3, then you'll need to generate the secure, time-limited URL (S3 Docs detail this...) and then redirect to that.
  Negative
563e30ba61a801306526723c	X	So, currently, when I open one of the MP4s in my S3, (in Firefox anyway), it embeds the MP4 and begins playing it.
  Negative
What content-type would I need to call it, and how would I do so?
  Negative
563e30ba61a801306526723d	X	I have a bunch of videos stored on my Amazon S3 storage.
  Negative
I'm working on creating a PHP script, very similar to the one here, where users can download the videos to their hard drive.
  Negative
I'd like to use something like this: However, I am under the impression that this increases the bandwidth because the video will be coming through my server.
  Negative
Any ideas on how I might be able to force the download of these videos, while avoiding reading it first through my own server?
  Negative
Many thanks!
  Positive
563e30ba61a801306526723e	X	Take a look at the S3 API Docs, and note the header values that you can set.
  Positive
Amazon will send these when the file is requested: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html (the same parameters can be sent with a POST to update an existing object)
563e30ba61a801306526723f	X	The php script you mentioned will work ok, but the main downside is that every time a visitor on your site requests a file, your own servers will load it from the S3 and then relay that data to the browser.
  Negative
For low traffic sites, it's probably not a big deal, but for high traffic ones, you definitely want to avoid running everything through your own servers.
  Negative
Luckily, there's a fairly straight-forward way to set your files to be forced to download from the S3.
  Negative
You just want to set the content-type and content-disposition (just setting content-disposition will work in some browsers, but setting both should work in all browsers).
  Negative
This code is assuming that you're using the Amazon S3 PHP class from Undesigned:  // set S3 auth and get your bucket listing // then loop through the bucket and copy each file over itself, replacing the "request headers": S3::copyObject($bucketName, $filename, $bucketName, $filename, "public-read", array(), array("Content-Type" => "application/octet-stream", "Content-Disposition" => "attachment")); ?
  Negative
> Now all your files will be forced to download.
  Negative
You may need to clear your cache to see the change.
  Neutral
And obviously, don't do that on any file that you actually do want to be loaded "inline" in the browser.
  Negative
The nice part with this solution is that applications that load media files directly (like let's say an mp3 player in Flash) don't care about the content-type or content-disposition, so you can still play your files in the browser and then link to download that same file.
  Negative
If the user already finished loading the file in flash, they'll most likely still have it in their cache, which means their download will be super quick and it won't even cost you any extra bandwidth charges from the S3.
  Positive
563e30ba61a8013065267240	X	
563e30ba61a8013065267241	X	I know this is an old thread, but within the past year, the S3 team has added support for request parameters, which allow you to override certain HTTP headers upon request.
  Negative
You could, for example, upload an SVG image as image/svg+xml, but then override the headers when retrieving as application/octet-stream to force a download.
  Negative
Using the AWS SDK for PHP, you can use the Aws\S3\S3Client::createPresignedUrl() method to achieve this.
  Negative
563e30bb61a8013065267242	X	Heroku also charges for data that passes through their dynos (over a certain amount, anyway).
  Negative
Look for a storage solution that will allow your uploads to bypass their servers, or use a Heroku dyno on EC2-east and use S3; transfers are free between the two.
  Negative
Very-low-usage (dev level) S3 is free, too, and the prices scale nicely from there.
  Positive
563e30bb61a8013065267243	X	thanks, I will open up an Amazon S3 account.
  Positive
Is there a good conventional and safe path to use though?
  Neutral
The images are uploaded by users with their posts so that's the only type of access I need.
  Negative
It will also be very low usage at the moment as I don't have any users
563e30bb61a8013065267244	X	I'm using carrierwave and I want to change the directory where images are stored.
  Negative
Right now the image URL is /uploads/modelname/image/51/nameoffile.
  Negative
jpg the store_dir in ImageUploader is I definitely do not want the modelname to show Is there an accepted ideal path where images should be stored on heroku?
  Negative
563e30bb61a8013065267245	X	Heroku doesn't allow uploads to their servers.
  Negative
You need to use another storage medium, like Amazon's S3.
  Negative
I'm actually using Parse's (www.parse.com) API to store images on their solution.
  Negative
But it depends how you need access to your images.
  Neutral
563e30bb61a8013065267246	X	You can upload files to the Heroku dyno filesystems but the filesystem are perishable and not shared among your dynos.
  Negative
Here's a Gist showing how to make Carrierwave store uploaded file in AWS S3 which is a better option: https://gist.github.com/cblunt/1303386 Here's a Heroku guide for accomplishing this with PaperClip: https://devcenter.heroku.com/articles/paperclip-s3
563e30bb61a8013065267247	X	I'm working with an android device and Amazon's S3 storage system.
  Negative
I have a main user account and I've creates sub-users to access the storage on the behalf of my user.
  Negative
Typically, I would use do the following to access S3 with my main user: Where ACCESS_KEY_ID and SECRET_KEY are given for my main user's account.
  Negative
I am trying to do this with my IAM user "Alice".
  Negative
This user was given it own ACCESS_KEY_ID.
  Neutral
Using the above code, I user Alice's ACCESS_KEY_ID with the same SECRET_KEY from above.
  Negative
The user Alice has no special permissions or restrictions, thus I assume the user can create buckets.
  Negative
When createBucket is called, there is an error: I cannot seem to find another way to authenticate via Android API to S3 using an IAM user.
  Very negative
ANy help is appreciated, thanks.
  Positive
563e30bb61a8013065267248	X	Your IAM user will have a different SECRET_KEY than your root account.
  Neutral
When you created the the IAM account you should have received both an ACCESS_KEY_ID and SECRET_KEY for the account which must be used in combination.
  Positive
The error you received is indicative of a key mismatch or a typo in one of the keys.
  Negative
563e30bc61a8013065267249	X	I'm using the Java API to upload and download files in Amazon S3.
  Negative
It was working perfectly until about a week ago, and it just stopped working altogether even though I hadn't touched the code.
  Positive
I'm wondering if there's a solution to this.
  Negative
Thanks!
  Positive
Over here's the code that was worked fine: And here's the error: I'd done a bit of research, and some had suggested that the problem could be with the permissions.
  Very negative
However, I think I've followed the instructions here: http://docs.aws.amazon.com/AWSToolkitEclipse/latest/GettingStartedGuide/tke_setup_creds.html#d0e387 as well as I could.
  Neutral
I'm really pretty stuck, guys.
  Positive
Thanks, and any help at all will be appreciated!
  Positive
563e30bc61a801306526724a	X	I had same problem.
  Negative
The solution turned out to be the device wrong date.
  Negative
Try to keep your device date updated.
  Positive
The reason is that SSL certificates have issuance/expiry date that are being verified against your device date.
  Negative
563e30bc61a801306526724b	X	I think we will generate the signature server side.
  Negative
I don't want to include the accounts secret key in a javascript file with the app.
  Negative
563e30bc61a801306526724c	X	Im doing some research to build an iOS and Android app using titanium appcelerator.
  Negative
The only requirement I have not being able to confirm is the possibility to upload a photo directly to Amazon S3 in a way that is compatible with iOS and Android devices.
  Negative
563e30bc61a801306526724d	X	Amazon S3 has provided a REST API so your can do it by creating a Titanium.Network.HTTPClient.
  Negative
For your upload case, you need to provide a PUT request to S3.
  Negative
If you want to do it in this way, you may need to include your S3 secret key in your client for signing your request.
  Negative
563e30bc61a801306526724e	X	You might ask the developers of Titanium Appcelerator.
  Negative
But if you're not writing an HTML5 app, you can definitely do this with the ASIHTTPRequest framework.
  Negative
563e30bc61a801306526724f	X	dropbox, box.net, amazon drive?
  Negative
563e30bc61a8013065267250	X	Do those have API's that make it easy to integrate with a web app?
  Neutral
563e30bc61a8013065267251	X	There are many services available for delivering content to users.
  Positive
I am looking for the reverse: a service for receiving content uploads from users.
  Negative
Specifically, I'm building a web app where the files uploaded may be anywhere from 10MB to 100MB.
  Negative
I expect there are many issues around receiving this much data, and what I'm looking for is a service that handles those issues for me.
  Negative
563e30bd61a8013065267252	X	You can use Amazon S3 to receive uploads from users.
  Negative
You can generate a signed HTML form that will let your users upload files to a designated bucket; you can control the expiry of the form using a policy document.
  Negative
Amazon S3 Developer Guide: Browser-Based Uploads Using POST Article: Browser Uploads to S3 using HTML POST Forms
563e30bd61a8013065267253	X	Uploadcare is built to ease your pain of receiving, storing, processing and distributing files and images from your users.
  Very negative
Give it a try, there's a free plan.
  Positive
563e30bd61a8013065267254	X	You can actually solve this without paying a third party by using a HTML5/Flash uploader in your web app.
  Negative
I particularly recommend Plupload (free, open-source).
  Positive
It's a javascript library that uses a choice of client frontends (HTML5, Flash, Silverlight, etc) to handle uploading the file(s) in a way that doesn't trigger browser timeouts and allows the user to see the progress of their uploads (demo with 10Mb limit).
  Negative
Other than the script you only require a backend on the server to receive the data.
  Negative
You'll want to watch out for anything that could trigger a server-side timeout like PHP's MAX_FILE_SIZE or request timeouts.
  Negative
The plupload package comes with example frontend and backend scripts so you should be up and running fairly quickly.
  Negative
I've uploaded 80Mb files using plupload so I know it works.
  Negative
It's going to be slow on a typical DSL connection but no upload service is immune to that.
  Negative
563e30bd61a8013065267255	X	Seems like you're looking for something like Amazon S3 or Google Storage.
  Negative
Both have dedicated API's and you only pay for the storage that you use.
  Negative
They both also have libraries avaliable in various languages to make integration easier.
  Negative
563e30bd61a8013065267256	X	A couple other options: https://www.yousendit.com/ http://www.uploadthingy.com/
563e30bd61a8013065267257	X	I'm not sure, but I'd be surprised if you could get your images into Google Image Search if you prevent hotlinking.
  Negative
563e30bd61a8013065267258	X	@sharth: Good point.I just searched, there is no referrer in Googlebot.
  Negative
Only one agent: Googlebot-Image/1.0.
  Neutral
563e30bd61a8013065267259	X	Did you succeed in preventing hotlinking?
  Negative
Cheers.
  Neutral
563e30bd61a801306526725a	X	Is there any command line tool to upload an image to blobstore?
  Neutral
563e30bd61a801306526725b	X	@DocWiki No, but the blobstore APIs are available over remote_api, so you could write one fairly simply.
  Negative
563e30bd61a801306526725c	X	Since you are here, I want to know something about Blobstore.
  Negative
I know there is a 30 secs per request limit in app engine.
  Negative
Will this limit apply when I upload a video to the app engine Blobstore?
  Negative
The max single file size for Blobstore is 2GB, and if I upload via an HTML form, it may take hours.
  Negative
Will the 30 secs per request limit apply?
  Negative
563e30be61a801306526725d	X	@DocWiki The 30 second execution time limit only applies to the time your code actually spends executing - which doesn't begin until the user has sent the entire request, and ends as soon as you send your response (before they receive it).
  Negative
563e30be61a801306526725e	X	LOL What are you talking about?
  Neutral
FYI My English sucks
563e30be61a801306526725f	X	Wow.
  Positive
1) The point of hotlink prevention is to prevent users from linking directly to your resources by making it unusable by other users.
  Negative
The users who are sending the referer headers are not your adversaries, the people who linked to your images are, and they have no control over other users' browsers.
  Very negative
2) I'm pretty sure Roosevelt and Churchill didn't use disks, since they weren't invented until 30 years after the end of world war 2.
  Negative
3) What you're talking about is One Time Pads, and completely irrelevant to the question at hand.
  Negative
4) Don't invent your own crypto.
  Negative
Just don't.
  Neutral
563e30be61a8013065267260	X	It's been drawn to my attention that you probably meant vinyl records when you said 'discs', which is accurate.
  Negative
It's still pretty much irrelevant to the OP's problem, though.
  Negative
563e30be61a8013065267261	X	Is this an ironic way of saying "don't worry about hot-linking"?
  Neutral
563e30be61a8013065267262	X	And fetch and return the image from a third-party service on every response?
  Neutral
Sure, if you love high bandwidth bills, do this.
  Positive
563e30be61a8013065267263	X	I implied google app engine blobstore, since as far as I know short of storing static images through app deployment that's the only way I know of storing images there.
  Negative
I guess you have a point in that I didn't specifically say blobstore since that was part of his question...
563e30be61a8013065267264	X	Then you're not really "returning the image from another location", are you?
  Negative
That was what led me to believe you were talking about fetching the image from elsewhere.
  Neutral
563e30be61a8013065267265	X	I meant to say that you can specify "examplewebsite.com/images/image1234.png" when the image's url is whatever the blobstore url is.
  Negative
Google's bandwidth charges are very reasonable for small to medium website's to serve images directly imho.
  Negative
=)
563e30be61a8013065267266	X	Well, the blobstore lets you serve images under any URL you want - the only 'blobstore URLs' are upload URLs and get_serving_url ones.
  Negative
I agree that App Engine's bandwidth charges are reasonable - I was more worried about the OP paying three times that for every request.
  Negative
563e30be61a8013065267267	X	I am going to build a site using Google App Engine.
  Negative
My public site contains thousands of pictures.
  Neutral
I want to store these pictures in the Cloud: Google Storage or Amazon S3 or Google App Engine BlobStore.
  Negative
The problem is image hotlinking.
  Negative
As for Google Storage, I googled and I cant find a way to prevent image hotlinking.
  Negative
(I like its command line tool gsutil very much though) Amazon S3 has "Query String Authentication" which generates expiring image urls.
  Negative
But this is very bad for SEO, isnt it?
  Negative
Constantly changing the URL would have quite negative affects as it takes upwards of a year to get an image, and its related URL, into Google Images.
  Negative
I am rather sure changing this URL would have an immediate negative affect when GoogleBot comes around to say hi.
  Negative
(UPDATE: A better way to preven image hotlinking in Amazon S3 by referrer is using Bucket Policy.
  Negative
Details here: http://www.naveen.info/2011/03/25/amazon-s3-hotlink-prevention-with-bucket-policies/) Google App Engine BlobStore?
  Negative
I have to upload the images via Web Interfaces manually and it generates changing urls too.
  Negative
(update: Due to my ignorance about Blobstore, I just made a mistake.
  Negative
By using Google App Engine BlobStore, you can use whatever url to serve the image you want.)
  Negative
What I need is simple referrer protection: Only show the image when the referrer is my site.
  Negative
Are there some better ways to prevent image hotlinking.
  Neutral
I dont want to file bankruptcy due to the extremely high cost of cloud bandwidth.
  Negative
UPDATE: Still difficult to choose from the three, each of them have pros and cons.
  Negative
BlobStore seems to be the ultimate choice.
  Negative
563e30be61a8013065267268	X	The easiest option would be to use the blobstore.
  Negative
You can provide whatever upload interface you want - it's up to you to write it - and the blobstore doesn't constrain your download URLs, only your upload ones.
  Neutral
You can serve blobstore images under any URL simply by setting the appropriate headers, or you can use get_serving_url to take advantage of the built-in fast image serving support, which generates cryptic but consistent URLs (but doesn't let you do referer checks).
  Negative
I would suggest giving some consideration to whether this is a real, practical problem you're facing, though.
  Negative
The bandwidth consumed by a few hotlinked images is pretty minimal by today's standards, and it's not a particularly common practice in the first place.
  Negative
As @sharth points out in the comments, it's likely to impact SEO too, since image search tends to show images in their own windows in addition to linking to the page that hosted them.
  Negative
563e30be61a8013065267269	X	Whenever I get back to coding for statistical web services, I had to generate images and charts dynamically.
  Negative
The images generated would depend on the request parameter, state of data repository, and some header info.
  Negative
Therefore if I were you, I would write a REST web service to serve the images.
  Negative
Not too difficult.
  Negative
It's pretty ticklish too because if you don't like a particular ip address, you could show cartoon of tongue-out-of-cheek (or animated gif of OBL samba dancing while getting bombed) rather than the image for the data request.
  Negative
For your case you would check the referer (or referrer) at the http header, right?
  Negative
I am doubtful because people can and will hide, blank out or even fake the referer field in the http header.
  Negative
So, not only check the referer field, but create a data field where the value changes.
  Negative
The value could be a simple value matching.
  Positive
During the world war, Roosevelt and Churchill communicated thro encryption.
  Negative
They each had an identical stack of disks, which somehow contained the encryption mechanism.
  Negative
After each conversation, both would discard the disk (and never reused) so that the next time they spoke again, they reach for the next disk on the stack.
  Negative
Instead of a stack of disks, your image consumers and your image provider would carry the same stack of 32 bit tokens.
  Negative
32 bits would give you ~4 billion ten minute periods.
  Negative
The stack is randomly sequenced.
  Negative
Since it is well known that "random generators" are not truly random and actually algorithmic in a way which can be predicted if supplied a sufficiently long sequence, you should either use a "true random generator" or resequence the stack every week.
  Negative
Due to latency issues, your provider would accept tokens from the current period, the last period and the next period.
  Negative
Where period = sector.
  Neutral
Your ajax client (presumably gwt) on your browser would get an updated token from the server every ten minutes.
  Negative
The ajax client would use that token to request for images.
  Negative
Your image provider service would reject a stale token and your ajax client would have to request a fresh token from the server.
  Negative
It is not a fireproof method but it is shatterproof, so that it could reduce/discourage the number of spam requests (nearly to zero, I presume).
  Negative
The way I generate "truly random" sequences is again quick and dirty.
  Positive
I further work on an algorithmically generated "random" sequence by spending a few minutes manually throwing in a few monkey wrenches by manually resequencing or deleting values of the sequences.
  Negative
That would mess up any algorithmic predictability.
  Negative
Perhaps, you could write a monkey wrench thrower.
  Neutral
But an algorithmic monkey wrench thrower would simply be adding a predictable algorithm above another predictable algorithm which does not reduce the overall predictability at all.
  Negative
You could further obsessively constrict the situation by employing cyclic redundancy matching as a quick and dirty "encrypted" token matching mechanism.
  Negative
Let us say you have a circle divided into 8 equidistant sectors.
  Neutral
You would have a 3 digit binary number to be able to address anyone of all the 8 sectors.
  Neutral
Imagine each sector is further subdivided into 8 subsectors, so that now you will be able to address each subsector with an additional 3 bytes, making a total of six bytes.
  Positive
You plan to change the matching value every 10 minutes.
  Neutral
Your image provider and all your approved consumers will have the same stack of sector addresses.
  Negative
Every ten minutes they throw away the sector address and use the next one.
  Negative
When a consumer sends your provider a matching value, it does not send the sector address but the subsector address.
  Negative
So that as long as your provider receives a subsector address belonging to the currently accepted sector, the provider service would respond with the correct image.
  Negative
But the subsector address is remapped through an obfuscation sequencing algorithm.
  Negative
so that each subsector address within the same sector do not look similar at all.
  Negative
In that way, not all browsers would receive the same token value or highly similar token value.
  Negative
Let us say that you have 16bit sector addresses and each sector has 16 bit subsector addresses, making up a 32 bit token.
  Negative
Which means you can afford to have 65536 concurrent browser clients carrying the same token sector but where no two token has the same low predictability value.
  Negative
So that you could assign a token subsector value for every session id.
  Negative
Unless you have more than 65536 concurrent sessions to your image provider service, no two session ids would need to share the same subsector token address.
  Negative
In that way, unless a spammer had access to session id faking equipment/facilities, there would be no way your image provider could be spammed except thro denial of service attack.
  Negative
Low predictability means that there is low probability for a snooper or peeper to concoct an acceptable token to spam your image provider service.
  Negative
Certainly, normal bots would not be able to get thro - unless you had really offended the ANNONYMOUS group and they decided to spam your server out of sheer fun.
  Negative
And even then if you had thrown monkey wrenches into the sector address stack and subsector maps, it would be really difficult to predict a next token.
  Negative
BTW, Cyclic Redundancy matching is actually an error correction technique and not so much an encryption technique.
  Negative
563e30be61a801306526726a	X	Simpler version of geek's essay, build a handler in google app engine to fetch and server the images.
  Negative
You can modify your headers to specify png or whatever, but you're returning the image from another location.
  Negative
You can then examine your request referrer information in the handler and take appropriate action if somebody is trying to access that image "hotlinked".
  Negative
Of course, because you're never exposing the actual image, it would be impossible to hotlink.
  Negative
=)
563e30be61a801306526726b	X	You should know that the File API is still experimental, check out this issue: http://code.google.com/p/googleappengine/issues/detail?id=6888#c20 I'm working on a startup which is moving out from Blobstore to Amazon S3
563e30bf61a801306526726c	X	Thanks for that @ejdyksen.
  Negative
The solution I've devised used exactly that (I haven't updated the question with my answer)!
  Negative
So my solution was to do authenticated URLs for GET requests.
  Negative
However, when a user contributes content, he creates resources to a specific /bucket/user/objectname location using federated IAM token (temporary credentials that expire) with the policy attached to allow for /bucket/user/* write access.
  Neutral
so no user in the system can do harm to other users' content.
  Negative
Seems to work okay.
  Neutral
Appreciate your answer.
  Neutral
563e30bf61a801306526726d	X	It's just a temporary url, not one time url.
  Negative
563e30bf61a801306526726e	X	@okwap you're right.
  Positive
"One time" is not very accurate, as it can be used an unlimited number of times before the expiration.
  Negative
I updated the answer to reflect that.
  Neutral
563e30bf61a801306526726f	X	If you're using v2 of the aws-sdk-ruby, note that the methods are somewhat different : docs.aws.amazon.com/sdkforruby/api/Aws/S3/…
563e30bf61a8013065267270	X	Isn't it a risk that the user can see my access key?
  Negative
There is also the secret key (converted)
563e30bf61a8013065267271	X	Thanks for the blog post with your solution, @dineth.
  Negative
I asked a question on Amazon's forum looking for Sample Code to this exact solution you blogged about (forums.aws.amazon.com/thread.jspa?threadID=125851&tstart=0) and their answer pointed in the direction you developed.
  Neutral
It would be great if you could post the key parts of your code in Ruby as described.
  Neutral
In any case, thanks again for pointing the right path.
  Positive
563e30bf61a8013065267272	X	I was interested, but the URL is a 404 now.
  Negative
563e30bf61a8013065267273	X	I updated the URL to my old tumblr blog's post.
  Negative
Something got messed up while moving to Squarespace.
  Positive
563e30bf61a8013065267274	X	I am new to writing Rails and APIs.
  Negative
I need some help with S3 storage solution.
  Neutral
Here's my problem.
  Negative
I am writing an API for an iOS app where the users login with the Facebook API on iOS.
  Negative
The server validates the user against the token Facebook issues to the iOS user and issues a temporary Session token.
  Negative
From this point the user needs to download content that is stored in S3.
  Negative
This content only belongs to the user and a subset of his friends.
  Negative
This user can add more content to S3 which can be accessed by the same bunch of people.
  Negative
I guess it is similar to attaching a file to a Facebook group... There are 2 ways a user can interact with S3... leave it to the server or get the server to issue a temporary S3 token (not sure of the possibilities here) and the user can hit up on the content URLs directly to S3.
  Very negative
I found this question talking about the approaches, however, it is really dated (2 yrs ago): Architectural and design question about uploading photos from iPhone app and S3 So the questions: Apologies for multiple questions and I appreciate any insight into the problem.
  Negative
Thanks :)
563e30bf61a8013065267275	X	Using the aws-sdk gem, you can get a temporary signed url for any S3 object by calling url_for: This will give you a signed, temporary use URL for only that object in S3.
  Negative
It expires after 20 minutes (in this example), and it's only good for that one object.
  Negative
If you have lots of objects the client needs, you'll need to issue lots of signed URLs.
  Negative
Or should let the server control all content passing (this solves security of course)?
  Neutral
Does this mean I have to download all content to server before handing it down to the connected users?
  Negative
Note that this doesn't mean the server needs to download each object, it only needs to authenticate and authorize specific clients to access specific objects in S3.
  Negative
API docs from Amazon: http://docs.amazonwebservices.com/AmazonS3/latest/dev/RESTAuthentication.html#RESTAuthenticationQueryStringAuth
563e30bf61a8013065267276	X	If anyone is interested, I have wrote about my final solution in a blogpost: http://tmblr.co/ZqibVwRI2W7o (a short explanation is present as a comment on the previous answer).
  Negative
563e30bf61a8013065267277	X	The above answers use the old aws-sdk-v1 gem rather than the new aws-sdk-resources version 2.
  Positive
The new way is: where your_object_key is the path to your file.
  Negative
If you need to look that up, you would use something like: That information was startlingly difficult to dig up, and I almost just gave up and used the older gem.
  Negative
http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/Object.html#presigned_url-instance_method
563e30c061a8013065267278	X	I have the access_key_id and secret_access_key but what do i have to specify in bucket?
  Negative
563e30c061a8013065267279	X	The name of the bucket you created on S3.
  Positive
You can do it from the AWS Web console.
  Neutral
563e30c061a801306526727a	X	No, I have not created bucket.
  Negative
I just created an account.
  Positive
563e30c061a801306526727b	X	Then you need to log in into your account aws.amazon.com, then look for AWS Management Console.
  Negative
Look for S3 and create a bucket.
  Positive
And you are good to go.
  Positive
Let me know if you need anything else.
  Negative
563e30c061a801306526727c	X	Ok.
  Neutral
I have seen it and i did not actually finished creating my account because i have not provided the payment details.
  Negative
I tried creating free tier but still it is asking for payment details.
  Neutral
why?
  Neutral
563e30c061a801306526727d	X	I have a page where i store images for each post and i store images in files using paperclip but i want to store it in the amazon s3 instead of storing it in folder.
  Negative
How can i do it?
  Neutral
563e30c061a801306526727e	X	Let me try to explain it.
  Negative
add gem 'aws-sdk' to you gemfile.
  Positive
(don't forget to bundle install) create a new file in {APP FOLDER}/config/s3.
  Negative
yml with the following content: access_key_id: xxxxxYOUR_ACCESS_KEY_IDxxxxx enter code heresecret_access_key: xxxxxYOUR_SECRET_ACCESS_KEYxxxxx Update your model accordingly: At last in your views: Any issues let me know.
  Negative
As for the alternatives to S3, Windows Azure Storage and Nimbus.io.
  Negative
563e30c061a801306526727f	X	There is a fairly thorough documentation about it here, take a look.
  Positive
563e30c061a8013065267280	X	As per my understanding in Amazon S3 there is nothing called folders to store the data.
  Negative
we have used S3 for string images only but it was stored in the virtual folder S3 termed it as bucket, there are C# API available to read those virtual folders( buckets) information to access the data.
  Negative
there is also AWS SDK available for C#.
  Negative
563e30c061a8013065267281	X	Here is a minimal howto.
  Negative
For testing and development you can use fake-s3 Descrtiption from github page: "FakeS3 is a lightweight server that responds to the same calls Amazon S3 responds to.
  Negative
It is extremely useful for testing of S3 in a sandbox environment without actually making calls to Amazon, which not only require network, but also cost you precious dollars."
  Negative
I think you should stay with Amazon S3 in production.
  Negative
I do not know a better alernative.
  Negative
It works.
  Positive
563e30c061a8013065267282	X	I'm using Boto to connect to Amazon S3 in my Python program.
  Negative
I'm able to open a connection and upload files to a bucket.
  Positive
I figured I should then close the connection to release resources and, more important, to avoid any security risks from leaving an open connection hanging around.
  Negative
I assumed I should call the close() method.
  Positive
But I tested this as follows: 1.
  Negative
Open connection.
  Neutral
2.
  Neutral
Close connection.
  Neutral
3.
  Neutral
Upload file to bucket.
  Neutral
I figured step 3 would fail, but the upload worked!
  Neutral
So what does close() do?
  Neutral
If it doesn't really close the connection, what should I use in place of close()?
  Negative
Or is it just unnecessary to close the connection?
  Neutral
I've looked for the answer in the Boto tutorial, the Boto API reference, and this StackOverflow post, but no luck so far.
  Negative
Thanks for your help.
  Positive
563e30c061a8013065267283	X	Your step 3 worked because boto has code that will automatically re-open closed connections and retry requests on errors.
  Negative
There is little to gain by manually closing the boto connections because they are just HTTP connections and will close automatically after a few minutes of idle time.
  Negative
I wouldn't worry about trying to close them.
  Negative
563e30c061a8013065267284	X	Under the covers, boto uses httplib.
  Negative
This client library supports HTTP 1.1 Keep-Alive, so it can and should keep the socket open so that it can perform multiple requests over the same connection.
  Negative
connection.close() does not actually close the underlying sockets.
  Negative
Instead, it removes the reference to the underlying pool of httplib connections, which allows the garbage collector to run on them, and that's when the actual socket close happens.
  Negative
Obviously, you also can let the garbage collector run by not keeping a reference to the boto connection itself.
  Negative
But there are performance benefits to reusing the boto connection (e.g. see the Keep-Alive note above).
  Positive
Fortunately, in most cases, you don't have to call connection.close() explicitly.
  Neutral
For more details on one case where you DO have to call close, see my answer to the StackOverflow post that's linked in the question.
  Negative
563e30c161a8013065267285	X	I did something simpler a while ago with a script that generates <form> and allows to send files through POST, but am stuck with PUT.
  Negative
563e30c161a8013065267286	X	hi, just want your help about updating your code if the credentials I'm using are only temporary security credentials which consist of aws_accesskey, aws_secretkey & security_token?
  Negative
Were you able to accomplish uploading of file using PUT?
  Neutral
563e30c161a8013065267287	X	@WonderingCoder, nope, sorry - I wasn't been able to use PUT and I've forgot about the project.
  Negative
I still would like to learn that though.
  Negative
563e30c161a8013065267288	X	Why do you need PUT?
  Negative
Isn't POST enough?
  Neutral
The documentation strictly talks about POST form, what's the issue with that?
  Neutral
563e30c161a8013065267289	X	@alexcasalboni, quite obvious - you need to reload the page to upload anything.
  Negative
S3 supports CORS and PUT, it's just a matter of learning how it works on an example.
  Positive
563e30c261a801306526728a	X	I am trying to upload a file using simple PUT method (jQuery if needed), but Amazon documentation seems too superficial for me to understand.
  Very negative
There are several examples - few old, few newer (but often overcomplicated).
  Negative
Would there be anyone able to write a simple tutorial.
  Positive
There are three simple parts in here - HTML file with single input[type="file"], PHP file that will generate the signature and most complicated JavaScript that will use File API and AJAX (PUT method as I understand) to store the file on S3.
  Negative
I think many would consider such a working example as a life savior, considering Amazon probably isn't going to make S3 documentation a little noob-friendly.
  Negative
563e30c261a801306526728b	X	is piping it through head -n 1 an option, or do you need a clean solution?
  Negative
563e30c261a801306526728c	X	what does it do exactly?
  Negative
well a clean solution would be better, but im taking everything that works... thanks @Fiximan
563e30c261a801306526728d	X	head will just print the head of a file, i.e. the first few (standard is 10) lines, while -n NUMBER will let you select how many exactly.
  Negative
So you would not change what is being retrieved, but what you actually use from it.
  Negative
563e30c261a801306526728e	X	I'm trying to list and to echo all the files having yesterday's date in their names, on a amazon s3 bucket.
  Negative
I know I can do: which is working just fine, but I need to do several stuff on these files, one bye one.
  Positive
So my code is: it also works fine but instead of retrieve me just the file name it also give me for every single element: the date it was uploaded and its weight: How can I just retrieve the file's name?
  Negative
ie: s3://my-bucket/20150720-1437436434_ip-10-0-1-36_android.log.gz
563e30c261a801306526728f	X	Rather than using s3cmd, these days it is recommend to use the official AWS Command-Line Interface (CLI).
  Negative
It has great features such as --query (to control output) and access to every AWS API call.
  Negative
Here's a sample that would fit your need: Breaking it down: See: list-objects CLI documentation As simpler version of listing objects is available (aws s3 ls BUCKET-NAME) but it has a fixed text output, like s3cmd.
  Negative
563e30c261a8013065267290	X	I think that only works if the bucket has an ACL, does that still work if you assigned permissions through bucket policies?
  Negative
563e30c261a8013065267291	X	I know you can try to read the ACLs or Bucket Policies through the Java SDK, but is there any easy way to just check if you have read and/or write permissions to a bucket and/or its contents?
  Negative
I don't see any "haveReadPermissions()" method or anything in the AmazonS3 class, but maybe I'm missing something?
  Negative
I find it hard to believe there's no easy way to check permissions.
  Negative
563e30c261a8013065267292	X	I think the answer is that there's no fool-proof way to do this, at least not at this time.
  Negative
There are a couple other methods you can use to try to get around this.
  Negative
I originally tried to use the getBucketLocation() method to determine if my given user had read access to the bucket, but it turns out you must be the owner of the bucket to use this method... so that didn't work.
  Negative
For read access, there is another hack you can use.
  Negative
Just use something along the lines of getObject(bucketName, UUID.randomUUID().
  Negative
toString()) - this will throw an exception because you are trying to fetch a key that doesn't exist.
  Negative
Catch the AmazonServiceException (or AmazonS3Exception) and check that the e.getErrorCode() equals "NoSuchKey".
  Negative
If this is the case, then you have read access to the bucket.
  Negative
If it's any other error code, then you don't have access, or the bucket doesn't exist, etc (it could be any number of things).
  Negative
Make sure you explicitly check the ErrorCode not the StatusCode, because you will also get a 404 StatusCode if the bucket doesn't exist (which is the same StatusCode you get when the key doesn't exist).
  Negative
Here's the complete list of S3 error/status codes: http://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html For write access, it's not as simple.
  Negative
The best way is to actually write a small test file to the bucket (and then maybe try to delete it).
  Negative
Besides that, if you want to check for more generic permissions, using a method like PutObjectAcl will determine if your user has "s3:Put*" permissions (you can set the ACL to the exact same as the current ACL by reading it first and then using that to set it).
  Negative
563e30c261a8013065267293	X	Try getBucketACL(String bucketName) which is a method of AmazonS3.
  Negative
563e30c261a8013065267294	X	I do not think there is an easier way unless you created your own classes that extended the AWS API.
  Negative
Which might not be a bad idea if you are doing a larger program having to do with Amazon S3.
  Negative
563e30c261a8013065267295	X	Are there any benchmarks comparing data transfer rates using s3cmd to awscli?
  Negative
563e30c361a8013065267296	X	There are s3cmd official packages for Ubuntu: packages.ubuntu.com/trusty/s3cmd
563e30c361a8013065267297	X	Also check out s4cmd.
  Negative
While it doesn't have all the features of s3cmd, its performance is definitely better over high-bandwidth connections (e.g. on EC2), since it multithreads the connections.
  Positive
github.com/bloomreach/s4cmd
563e30c361a8013065267298	X	I am thinking about redeploying my static website to Amazon S3.
  Negative
I need to automate the deployment so I was looking for an API for such tasks.
  Negative
I'm a bit confused over the different options.
  Negative
Question: What is the difference between s3cmd, the Python library boto and AWS CLI?
  Negative
563e30c361a8013065267299	X	s3cmd and AWS CLI are both command line tools.
  Negative
They're well suited if you want to script your deployment through shell scripting (e.g. bash).
  Negative
AWS CLI gives you only the basic commands, but they should be enough to deploy a static website to an S3 bucket.
  Negative
It also has some small advantages such as being pre-installed on Amazon Linux, if that was where you were working from.
  Negative
One AWS CLI command that may be appropriate to sync a local directory to an S3 bucket: Full documentation on this command: http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html s3cmd supports everything AWS CLI does, plus adds some more extended functionality on top, although I'm not sure you would require any of it for your purposes.
  Negative
You can see all its commands here: http://s3tools.org/usage Installation of s3cmd may be a bit more involved because it doesn't seem to be packages for it in any distros main repos.
  Negative
boto is a Python library, and in fact the official AWS Python SDK.
  Positive
The AWS CLI, also being written in Python, actually uses part of the boto library (botocore).
  Negative
It would be well suited only if you were writing your deployment scripts in Python.
  Negative
There are official SDKs for other popular languages (Java, PHP, etc.) should you prefer: http://aws.amazon.com/tools/ The rawest form of access to S3 is through AWS's REST API.
  Negative
Everything else is built upon it at some point.
  Negative
If you feel adventurous, here's the S3 REST API documentation: http://docs.aws.amazon.com/AmazonS3/latest/API/APIRest.html
563e30c461a801306526729a	X	I studied Amazon web service API reference, I found many services (EC2,Cloudwatch) only support the programming SDK, java/python and so on.
  Very negative
Only little services provide RESTful API, such as S3.
  Negative
I think Restful API is more easier to use than programming SDK.
  Negative
Why Amazon didn't provide the Restful API?
  Negative
563e30c461a801306526729b	X	I agree that REST APIs are preferable to SDKs but, actually, all of the AWS services do expose an HTTPS interface, they're just not "RESTful."
  Negative
They call it the "Query API."
  Neutral
http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/Using_Query_API.html http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-query-api.html
563e30c461a801306526729c	X	Well, in fact, all services of AWS in the SDK communicates with POST/GET.
  Negative
In the documentation of the AWS services, they provide the url of each action.
  Negative
The only thing you need to do is read the documentation.
  Negative
See this S3 documentation which you can see how things works: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html
563e30c461a801306526729d	X	Some may say it just comes down to opinion... but imo, there's no reason to prefer lower RESTful API over SDK, when your language of choice has one.
  Very negative
The SDK's are hand-crafted by AWS to get the most out of their API's (and who would know how to do so better than them?)
  Negative
, and give you abstractions that you can take advantage of.
  Neutral
For anything besides a trivial project toying around with AWS, choosing to work with the lower-level API's means that you will end up re-implementing many things the SDK's give you out of the box, aka reinventing the wheel... The SDK's are there to get you productive and working, fast.
  Negative
563e30c461a801306526729e	X	At the moment i am developing angular with nodejs app, and i ran into a problem where i need to do image upload, also resize image on client side before uploading to the server.
  Negative
I found plupload and when i tested it and it seem to be the right library for me to used.
  Positive
So because the example here https://github.com/moxiecode/plupload/wiki/Upload-to-Amazon-S3 is js and php.
  Negative
This will not be suitable for my app while running in Heroku.
  Negative
So i can't convert php into js because due to API keys will be expose to anyone on client side.
  Negative
I tried doing some CORS but it doesnt seem to work for me as everything now is getting really complicated just to do image upload on the client side.
  Negative
Can someone point me to the easiest method or direction... Thank you
563e30c461a801306526729f	X	Aggghhh... it says 403 forbidden and everything.
  Negative
.
  Neutral
it made total sense as soon as you mentioned it.
  Negative
.
  Neutral
I'm an idiot.
  Negative
.
  Neutral
I changed 404 for 403 and the redirect worked!
  Negative
Thanks so much!
  Positive
563e30c461a80130652672a0	X	Is it possible to change this S3 behavior & return a default object when the requested object is not found without any kind of error ?
  Negative
563e30c461a80130652672a1	X	I am trying to get an S3 bucket when it encounters a 404 rather than throwing up a 404 page it redirects to my own server so I can then do something with the error.
  Negative
This is what I have cobbled together, what I think it should do is go to mydomain.com and hit the error.php and let the php script workout the filename the user was trying to access on S3.
  Negative
I would like this to happen no matter what directory the request comes from.
  Negative
When I have an error document defined in website hosting the 404 page shows up and when I don't have a 404 page defined I get an access denied xml error.
  Very negative
This is my current redirection rule Can anyone give me a hint as to what I am missing please?
  Negative
563e30c561a80130652672a2	X	Change the error code: S3 doesn't generate a 404 unless the requesting user is allowed to list the bucket.
  Negative
Instead, it generates a 403 ("Forbidden"), because you're not allowed to know whether the object exists or not.
  Negative
In this case, that's the anonymous user, and you probably don't want to allow anonymous users to list the entire contents of your bucket.
  Negative
If the object you request does not exist, the error Amazon S3 returns depends on whether you also have the s3:ListBucket permission.
  Negative
If you have the s3:ListBucket permission on the bucket, Amazon S3 will return an HTTP status code 404 ("no such key") error.
  Negative
if you don’t have the s3:ListBucket permission, Amazon S3 will return an HTTP status code 403 ("access denied") error.
  Negative
— http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html See also: Amazon S3 Redirect rule - GET data is missing
563e30c561a80130652672a3	X	COPY is a SQL command sent over JDBS/ODBS connection.
  Negative
563e30c561a80130652672a4	X	If you'are using python, you'll need a postgreSQL binder like psycopg2 to connect to redshift and then run a "copy" command.
  Negative
563e30c561a80130652672a5	X	Is it possible to load redshift using bulk copy command using python boto package.
  Negative
I do not see a way to do this.
  Negative
It seems a JDBC client is necessary.
  Negative
563e30c561a80130652672a6	X	No, boto is not used to load data into Amazon Redshift.
  Negative
Data can be loaded into Amazon Redshift from: The python boto package provides access to AWS APIs.
  Negative
These can be used with Redshift to create, snapshot, reboot, describe and resize clusters (plus other commands).
  Neutral
However, the process to load data into Redshift (eg via Amazon S3) is done with the COPY command that is called via the normal ODBC or JDBC connection -- just like calling a SELECT command: You could, however, use a standard Python JDBC library to connect to Redshift and execute the COPY command to bulk load data.
  Neutral
See also:
563e30c561a80130652672a7	X	sounds interesting, I would like to learn more from people with S3 experience too.
  Positive
563e30c561a80130652672a8	X	Hi Tuviah, it's been two year since you asked this question, what solution you've used eventually?
  Negative
I'm looking for something similar.
  Neutral
563e30c661a80130652672a9	X	I work for a company which is developing a media asset management website.
  Neutral
We've decided to modify our web app to host our media on the cloud.
  Negative
We will license this web application to our customers and allow them to choose whatever cloud service they want.
  Negative
Currently we are using Amazon S3 and I manually upload the files using cloudberry.
  Negative
But now we need to write an application which will mirror whatever is in our local media directory to Amazon.
  Neutral
It's all ASP.net and I've researched ThreeSharp and other simple C# cloud APIs for S3.
  Negative
But now I see that Zend and Microsoft have partnered for this simplecloud initiative which apparently only works for PHP.
  Negative
Question: Does anyone know of a simple cloud api (simplecloud.org) which works with all popular cloud services for .
  Negative
NET?
  Neutral
I would really hate to have to write the same code to move and delete files three different times for Azure, S3 and RackSpace or have to reinvent the wheel.
  Negative
best, Tuviah
563e30c661a80130652672aa	X	There's some of those on CodePlex:
563e30c661a80130652672ab	X	Have you tried it?
  Positive
563e30c661a80130652672ac	X	I have a number of large PDFs stored in Amazon S3.
  Negative
Users of my web application need to be able to view these documents within the browser, and I am considering using pdf.js for this purpose.
  Negative
I am aware that Amazon S3 provides a REST API for getting Multiple Parts of an S3 object, allowing a specified range of bytes to be downloaded using the HTTP Range header I believe that pdf.js provides support for making requests with a Range Header to fetch only the bytes for a specific page.
  Negative
Is it therefore possible to download the first few pages of a pdf from S3, render these pages and then request the rest of the pdf from S3 in the background?
  Negative
563e30c661a80130652672ad	X	As noted in Julien's answer above, WS does not support multipart/form-data POSTing.
  Negative
multipart/form-data is implied by curl -F option in the question.
  Negative
Though if you are free to send the file contents as the request body you can just do WS.url(...).
  Negative
post(fileContentsAsByteArray).
  Neutral
Also fit in a .
  Neutral
withHeaders(("Content-Type",...)) before sending, so the server knows how to handle it.
  Negative
563e30c661a80130652672ae	X	In what way can I simulate file upload in play framework?
  Negative
In other words I can upload file to server with such curl command: curl -k -v -H "X-Agile-Authorization: token" -F uploadFile=c:\1.txt -F directory=/testpost -F basename=1.txt https:// api /post/file how can I do the same without curl or browser in play framework.
  Negative
The aim is to upload file from one server to another.
  Neutral
563e30c661a80130652672af	X	Currently, it's not possible to post a multipart/form-data through the WS API.
  Negative
You can easily send a File WS.url(myUrl).
  Negative
post(myFile), but not a form... A workaround should be to use another library, like Apache Http Client.
  Negative
Check this topic on the Play mailing-list: [2.0] multipart/form-data in WS POST
563e30c661a80130652672b0	X	Post on google-groups: With org.apache.http.entity.mime.MultipartEntity, you can manipulate multipart data easily.
  Negative
And then just write it to byte array buffer as you did.
  Negative
Ex: Upload photo to my facebook wall: Dependency: "org.apache.httpcomponents" % "httpclient" % "4.3.1" and "org.apache.httpcomponents" % "httpmime" % "4.3.1".
  Negative
If performance were a concern, you could do the same with WS's underlying client WS.client.
  Negative
563e30c661a80130652672b1	X	Take a look on the Windows Azure Storage or Amazon S3 REST APIs, then using similar approach you can send files with Play's WebServices API.
  Negative
You just need to construct POST or PUT request and send it.
  Negative
Probably you'll need to care about authentication and/or authorization between both apps yourself (both APIs Windows and Amazon uses HMAC for this task)
563e30c661a80130652672b2	X	The following example allows you to post multipart/form-data.
  Negative
It is a simple version that only works with String values, but it could easily be modified to use other types of data.
  Negative
Usage is like this:
563e30c761a80130652672b3	X	I need to fetch images from different URLs and store it in Google Datastore after compressing it.
  Negative
I understand tinyPNG is the a great API to do the compression but the API only supports Amazon S3.
  Neutral
Can someone guide how to accomplish this in Google App Engine.
  Negative
563e30c761a80130652672b4	X	I have created a bucket in Amazon S3 and have uploaded 2 files in it and made them public.
  Negative
I have the links through which I can access them from anywhere on the Internet.
  Neutral
I now want to put some restriction on who can download the files.
  Negative
Can someone please help me with that.
  Positive
I did try the documentation, but got confused.
  Negative
I want that at the time of download using the public link it should ask for some credentials or something to authenticate the user at that time.
  Negative
Is this possible?
  Neutral
563e30c761a80130652672b5	X	By default, all objects in Amazon S3 are private.
  Negative
You can then add permissions so that people can access your objects.
  Negative
This can be done via: As long as at least one of these methods is granting access, your users will be able to access the objects from Amazon S3.
  Positive
1.
  Neutral
Access Control List on individual objects The Make Public option in the Amazon S3 management console will grant Open/Download permissions to all Internet users.
  Negative
This can be used to grant public access to specific objects.
  Neutral
2.
  Neutral
Bucket Policy A Bucket Policy can be used to grant access to a whole bucket or a portion of a bucket.
  Negative
It can also be used to specify limits to access.
  Negative
For example, a policy could make a specific directory within a bucket public to users from a specific range of IP addresses, during particular times of the day, and only when accessing the bucket via SSL.
  Positive
A bucket policy is a good way to grant public access to many objects (eg a particular directory) within having to specify permissions on each individual object.
  Positive
It is commonly used for static websites served out of an S3 bucket.
  Negative
3.
  Neutral
IAM Users and Groups This is similar to defining a Bucket Policy, but permissions are assigned to specific Users or Groups of users.
  Negative
Thus, only those users have permission to access the objects.
  Negative
Users must authenticate themselves when accessing the objects, so this is most commonly used when accessing objects via the AWS API, such as using the aws s3 commands from the AWS Command-Line Interface (CLI).
  Negative
Rather than being prompted to authenticate, users must provide the authentication when making the API call.
  Negative
A simple way of doing this is to store user credentials in a local configuration file, which the CLI will automatically use when calling the S3 API.
  Negative
4.
  Neutral
Pre-Signed URL A Pre-Signed URL can be used to grant access to S3 objects as a way of "overriding" access controls.
  Negative
A normally private object can be accessed via a URL by appending an expiry time and signature.
  Negative
This is a great way to serve private content without requiring a web server.
  Positive
Typically, a Pre-Signed URL is constructed by an application when it wishes to grant access to an object.
  Negative
For example, let's say you have a photo-sharing website and a user has authenticated to your website.
  Positive
You now wish to display their pictures in a web page.
  Neutral
The pictures are normally private, but your application can generate Pre-Signed URLs that grant them temporary access to the pictures.
  Negative
The Pre-Signed URL will expire after a particular date/time.
  Neutral
563e30c761a80130652672b6	X	should use the new aws cli like @number5 's answer below docs.aws.amazon.com/cli/latest/reference/s3/rm.html
563e30c761a80130652672b7	X	this should be the answer.
  Negative
It's a (new-ish) standard, powerful tool, designed for things just like this question
563e30c761a80130652672b8	X	This does not work for buckets under versioning.
  Negative
563e30c761a80130652672b9	X	This is deleting the files just fine but its also deleting the bucket after deleting the files.
  Negative
Did I miss anything?
  Neutral
563e30c761a80130652672ba	X	@Naveen as I said above, rm will only delete files but rb --force will delete the files and the bucket.
  Negative
563e30c761a80130652672bb	X	@number5 I used the following -> aws s3 rm --recursive s3://your_bucket_name and this deletes the bucket as well.
  Negative
I even tried aws s3 rm --recursive s3://your_bucket_name --exclude "" --include ".
  Negative
gz" and this did not help either.
  Neutral
563e30c761a80130652672bc	X	Great idea for using Lifecycle instead of some delete command.
  Negative
563e30c761a80130652672bd	X	Exactly, let S3 do it for you.
  Negative
563e30c761a80130652672be	X	You can also apply this to the entire bucket, enabling you to delete the bucket.
  Negative
563e30c761a80130652672bf	X	Thanks for posting this answer, I was trying to do this exact thing and had put -Key "%_.
  Negative
Key" which doesn't work.
  Negative
563e30c761a80130652672c0	X	According to the help it is either single-object delete s3cmd del s3://BUCKET/OBJECT or whole bucket delete s3cmd rb s3://BUCKET.
  Negative
There is no s3cmd rm, at least according to s3cmd --help.
  Negative
563e30c861a80130652672c1	X	I have the following folder structure in S3.
  Negative
Is there a way to recursively remove all files under a certain folder (say foo/bar1 or foo or foo/bar2/1 .
  Negative
.)
  Neutral
563e30c861a80130652672c2	X	This used to require a dedicated API call per key (file), but has been greatly simplified due to the introduction of Amazon S3 - Multi-Object Delete in December 2011: Amazon S3's new Multi-Object Delete gives you the ability to delete up to 1000 objects from an S3 bucket with a single request.
  Negative
See my answer to the related question delete from S3 using api php using wildcard for more on this and respective examples in PHP (the AWS SDK for PHP supports this since version 1.4.8).
  Negative
Most AWS client libraries have meanwhile introduced dedicated support for this functionality one way or another, e.g.: You can achieve this with the excellent boto Python interface to AWS roughly as follows (untested, from the top of my head): This is available since version 1.24 of the AWS SDK for Ruby and the release notes provide an example as well: Or:
563e30c861a80130652672c3	X	With the latest aws-cli python command line tools, to recursively delete all the files in a bucket is just: If what you want is to actually delete the bucket, there is one-step shortcut: which will remove the contents in that bucket recurisvely then delete the bucket.
  Very negative
563e30c861a80130652672c4	X	You might also consider using Amazon S3 Lifecycle to create an expiration for files with the prefix foo/bar1.
  Negative
Open the S3 browser console and click a bucket.
  Negative
Then click Properties and then LifeCycle.
  Neutral
Create an expiration rule for all files with the prefix foo/bar1 and set the date to 1 day since file was created.
  Negative
Save and all matching files will be gone within 24 hours.
  Negative
Just don't forget to remove the rule after you're done!
  Negative
No API calls, no third party libraries, apps or scripts.
  Negative
I just deleted several million files this way.
  Negative
A screenshot showing the Lifecycle Rule window (note in this shot the Prefix has been left blank, affecting all keys in the bucket): 
563e30c861a80130652672c5	X	I just removed all files from my bucket by using PowerShell:
563e30c861a80130652672c6	X	With s3cmd package installed on a Linux machine, you can do this s3cmd rm s3://foo/bar --recursive
563e30c861a80130652672c7	X	Best way is to use lifecycle rule to delete whole bucket contents.
  Very negative
Programmatically you can use following code (PHP) to PUT lifecycle rule.
  Negative
In above case all the objects will be deleted starting Date - "Today GMT midnight".
  Neutral
You can also specify Days as follows.
  Negative
But with Days it will wait for at least 24 hrs (1 day is minimum) to start deleting the bucket contents.
  Negative
563e30c861a80130652672c8	X	Just seen that Amazon have added Empty Bucket to there AWS console menu.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/UG/DeletingaBucket.html
563e30c861a80130652672c9	X	I run a site where the content is user generated and storage space is mainly filled via images.
  Negative
Eventually my VPS, though it says it has unlimited space, will get too large and I will be limited.
  Very negative
So then I thought to upload the user generated images on a third party storage server, but how will I program how my website will load images from my server, and the new ones from another server, like for example Amazon S3?
  Negative
And suppose I want to do a backup of all the images..... how long does it take to backup 10gb?
  Negative
oh and, what about uploading all the old images onto the third party server and load them from there?
  Neutral
how can that be coded?
  Neutral
I mean transfer all old images on my server, to S3, then load images from s3
563e30c861a80130652672ca	X	Amazon S3 is certainly a good option for storing images because: An easy way to move existing data into Amazon S3 is to use the Amazon Command Line Interface (CLI).
  Negative
This is free software that can call the AWS API for practically any service, including S3.
  Negative
As per the CLI documentation for S3, you can use commands like these: To send individual images from your website to Amazon S3, you can use an SDK to add functionality to your website code.
  Negative
There are SDKs available for many languages including Java, .
  Neutral
Net, PHP, Node.js, Ruby and Python.
  Negative
Once the images are on Amazon S3, you can refer to them in your image tags.
  Positive
This means you could serve HTML from your existing web app, but include pictures from S3.
  Neutral
You can also add security so that the pictures are private, but use a Pre-Signed URL to display the pictures with a special URL for a limited time period.
  Negative
Read the Amazon S3 Developer Guide for information about the capabilities of S3 and how to store and retrieve data.
  Negative
The speed of data transfer (for backup or retrieval) would be limited by your own Internet bandwidth.
  Negative
Use an online data transfer calculator to estimate the time it would take to copy your data.
  Negative
563e30c861a80130652672cb	X	thanks for your advice
563e30c861a80130652672cc	X	I wanted to ask if there is a way to backed up an Amazon S3 Bucket on Standard Storage to an Amazon S3 RRS Redundant Storage Bucket in a different account directly without using a Server/Custom Script in between.
  Negative
I know the storage has high redundancy but a second maintained copy would be an advantage and not have to use a server nightly etc would also be an advantage.
  Negative
thx
563e30c861a80130652672cd	X	If you want to have your files under a different user account, I think the only way is to write a custom script that uses the S3 API (for example in Python with boto).
  Negative
However if you are looking for long-term durable storage, I would suggest to have a look at Amazon Glacier
563e30c961a80130652672ce	X	Yes, Glacier is good option for chipper backup.
  Negative
But for that you have to use such tool which provide you enough description about the uploaded archive file because Glacier does not upload file as file name, it keep file in different alphanumeric name generated by it.
  Negative
2.
  Neutral
It will not replace your changed file by new one, it will upload new with old one.
  Positive
So If you want to keep your data for long time and do not want to retrieve repeatedly then use it.
  Negative
Amazon S3 STANDARD STORAGE KEEP YOUR DATA SAFE BUT EVEN YOU WANT TO HAVE BACKUP THEN YOU CAN USE ANY TOOL WHICH SUPPORTING COPY BETWEEN TWO BUCKET.
  Negative
AND THEN YOU CAN APPLY RRS STORAGE TYPE ON ALL THE OBJECTS OF THE BUCKET.
  Negative
I am one of the developer of Bucket Explorer S3 Tool.
  Negative
You can use Bucket Explorer for copying your data between two difference bucket either in same or different aws account.
  Negative
and also provide batch update metadata option to update RRS setting on the all existing files.
  Neutral
Thanks
563e30c961a80130652672cf	X	The limit on static file sizes is 10MB, and has been for quite some time.
  Negative
563e30c961a80130652672d0	X	Thanks, I've updated the max file size (up from 1MB to 10MB since I wrote this post), and the free quota for bandwidth (down from 10GB to 1GB).
  Negative
For future readers - check Google's pages to confirm these numbers, they will change from time to time.
  Negative
563e30c961a80130652672d1	X	More more work with amazon?
  Negative
I would think the opposite.
  Neutral
If you use Amazon EC2, you can pick what ever platform that suits you!
  Positive
GAE is the one that forces you to do things exactly the way they want you to.
  Positive
I use both.
  Neutral
Amazon for the more important stuff (where I need the freedom to do what I want) and GAE for the less critical apps (cause its basically free!)
  Negative
563e30c961a80130652672d2	X	But freedom means more work.
  Positive
GAE presents it to you on a plate while Amazon requires you to set it up.
  Neutral
563e30c961a80130652672d3	X	What do you see as the advantages and disadvantages of Amazon Web Services S3 compared with Google Application Engine?
  Negative
The cost per gigabyte for the two is, at the time I ask, roughly similar; I have not seen any widespread complaints about the quality of service; so I think the decision of which one to use may depend on the API (of all things).
  Negative
Google's API breaks your content into what they call static content, such as your CSS files, favicons, images, etc and non-static dynamically-generated HTTP responses.
  Negative
Requests for static stuff will be served to whoever requests it until your bandwidth limit is reached; non-static requests will be fulfilled until your bandwidth or CPU limit is reached.
  Negative
With respect to your non-static requests, you can provide any logic you are able to express in Python, so you can be choosy about who you serve.
  Positive
Amazon's API treats all your content as blobs in a bucket, and provides an access protocol that lets you distinguish between a variety of fulfillable requests ranging from world-readable to owner-only.
  Negative
If you want to something that's not in the kit, though, I don't know what you do beyond being thoughtful about distributing your URIs.
  Negative
What differences do you see between the two?
  Neutral
Are there other cloud storage services you like?
  Negative
Zetta had a press release today, but they're looking for a minimum of ten terabytes on the beta application, and none of my clients are there (yet); and Joyent will probably do something in the near future.
  Negative
563e30c961a80130652672d4	X	GAE has a limit of 10MB each on static files uploaded through appcfg.py (look right at the bottom of http://code.google.com/appengine/docs/python/tools/uploadinganapp.html).
  Negative
Obviously you can write code to slice large files into bits and reassemble at download time, but it suggests to me that Google doesn't expect App Engine to be used just as a simple CDN, and that if you want to use it as one you'll have to do some work.
  Negative
S3 does the job out of the box, all you have to do is grab a third-party interface app.
  Negative
If you want to do something non-standard with file access on S3, then probably Amazon expects you to spring for a server instance on EC2.
  Negative
Once this is done, you have much more flexibility than GAE's environment, but you pay more (in cash and probably in maintenance).
  Negative
The plus point for GAE is that it has "cheap" on its side for small apps (up to 1GB storage, 1GB bandwidth and 1.3 million hits a day are free: http://code.google.com/appengine/docs/quotas.html).
  Negative
Depending on your use, this might be significant, or it might be irrelevant on the scale of your total bandwidth costs.
  Negative
Coincidentally, I have just this last couple of days looked at GAE for the first time.
  Negative
I took an old Perl CGI script and turned it into a GAE app, which is up and running.
  Negative
About 10 hours total, including reading the GAE introductory docs and remembering how Python is supposed to work enough to write a couple of hundred lines.
  Negative
I'd speculate that's more effort than loading a bunch of files onto S3, but less effort than maintaining EC2 server(s).
  Negative
However, I haven't used Amazon.
  Neutral
[Edited to add: this sounds like the advantages are all with Amazon for commercial purposes.
  Negative
This may well be true, but then GAE is not yet mature and presumably will get better from here fairly rapidly.
  Negative
They only let people start paying in December or so, before that it was free-quota-only except by special arrangement with Google.
  Negative
While Google sometimes takes flack for its claims of "perpetual beta", I think GAE genuinely is still starting up.
  Positive
If your app is a good fit for the BigTable data paradigm, then it might scale better on GAE than EC2.
  Negative
For storage I assume that S3 is already good enough for all reasonable purposes, and Google's clever architecture gives GAE no advantages to compensate when all you're doing is serving files.]
  Negative
* Except that Google has just offered me a preview of GAE's Java support.
  Negative
** Just noticed that you can set up chron jobs, but they're limited by the same rules as any other request (30 second runtime, can't modify files, etc).
  Negative
563e30c961a80130652672d5	X	The way I see it is the Google App Engine basically provides a sandbox for you to deploy your app as long as it is written with their requirements (Python etc).
  Negative
Amazon gives you a virtual machine with a lot more flexibility in what can be done but probably more work on your side needed.
  Positive
MS new Azure seems to be going down the GAE route, but replace Python with .
  Negative
NET.
  Neutral
563e30c961a80130652672d6	X	Hi, thanks for the reply , i tried this and i am getting error like ERROR: Invalid command: u'modify'
563e30c961a80130652672d7	X	Files giving access denied after this change...
563e30ca61a80130652672d8	X	@Rajaraman u know anything about it??
  Negative
after this step its showing access denied
563e30ca61a80130652672d9	X	This works.
  Negative
.
  Neutral
tested with s3cmd version 1.5.0-rc1 BUT!
  Neutral
: this changes the object's Content-type header to "binary/octet-stream"!
  Neutral
Tested with one PNG -file.
  Neutral
563e30ca61a80130652672da	X	I experienced the same problem as @Hardy.
  Negative
It also removed public readability from all my files.
  Negative
Watch out.
  Neutral
563e30ca61a80130652672db	X	This works if you are uploading to AWS from a local path on your computer and want everything to keep proper cache-control
563e30ca61a80130652672dc	X	That works to set cache headers at time of initial sync but does not update existing files.
  Negative
You can force an update like so find .
  Neutral
-type f -exec touch '{}' \;; aws s3 sync /path s3://yourbucket/ --recursive --cache-control max-age=604800
563e30cb61a80130652672dd	X	I have moved 20000 files to AWS S3 by s3cmd command.
  Negative
Now i want to add cache-control for all images (.
  Neutral
jpg) These files are located in ( s3://bucket-name/images/ ).
  Negative
How can i add cache-control for all images by s3cmd or is there any other way to add header ?
  Neutral
Thanks
563e30cb61a80130652672de	X	Please try the current upstream master branch (https://github.com/s3tools/s3cmd), as it now has a modify command, used as follows:
563e30cb61a80130652672df	X	Also with the AWS's own client:
563e30cb61a80130652672e0	X	Every Metadata setting contains a Key-Value pair.
  Negative
Cache control metadata key is “Cache-Control” and Value is “max-age=<time for which you want your object to be accessed from cache in seconds>” You can set Cache Control Custom Header for Amazon S3 Objects by sending HTTP PUT Request to Amazon S3 Server with appropriate headers in two ways: Set Cache Control Metadata using Amazon S3 REST API PUT Object Request - If you are a programmer, you can write your own software program to use Amazon S3 REST or SOAP APIs to set Custom Headers with PUT Object Request.
  Very negative
This website only refers to Amazon S3 REST APIs, please refer to AWS documentation website for details on how to use SOAP APIs.
  Negative
Set Cache Control Metadata using Bucket Explorer User Interface - If you like to set custom HTTP Headers like Cache Control using mouse clicks instead of writing a software program, you can use Bucket Explorer's user interface for that.
  Very negative
With this Custom HTTP Header, you can specify the caching behavior that must be followed with the request/response chain and to prevent caches from interfering with the request or response.
  Negative
for more information please check How to Set Cache Control Header for Amazon S3 Object?
  Negative
`
563e30cb61a80130652672e1	X	Just upgrade the s3cmd to version 1.5.1 and the issue will resolve.
  Negative
563e30cb61a80130652672e2	X	(Since the OP asked for any other way) You can also do it via aws-cli, e.g. (v: aws-cli/1.8.8 Python/2.7.2 Darwin/12.5.0): Although please note that you will rewrite any existing object.
  Negative
563e30cb61a80130652672e3	X	Well, I created an API to manage for our websites some attachments uploads and store into Amazon S3 buckets The scenario : Once visitor / user in the form and wants to submit it with attachment, once the file is selected then button clicked an Ajax request fire to the micro service API so it can store the file into S3 do some processing then return the direct link or identifier.
  Very negative
The question is : how can we authenticate the user using for example a short live token or something like that without being hijacked, mis-usage of the token.
  Negative
.
  Neutral
In Javascript everything is visible to the visitor, and we try to not integrate any heavy process in the backend
563e30cc61a80130652672e4	X	If I got your question straight, you have a web interface in which files are uploaded to an S3 bucket and you need to make sure that in a certain back end API (such as REST) all file upload commands will have authentication and authorization.
  Very negative
The answer is highly dependent on your architecture but generally speaking, all Javascript calls are nothing but HTTP calls.
  Negative
So you need HTTP authentication/authorization.
  Negative
In general, the most straightforward method for REST over HTTP is the basic authentication, in which the client sends a credential in every single request.
  Negative
This may sound odd at first but it is quite standard since HTTP is supposed to be stateless.
  Negative
So the short answer, at least for the scenario I just described, would be to ask the user to provide the credentials that Javascript will keep in the client side, then send basic authentication which the REST interface can understand.
  Negative
The server-side processes will then get such information and decide whether a certain file can be written in a certain S3 bucket.
  Negative
563e30cc61a80130652672e5	X	I am working on Amazon Web Service (EC2, S3) to set up an instances given the following detail on the account.
  Negative
(I don't have administrative rights to the Amazon account through the web browser) Do anyone know how can I check the total usage cost spent through command line interface?
  Negative
I wouldn't want to give the owner a surprise of how much Amazon have charged him at the end of the month.
  Negative
P.S.: To date of writing Amazon does not provide account charges information through the command line or API (According to @Eric Hammond).
  Negative
If Amazon does in the later date, please give me a head up.
  Positive
Thanks!
  Positive
563e30cc61a80130652672e6	X	Amazon AWS does not currently provide account charges information through the command line or API.
  Negative
Accrued account charges can be viewed through a login on the AWS web site: http://aws-portal.amazon.com/gp/aws/developer/account/index.html?ie=UTF8&action=activity-summary
563e30cc61a80130652672e7	X	Re: your shoestring budget: turn on detailed billing and set up cloudwatch monitoring for your billing -- AWS obviously is in business to make money, but they don't want or need to do it by sticking people with unexpected charges, so they have made it pretty easy to avoid surprises.
  Very negative
You can view graphs of your approximate charges-to-date and even set alarms if your accrued charges exceed thresholds you define.
  Negative
Data transfer has its own metrics.
  Negative
563e30cc61a80130652672e8	X	Actually, the US Standard region is its own region: us-east-1.
  Negative
(source: github.com/aws/aws-sdk-js/issues/276#issuecomment-43145369)
563e30cc61a80130652672e9	X	That's correct, but what I was trying to say is that us-east-1 (S3 "standard") uses data center facilities in both Virginia and Oregon.
  Negative
It's a really weird duck in the AWS service family & the only service I know of that lets me spin up resources in two regions.
  Negative
I can even specify just the Virginia region with s3-external-1.
  Negative
amazonaws.com (Northern Virginia only).
  Neutral
docs.aws.amazon.com/general/latest/gr/rande.html (browse down to S3)
563e30cd61a80130652672ea	X	I am planning on running a script located on an EC2 instance in us-east-1d.
  Negative
The script basically pulls in images from a few different places and will throw them into an S3 bucket in the US-Standard Region.
  Neutral
Since there is no way to upload directly into an s3 bucket (by sending an API request that causes S3 to fetch a file from a remote URL as written about here and I don't think this has changed) I would like to make sure that the each image I save as temp file on my ec2 will not result in additional bandwidth charges when written to S3 (ie.
  Negative
leaves the Amazon data center).
  Negative
Will a us-east-1d EC2 instance uploading to a US-Standard S3 bucket will be communicating within the same AWS region?
  Negative
Any insight into this would be greatly appreciated as it will be terabytes of data and I'm on a shoestring bucket I'd like to know before proceeding.
  Negative
563e30cd61a80130652672eb	X	"US Standard" means "us-east-1".
  Neutral
According to S3 Pricing FAQ There is no Data Transfer charge for data transferred between Amazon EC2 and Amazon S3 within the same Region or for data transferred between the Amazon EC2 Northern Virginia Region and the Amazon S3 US Standard Region.
  Negative
This will mean that if your instance is in any of the us-east-1 AZs and your bucket is in the US Standard region, any movement of data between the 2 should cost nothing.
  Negative
Also, depending on your use case, you may want to look at the new AWS SDK for JavaScript in the Browser as it may offer the direct to S3 uploads you're looking for.
  Negative
563e30cd61a80130652672ec	X	US standard includes us-east and part of us-west (oregon).
  Negative
It's a legacy construct that only applies for S3
563e30cd61a80130652672ed	X	My suggestion might be a bit off topic but you also have the option of base64 encoded images.
  Negative
Though not very efficient but they have served me well.
  Negative
davidbcalhoun.com/2011/…
563e30cd61a80130652672ee	X	which servlet container you are using?
  Negative
563e30cd61a80130652672ef	X	@AnupamSaini im very new to server programming so simply using restkit seemed to be the easiest solution.
  Negative
563e30cd61a80130652672f0	X	@RameshPVK apache tomcat i believe
563e30cd61a80130652672f1	X	@RameshPVK the instance image on my ec2 says: AMI: ElasticBeanstalk-Tomcat6-64bit-201202071737 (ami-d5ec3cbc)
563e30cd61a80130652672f2	X	Thanks for your reply, the apache commons fileupload says the specifies the following: if an HTTP request is submitted using the POST method, and with a content type of "multipart/form-data", then FileUpload can parse that request, and make the results available in a manner easily used by the caller.
  Negative
I my request compliant with this?
  Neutral
563e30cd61a80130652672f3	X	@LuisOscar: Try and you shall find the answer.
  Positive
563e30ce61a80130652672f4	X	@LuisOscar: Your request is indeed a POST.
  Negative
From what I gather from RestKit's RKParams reference documentation, it should be a multi-part message.
  Negative
563e30ce61a80130652672f5	X	thanks, im trying to install the commons fileupload to java so i can test it ill get back to you in a bit.
  Negative
563e30ce61a80130652672f6	X	Ok i managed to import the commons but im getting the following warnings: List is a raw type.
  Negative
References to generic type List<E> should be parameterized.
  Negative
Iterator is a raw type.
  Positive
References to generic type Iterator<E> should be parameterized.
  Negative
563e30ce61a80130652672f7	X	It's the way that works without any additional library +1.
  Positive
563e30ce61a80130652672f8	X	It seems i cant use this approach because im running tomcat 6 and according to this site: tomcat.apache.org/whichversion.html I only have 2.x serlvets
563e30ce61a80130652672f9	X	then you should use commons apache file upload
563e30ce61a80130652672fa	X	Ok thanks for your help
563e30ce61a80130652672fb	X	@Lion: it's also the way that works only on recent containers :) Not everybody is that lucky, whereas the lib-way is relatively easy and backward- and forward-compatible.
  Very negative
If you have a Servlet 3+ capable container, then I'd definitely go for that then.
  Negative
563e30ce61a80130652672fc	X	I am trying to send a picture to my java servlet (hosted on amazon ec2) to later transfer it to amazon s3 and wonder how to retrieve the Image from the post request.
  Negative
The request is sent through iOS RestKit API like this (pic.imageData is a NSData type): This is how I parse the other 2 parameters on the Java servlet: The question is: how do I retrieve and parse the image on my server?
  Negative
563e30ce61a80130652672fd	X	Something along the lines of this, using Apache Commons FileUpload: Refer to the FileItem API reference doc to determine what to do next.
  Negative
563e30ce61a80130652672fe	X	Servlet 3.0 has support for reading multipart data.
  Negative
MutlipartConfig support in Servlet 3.0 If a servelt is annotated using @MutlipartConfig annotation, the container is responsible for making the Multipart parts available through References:
563e30cf61a80130652672ff	X	So you want someone to read the referenced link and process the flow of the code and then answer your very general question?
  Negative
Down vote!
  Neutral
563e30cf61a8013065267300	X	I apologize - I thought it would help eliminate the game of fifty questions since my code is almost verbatim... I'll update my question to include the code in my controller
563e30cf61a8013065267301	X	thank you for the feedback - this is what I discovered yesterday but unfortunately won't work for my scenario.
  Very negative
My user create screen sends a nested JSON response to the API and I was hoping I could somehow add the file attachment to it.
  Negative
It looks like as of right now I'll just have to create the user, return the id and then add attachments with the correspondind user id.
  Neutral
It's not the end of the world, I just thought it would be nice to handle it all in one call
563e30cf61a8013065267302	X	I followed this guide http://jaliyaudagedara.blogspot.com/2014/08/how-to-directly-upload-files-to-amazon.html and I was able to successfully upload files to Amazon S3 in my Web API Project.
  Very negative
Happy with the results I decided to add the remaining logic which was JSON from the client.
  Positive
The screen is a create user screen that also has a file uploader.
  Positive
I'd like to send all the user data and uploaded file at once instead of using multiple calls.
  Negative
Is this possible?
  Neutral
If so how should I should tweak the referenced example so it can accept both JSON and the uploaded file?
  Negative
563e30cf61a8013065267303	X	Yes, it is possible to capture form data on the POST as well as the file, though not as JSON, like other API calls.
  Negative
Take a look at the form post in Fiddler.
  Neutral
You should see something like this on the text view tab: In this example, my form has a "RecordTypeId", "RecordId" and "Tags" field on the form.
  Positive
So you can read that data from the "provider.FormData", just like you did with the username.
  Negative
So, each piece of the data that would have been in your JSON object, would show up in the form data.
  Negative
563e30d061a8013065267304	X	Why are you doing this?
  Negative
Why not upload the file directly to S3?
  Negative
563e30d061a8013065267305	X	@Nick: one reason to do this is to have GAE resize the photo.
  Negative
563e30d061a8013065267306	X	@Nick: I´m using S3 and GoogleStorage for permanent storage, the users are uploading to the blobstore first and deciding later where there data has to be saved.
  Very negative
563e30d061a8013065267307	X	@Michael This seems unnecessarily kludgy - and it'll also limit you to 1MB, since that's all you can send with URLFetch.
  Negative
It's possible to do form-based uploads to S3; this would be a far neater approach.
  Negative
563e30d061a8013065267308	X	@Nick: This App should be a kind of storage management solution for different private and public cloud services (Amazon S3, Google Storage, Walrus from Eucalyptus, etc.) The allocation to different services should be done by the AppEngine to save users time and bandwith.
  Negative
Using form-based uploads will triple users bandwith needs.
  Negative
I will try Calvins approach using the BlobReader class, perhaps I can realize some "dirty" workaraound.
  Negative
563e30d061a8013065267309	X	Be careful not to timeout uploading really large files from GAE to S3.
  Negative
563e30d061a801306526730a	X	@Amir: How this should work?
  Negative
Calling blob_reader = blobstore.BlobReader(blob_key, buffer_size=1048576) in a loop and appending to a S3 upload stream?
  Negative
Boto doesn´t support that, if you take a look at botos upload methods like the set_contents_from_string method - there is no possibility to send small chunks (like 1MB) one after another.
  Negative
So the BlobReader class will not help in that case, or did I overlook something?
  Negative
563e30d061a801306526730b	X	I've never used boto, but you should be able to read the entire blob into a buffer object and encode and send that (perhaps using set_contents_from_file?)
  Negative
.
  Neutral
You can pretend a buffer is a file using StringIO.
  Negative
563e30d061a801306526730c	X	I'm uploading data to the blobstore.
  Negative
There should it stay only temporary and be uploaded from within my AppEngine app to Amazon S3.
  Negative
As it seems I can only get the data through a BlobDonwloadHandler as described at the Blobstore API: http://code.google.com/intl/de-DE/appengine/docs/python/blobstore/overview.html#Serving_a_Blob So I tried to fetch that blob specific download URL from within my application (remember my question yesterday).
  Negative
Fetching internal URLs from within the AppEngine (not the development server) is working, even it´s bad style - I know.
  Negative
But getting the blob is not working.
  Negative
My code looks like: And I'm getting DownloadError: ApplicationError: 2 Raised by: File "/base/python_runtime/python_lib/versions/1/google/appengine/api/urlfetch.py", line 332, in _get_fetch_result raise DownloadError(str(err)) Even after searching I really don't know what to do.
  Negative
All tutorials I've seen are focusing only on serving through a URL back to the user.
  Negative
But I want to retrieve the blob from the Blobstore and send it to a S3 bucket.
  Negative
Anyone any idea how I could realize that or is that even not possible?
  Negative
Thanks in advance.
  Neutral
563e30d061a801306526730d	X	You can use a BlobReader to access that Blob and send that to S3.
  Negative
http://code.google.com/intl/de-DE/appengine/docs/python/blobstore/blobreaderclass.html
563e30d161a801306526730e	X	+1 totally agree on both points.
  Negative
563e30d161a801306526730f	X	Thanks for the link.
  Neutral
I am aware of the second issue, but my client is worried about somebody guessing the URL(don't ask me how.
  Negative
.)
  Neutral
.
  Neutral
563e30d161a8013065267310	X	On the bright side, they've latched onto the side of things you can actually control a bit.
  Positive
:-)
563e30d161a8013065267311	X	+1.
  Negative
Thanks for the answer.
  Neutral
563e30d161a8013065267312	X	I have several PDF files stored in Amazon S3.
  Negative
Each file is associated with a user and only the file owner can access the file.
  Negative
I have enforced this in my download page.
  Neutral
But the actual PDF link points to Amazon S3 url, which is accessible to anybody.
  Negative
How do I enforce the access-control rules for this url?
  Neutral
(without making my server a proxy for all the PDF download links)
563e30d161a8013065267313	X	I would suggest using Amazon S3's authenticated REST URLs with an expiration date.
  Negative
They allow temporary, expiring access to a non-public S3 object.
  Neutral
That said, if they're going to share the URL, what's stopping them from sharing the file itself?
  Negative
563e30d161a8013065267314	X	Each file on your S3 account can have special access rights (ACL).
  Negative
You should set all your PDFs ACL to private.
  Negative
Then nobody will be able to access them.
  Neutral
S3 has an API which allows you to "temporarily" grant read-access.
  Positive
For example Amazon's S3 PHP Library has a getAuthenticatedURL (string $bucket, string $uri, integer $lifetime, [boolean $hostBucket = false], [boolean $https = false]) function.
  Negative
This will enable you to allow access to a PDF for a defined amount of time (like 5 minutes) - which is more than enough if you immediatly redirect the user to S3.
  Negative
563e30d261a8013065267315	X	I use Node.js to host a web server.
  Negative
The server will communicate with different third party storage providers, such as Box, dropbox, googledrive, amazon s3, skydrive, ftp server, etc.
  Negative
The server will download files from or upload files to the storage providers.
  Negative
I'm using the OAuth module to implement the authorization process and get the auth_token and auth_secret.
  Negative
OAuth module unifies the authorization interfaces for different servers.
  Negative
OAuth module encapsulates the APIs of the oauth servers and provides a uniform and friendly interfaces.
  Positive
With OAuth module, I don't need to call the API of the oauth servers in my code.
  Negative
It is pretty good and saves lot of my work.
  Positive
I encounter some difficulties when download/upload files from/to the different storage providers.
  Negative
I must write the different implementations for each storage server.
  Neutral
Such as calling box APi to upload file to box, calling s3 API to upload file to s3, etc.
  Negative
I wonder if there is some node module which encapsulates all the APIs for the storage providers, which works like the OAuth module for the authorization APIs.
  Negative
The pseudo code below expresses how I expect this kind of module works.
  Neutral
I suppose it's name is 'storage'.
  Neutral
Please let me know which node module can meet my requirement.
  Negative
Thanks, Jeffrey
563e30d261a8013065267316	X	You should check out Temboo - it does exactly what you're looking.
  Negative
Temboo normalizes APIs so that they all look and and feel the same.
  Negative
Once you know how to talk to a single storage API via Temboo you know to talk them to all.
  Neutral
Temboo has a Node.js SDK, and you can find out more about the storage APIs that Temboo supports here.
  Negative
(Full disclosure: I work at Temboo)
563e30d261a8013065267317	X	I've created a s3 server which contain a large number of images.
  Negative
I'm now trying to create a bucket policy, which fits my needs.
  Positive
First of all i want everybody to have read permission, so they can see the images.
  Negative
However i also want to give a specific website the permission to upload and delete images.
  Neutral
this website is not stored on a amazon server?
  Neutral
how can i achieve this?
  Neutral
so far i've created an bucket policy which enables everybody to see the images
563e30d261a8013065267318	X	You can delegate access to your bucket.
  Negative
To do this, the other server will need AWS credentials.
  Negative
If the other server were an EC2 instance that you owned then you could do this easily by launching it with an IAM role.
  Positive
If the other server were an EC2 instance that someone else owned, then you could delegate access to them by allowing them to assume an appropriate IAM role in your account.
  Negative
But for a non-EC2 server, as seems to be the case here, you will have to provide AWS credentials in some other fashion.
  Negative
One way to do this is by adding an IAM user with a policy allowing s3:PutObject and s3:DeleteObject on resource "arn:aws:s3:::examplebucket/*", and then give the other server those credentials.
  Negative
A better way would be to create an IAM role that has the same policy and then have the other server assume that role.
  Neutral
The upside is that the credentials must be rotated periodically so if they are leaked then the window of exposure is smaller.
  Negative
To assume a role, however, the other server will still need to authenticate so will need some base IAM user credentials (unless you have some way to get credentials via identity federation).
  Negative
You could add a base IAM user who has permissions to assume the aforementioned role (but has no other permissions) and supply the base IAM user credentials to the other server.
  Negative
When using AssumeRole in this fashion you should require an external ID.
  Negative
You may also be able to restrict the entity assuming this role to the specific IP address(es) of the other server using a policy condition (not 100% sure if this is possible).
  Neutral
563e30d261a8013065267319	X	The Bucket Policy will work nicely to give everybody read-only access.
  Neutral
To give specific permissions to an application: See also: Amazon S3 Developer Guide
563e30d261a801306526731a	X	Looks like an Answer!
  Negative
I should have read back to May on your Blog (Designing RavenFS (ayende.com/blog/4828/designing-ravenfs ) and RavenFS & Rsync (ayende.com/blog/4829/ravenfs-rsync )).
  Negative
I've posted a bit of background here (solutionevangelist.com/post/13313 ).
  Negative
The only issue is I'd like to use it for a few personal non-commercial projects too which might be a bit price sensitive.
  Negative
However, this may be solvable via a developer license, and I'm sure we'd be able to find paying clients/projects for this.
  Neutral
Can you tell me more?
  Neutral
563e30d261a801306526731b	X	I am looking for an API that performs functionality roughly analogous to Rackspace Cloud Files / OpenStack Swift, Microsoft Azure Blob Storage, or Amazon S3 that can be run on a Windows Server.
  Negative
I am not speaking of all the add-ons including replication, etc, but an API that enables a similar RESTful API for the storage/serving (including Anonymous).
  Negative
Some examples of functionality I like, and would be missing if I rolled my own right now, are: Options like MongoDB's GridFS are getting close, but wouldn't quite cut it.
  Negative
RavenDB's "Attachments" functionality is pretty close, I understand it only supports up to 2Gb via the ESENT storage engine Just to clarify, I'm not exactly sure what form this would take.
  Negative
I'm not looking for a pre-built product (which I don't see exists), but perhaps a stub of a project, an open source project planning to provide this functionality, people who might have developed their own similar solution in C#, etc.
563e30d361a801306526731c	X	We have RavenFS that handles that scenario, I think.
  Negative
It is a commercial offering, though.
  Positive
563e30d361a801306526731d	X	you can do this with a condition block, but it's fairly complicated.
  Positive
Instead, put your private files in a second bucket.
  Neutral
563e30d361a801306526731e	X	Can you please answer my questions here: stackoverflow.com/questions/29749203/…
563e30d361a801306526731f	X	I am using Amazon S3 to upload files into different folders.
  Negative
All folders and files are public and can be seen by anyone.
  Positive
I created a private folder, where i want to put private images so that only i can see them.
  Negative
I already created a bucket policy rule that will deny the access to that folder.
  Positive
But how can i see the files ?
  Neutral
Is there a special link like this https://s3.amazonaws.com/bucket/private_folder/file.jpg?secret_key=123 that will let me and someone who know`s that secret key to see the files ?
  Neutral
Is there any way of uploading private files that can be seen by using a secret_key, url or something like that ?
  Negative
563e30d361a8013065267320	X	By default, all objects in Amazon S3 are private.
  Negative
Objects can then be made "public" by adding permissions, via one of: As long as one of these methods grants access, the person will be able to access the object.
  Positive
It is also possible to assign Deny permissions that override Allow permissions.
  Negative
When an object is being accessed via an un-authenticated URL (eg s3.amazonaws.com/bucket-name/object-key), the above rules determine access.
  Negative
However, even "private" files can be accessed if you authenticate against the service, such as calling an S3 API with your user credentials or using a pre-signed URL.
  Neutral
To see how this works, click a private file in the Amazon S3 Management Console, then choose Open from the Actions menu.
  Negative
The object will be opened.
  Neutral
This is done by providing the browser with a pre-signed URL that includes a cryptographically-sized URL and a period of validity.
  Negative
The URL will work to Get the private file only until a defined time.
  Negative
So, to answer your question, you can still access private files via: Just be careful that you don't define DENY rules that override even your ability to access files.
  Negative
It's easier to simply ALLOW the directories you'd like to be public.
  Negative
See: Query String Request Authentication Alternative
563e30d361a8013065267321	X	Is there a way to set an expiry date in Amazon Glacier?
  Negative
I want to copy in weekly backup files, but I dont want to hang on to more than 1 years worth.
  Negative
Can the files be set to "expire" after one year, or is this something I will have to do manually?
  Neutral
563e30d461a8013065267322	X	While not available natively within Amazon Glacier, AWS has recently enabled Archiving Amazon S3 Data to Amazon Glacier, which makes working with Glacier much easier in the first place already: [...] Amazon S3 was designed for rapid retrieval.
  Negative
Glacier, in contrast, trades off retrieval time for cost, providing storage for as little at $0.01 per Gigabyte per month while retrieving data within three to five hours.
  Negative
How would you like to have the best of both worlds?
  Neutral
How about rapid retrieval of fresh data stored in S3, with automatic, policy-driven archiving to lower cost Glacier storage as your data ages, along with easy, API-driven or console-powered retrieval?
  Negative
[emphasis mine] [...] You can now use Amazon Glacier as a storage option for Amazon S3.
  Negative
This is enabled by facilitating Amazon S3 Object Lifecycle Management, which not only drives the mentioned Object Archival (Transition Objects to the Glacier Storage Class) but also includes optional Object Expiration, which allows you to achieve what you want as outlined in section Before You Decide to Expire Objects within Lifecycle Configuration Rules: So at the small price of having your objects stored in S3 for a short time (which actually eases working with Glacier a lot due to removing the need to manage archives/inventories) you gain the benefit of optional automatic expiration.
  Very negative
563e30d461a8013065267323	X	You can do this in the AWS Command Line Interface.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html
563e30d461a8013065267324	X	+1 @Ryan.
  Negative
I had assumed it was a S3 certificate change.
  Negative
The bug was introduced in the SDK in version 1.3.21.
  Negative
563e30d461a8013065267325	X	To be fair, if you request https://foo.example.com.s3.amazonaws.com/... in your URL, the *.
  Negative
s3.amazonaws.com doesn't cover that host name: the wildcard doesn't propagate across the dots according to the specs ("*.a.com matches foo.a.com but not bar.foo.a.com").
  Negative
What Amazon could do is get a certificate for *.
  Negative
*.
  Neutral
*.
  Neutral
s3.amazonaws.com.
  Neutral
563e30d461a8013065267326	X	Agreed @Bruno.
  Negative
What the question is talking however is using their Java API.
  Neutral
Basically I can't do anything using their API with a bucket that has a period in its name.
  Neutral
563e30d461a8013065267327	X	Just reading RFC 6125 (best practice spec, but not really implemented yet).
  Negative
I'm not sure how to interpret whether *.
  Negative
*.
  Neutral
*.
  Neutral
something would be allowed with rule 1.
  Negative
The right thing (if you really need a custom handler) would indeed be to write your own verifier that expects the right pattern.
  Negative
The problem with the ALLOW_ALL_HOSTNAME_VERIFIER is that it allows MITM attacks.
  Negative
563e30d461a8013065267328	X	Amazon has a way to address the buckets without affecting the hostname.
  Negative
That's how they are going to fix it.
  Neutral
I was looking for a short/mid term solution to issue.
  Neutral
563e30d461a8013065267329	X	for reference the 1.3.22 does not fix this for non-US buckets.
  Negative
Path-style addressing requires to change the endpoint to the region specific one using #setEndpoint so a little more work is needed setting the endpoint for the s3client instance in use
563e30d461a801306526732a	X	Amazon "upgraded" the SSL security in its AWS Java SDK in the 1.3.21 version.
  Negative
This broke access any S3 buckets that have periods in their name when using Amazon's AWS Java API.
  Neutral
I'm using version 1.3.21.1 which is current up to Oct/5/2012.
  Negative
I've provided some solutions in my answer below but I'm looking for additional work arounds to this issue.
  Negative
If you are getting this error, you will see something like the following message in your exceptions/logs.
  Negative
In this example, the bucket name is foo.example.com.
  Positive
You can see documentation of this problem on the AWS S3 discussion forum: https://forums.aws.amazon.com/thread.jspa?messageID=387508&#387508 Amazon's response to the problem is the following.
  Negative
We should be able to fix this by using the older path style method of bucket addressing (instead of the newer virtual host style addressing) for buckets with this naming pattern.
  Negative
We'll get started on the fix and ensure that our internal integration tests have test cases for buckets names containing periods.
  Negative
Any workaround or other solutions?
  Neutral
Thanks for any feedback.
  Neutral
563e30d461a801306526732b	X	Turns out that Amazon "upgraded" the SSL security on S3 in late September 2012.
  Negative
This broke access any S3 buckets that have periods in their name when using Amazon's AWS Java API.
  Neutral
This is inaccurate.
  Neutral
S3's SSL wildcard matching has been the same as when S3 launched back in 2006.
  Negative
What's more likely is that the AWS Java SDK team enabled stricter validation of SSL certificates (good), but ended up breaking bucket names that have been running afoul of S3's SSL cert (bad).
  Negative
The right answer is that you need to use path-style addressing instead of DNS-style addressing.
  Negative
That is the only secure way of working around the issue with the wildcard matching on the SSL certificate.
  Negative
Disabling the verification opens you up to Man-In-The-Middle attacks.
  Negative
What I don't presently know is if the Java SDK provides this as a configurable option.
  Negative
If so, that's your answer.
  Neutral
Otherwise, it sounds like the Java SDK team said "we'll add this feature, and then add integration tests to make sure it all works."
  Negative
563e30d461a801306526732c	X	Edit: So after work on 10/5/2012, Amazon released version 1.3.22 which resolves this issue.
  Negative
I've verified that our code now works.
  Positive
To quote from their release notes: Buckets whose name contains periods can now be correctly addressed again over HTTPS.
  Negative
There are a couple of solutions that I can see, aside from waiting till Amazon releases a new API.
  Positive
Obviously you could roll back to 1.3.20 version of the AWS Java SDK.
  Negative
Unfortunately I needed some of the features in 1.3.21.
  Negative
You can replace the org.apache.http.conn.ssl.StrictHostnameVerifier in the classpath.
  Neutral
This is a hack however which will remove all SSL checking for Apache http connections I think.
  Very negative
Here's the code that worked for me: http://pastebin.com/bvFELdJE I ended up downloading and building my own package from the AWS source jar.
  Negative
I applied the following approximate patch to the HttpClientFactory source.
  Neutral
The right fix is to change from domain-name bucket handling to path based handling.
  Positive
Btw, the following seems like it might work but it does not.
  Negative
The AWS client specifically requests the STRICT verifier and does not use the default one:
563e30d561a801306526732d	X	Are there any other products much similar to Amazon S3's offerings
563e30d561a801306526732e	X	I have decided to write an abstraction layer myself, so that it will be easier for me to port this to GAE later on.
  Negative
563e30d561a801306526732f	X	Google App Engine provides a image API for storing / retrieving images.
  Negative
We are currently not in a position to deploy our application on top of App Engine because of limitations in the java frameworks (jboss seam 2.2.0) we are using to build our j2ee application.
  Negative
We would eventually want to deploy our production application on top of Google App Engine, but what are the short term options (java based open source products) which provides comparable functionality to Google App Engine's Image API and will have an easier migration path at a later point in time.
  Negative
563e30d561a8013065267330	X	I know it's not a java-based open source product but if you are talking about the Blobstore API (just for storing/retrieving images), I'd recommend replacing it with Amazon S3 : http://aws.amazon.com/s3/ It doesn't have the features of the Image Manipulation API from Google App Engine though.
  Negative
563e30d561a8013065267331	X	If you're looking good image storage try Bildero.
  Negative
563e30d561a8013065267332	X	I am trying to create a signed URL for a PUT request into an Amazon S3 bucket.
  Negative
I get this error: The request signature we calculated does not match the signature you provided.
  Very negative
Check your key and signing method.
  Neutral
Now I've tried everything under the sun to fix this, like different solutions to similar questions from others posted here on SO.
  Negative
I have made sure that the that Amazon returns in the error matches the string that the script signs.
  Negative
Like when I get this as response: It looked like Amazon did not expect the amz headers and the mimetype, so I left that out.
  Negative
But still to no avail.
  Negative
Here's the code I'm using, courtesy and slightly adapted of: http://danielgallo.co.uk/dev/extjs-amazons3/docs/#!/api/AmazonS3-method-uploadFile Here's what I can exclude from being a solution as I tried this: Any help is greatly appreciated, thanks!
  Positive
563e30d561a8013065267333	X	Can't you just query whois.ripe.net?
  Negative
Or am I on the wrong page here?
  Neutral
563e30d561a8013065267334	X	I was looking for a Whois Api, but most of them charge heavy price and not reliable enough.
  Negative
We can code in Python or Php.
  Neutral
We need to make a Whois lookup service, to integrate with our site.
  Negative
What AWS Resource we need for this?
  Neutral
We need at least 5k lookups per day.
  Negative
AWS provides: S3 , elastic, and others.
  Positive
We are confused.
  Negative
As Amazon provides free tire.
  Positive
Does it allow who is lookup?
  Neutral
As google app engine never allowed this.
  Negative
563e30d561a8013065267335	X	The Amazon service you want to use is the server service: EC2.
  Positive
You get full access to a server and, of course, you can performs socket connections on port 43 (the one required by the Whois protocol).
  Negative
563e30d561a8013065267336	X	Has nothing to do with nodejs, but in S3, you cannot delete all files at one time.
  Negative
You have to delete one file/object at a time, and so you have to have a loop to delete all files/objects.
  Very negative
563e30d561a8013065267337	X	@rsmoorthy: This used to be the case indeed, but has been mostly remedied via Amazon S3 - Multi-Object Delete as of December 2011 - it's still limited to 1000 objects at a time, but a significant improvement regardless of course.
  Negative
563e30d661a8013065267338	X	@steffenOpel - Thanks!
  Neutral
I missed that announcement.
  Negative
Yes, that will speed up the deletions.
  Negative
Last year, I had to delete more than 10M objects and it took days!
  Negative
563e30d661a8013065267339	X	Recommending only you own stuff 8 times on you first day here is not totally appreciated.
  Negative
See this FAQ on promotion.
  Positive
563e30d661a801306526733a	X	Sorry about that, I'll go read the FAQ a bit more.
  Negative
However, knox is a library that is no longer maintained and therefore people get caught out.
  Neutral
AwsSum is maintained, I try to help people and AWS is my specialty so I'd hoped StackOverflow would like that knowledge.
  Negative
I apologise and I'll stop answering questions in this manner.
  Negative
563e30d661a801306526733b	X	I'm not a moderator, just a user who noticed your posts in the sites Review feature.
  Negative
A new user who recommends a single product in several posts on his first day, will make the spam indicator lights start to warm up.
  Positive
The fact that you stated up front that it is your project was good.
  Positive
And that your answers are still here shows that the spam filter wasn't triggered.
  Negative
Just wanted you to stay clear of that.
  Neutral
563e30d661a801306526733c	X	I am using knox npm module to upload files to a bucket on S3.
  Negative
It works great.
  Very positive
What I am not sure how to do is how to remove all the files from a bucket in one call instead of deleting one file at the time?
  Negative
any idea would help...
563e30d661a801306526733d	X	This has not been possible in the past, however, Amazon has finally introduced Amazon S3 - Multi-Object Delete in December 2011: Amazon S3's new Multi-Object Delete gives you the ability to delete up to 1000 objects from an S3 bucket with a single request.
  Very negative
Obviously client libraries like knox must add dedicated support for this API now in turn, and a respective issue does indeed exist already in knox' issue tracker (still pending as of today), see Multi-Object Delete.
  Very negative
Accordingly you should monitor this issue and/or participate in the implementation :)
563e30d661a801306526733e	X	My AwsSum library can already do multi object delete.
  Negative
The operation in the library is called 'DeleteMultipleObjects': You can install AwsSum via npm doing: $ npm install awssum There is an example here: Hope it goes well and give me a shout if you need any assistance.
  Negative
:)
563e30d661a801306526733f	X	Hi Patashu, Thanks for your response.
  Negative
I had gone through this link earlier.
  Neutral
The code which they have provided wont run on my localhost.
  Negative
The explanation is not clear.
  Negative
As they have not mentioned where all we need to make changes in order to run smoothly.
  Negative
563e30d661a8013065267340	X	We want to integrate YouTube API into our website.
  Negative
We have website which gets photos and videos from amazon s3 storage.
  Negative
We would like to add feature which will allow user to upload video to his YouTube account from our website.
  Negative
I spent whole day searching for it, but ended with nothing.
  Negative
Any suggestion or help highly appreciated.
  Positive
Thanks in advance Vinay Kulkarni
563e30d661a8013065267341	X	Speaking as someone who has not tried it before, I would start here: https://developers.google.com/youtube/2.0/developers_guide_protocol "In addition, the API lets your application upload videos to YouTube or update existing videos."
  Negative
563e30d661a8013065267342	X	What are some Cloud storage Services that can download files from web directly.
  Negative
For ex- I want to download a file -> www.example.com/god.avi Now what are some cloud storage services that will allow me to directly upload the file to my account.
  Negative
Google Drive and Dropbox are cloud services but they dont have this facility.
  Negative
563e30d661a8013065267343	X	I assume since you're asking this question on Stack Overflow that you're asking this in the context of programming.
  Negative
In that case, Dropbox has the Saver, which lets a user download a file directly to Dropbox.
  Negative
(Dropbox transfers the file from the URL server-side.)
  Negative
563e30d661a8013065267344	X	You can download/upload files pragmatically to Dropbox and also to Google Drive.
  Negative
They have API to do it.
  Neutral
However, if you are really looking for really complete service where you can scale up your data storage until you want, you really want to take a look at Amazon S3.
  Neutral
In fact, as far as I know, Dropbox works on the top of Amazon S3 to provide their services.
  Negative
If you would like to have an idea about how to upload/download a file to Amazon S3, you can take a look to this application example.
  Positive
If you want the same thing on Dropbox or Google Drive, there are a lot of examples on Internet.
  Negative
However, on these two providers you need a token to upload files, what I don't like.
  Neutral
I prefer the way in which it works for Amazon S3 (just for programming purposes - GUI is better on GD or Dropbox).
  Negative
563e30d761a8013065267345	X	Is your goal to not use nodejs during development?
  Neutral
Or is it enough to not have a requirement for node in production?
  Neutral
563e30d761a8013065267346	X	My goal is to put all html, js and css etc on amazon s3 where they can only host static web pages.
  Negative
So I guess I could use node in development.
  Negative
563e30d761a8013065267347	X	Yep I'd suggest that, employ node/webpack as your build pipeline then just copy the static files up to S3
563e30d761a8013065267348	X	Thank you!
  Negative
So all webpack features would still work right?
  Neutral
Like lazy loading (code splitting)?
  Negative
563e30d761a8013065267349	X	Sure.
  Negative
Have a look here and here for a detailed code base.
  Positive
The important parts are require.ensure (CommonJs) or require (AMD).
  Negative
563e30d761a801306526734a	X	Thank you very much!
  Positive
563e30d761a801306526734b	X	I am trying to build a web app where I want to store all html, js and css files on amazon s3, and communicate with a restful server through api.
  Negative
I am trying to achieve lazy loading and maybe routing with react router.
  Negative
It seems that webpack has this feature code splitting that would work similarly as lazy loading.
  Negative
However, all of the tutorial and examples I found involves webpack-dev-server, which is a small node express server.
  Neutral
Is there anyway I could generate bundle at build time and upload everything to amazon s3 and achieve something similar to Angular's ocLazyLoading?
  Negative
563e30d761a801306526734c	X	It's definitely possible to create a static bundle js file, which you can use in your production code that does not include webpack-dev-server.
  Negative
See this example as a reference (note: I am the owner of this repo).
  Negative
webpack.prod.config.js does create a production ready bundle file using webpack via node.js which itself does not require node.js anymore.
  Negative
Because of that you can simply serve it as a simple static file (which is done in the live example).
  Negative
The key difference is how the entry points are written in the dev- and production environments.
  Positive
For development webpack-dev-server is being used In the production environment you skip the webpack-dev-server and the hot reloading part If you want to split your code into more than one bundle, you might want to have a look at how to define multiple entry points and link the files accordingly.
  Negative
563e30d761a801306526734d	X	Hi!
  Positive
I am having problems with building libs3, can you help me?
  Negative
563e30d761a801306526734e	X	Thanks!
  Positive
As a novice programmer myself, I'm curious: how did you know that -ls3 was the right flag?
  Negative
563e30d861a801306526734f	X	In GCC, libraries are added to the linker command line (when invoked via the gcc/g++ driver) via the -l flag.
  Negative
The syntax is -lname, where name is the part after lib and before the .
  Negative
so/.
  Neutral
a. I guessed on the library file being libs3.so
563e30d861a8013065267350	X	actually it will be -ls3
563e30d861a8013065267351	X	I'm trying to install a library that I can use to access Amazon's S3 service (I just need to be able to upload files there).
  Negative
The code needs to be in C++ because it's going to be bundled as part of an application I'm working on.
  Negative
I'm trying to work with Bryan Ischo's library located here: http://libs3.ischo.com.s3.amazonaws.com/index.html I'm having some installation issues though.
  Negative
I changed GNUMakefile.macosx to GNUMakefile and then ran "sudo make install", as I'm developing on a Mac.
  Negative
Then I made a test .
  Neutral
cpp file.
  Neutral
All I want to do is to be able to initialize the library, since this is what his API says to do.
  Positive
However, I get back this error: I'd like some help either fixing my installation of libs3 or getting a few tips on accessing S3 through C++.
  Negative
Thanks!
  Positive
563e30d861a8013065267352	X	Your test application is not linking with libs3.
  Negative
You will need to add it to the linker flags, such as -ls3 (if the library is libs3.so/a)
563e30d861a8013065267353	X	I faced a similar problem with executing a C file using Byan Ischo's library on mac and I had to add some more parameters before I was able to successfully run my test file.
  Very negative
How to compile libs3 on mac?
  Neutral
How to compile test.c?
  Neutral
How to execute a.out?
  Neutral
563e30d861a8013065267354	X	I have XSSFWorkbook object which I want to save to a particular location (using Amazon S3).
  Negative
For saving I am using some API which takes File object parameter.
  Negative
I know that XSSFWorkbook exposes write method to write to a path.
  Positive
Is there any way I can get file object from XSSFWorkbook.
  Negative
563e30d861a8013065267355	X	I'm developing an api, using Loopback.js and the loopback-component-storage.
  Negative
This component works great if one sends a file with the request, however, I'm generating some images in my server.
  Positive
Each image is a buffer object.
  Negative
How can I upload this buffers to Amazon S3?
  Neutral
This is what I have so far: what can I do now with this stream?
  Positive
Thanks!
  Positive
563e30d861a8013065267356	X	Thanks for the reply!
  Neutral
Any chance you know if I can set the response Access-Control-Allow-Origin header on Amazon S3?
  Negative
563e30d861a8013065267357	X	@Kevin I've been researching this.
  Negative
Amazon S3 does not support setting this header on buckets or objects.
  Negative
For more information, check out this thread.
  Negative
Amazon seems to have zero interest in supporting it in the future, according to their (lack of) response.
  Negative
563e30d961a8013065267358	X	@Dan Thanks.
  Neutral
I was actually a 'poster' on that thread and have given up hope.
  Positive
563e30d961a8013065267359	X	CORS is now available on S3 (3.5 years after being requested!)
  Negative
563e30d961a801306526735a	X	No, you don't.
  Negative
Amazon supports HMAC SHA-2 encoding of the secret key in combination with an expiration token.
  Negative
They allow direct uploads through regular posts (doc.s3.amazonaws.com/proposals/post.
  Negative
html) I just need it to be AJAX.
  Negative
563e30d961a801306526735b	X	Awesome.
  Positive
Didn't know about that!
  Neutral
563e30d961a801306526735c	X	Yeah, still not sure how to fix my bugs Ariejan.
  Negative
Anyone else know?
  Neutral
See post above!
  Very positive
563e30d961a801306526735d	X	I am looking for an HTML 5 AJAX library / framework for users to upload files directly to Amazon S3.
  Negative
The goal is to avoid uploading attachments to the web server (as the web server blocks when it transfer them to Amazon).
  Negative
My understanding is that this should be possible using XDomainRequest, but I can't figure out how.
  Negative
I am running ruby-on-rails and wanted to assign the uploaded file a temporary name (using a UUID) that will be posted back to the web server so the file can later be renamed and integrated with paperclip.
  Negative
Any ideas?
  Neutral
Is this something jQuery can handle?
  Neutral
Flash isn't an option for this project.
  Negative
Thanks!
  Positive
Edit: I managed to get a basic post working but am still have issues.
  Neutral
I'm not exactly sure what headers are required, or how to encode the Amazon required parameters in the request (can I put them in the request header?)
  Negative
.
  Neutral
Here is my progress thus far: Edit: After further updates, I've managed to get the following error: XMLHttpRequest cannot load http://bucket.s3.amazonaws.com/.
  Negative
Origin http://local.app is not allowed by Access-Control-Allow-Origin.
  Negative
I've uploaded a crossdomain.xml file that allows access from the wildcard (*) domain.
  Negative
Not sure how to continue... Edit: After having done more investigation, I'm starting to think that the JavaScript POST might not be possible to S3.
  Negative
Will I be required to post to an EC2 instance before doing a transfer?
  Neutral
I might be able to secure a micro instance, but I'd prefer to go direct to S3 if possible!
  Negative
Thanks!
  Positive
Edit: I posted the question on the Amazon Forums and haven't received any feedback.
  Negative
For cross references the other post can be found here: https://forums.aws.amazon.com/message.jspa?messageID=206650#206650.
  Positive
563e30d961a801306526735e	X	You need to make the other side issue an Access-Control-Allow-Origin header.
  Negative
In you case the other side is Amazon S3 server.
  Negative
Unless they mention your domain in that header you won't be able to make any cross-site requests to them.
  Negative
Amazon S3 now supports Cross Origin Resource Sharing and you can configure any of your S3 buckets for cross-domain access by adding one or more CORS rules to your bucket.
  Negative
Each rule can specify a domain that should have access to your bucket and a set of HTTP verbs you wish to allow.
  Negative
563e30d961a801306526735f	X	Today Amazon announces complete support for Cross-Origin Resource Sharing (CORS) in Amazon S3.
  Negative
You can now easily build web applications that use JavaScript and HTML5 to interact with resources in Amazon S3, enabling you to implement HTML5 drag and drop uploads to Amazon S3, show upload progress, or update content.
  Negative
563e30d961a8013065267360	X	This would mean that you have to expose your S3 credentials in JavaScript.
  Negative
You don't want that.
  Neutral
The best solution is to use Paperclip.
  Neutral
Yes, you must upload to your server first, but at least its secure.
  Neutral
Nevermind that, check the comments.
  Positive
563e30d961a8013065267361	X	S3 can host html pages with jquery no problem.
  Negative
The bucket becomes the server URL.
  Negative
If you use a tool like S3 Bucket Explorer you can get a URL in one click for any HTML page in a bucket.
  Negative
Then you can simply use the PUT command in an XMLHttpRequest to upload files.
  Neutral
This is essentially how the JQuery-HTML5-Upload plugin works for Amazon S3 (See their Issue #12).
  Negative
In fact you can experiment with the Amazon S3 REST API syntax by just plugging it into variables and then using that in conjunction with the XMLHttpRequest open() method.
  Negative
Peace in the multiverse.
  Neutral
563e30d961a8013065267362	X	Amazon has finally added support for Cross Origin Resource Sharing (CORS) which allows this: http://aws.typepad.com/aws/2012/08/amazon-s3-cross-origin-resource-sharing.html
563e30d961a8013065267363	X	Have not tried myself, but they seem to have this working in the jquery-html5-upload plugin http://code.google.com/p/jquery-html5-upload/issues/detail?id=12
563e30d961a8013065267364	X	There is a good rails gem called s3_drect_upload which does exactly what you want out of the box.
  Negative
It's an html5/javascript uploader with support for renaming files.
  Negative
563e30d961a8013065267365	X	That was the problem, Ray.
  Negative
The bucket properties for "static website hosting" specify the endpoint as upload.roughdrag.com.s3-website-us-east-1.
  Negative
amazonaws.com but as soon as I changed it to upload.roughdrag.com.s3.amazonaws.com this started working for me.
  Negative
Thank you very much.
  Positive
563e30d961a8013065267366	X	I'm having a very similar situation to the original poster.
  Negative
Can you share your DNS settings for me?
  Negative
And, are you using Amazon for the DNS?
  Negative
I'm just mapping uploads.mydomain.com to my bucket's endpoint BUCKETNAME.s3-website-us-east-1.
  Negative
amazonaws.com.
  Neutral
563e30da61a8013065267367	X	I have been banging my head against the wall on this and am entirely stumped.
  Negative
I am trying to use FineUploader to upload files directly to my Amazon S3 bucket.
  Negative
I have essentially copied the code from the fineuploader.com web page (Upload Files Directly to Amazon S3) and the server-side PHP.
  Negative
When I attempt to upload a file I see the post to the signature endpoint seems to work successfully but when it attempts to upload to S3 I get a 405 "Method Not Allowed" error.
  Negative
HTML PHP Signature Endpoint - uploadHandler.php Amazon S3 CORS Configuration IAM Group Security Policy uploader.js was captured from http://fineuploader.com/source/all.fineuploader-3.9.1.min.js Console response The software looks amazing but I just can't get past this.
  Negative
Any help is appreciated.
  Positive
563e30da61a8013065267368	X	I'm guessing this is a DNS issue created when you mapped your custom domain name to your S3 bucket.
  Negative
After resolving upload.roughdrag.com, it looks like you have mapped this CNAME to "upload.roughdrag.com.s3-website-us-east-1.
  Negative
amazonaws.com".
  Neutral
Try mapping that CNAME to "upload.roughdrag.com.s3.amazaonaws.com" instead.
  Neutral
Update 1: If you are still seeing issues after this change, I would post in the AWS S3 forums.
  Negative
Hopefully an employee will answer.
  Neutral
There may be an issue with your bucket/CNAME that I cannot see from my end.
  Negative
It looks like a POST request to upload.roughdrag.com.s3.amazonaws.com goes though, but there are issues sending a POST request to upload.roughdrag.com.
  Negative
I verified this with Postman.
  Positive
Update 2: With your latest CNAME change, it looks like POST requests are being accepted by S3.
  Negative
563e30da61a8013065267369	X	I had the same issue.
  Negative
Fixed it by adding the region to the function below: Find the region of your bucket by clicking on properties.
  Negative
Then Static website hosting and only take the details from the endpoint from ap-xxxxx-x
563e30da61a801306526736a	X	Is it possible to use objectstore (swift) provided by openstack via opscenter as a backup-location?
  Neutral
In version 5.1.1.
  Neutral
backups to Amazon S3 are supported.
  Negative
But how do I configure object-store from another provider?
  Neutral
I only found the link below: http://www.datastax.com/documentation/opscenter/5.1/api/docs/backups.html#method-update-a-destination
563e30da61a801306526736b	X	You can use a custom pre-post backup script if you want to back up cassandra to a location that is not on your local file system or on Amazon S3: http://www.datastax.com/documentation/opscenter/5.1/opsc/online_help/services/opscSchedulingBackup_t.html In essence this is a hook for you to run a program before or after the OpsCenter backup.
  Very negative
You can use that to push out to Swift.
  Neutral
Here is an example of what a backup script may look like (this one is to back up to S3 which is now obsolete by the S3 functionality in OpsCenter 5.1, but you can use it as an example for your custom Swift script).
  Negative
https://gist.github.com/phact/7500b6cc9fb6f963c849
563e30db61a801306526736c	X	Look at GitHub Pages.
  Negative
563e30db61a801306526736d	X	Yup, realized this a few hours ago.
  Negative
Thanks!
  Positive
563e30db61a801306526736e	X	I used up my free year of Amazon S3.
  Negative
Is there somewhere I can host an mp3 file (with url: "http://.mp3") Also, where can I host a directory full of html, js, and css files?
  Negative
These assets will respond to a Rails API.
  Neutral
563e30db61a801306526736f	X	Yup.
  Neutral
This is very confusing.
  Negative
And I found that not much documentation is available for boto too.
  Negative
563e30db61a8013065267370	X	I am trying to use boto api to upload photos to Amazon S3.
  Negative
I can successfully upload photos there if I haven't specified the Canned ACL.
  Positive
But if I specified ACL as follow.
  Negative
I got the following error.
  Negative
Error as follow.
  Neutral
I tried for a long time but still cannot get any hints.
  Negative
Anyone knows why?
  Positive
Thanks!
  Positive
563e30db61a8013065267371	X	The upload_part_from_file method should not have a policy parameter.
  Negative
This is a bug in boto.
  Neutral
To assign a policy to a multipart file, you specify the canned policy as the policy parameter on the initiate_multipart_upload call and then upload the parts and complete the upload.
  Negative
Don't try to pass the policy when uploading the individual parts.
  Neutral
We should create an issue on github for boto to remove the policy parameter.
  Negative
It's confusing and doesn't work.
  Negative
563e30db61a8013065267372	X	Can you capture your actual http request headers?
  Neutral
563e30db61a8013065267373	X	I can reproduce this.
  Positive
Investigating ....
563e30db61a8013065267374	X	@SébastienStormacq .
  Negative
Thanks :)
563e30db61a8013065267375	X	It might be a bug.
  Negative
Waiting for confirmation
563e30db61a8013065267376	X	If-Modified-Since is currently formatted as yyyy-MM-dd'T'HH:mm:ss'Z' where it should be EEE, dd MMM yyyy HH:mm:ss z. S3 uses yyyy-MM-dd'T'HH:mm:ss'Z' as the date format, but If-Modified-Since is a part of HTTP specifications and needs to be formatted differently.
  Negative
We are working on a fix, and the next update should address this bug.
  Negative
Thanks.
  Neutral
563e30db61a8013065267377	X	I have tested this and can confirm that it now works.
  Positive
563e30dc61a8013065267378	X	Building for iOS 7+ , Building on Xcode 6.1 , Using Amazon SDK AWSiOSSDKv2 2.0.12 , Testing on iPhone 5s and iPad 2 I am downloading images from my Amazon S3 bucket with the Amazon SDK for iOS.
  Very negative
The downloading is working fine but I want to use the ifModifiedSince property to retrieve only images that have been modified since a certain date (see http://docs.aws.amazon.com/AWSiOSSDK/latest/Classes/AWSS3GetObjectRequest.html#//api/name/ifModifiedSince ) However, this is not working.
  Negative
Even when I specify a ifModifiedSince date that is LATER THAN the modified date of the file on S3, the file is returned.
  Negative
According to the Amazon documentation: ifModifiedSince - Return the object only if it has been modified since the specified time, otherwise return a 304 (not modified).
  Negative
So I am not sure if I am doing something wrong or Amazon has a bug in the SDK.
  Negative
Here is my code:
563e30dc61a8013065267379	X	We've released the AWS Mobile SDK for iOS 2.0.14.
  Negative
It should address the time formatting issue.
  Neutral
563e30dc61a801306526737a	X	Thanks Rico for laying out different options.
  Negative
Looks like some of them do cost.
  Neutral
So I am going to stick with the current solution until the costs get prohibitively high at which point 3rd party moving apis might make sense.
  Negative
Also yah it is ironic that Dropbox using Amazon S3 as well.
  Negative
Thanks again!
  Positive
563e30dc61a801306526737b	X	We are looking to support export of photos from our S3 location to users' Dropbox.
  Negative
Currently I am using the code like below: The above method costs twice the bandwidth.
  Negative
Is there anyway I can transfer the images directly from S3 to Dropbox without copying them locally?
  Negative
Thanks in advance!
  Neutral
563e30dc61a801306526737c	X	How about trying something like mover and use their APIs?
  Negative
https://mover.io/ http://support.mover.io/knowledgebase/articles/214572-how-to-transfer-or-backup-your-amazon-s3-buckets-t Or you can also try SME Storage (Storage Made Easy) http://storagemadeeasy.com/ It's kinda ironic that DropBox uses Amazon S3 to stores all its files.
  Negative
Or you can also write your own streamer in Ruby and run it in an Amazon instance it will be much faster since all the data would be within Amazon.
  Neutral
How do I HTTP post stream data from memory in Ruby?
  Neutral
563e30dc61a801306526737d	X	The article might have been written before Facebook added the feature.
  Negative
Canvas iframes are now always loaded via form POST (for extra security or something), and it can't be turned off any more.
  Positive
Looks like a dead end
563e30dc61a801306526737e	X	These days a free SSL cert can be had in less than 24 hours for most domains.
  Negative
Using S3 because you don't have a cert seems silly to me.
  Negative
563e30dc61a801306526737f	X	I was involved in a fanpage project using the JS API so we decided to host the site on an Amazon S3 bucket as a) it's static content and b) Amazon have an SSL certificate required by Facebook apps since Oct 2011.
  Negative
But it turns out that instead of going a HTTP GET, Facebook is requesting the fanpage via an HTTP POST (an additional security check?
  Negative
why don't they just to an HTTP HEADERS?)
  Negative
.
  Neutral
Amazon wisely sends back the following: ...as it figures that Facebook is trying to upload via the POST The irony is that Facebook actually recommend using S3 for those who don't have an SSL certificate on http://www.facebook.com/note.php?note_id=10150223945345844 Bottom line: Has anyone managed to host a fanpage on an S3 bucket post October 2011?
  Negative
Is there a bucket policy that can help with this?
  Neutral
563e30dc61a8013065267380	X	An option would be to use cloudfront to point to an EC2 instance.
  Negative
This will happily accept Post requests.
  Positive
Just make sure you set a very long cache TTL on your response headers to ensure the instance doesn't keep getting hit by requests.
  Negative
You can still host your images etc in s3.
  Neutral
The EC2 instance will just be in charge of translating the post request.
  Negative
563e30dc61a8013065267381	X	I have some doubts about accessing and processing data in cloud storage services.
  Negative
1.
  Neutral
Is there any common API that i can use to write applications for the mainstream cloud storage providers(Amazon s3,Google cloud storage,Windows Azure.
  Negative
.
  Neutral
Please point out providers whom i missed in this list.
  Negative
I am concentrating on Enterprise domain only, not personal storage) 2.
  Neutral
Now the sdk part.
  Neutral
If i want to write an application(let us say a j2ee application) that process data in iCloud, does iCloud sdk provides me that kind of flexibility?
  Negative
3.
  Neutral
Will the cloud storage service provide me transaction support?I mean support for ACID properties.Or i have to take the responsibility for it?
  Negative
563e30dc61a8013065267382	X	Here is how to use the jClouds storage context:
563e30dd61a8013065267383	X	The basis of any Cloud service provider is to use REST interface.
  Negative
You can use any language to wrap REST request as long as the language supports networking and PKI security infrastructure.
  Negative
All modern language have such functionality.
  Neutral
Any SDK in most cases is just a wrapper to these REST interface so you can very easily write the coder and sync or async way and get what you are looking.
  Negative
SDK just expedite the work to manifold, comparative to using REST directly.
  Negative
That does not mean you can not use REST direclty, it is just an SDK is there to help you to connect to specific cloud service.
  Neutral
How SDK are implemented and what kind of functionality it provides, varies from providder to provider and the services they have.
  Positive
You can not use the Windows Azure SDK for Amazon Cloud because the internal service connection endpoints and underneath interfaces are encapsulated within the SDK itself.
  Negative
It does not mean an SDK can not be created to connect all of the cloud service provide however each cloud service provide individualize the SDK for their cloud service.
  Negative
Cloud services provide infrastructure and platform your users to deploy their application and about ACID, you will have to take care of application level work, however ACID could be a totally separate and lengthy discussion.
  Negative
.
  Neutral
563e30dd61a8013065267384	X	Regarding your 3 questions:
563e30dd61a8013065267385	X	For question 1.
  Negative
If you're asking if all the services you list share a common API, no.
  Negative
They all use REST which means you can use JSON with any of them, but the call will have different names and possibly syntax.
  Neutral
563e30dd61a8013065267386	X	You ca use iClous only through Apple's SDK for iOs and Mac OS X. I am almost sure you cannot access it from a j2ee application.
  Negative
563e30dd61a8013065267387	X	try disposing of your result, and be sure to handle IOExceptions
563e30dd61a8013065267388	X	I've not used these bits before, but since you reference filedata you seem to be using the FileStreamProvider.
  Negative
using the memoryStreamProvider would mean the files are only stored in memory & not written to disk.
  Negative
Not sure if your S3helper supports that though
563e30dd61a8013065267389	X	FileInfo creates a lock on the file even if you dont actually do anything with it.
  Negative
I'd suggest copy the filepath to a string variable and use that instead.
  Negative
563e30dd61a801306526738a	X	I was wondering if that was the issue when I came back around and read this today.
  Negative
Anyone know if there is a way around having these files put on the servers hard drive in the first place?
  Negative
563e30dd61a801306526738b	X	If the files are small (1MB or less) and the traffic on your site is medium - low you could store the file in a memory buffer (either byte[] or MemoryStream ), I'm not familiar with S3 uploads, maybe it has an option to upload a stream, then you can grab the stream as it comes from the upload and push it over.
  Negative
563e30dd61a801306526738c	X	The S3Helper was locking the file.
  Negative
Duh.
  Neutral
I also didn't know FileInfo had Delete on it.
  Negative
How handy... Thanks!
  Positive
563e30dd61a801306526738d	X	I am trying to upload files to S3 after a user uploads a file to my API.
  Negative
I obviously don't want them to live on MY server, in fact I'd prefer they never exist on the server at all.
  Negative
My problem is that the files appear to remain in use for the lifetime of server app!
  Negative
Here is the code: How, when or where do I delete these files?
  Negative
OR is there a way to keep the files from being written to my server's disk in the first place?
  Negative
563e30dd61a801306526738e	X	The similar question shows that this approach should work in general.
  Positive
The only visible differences are:
563e30de61a801306526738f	X	Not sure but s3helper.upload might be uploading on background thread.
  Negative
Try this Just looked at s3 docs and yes it uses not just a background thread but multiple!
  Neutral
Uploads the specified file.
  Neutral
The object key is derived from the file's name.
  Negative
Multiple threads are used to read the file and perform multiple uploads in parallel.
  Negative
For large uploads, the file will be divided and uploaded in parts using Amazon S3's multipart API.
  Negative
The parts will be reassembled as one object in Amazon S3.
  Negative
563e30de61a8013065267390	X	I am looking for online RESTful Web Service Demo and not good examples of RESTful API.
  Negative
Are the following links the only available demos?
  Negative
http://mooshup.com/mashup.jsp?author=keith&mashup=RESTDemo (REST and WSDL 2.0 Discussion continues) http://www.thomas-bayer.com/rest-demo.htm
563e30de61a8013065267391	X	You might want to check out the Amazon S3 REST API.
  Negative
As an example directly from the S3 documentation, the following would be a request to delete the puppy.jpg file from the mybucket bucket: You would have to create an AWS account if you do not have one.
  Negative
It will not only serve as an online demo, but will also be a very practical exercise.
  Negative
563e30de61a8013065267392	X	Here is an experimental one that provides access to the MSDN documentation.
  Positive
I'm not sure how much work has been done on in the last couple of years, but it is a good example of using XHTML as an API format.
  Positive
563e30de61a8013065267393	X	You may take a look at an implementation of the RESTful AtomPub protocol by WordPress.com.
  Negative
You have to have an account to do something useful.
  Neutral
The AtomPub service document is located at:
563e30de61a8013065267394	X	I am working on a show case that will also include a demo implementation within a couple of weeks.
  Negative
It is still growing but you can start reading and perhaps check back in a week or so: http://www.nordsc.com/blog/?cat=13 (read bottom to top) Jan
563e30de61a8013065267395	X	You might want to take a look here: http://www.thomas-bayer.com/sqlrest/ found it on this SO question
563e30de61a8013065267396	X	Sorry to destroy your last shred of hope, but no, there's not a documented way to do this... but also, given that rfc-7233 indicates that Range: support is optional in servers and proxies, and that the concept of a "partial" img is not necessarily something browsers might universally understand as a sane concept... what are you trying to accomplish with only a subset of an image's data?
  Negative
563e30de61a8013065267397	X	Thanks for your help--I'm hosting deep-zoom image pyramids, which typically contain thousands of tiny image files.
  Negative
Uploading them all to S3 is slow and costly (cost per PUT).
  Negative
I'm trying to glob the images together into one file, tracking the byte ranges for the individual chunks.
  Negative
I can then fetch a chunk via an AJAX req and set the img data accordingly (base64 enc, unfortunately).
  Negative
I have a working implementation, but caching seems to be an issue.
  Neutral
563e30de61a8013065267398	X	S3 PUT requests are $5 for 1 million.
  Negative
Still, it's an interesting approach you're taking.
  Positive
563e30de61a8013065267399	X	I'm familiar with the Range HTTP header; however, the interface I'm using to query S3 (an img element's .
  Negative
src property) doesn't allow me to specify HTTP headers.
  Negative
Is there a way for me to specify my desired range via a parameter in the query string?
  Negative
It doesn't seem like there is, but I'm just holding out a shred of hope before I roll my own solution with ajax requests.
  Negative
563e30de61a801306526739a	X	Amazon S3 supports Range GET requests, as do some HTTP servers, for example, Apache and IIS.
  Negative
How CloudFront Processes Partial Requests for an Object (Range GETs) I tried to get my S3 object via cURL: and AWS SDK for JavaScript: These two methods work fine for me.
  Negative
AWS SDK for JavaScript API Reference (getObject)
563e30df61a801306526739b	X	Thanks for the response.
  Negative
Do you think I need to use BeginUpload in asp.net app or I can just use the normal Upload method?
  Neutral
563e30df61a801306526739c	X	You're welcome.
  Positive
It really depends on whether you want the upload to be synchronous or asynchronous.
  Negative
563e30df61a801306526739d	X	If I am doing asynchronously how the status will be shown in ASP.Net app?
  Negative
Is there any example available?
  Neutral
563e30df61a801306526739e	X	Please have a look into this article from MSDN on sync vs. async.
  Negative
563e30df61a801306526739f	X	TransferUtility.Upload is 4 times faster than PutObject when I uploaded a 110MB file, TransferUtility.Upload is the way to go
563e30df61a80130652673a0	X	I know there are two methods available to upload files in AWS S3 (i.e. PutObject and TransferUtility.Upload).
  Negative
Can someone please explain which one to use?
  Neutral
FYI, I have files ranging from 1kb to 250MB.
  Negative
Thanks in advance.
  Neutral
563e30df61a80130652673a1	X	Based in Amazon docs, I would stick with TransferUtility.Upload: Provides a high level utility for managing transfers to and from Amazon S3.
  Negative
TransferUtility provides a simple API for uploading content to and downloading content from Amazon S3.
  Neutral
It makes extensive use of Amazon S3 multipart uploads to achieve enhanced throughput, performance, and reliability.
  Positive
When uploading large files by specifying file paths instead of a stream, TransferUtility uses multiple threads to upload multiple parts of a single upload at once.
  Negative
When dealing with large content sizes and high bandwidth, this can increase throughput significantly.
  Negative
But please be aware of possible concurrency issues and the recommendation about using BeginUpload (the asynchronous version), like in this related post
563e30df61a80130652673a2	X	Mostly we people try to upload large files on S3 that take too much time to upload,at that situations we need to have progress information such as the total number of bytes transferred and remaining amount of data to transfer.
  Negative
Following link has detailed way that helps us to create event that occurs periodically and returns multipart upload progress information to us.
  Very positive
https://docs.aws.amazon.com/AmazonS3/latest/dev/HLTrackProgressMPUDotNet.html
563e30df61a80130652673a3	X	Great thanks.
  Positive
Now I suppose I need to figure out some regex to separate that key.name in to the folders so I can build some organisation in to bucket browsing rather than just a huge list of images.
  Negative
563e30df61a80130652673a4	X	well, if remember correctly you can just use the bucket.list(s3_path) function if you like to organize your files in a "unix path like" manner, because it lists all keys having this start string in common.
  Negative
563e30df61a80130652673a5	X	readthedocs.org/docs/boto/en/latest/ref/s3.html#boto-s3-bucket look at the "list()" function's parameters (i.e. prefix, etc..)
  Negative
563e30df61a80130652673a6	X	docs.amazonwebservices.com/AmazonS3/latest/API/…
563e30df61a80130652673a7	X	Thank You.
  Negative
Boto is turning out to be great.
  Negative
Now I just need to keep reading the Djangobook to figure out how to store this info.
  Negative
563e30df61a80130652673a8	X	I've been using Django for a couple of days & setup a basic blog from a tutorial with django comments.
  Negative
I've got a totally separate python script that generates screenshots and uploads them to Amazon S3, now I'd like my django app to display all the images in the bucket and use a comment system on the images.
  Very negative
Preferably I'd do this by just storing the URLs in my sqlite db, which I've got hard-coded currently to display all images in the db and has comments enabled on these.
  Negative
My model: (Does this need a foreign key to the django comments or is that just part of the Django Magic?!)
  Neutral
My bucket structure: https://s3-eu-west-1.amazonaws.com/bucket/revision/process/images.png Almost all the tutorials and packages I'm finding are based on upload/download rather than a simple for keys in bucket type approach that I want.
  Negative
One of my problems is understanding how I can integrate my Boto functions with Django if I'm using Base.html.
  Negative
In an earlier tutorial I had an index page which had a view and could call functions from there.
  Negative
But base doesn't need that so I'm starting to get a little lost.
  Negative
563e30df61a80130652673a9	X	haven't looked up if boto api changed, but this is how it worked last time i looked Update: Amazon s3 is a key value store, where key is a string.
  Negative
So nothing prevents you from putting in keys like: now bucket.list(prefix="/folder/images/") would yield the latter three.
  Negative
Look here for further details:
563e30e061a80130652673aa	X	This is my code to store result from s3 to mysql by boto, django.
  Negative
563e30e061a80130652673ab	X	Apparently, there is one.
  Negative
Will be released with the next version of S3fm.
  Neutral
Stay tuned :)
563e30e061a80130652673ac	X	http://www.s3fm.com/ is really useful for uploading/viewing files for personal viewing.
  Negative
I was wondering if there was something good for social networks (using Amazon s3 or something similar), where it will only show the subset of files uploaded by a specific user (for example, limit access to the user's specific bucket).
  Negative
Can s3fm be adapted to this solution?
  Neutral
or is there something else out there?
  Neutral
563e30e061a80130652673ad	X	Chris, thanks for bringing this up.
  Positive
Next version of S3fm will allow just that: sharing files and "folders" with your friends and colleagues using your own S3 account.
  Negative
A bucket owner will be able to use his or her credentials to create new (non-AWS) "accounts" and assign different permissions for each user.
  Positive
Then s/he will be able to select files to share or "folders" for uploads for each of those users.
  Neutral
A secure authentication method has been developed on top of regular Amazon S3 API so no 3rd party service will be required for that purpose.
  Negative
In fact, your newly created account credentials are not even accessible to anyone but you and you users .
  Negative
On the flip side, if you loose them - they are gone, we wont be able to restore them.
  Negative
:) This version was expected this coming Fri (Aug 9, 2009), but apparently will be delayed another week or so.
  Negative
Happy to help, feel free to follow up with questions or ideas, Alex
563e30e061a80130652673ae	X	I believe you would need to build your own system to do this.
  Negative
What you use doesn't really matter, you could use S3, Azure, etc as your base storage "cloud."
  Negative
563e30e061a80130652673af	X	There is no method of authentication on S3, it only serves files publicly.
  Negative
You can of course obfuscate the file names by naming them with hashes.
  Negative
But still only a fileserver.
  Negative
Maybe roll your own system?
  Neutral
Then make it public so I can use it... it would be awesome!
  Positive
563e30e061a80130652673b0	X	I have updated my S3 class - Link: http://amazon-s3-php-class.googlecode.com/files/s3-php5-curl_0.4.0.tar.gz
563e30e061a80130652673b1	X	Thanks for you support, its done.
  Negative
563e30e061a80130652673b2	X	Yep Grantee: Frank Open/Download: Yes View Permissions:Yes Edit Permissions:Yes
563e30e061a80130652673b3	X	Then tell me what else
563e30e061a80130652673b4	X	Am using below third party API in my project development http://undesigned.org.za/2007/10/22/amazon-s3-php-class I have done all task like upload, delete, bucket-list, object-list with this API, but one of major task to create object download link form a bucket is hazy.
  Negative
Official Amazon API has : get_object_url ( $bucket, $filename, $preauth, $opt ) to get any object's URL, but with above API is lack of method and documentation.
  Negative
Its shows following error with this code when i click on download link: Code Error If someone has any idea or experience with this API then suggest me.
  Negative
Thanks
563e30e061a80130652673b5	X	So you are using : http://undesigned.org.za/2007/10/22/amazon-s3-php-class S3 class, and want to create download link of an object from bucket You should try this: This download link will expire after 1hr(3600sec.)
  Positive
, you can extend expire time by the last parameter to increment it.
  Negative
563e30e161a80130652673b6	X	Does all files have all user read permission (public accessible) ?
  Negative
if you want to access that file without authenticated way then you have to give read permission to that file.
  Neutral
else you can also create signed url.
  Positive
563e30e161a80130652673b7	X	I m getting permission denied.
  Negative
And more specifically, I need to know abt the rest based S3 API using federated user
563e30e161a80130652673b8	X	It was improper token usages.
  Negative
COrrected it with the help of the docs.
  Positive
563e30e161a80130652673b9	X	How can I upload file directly to S3 using curl My current flow is: I am yet to identify how the step 3 need to be implemented.
  Negative
I have given CreateObject permission the federated user.
  Negative
But the upload is failing.
  Negative
I am trying to make the service run independent of the logic I am running behind and without knowledge Any help will be appreciated.
  Negative
Looks like a silly mistake i am doing here.
  Negative
563e30e161a80130652673ba	X	What kind of error do you receive (take a look at downloaded file) ?
  Negative
Most likely you incorrectly sign outgoing request.
  Positive
Please take a look at this document: Signing and Authenticating REST Requests.
  Neutral
I would also recommend you to carefully read Amazon S3 REST API to understand how you should operate with S3 buckets / objects.
  Negative
Hope it helps you !
  Positive
563e30e161a80130652673bb	X	Why the @ before the <script> ?
  Negative
563e30e161a80130652673bc	X	yes , I assume it with razor engine,
563e30e161a80130652673bd	X	Still, you don't need it.
  Negative
563e30e161a80130652673be	X	Minus the @ symbol before Script, and this is the correct answer.
  Negative
Thanks
563e30e161a80130652673bf	X	you need the @ sign when using traditional <script> tags, just tested
563e30e161a80130652673c0	X	Thanks I have also tried that already.
  Negative
563e30e161a80130652673c1	X	@Doomsknight.
  Neutral
And I hope you won't use it.
  Positive
563e30e261a80130652673c2	X	Well in that case, check the value of ViewBag.CC server side, you might not be setting it in some case.
  Negative
563e30e261a80130652673c3	X	Im not sure wht showWarning does.
  Negative
(didnt do anything for me) but the rest did contain the correct value.
  Negative
So I saw showWarning('2'); in my code.
  Negative
563e30e261a80130652673c4	X	I was using this code, it calls just alert and it works fine.
  Very positive
you can change alert('@ViewBag.
  Neutral
Message'); it will work.
  Positive
563e30e261a80130652673c5	X	I think you have an additional bracket?
  Negative
Ive tried similar to this but with ".
  Neutral
563e30e261a80130652673c6	X	@Doomsknight. '
  Neutral
or " is the same thing.
  Neutral
Well Did it work?
  Neutral
563e30e261a80130652673c7	X	On run time i see var c = $('#' + '2').
  Negative
val();
563e30e261a80130652673c8	X	@Doomsknight.
  Negative
O.k.
  Neutral
So it's working... :) You have 2 in the ViewBag.CC
563e30e261a80130652673c9	X	Seeing the 2 now, Ive gone back to var c = '@ViewBag.
  Negative
CC'; seems to be working now :/ Maybe running a few methods together were breaking it all.
  Negative
Thanks
563e30e261a80130652673ca	X	My attempted methods.
  Neutral
Looking at the JS via browser, the @ViewBag.
  Neutral
CC is just blank... (missing)
563e30e261a80130652673cb	X	if you are using razor engine template then do the following in your view write : UPDATE: A more appropriate approach is to define a set of configuration on the master layout for example, base url, facebook API Key, Amazon S3 base URL, etc ...``` And you can use it in your client side code as follow:
563e30e261a80130652673cc	X	try: var cc = @Html.
  Very negative
Raw(Json.Encode(ViewBag.CC)
563e30e261a80130652673cd	X	You can use ViewBag.PropertyName in javascript like this.
  Negative
563e30e261a80130652673ce	X	ViewBag is server side code.
  Negative
Javascript is client side code.
  Neutral
You can't really connect them.
  Neutral
You can do something like this: But it will get parsed on the server, so you didn't really connect them.
  Negative
563e30e261a80130652673cf	X	I will give you a hint though your issue is in your headers
563e30e261a80130652673d0	X	I am offering a bounty because I am really close to solving the issue I think.
  Negative
Still getting a 403 error.
  Negative
I used the following example code to get to where I am at currently.
  Negative
aws.amazon.com/code/1092
563e30e361a80130652673d1	X	I'm having the same issue.
  Negative
I tried using the lib from the amazon example, Eugeny89's code and the elctech library.
  Negative
None of those worked, I always get the 403 response back from s3.
  Negative
I'm using the same policy together with uploadify and that's working fine, so I don't think the policy is the issue.
  Positive
I do get a warning before the upload though: Warning: Domain mybucket.s3.amazonaws.com does not specify a meta-policy.
  Negative
Applying default meta-policy 'master-only'.
  Negative
563e30e361a80130652673d2	X	... and sure enough it was the policy the problem.
  Negative
Copying the policy from the elctech library over made it work.
  Neutral
563e30e361a80130652673d3	X	Bastien - do you have a working example that you could share ?
  Neutral
563e30e361a80130652673d4	X	interesting ... I will try give this a try and let you know.
  Positive
563e30e361a80130652673d5	X	you seem to be missing: s3options.AWSAccessKeyId = Settings.accessKey; and you're not setting the uploadURL.url.
  Negative
But even with those changes I can't get it to work...
563e30e361a80130652673d6	X	there are quite a few ways to upload to S3 from flash.
  Negative
I am trying to implement code from the following example on Amazon's site http://aws.amazon.com/code/1092?_encoding=UTF8&jiveRedirect=1 Some posts that I ran across indicated that a lot of the fields from Amazon are now Requiered and unless you fill them in you will get this dreaded 403 error.
  Negative
I have tried several things and I am hoping there will be a solution soon.
  Negative
I used the following libs from here http://code.google.com/p/as3awss3lib/.
  Negative
Here is my class that handles all the uploading
563e30e361a80130652673d7	X	I'd worked with s3 and I didn't used special libs for that (maybe except generating policy and signature).
  Negative
Try something like the following:
563e30e361a80130652673d8	X	I managed to get my code to work, the difficulty is to get the right policy.
  Positive
I'm using elctech's library to generate the s3 upload call.
  Negative
I get the amazon s3 certificate from our main Rails application via our API, here's what the policy generation looks like:
563e30e361a80130652673d9	X	A quick search for "s3cmd iam role" would have shown you that you can use IAM roles with s3cmd.
  Negative
Also, you can use the aws cli tool to access S3 via IAM roles.
  Negative
563e30e361a80130652673da	X	"with IAM users and not using access keys" doesn't entirely make sense... IAM users are identified by... their access keys.
  Negative
If "IT policy" is that you can't have access keys, then the question for IT is whether you are authorized to access S3 and how that should be done.
  Negative
(One assumes the answer would be one of: "use the instance's IAM Role" or "use the following key and secret" or "we already said you can't do that.")
  Negative
563e30e361a80130652673db	X	Can you connect to S3 via s3cmd or mount S3 to and ec2 instance with IAM users and not using access keys?
  Negative
All the tutorials I see say to use access keys but what if you can't create your own access keys (IT policy).
  Neutral
563e30e361a80130652673dc	X	There are two ways to access data in Amazon S3: Via an API, or via URLs.
  Negative
Via an API When accessing Amazon S3 via API (which includes code using an AWS SDK and also the AWS Command-Line Interface (CLI)), user credentials must be provided in the form of an Access Key and a Secret Key.
  Negative
The aws and s3cmd utilities, and also software that mounts Amazon S3 as a drive, require access to the API and therefore require credentials.
  Negative
If you have been given a login to an AWS account, you should be able to ask your administrators to also create credentials that are associated with your User.
  Positive
These credentials will have exactly the same permissions as your normal login (via Username/password), so it's strange that they would be disallowing it.
  Negative
They can be very useful for automating AWS activities, such as starting/stopping Amazon EC2 instances.
  Negative
Via URLs Objects stored in Amazon S3 can also be made available via a URL that points directly to the data, eg s3.amazonaws.com/bucket-name/object.
  Negative
txt To provide public access to these objects without requiring credentials, either add permission to each object or create a Bucket Policy that grants access to content within the bucket.
  Negative
This access method can be used to retrieve individual objects, but is not sufficient to mount Amazon S3 as a drive.
  Negative
563e30e361a80130652673dd	X	I'm trying to figure out how to upload data to an Amazon S3 bucket via a RESTful API that I'm writing in Node.js/Restify.
  Negative
I think I've got the basic concepts all working, but when I go to connect to the body of my POST request, that's when things go awry.
  Negative
When I set up my callback function to simply pass a string to S3, it works just fine and the file is created in the appropriate S3 bucket: Obviously, I need to eventually stream/pipe my request from the body of the request.
  Positive
I assumed all that I needed to do would be to simply switch the body of the params to the request stream: But that ends up causing the following log message to be displayed: "Error uploading data: [TypeError: path must be a string]" which gives me very little indication of what I need to do to fix the error.
  Negative
Ultimately, I want to be able to pipe the result since the data being sent could be quite large (I'm not sure if the previous examples are causing the body to be stored in memory), so I thought that something like this might work: Since I've done something similar in a GET function that works just fine:(s3.client.getObject(params).
  Negative
createReadStream().
  Neutral
pipe(res);).
  Negative
But that also did not work.
  Negative
I'm at a bit of a loss at this point so any guidance would be greatly appreciated!
  Negative
563e30e361a80130652673de	X	So, I finally discovered the answer after posting on the AWS Developer Forums.
  Negative
It turns out that the Content-Length header was missing from my S3 requests.
  Negative
Loren@AWS summed it up very well: In order to upload any object to S3, you need to provide a Content-Length.
  Positive
Typically, the SDK can infer the contents from Buffer and String data (or any object with a .
  Negative
length property), and we have special detections for file streams to get file length.
  Negative
Unfortunately, there's no way the SDK can figure out the length of an arbitrary stream, so if you pass something like an HTTP stream, you will need to manually provide the content length yourself.
  Very negative
The suggested solution was to simply pass the content length from the headers of the http.IncomingMessage object: If anyone is interested in reading the entire thread, you can access it here.
  Negative
563e30e461a80130652673df	X	Thank you, but the second link is for google drive, and I need to use Google Cloud Storage
563e30e461a80130652673e0	X	I'm developing an application that lets my users upload files, and I've made it works with "local" disk using filesystem feature, but now I want to migrate and use google Google Cloud Storage for it.
  Negative
It has been a lot difficult to find some useful information.
  Negative
In the docs there is an "example" for work with dropbox, but it's not complete.
  Negative
It doesn't shows how to config drivers and disk, so it isn't clear enough for me.
  Negative
I would like to know what to do, since I have no idea from now.
  Negative
I've just used this tutorial http://www.codetutorial.io/laravel-5-file-upload-storage-download/ and it's working for local, so I really need to migrate to google cloud storage, and only that.
  Negative
I'm using openshift and I feel comfortable about it.
  Positive
Could you please help me to know what should I configure filesystem to be used as I need?
  Negative
Thank you
563e30e461a80130652673e1	X	I've find a solution.
  Positive
It seems that Google Cloud Storage uses the same api than Amazon S3, so I could use it as I would use amazon, the same driver.
  Very negative
The only thing I needed to change was when I config disks in laravel, using this code in config/filesystems when adding a disk for google:
563e30e461a80130652673e2	X	I didn't find solutions\adapters like here This is what i found but it's for google drive.
  Negative
563e30e461a80130652673e3	X	The files are custom binary files.
  Negative
I added the crossdomain file to the question.
  Positive
563e30e461a80130652673e4	X	What about the policy log -- did it provide any additional information?
  Negative
563e30e461a80130652673e5	X	Also, when you try and load myowndomain.com/filename in your browser, outside of Flash ... do you get a certificate error or is the certificate valid and the request succeeds?
  Negative
563e30e461a80130652673e6	X	Your crossdomain.xml looks OK on the surface.
  Neutral
Have you witnessed your browser make a successful request for your crossdomain.xml?
  Neutral
Alternatively, are you explicitly loading the policy file in advance, from its https location?
  Negative
A tool like ieHttpHeaders (for IE) or HttpFox (for Firefox) to inspect is often helpful.
  Negative
563e30e461a80130652673e7	X	And don't forget to look at the policy log!
  Negative
563e30e461a80130652673e8	X	I am trying to access files in Amazon S3 bucket with SSL with ActionScript3.
  Negative
When I use this format... I get security sandbox error.
  Negative
"Error #2048: Security sandbox violation: " When I switch to this format... It works like a charm (until I try it on a browser other than Firefox).
  Positive
It generates a certificate error (host name mismatch) for the other browsers.
  Negative
Once I add exceptions it works fine.
  Positive
But that's not practical.
  Negative
Third option which would be the ideal version... ... generates the same security violation for all browsers.
  Negative
Needless to say, the domain is mapped to the bucket.
  Positive
The bucket has its own crossdomain.xml.
  Neutral
The files are custom binary files.
  Negative
I went thru the security white paper and new rules for Flash Player 10.
  Negative
No luck so far.
  Negative
Any ideas?
  Neutral
Ok it gets more interesting, and I suspect this is causing the problem.
  Positive
While sanitizing the name of my bucket, I oversimplified.
  Negative
My bucket name has a dot in it and appearently it is not a good thing.
  Negative
http://faindu.wordpress.com/2008/12/18/amazon-s3-flash-crossdomainxml-ie7-certifacte-error/ So I would appreciate it, if there is an alternative to that.
  Negative
563e30e461a80130652673e9	X	This is due to browser restrictions.
  Negative
Also, if you trying to access S3 from AS3 then you'll probably fine the AS3 API quite useful though this too runs into browser restrictions: This is an AS3 library for accessing Amazon's S3 service.
  Negative
It only works in Apollo because of restrictions in the browser player.
  Neutral
563e30e461a80130652673ea	X	During your troubleshooting, did you enable the Flash Player's policy file logging feature?
  Negative
You can get more specific information behind the sandbox violation error.
  Negative
Read the following to learn how to set up policy file logging: Policy file changes in Flash Player 9 and Flash Player 10 Personally, I suspect you should be able to get your third option to work, at least, since you'd be able to host a crossdomain.xml at the root location of https://www.myowndomain.com/crossdomain.xml -- but let's see what you have in your crossdomain.xml.
  Negative
I suggest you post a copy here, sanitized if necessary.
  Negative
And, tell us, what kind of files are you trying to load in the player?
  Negative
563e30e461a80130652673eb	X	thanks for the suggestion...I like to use the php sdk but not sure how to, are you saying that I don't need to include the factory method for declare my IAM credentials?
  Negative
Is it possible to make a variable for the bucket url and use it like: my_s3_bucket='http://link/to/my/s3 and then $thisFu['original_img']='my_s3_bucket/uploads/fufu/'.
  Negative
$_POST['cat'].'
  Neutral
/original_'‌​.
  Neutral
uniqid('fu_').'
  Neutral
.
  Neutral
jpg'; ?
  Neutral
563e30e461a80130652673ec	X	Sorry for delay but I don't receive notification about new update.
  Negative
Wow, thank you for your great and detailed explanation, that seems so easy...instead days ago I tried with s3fs and now my upload folder is hidden and not accessible in any way (you can read my 4th update above).
  Positive
I wish to try this method but until I have the problem with my upload folder I cannot even try.
  Negative
563e30e461a80130652673ed	X	I think if I just make edit of post then notification doesn't come...I was glad that info was usefull for you :) about you problem at update 4: I'm little bit lost what exactly your problem?
  Very negative
Disable s3fs?
  Neutral
Why you can't just come to source folder and make: "sudo make uninstall"?
  Negative
btw I see you mentioned some permissions issue, did you make commands from root or sudo?
  Negative
563e30e561a80130652673ee	X	did you reboot after uninstall?
  Negative
563e30e561a80130652673ef	X	apache knows nothing about s3fs, reboot your instance
563e30e561a80130652673f0	X	Hi, after the connection, how could I automatically upload to the bucket pictures that users upload?
  Negative
My code is like this: $this['original_img'] = '/uploads/img/'.
  Negative
$_POST['cat'].'
  Neutral
/original_'.
  Neutral
uniqid('my_').'
  Negative
.
  Neutral
jpg'; Thanks
563e30e561a80130652673f1	X	@Simone Have you seen the "uploading a file" section on that page ?
  Negative
563e30e561a80130652673f2	X	@Simone What do you mean by constant to use for the url ?
  Negative
You can specify options to the function to make a file public or private.
  Neutral
563e30e561a80130652673f3	X	As I wrote, I'm a beginner, I don't know what's the correct path to upload images into s3...what should I code before /upload/img/ ?
  Negative
563e30e561a80130652673f4	X	Hi, I changed my question, could you please change your vote?
  Negative
Thanks
563e30e561a80130652673f5	X	Finally someone!
  Neutral
Thanks for reply, however I cannot install anything like that, indeed I'm on Amazon EC2...is it that difficult manually?
  Negative
I thought was enough take all folders and place in root :)
563e30e561a80130652673f6	X	In the first link for installing the SDK, go to the Install from zip instructions and instead of the line use Aws\S3\S3Client; use require '/path/to/aws-autoloader.
  Negative
php';
563e30e561a80130652673f7	X	That is clear but where I supposed to place into my project?
  Negative
I have my own framework, do I place this inside root folder?
  Negative
or admin folder perhaps?
  Neutral
What's the difference if any
563e30e561a80130652673f8	X	Place it whereever you would like inside your directory with the PHP code you use!
  Neutral
563e30e561a80130652673f9	X	:D ok that's great...I asked only for security reasons
563e30e561a80130652673fa	X	Thanks, so basically using this way I cannot use Cloudfront as CDN?
  Negative
I activated Cloudfront and for origin I have placed my bucket
563e30e561a80130652673fb	X	If the only change you make is using s3fs then yes, You can write the image to s3 with s3fs and then use the Cloudfront links with the API.
  Negative
But would need code changes
563e30e561a80130652673fc	X	I updated my question, could you please check it?
  Positive
563e30e561a80130652673fd	X	are there files in the dir already ?
  Negative
try mv /uploads/fufu /uploads/fufu.
  Negative
old; mkdir /uploads/fufu; then try the s3fs command again
563e30e561a80130652673fe	X	the non empty command probably didnt help.
  Negative
try unmounting the s3fs (fusermount -u /uploads/fufu) confirm its no longer mounted (doesnt appear in output of df), make sure nothing exists in folder.
  Negative
(ls -la) is there is nothing there do the s3fs command again.
  Negative
Everything should work after that.
  Neutral
Ived edited my answer.
  Positive
with an empty dir you shouldnt need the non empty option though
563e30e561a80130652673ff	X	I'm on an EC2 instance and I wish to connect my PHP website with my Amazon S3 bucket, I already saw the API for PHP here: http://aws.amazon.com/sdkforphp/ but it's not clear.
  Negative
This is the code line I need to edit in my controller: I need to connect to Amazon S3 and be able to change the code like this: I already configured an IAM user for the purpose but I don't know all the steps needed to accomplished the job.
  Negative
How could I connect and interact with Amazon S3 to upload and retrieve public images?
  Neutral
UPDATE I decided to try using the s3fs as suggested, so I installed it as described here (my OS is Ubuntu 14.04) I run from console: Everything was properly installed but what's next?
  Negative
Where should I declare credentials and how could I use this integration in my project?
  Negative
2nd UPDATE I created a file called .
  Positive
passwd-s3fs with a single code line with my IAM credentials accessKeyId:secretAccessKey.
  Negative
I place it into my home/ubuntu directory and give it a 600 permission with chmod 600 ~/.
  Negative
passwd-s3fs Next from console I run /usr/bin/s3fs My_S3bucket /uploads/fufu Inside the /uploads/fufu there are all my bucket folders now.
  Negative
However when I try this command: I get this error message: 3rd UPDATE As suggested I run this fusermount -u /uploads/fufu, after that I checked the fufu folder and is empty as expected.
  Negative
After that I tried again this command (with one more -o): and got this error message: Any other suggestion?
  Negative
4th UPDATE 18/04/15 Under suggestion from console I run sudo usermod -a -G fuse ubuntu and sudo vim /etc/fuse.
  Negative
conf where I uncommented mount_max = 1000 and user_allow_other Than I run s3fs -o nonempty -o allow_other My_S3bucket /uploads/fufu At first sight no errors, so I thought everythings fine but it's exactly the opposite.
  Negative
I'm a bit frustrated now, because I don't know what happened but my folder /uploads/fufu is hidden and using ls -Al I see only this I cannot sudo rm -r or -rf or mv -r it says that /uploads/fufu is a directory I tried to reboot exit and mount -a, but nothing.
  Very negative
I tried to unmount using fusermount and the error message is fusermount: entry for /uploads/fufu not found in /etc/mtab But I tried sudo vim /etc/mtab and I found this line: s3fs /uploads/fufu fuse.s3fs rw,nosuid,nodev,allow_other 0 0 Could someone tell me how can I unmount and finally remove this folder /uploads/fufu ?
  Very negative
563e30e561a8013065267400	X	Despite to "S3fs is very reliable in recent builds", I can share my own experience with s3fs and info that we moved write operation from direct s3fs mounted folder access to aws console(SDK api possible way also) after periodic randomly system crashes .
  Positive
Possible that you won't have any problem with small size files like images, but it certainly made the problem while we tried to write mp4 files.
  Negative
So last message at log before system crash was: and it was rare randomly cases, but that made system unstable.
  Neutral
So we decided still to keep s3fs mounted, but use it only for read access Below I show how to mount s3fs with AIM credentials without password file Also you will need to create IAM role that assigned to the instance with attached policy: In you case, seems it is reasonable to use php sdk (other answer has usage example already), but you also can write images to s3 with aws console: If you will have IAM role created and assigned to your instance you won't need to provide any additional credentials Update - answer to your question: Yes if you will have IAM assigned to ec2 instance, then at code you just need to create the client as: option 2: If you do not need file local storage stage , but will put direclt from upload form: And to get s3 link if you will have public access Or generate signed url to private content:
563e30e561a8013065267401	X	I agree the documentation at that link is a bit hard to dig and leaves a lot of dots to be connected.
  Negative
However, I found something a lot better here: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/service-s3.html It has sample code and instructions for almost all the S3 operations.
  Neutral
563e30e661a8013065267402	X	To give you a little more clarity, since you are a beginner: download the AWS SDK via this Installation Guide Then set up your AWS account client on your PHP webserver using this bit If you would like more information on how to use AWS credentials files, head here.
  Negative
Then to upload a file that you have on your own PHP server: If you are interested in learning how to upload images to a php file, I would recommend looking at this W3 schools tutorial.
  Negative
This tutorial can help you get off the ground for saving the file locally on your server in a temporary directory before it gets uploaded to your S3 bucket.
  Negative
563e30e661a8013065267403	X	A much easier and transparent to your application setup is simply to mount the s3 partition with s3fs https://github.com/s3fs-fuse/s3fs-fuse (Use option allow_other) Your s3fs then behaves like a normal folder would just move the file to that folder.
  Negative
S3fs then uploads S3fs is very reliable in recent builds You can read images this way also, But loose any effect that the AWS CDN has, though last time i tried it it wasn't a huge difference you need a s3fspassword file in the format accessKeyId:secretAccessKey it can be in either of these places The file needs 600 permissions https://github.com/s3fs-fuse/s3fs-fuse/wiki/Fuse-Over-Amazon Has some info.
  Negative
When that is complete the command is s3fs bucket_name mnt_dir you can find the keys here https://console.aws.amazon.com/iam/home?#security_credential from the example about i would assume your mnt_dir would be /uploads/fufu so s3fs bucket /uploads/fufu to your second problem is wrong, you need to specify -o again the user you are mounting as needs to be in the fuse group or sudo addgroup your_user fuse
563e30e661a8013065267404	X	Thanks!
  Very negative
this looks promising.
  Positive
.
  Neutral
any idea if i begin using Cloudfront after S3?
  Neutral
563e30e661a8013065267405	X	Don't want to speak to that as I've never used Cloudfront.
  Negative
Sorry!
  Positive
563e30e661a8013065267406	X	I would like to allow users of my website see a special "Image/Video/HTML" but only if they login through facebook connect on my Rails site... I would like to use Amazon S3 to store the media.
  Negative
.
  Neutral
My question is how do I give limited access to my users only if they are logged in?
  Neutral
Once they have the URL of the "Image/Video/HTML" I would not like them to be able to access it unless they are logged in with facebook on my site.
  Negative
.
  Neutral
Also, will I be able to continue this kind of private site if I want to use Amazon Cloudfront?
  Neutral
This is a starting point for whitelisting only my domain... but I want to make sure the user is logged in through fb connect in order to serve them the appropriate resource... https://gist.github.com/3716433
563e30e661a8013065267407	X	I think this depends on how you're including S3 assets in your app.
  Negative
Here are instructions if you're using paperclip: https://github.com/thoughtbot/paperclip/wiki/Restricting-Access-to-Objects-Stored-on-Amazon-S3 Here's another question that deals with this issue using the aws-sdk gem: How to store data in S3 and allow user access in a secure way with rails API / iOS client?
  Negative
563e30e661a8013065267408	X	What happens if you reverse the order of these two statements?
  Negative
Does $tbfile get uploaded and $file not?
  Negative
Have you verified that $tbfile and $tbdirectoryandfilename contain what you think they do, and the file you're trying to upload exists?
  Negative
563e30e661a8013065267409	X	I flip them around and it's now working.
  Neutral
How odd.
  Neutral
Thanks for the suggestion.
  Neutral
563e30e661a801306526740a	X	I am executing this function which uploads a file to my Amazon S3 bucket for two separate files so I have the line of code in there twice but looking at two different files: But only the first line will ever run.
  Negative
Where am I going wrong?
  Negative
The S3 PHP class documentation and code is at http://undesigned.org.za/2007/10/22/amazon-s3-php-class
563e30e761a801306526740b	X	I've never used any Amazon API, but after a quick look from their documentation, the putObjectFile() method is a legacy method, and may no longer be properly supported.
  Negative
Instead, you should consider using the inputFile() method.
  Neutral
See http://undesigned.org.za/2007/10/22/amazon-s3-php-class/documentation#inputFile
563e30e761a801306526740c	X	ok so Picture.objects.get(self.get_object()).
  Negative
file.delete() it's correct?
  Neutral
563e30e761a801306526740d	X	I imagine that you are using class based views.
  Positive
If so, then self.get_object() returns an object (Picture) in question.
  Negative
So to delete the file, then you can do self.get_object().
  Negative
file.delete().
  Neutral
563e30e761a801306526740e	X	ok sry but I want to delete in database reference and in the folder
563e30e761a801306526740f	X	What do you mean by that?
  Negative
Do you want to remove the Picture instance which will also delete the file associated with that picture?
  Negative
563e30e761a8013065267410	X	i want to delete the file ( HDD file) and the object in the database
563e30e761a8013065267411	X	I would like to delete file associated to a file-field But it doesn't work.
  Very negative
Can you fix it please?
  Neutral
563e30e761a8013065267412	X	You can remove file objects using the FileField api: That will use storage API to remove the file.
  Negative
The advantage of that is that this approach works even if you want to switch your storage to different system such as Amazon S3.
  Negative
563e30e761a8013065267413	X	My team wants to store the blobs for our BOSH release in a remote blobstore.
  Negative
However we have an internal CEPH / Rados store that we want to use.
  Neutral
I know that Rados has S3 compatible interfaces so I was wondering how I could enable this as the final blobstore.
  Negative
I know that typically I'd only need to give the access key, secrete key, and bucket to BOSH.
  Negative
But now I also have an IP host (and probably in the future a url) that specifies where the bucket exists.
  Negative
Is there currently a way to set this up?
  Neutral
563e30e761a8013065267414	X	As an example on how BOSH can interact with a CEPH / Rados store: Storing packages in an internal CEPH / Rados store This requires configuring the release to know about the intended blobstore.
  Negative
Keep in mind that when you run bosh upload blobs the command is parsed by the BOSH CLI rather than a BOSH or MicroBOSH VM if you have happened to target one.
  Negative
Assume that you have a store at address IP_ADDRESS:PORT with an existent bucket named BUCKET and keys ACCESS_KEY and SECRET_ACCESS_KEY.
  Negative
The config/final.
  Neutral
yml file should look like: But the config/private.
  Negative
yml file should look like: Depending on how the CEPH store is set up, it may be necessary to turn off ssl verification as well which would include adding ssl_verify_peer: false under the s3_port option (i.e. the nesting would be blobstore -> s3 -> ssl_verify_peer.
  Negative
At this point calling bosh upload blobs will work as expected.
  Negative
Telling a BOSH VM about the CEPH / Rados store The deployment manifest file needs to have the items in the private.yml and the bucket name in the blobstore properties section.
  Negative
EDIT Aug 20, 2015 According to the CEPH website http://ceph.com/docs/master/radosgw/s3/#api there is currently no support for Bucket policies.
  Negative
Therefore the fact that a bucket is readable will not be inherited by a bucket's objects.
  Negative
This means that if a CEPH store is used in place of the official Amazon S3 for storage, the config/private.
  Negative
yml file is mandatory.
  Negative
563e30e861a8013065267415	X	I use Carrierwave to upload images straight to Amazon S3.
  Negative
I have a JSON API for the upload where I encode the image as Base64 in the client and send it.
  Negative
I followed this tutorial blog post to do it.
  Negative
Image upload fails with this message on the console: I see that the value it is trying to insert into the DB for image column is not a string by an object.
  Very negative
When I upload via the browser(without using API), the value inserted in image column is just the file name.
  Negative
Where am I going wrong here ?
  Negative
Here is my controller: Here is my model.
  Negative
I have mounted ImageUploader on image: The JSON I am sending is: The above JSON was sent from a hand crafted test script:
563e30e861a8013065267416	X	I finally got the photo upload via API to work.
  Positive
In the create() function instead of I changed it to Here is the complete code:
563e30e861a8013065267417	X	How exactly do you want to authenticate?
  Negative
Are you going to send a key with each request?
  Neutral
563e30e961a8013065267418	X	I'm trying to find information on securing a HTTP REST API in a Symfony project, but all I can find is information about using sfGuardPlugin.
  Negative
From what I can see, this plugin isn't very useful for web services.
  Negative
It tries to have user profile models (which aren't always that simple) and have "sign in" and "sign out" pages, which obviously are pointless for a stateless REST API.
  Negative
It does a lot more than I'll ever have need for and I what to keep it simple.
  Positive
I want to know where to implement my own authorisation method (loosely based on Amazon S3's approach).
  Negative
I know how I want the authorisation method to actually work, I just don't know where I can put code in my Symfony app so that it runs before every request is processed, and lets approved requests continue but unsuccessful requests return a 403.
  Negative
Any ideas?
  Neutral
I can't imagine this is hard, I just don't know where to start looking.
  Negative
563e30e961a8013065267419	X	There is a plugin for RESTful authentication -> http://www.symfony-project.org/plugins/sfRestfulAuthenticationPlugin Not used it though .... How where you planning to authenticate users ?
  Negative
The jobeet tutorial uses tokens ... http://www.symfony-project.org/jobeet/1_4/Doctrine/en/15
563e30e961a801306526741a	X	I ended up finding what I was looking for by digging into the code for sfHttpAuthPlugin.
  Negative
What I was looking for was a "Filter".
  Neutral
Some details and an example is described in the Askeet sample project.
  Negative
563e30e961a801306526741b	X	Stick a HTTP basicAuth script in your <appname>_dev.php (Symfony 1.4 =<) between the project configuration "require" and the configuration instance creation.
  Negative
Test it on your dev.
  Positive
If it works, put the code in your index.php (the live equivalent of <appname>_dev.php) and push it live.
  Negative
Quick and dirty but it works.
  Positive
You may want to protect that username/password in the script though.
  Negative
e.g.
563e30ea61a801306526741c	X	Instead: Consider lambda.
  Negative
563e30eb61a801306526741d	X	In short: Although it is legal to do so, it usually doesn't make sense to use the comma operator in the condition part of an if or while statement (EDIT: Although the latter might sometimes be helpful as user5534870 explains in his answer).
  Negative
A more elaborate explanation: Aside from its syntactic function (e.g. separating elements in initializer lists, variable declarations or function calls/declarations), in C and C++, the , can also be a normal operator just like e.g. +, and so it can be used everywhere, where an expression is allowed (in C++ you can even overload it).
  Very negative
The difference to most other operators is that - although both sides get evaluated - it doesn't combine the outputs of the left and right expressions in any way, but just returns the right one.
  Positive
It was introduced, because someone (probably Dennis Ritchie) decided for some reason that C required a syntax to write two (or more) unrelated expressions at a position, where you ordinarily only could write a single expression.
  Negative
Now, the condition of an if statement is (among others) such a place and consequently, you can also use the , operator there - whether it makes sense to do so or not is an entirely different question!
  Negative
In particular - and different from e.g. function calls or variable declarations - the comma has no special meaning there, so it does, what it always does: It evaluates the expressions to the left and right, but only returns the result of the right one, which is then used by the if statement.
  Negative
The only two points I can think of right now, where using the (non-overloaded) ,-operator makes sense are: If you want to increment multiple iterators in the head of a for loop: If you want to evaluate more than one expression in a C++11 constexpr function.
  Negative
To repeat this once more: Using the comma operator in an if or while statement - in the way you showed it in your example - isn't something sensible to do.
  Negative
It is just another example where the language syntaxes of C and C++ allow you to write code, that doesn't behave the way that one - on first glance - would expect it to.
  Negative
There are many more....
563e30eb61a801306526741e	X	My server is hosting an application which preloads >100 thumbnail images on each page load.
  Negative
The thumbnail images do not change often.
  Negative
I am trying to make consecutive thumbnail loads faster by using Cache-Control: public,max-age=MANY_SECONDS where MANY_SECONDS is up to a year.
  Negative
The thumbnails are requested via a Flask endpoint that looks like this: I set the Cache-Control header to public,max-age=MANY_SECONDS for all the *-thumb.png keys, still Firefox fires a request against /api/thumbnail/... and gets the 301, then a 304 from Amazon S3.
  Very negative
I'm under the impression that 301 responses are cached seemingly for an eternity, and the Cache-Control headers for the thumbnail files served from Amazon S3 should allow Firefox to cache the thumbnail files locally for up to a year.
  Negative
All these thumbs × 2 requests are really an overhead.
  Negative
I want them cached for an eternity.
  Neutral
563e30eb61a801306526741f	X	My solution was to use a HTML5 manifest file.
  Negative
563e30eb61a8013065267420	X	This part of the S3 dev guide hints that it is possible, but doesn't explain where to get the components of the URL docs.amazonwebservices.com/AmazonS3/2006-03-01/dev/…
563e30eb61a8013065267421	X	Thanks for your help!
  Negative
I was hoping to do it without writing a little app for it.
  Negative
I ended up finding an online one, but in the end, decided to use CloudFTP as an FTP endpoint to my S3 bucket.
  Negative
See: stackoverflow.com/questions/1855109/amazon-s3-ftp-interface
563e30eb61a8013065267422	X	I am attempting to use an S3 bucket as a deployment location for an internal, auto-updating application's files.
  Negative
It would be the location where the new version's files are dumped for the application to puck up on an update.
  Negative
Since this is an internal application, I was hoping to have the URL be private, but to be able to access it using only a URL.
  Negative
I was hoping to look into using third party auto updating software, which means I can't use the Amazon API to access it.
  Negative
Does anyone know a way to get a URL to a private bucket on S3?
  Neutral
563e30ec61a8013065267423	X	You probably want to use one of the available AWS Software Development Kits (SDKs), which all implement the respective methods to generate these URLs by means of the GetPreSignedURL() method (e.g. Java: generatePresignedUrl(), C#: GetPreSignedURL()): The GetPreSignedURL operations creates a signed http request.
  Very negative
Query string authentication is useful for giving HTTP or browser access to resources that would normally require authentication.
  Negative
When using query string authentication, you create a query, specify an expiration time for the query, sign it with your signature, place the data in an HTTP request, and distribute the request to a user or embed the request in a web page.
  Negative
A PreSigned URL can be generated for GET, PUT and HEAD operations on your bucket, keys, and versions.
  Negative
There are a couple of related questions already and e.g. Why is my S3 pre-signed request invalid when I set a response header override that contains a “+”?
  Negative
contains a working sample in C# (aside from the content type issue Ragesh is experiencing of course).
  Negative
Good luck!
  Positive
563e30ec61a8013065267424	X	Thanks!
  Positive
That looks really cool.
  Positive
How does it scale when having a lot of traffic and users?
  Neutral
Are there fees involved?
  Negative
563e30ec61a8013065267425	X	Sorry, I see there billing service.
  Positive
I was wondering if the user has to log in with there service or something?
  Neutral
563e30ec61a8013065267426	X	It's free until 1 000 000 request per month, you can create free account easily.
  Positive
563e30ec61a8013065267427	X	Does the user have to log in with a Google account?
  Negative
563e30ec61a8013065267428	X	Nope.
  Neutral
You need to make a google apps account yourself (you as a developer) and setup the cloud service with the servlet and code which will handle the requests sent by the mobile devices.
  Negative
The users themselves do not have to log in with any accounts unless you make use of google's authentication tokens for part of the information exchange.
  Negative
563e30ec61a8013065267429	X	Out of your point of view, what do you believe is the better choice to go with: Google App Engine or parse.com?
  Negative
563e30ec61a801306526742a	X	I have never used parse.com, but you can get more information about Google App Engine's billing here: developers.google.com/appengine/docs/billing I personally liked the flexibility and detail that Google App Engine would let you go into and I would pick it again over another alternative.
  Negative
It has got great tools for managing your application.
  Positive
I have looked at parse.com website briefly and it seems that they have APIs which might let you develop faster, but they focus in cloud storage alone, while Google App Engine gives you more freedom in implementing logic along with that storage.
  Negative
563e30ec61a801306526742b	X	I was wondering if there are any free options to store and retrieve data from the cloud with Android.
  Negative
563e30ec61a801306526742c	X	I use parse.com for object cloud storage, and it's really simple to use, see http://www.parse.com
563e30ed61a801306526742d	X	You can use Google App Engine.
  Very negative
It is free up to a certain limit.
  Neutral
After that threshold you have to pay if you want additional storage and/or traffic.
  Neutral
563e30ed61a801306526742e	X	if you are trying to only store and retrieve data from cloud, you could use Google App engine which is pretty easy to use.
  Negative
It has APIs in Java and python If you want more control over how you store the data, you should create an account with Amazon Web services and try using S3.
  Negative
Overall, Amazon S3 will also turn out to be cheaper than Google App Engine.
  Neutral
563e30ed61a801306526742f	X	If I am not missing something this appears to be a duplicate of/ highly related to stackoverflow.com/questions/13074791/…
563e30ed61a8013065267430	X	I have seen that code snippet as well an do not understand it either.
  Negative
I do not see where the file is being obtained from.
  Negative
It is not as cut and dry as the the MVC Request.Files[0] so because of that I am lost as to how I could implement even that snippet to my current project.
  Negative
563e30ed61a8013065267431	X	I am using an third party uploader called Plupload and I wish to pass a selected image to a WebAPI method but I just cant seem to figure out how.
  Negative
on the front end I Post a file object on the back end I receive the file object but I don't know what to do with it... there is a MVC code snippet out there that I tried to use but apparently Web API does not support Request.Files[0] so I am lost.
  Negative
this is the snippet my method looks like this.
  Neutral
.
  Neutral
its in flux right now.
  Positive
to sum it up.
  Neutral
I am passing the file object to my method and trying to push it up to amazon s3 the stated MVC method work as it would seem but the same does not apply for Web API.
  Negative
How might I do this?
  Neutral
563e30ee61a8013065267432	X	Thanks - this is sort of what I suspected, but I'd love to find a solution that requires less server maintenance.
  Negative
So I suppose using an API such as Truevault, my main application would need to run on a local server
563e30ee61a8013065267433	X	@DaveTsay Even if you use Truevault to store the more sensitive data, you will likely need to have a database running locally for other parts of your application.
  Very negative
563e30ee61a8013065267434	X	@DaveTsay, check out www.atlashealth.com, specifically the Managed Cloud offering (I'm on the management team and founder).
  Negative
You'll get a fully managed AWS environment which we set up and maintain, so that all you have to worry about are deployments.
  Negative
The cost of the service is $199 per month (plus AWS usage fees, excluding the $2 dedicated fee surcharge, which we cover).
  Negative
We sign BAA's too.
  Negative
563e30ee61a8013065267435	X	We also offer an API for storage and other data needs but it's better suited to specific use cases, such as mobile apps.
  Negative
In the case of a Web application, PHI will likely flow through the entire stack.
  Negative
Therefore, even if you're passing off the data to an API, the service handling the post-back (e.g. Heroku) would be considered a HIPAA subcontractor per the recent "omnibus" rule and would need to sign a BAA.
  Negative
563e30ee61a8013065267436	X	are you still in touch with the founders?
  Neutral
563e30ee61a8013065267437	X	i am.
  Negative
why do you ask?
  Neutral
563e30ee61a8013065267438	X	we're interested in using their service.
  Neutral
can you hook me up with a phone call or demo?
  Negative
563e30ee61a8013065267439	X	contact me and i'll forward along.
  Negative
you can find me on linkedin
563e30ef61a801306526743a	X	There's a recent startup out of YC which seems interesting called Truevault.com, which allows you to store JSON documents in their database via an API and is HIPAA compliant.
  Negative
I am working on a healthcare app, and am wondering which is a better strategy in terms of HIPAA compliance: 1) Heroku + Truevault - easier deployment initially but Heroku won't sign Business Associate Agreement, so not sure if this is truly HIPAA complicant even if I don't store PHI on the heroku server or temporarily store it there.
  Negative
2) Run everything on Amazon EC2 - Amazon will sign BAA so no issue here, but will have to do server maintenance myself (rather not) 3) Heroku + Amazon S3 database - run server on Heroku but store everything on S3, Amazon to sign BAA Anyone with experience what would be most compliant yet practical?
  Negative
Thanks in advance.
  Neutral
563e30ef61a801306526743b	X	Without knowing specifics about how your application works, its likely that you will have to run all of your application on EC2 and other amazon web services.
  Negative
Heroku nodes are basically EC2 instances with a bit of automation on top, turning it more of a platform than infrastructure.
  Negative
However, if you are working in a field that requires legal compliance on how your data is handled, not having full control may be a bad thing.
  Neutral
You can do much of the automation heroku does with tools like Chef and Puppet.
  Negative
Also, if you do use EC2, make sure your infrastructure is configured in VPC is the way to go.
  Negative
Ads a bit of extra work, but gives you more control over network access to different instances.
  Positive
S3 is not really a database, its an object store.
  Negative
Its basically a key/value store with keys that look like file paths.
  Negative
And it can store some very very large values.
  Negative
563e30ef61a801306526743c	X	Aptible are working on a platform to do exactly that, i.e. automating HIPAA compliance where possible, training you on the stuff you need to do yourself, and letting you build systems on standard databases and ecosystems.
  Negative
They're in private beta at the moment.
  Neutral
Disclaimer: I'm not associated with them, but I did meet the founders today and they're an approachable, clever bunch.
  Negative
563e30ef61a801306526743d	X	So basically your iPhone developer is wrong...
563e30ef61a801306526743e	X	but remember that this adds crypto code to your application and may result in problems during the approval process
563e30ef61a801306526743f	X	This may be a simple question... To convert the *out would you use something like [[NSString alloc] initWithData:out encoding:NSUTF8StringEncoding];?
  Negative
563e30ef61a8013065267440	X	just like that.
  Neutral
you can see github.com/soundcloud/cocoa-api-wrapper/blob/… for a real life example.
  Positive
563e30ef61a8013065267441	X	SHA-1 is not the same thing as HMAC-SHA-1.
  Negative
563e30ef61a8013065267442	X	Implicit conversion loses integer precision (NSUInteger to CC_LONG)
563e30f061a8013065267443	X	Hi!
  Negative
And did you work with libs3?
  Neutral
563e30f061a8013065267444	X	For all operation with Amazon services(S3, EC2, SimpleDB) You need to sign all resquest with HMAC-SHA-1 Signature(http://en.wikipedia.org/wiki/HMAC , http://docs.amazonwebservices.com/AWSFWS/latest/DeveloperGuide/index.html?SummaryOfAuthentication.html).
  Negative
I'm working under asp.net backend and there is no problems.
  Negative
Problem is in the iPhone application.
  Negative
iPhone developer says that there is no way to use HMAC-SHA-1 encoding, and he have no rigths to implement his own algorithm.
  Very negative
As programmer I cannot understand why there can be a problem.
  Negative
So I want too know is iPhone developer right?
  Negative
I've never coded for iPhone, so I don't even where to search such an information.
  Negative
563e30f061a8013065267445	X	CommonCrypto will do it.
  Neutral
But if you want code, I have some here: http://oauth.googlecode.com/svn/code/obj-c/OAuthConsumer/Crypto/ Which I wrote for use in the Cocoa OAuth implementation: http://code.google.com/p/oauthconsumer/wiki/UsingOAuthConsumer
563e30f061a8013065267446	X	CommonCrypto does the trick.
  Negative
then later
563e30f061a8013065267447	X	This article demonstrates a little function that will generate an SHA-1 hash digest that will match what the php sha1() function will generate if you give it the same input:
563e30f061a8013065267448	X	A bit of googling and I found this document.
  Negative
Exporting of SHA1 is subject to (United Statese)Federal Government export controls and exporters are advised to contact the Department of Commerce, Bureau of Export Administration for more information.
  Negative
I also found this: People's Republic of China and the former Soviet Block can import SHA as long as it's intended for civil end-user applications rather than for military purpose.
  Negative
The following countries are prohibited from importing SHA: Cuba, Iran, Iraq, Libya, North Korea, Serbia, Syria, and Sudan.
  Negative
Please note that this list of embargo countries changes over time.
  Negative
(Not a direct answer to your question, but certainly pertinent.)
  Positive
563e30f061a8013065267449	X	Not for iPhone in particular, but the library libs3 provides a C API for accessing Amazon's S3 services.
  Negative
It, or the FUSE s3fs component, may be good sources for extracting the routines needed to communicate with Amazon's Web Services.
  Negative
As Objective-C is still C at its core, these routines should work just fine on the iPhone.
  Negative
I know at least one developer who is using something similar within their iPhone application to communicate with S3 buckets.
  Negative
563e30f061a801306526744a	X	I think the CommonCrypto library will do what you want.
  Negative
Look at this file: /Developer/Platforms/iPhoneOS.
  Neutral
platform/Developer/SDKs/iPhoneOS2.2.
  Negative
sdk/usr/include/CommonCrypto/CommonHMAC.
  Neutral
h
563e30f061a801306526744b	X	I don't know if this is the case anymore, but there used to be restrictions on encryption algorithms and your right to distribute them to certain countries were restricted.
  Negative
If this is still the case it could be that Apple don't want/can't restrict certain applications from being downloaded in these countries.
  Negative
563e30f161a801306526744c	X	Can you provide more detail about specifically which error you are receiving?
  Negative
563e30f161a801306526744d	X	I have set up the DNS CNAME but javascript still can't access iframe's height.
  Negative
What can I do?
  Neutral
563e30f161a801306526744e	X	I store my newsletters .
  Negative
html files in S3 and I have created a function that pulls the newsletters and puts them into iFrames.
  Positive
My problem is I can't set an auto height for the iFrames because I can't read the content.
  Negative
I have a plugin that works if the files are on the same domain.
  Neutral
Can I set some headers to allow access from my sub-domain?
  Neutral
563e30f161a801306526744f	X	S3 has a feature called "Virtual Hosting".
  Negative
Virtual Hosting, in general, is the practice of serving multiple web sites from a single web server.
  Negative
One way to differentiate sites is by using the apparent host name of the request instead of just the path name part of the URI.
  Negative
An ordinary Amazon S3 REST request specifies a bucket using the first slash delimited component of the Request-URI path.
  Negative
Alternatively, using Amazon S3 Virtual Hosting, you can address a bucket in a REST API call using the HTTP Host header.
  Negative
In practice, Amazon S3 interprets Host as meaning that most buckets are automatically accessible (for limited types of requests) at http://bucketname.s3.amazonaws.com.
  Negative
Furthermore, by naming your bucket after your registered domain name and by making that name a DNS alias for Amazon S3, you can completely customize the URL of your Amazon S3 resources, for example: http://my.bucketname.com/ Try mapping your S3 bucket to your domain.
  Negative
See here for an example.
  Positive
Accessing S3 Buckets via Virtual Host URLs S3 provides two ways to access your content.
  Positive
One way uses s3.amazonaws.com host name URLs, such as this: http://s3.amazonaws.com/mybucket.mydomain.com/myObjectKey The other way to access your S3 content uses a virtual host name in the URL: http://mybucket.mydomain.com.s3.amazonaws.com/myObjectKey Both of these URLs map to the same object in S3.
  Negative
You can make the virtual host name URL shorter by setting up a DNS CNAME that maps mybucket.mydomain.com to mybucket.mydomain.com.s3.amazonaws.com.
  Negative
With this DNS CNAME alias in place, the above URL can also be written as follows: http://mybucket.mydomain.com/myObjectKey This shorter virtual host name URL works only if you setup the DNS CNAME alias for the bucket.
  Negative
563e30f161a8013065267450	X	You can use this link or you can upload via s3 commandline tool aws.amazon.com/cli
563e30f161a8013065267451	X	I am working on a web application using angular js (for UI) and java (@back end) in which i am creating a user interface through which a user can manipulate data in S3 buckets.
  Negative
I need to upload data to buckets which are of size about 500 mb.
  Negative
I am currently being able to send 5mb chunks of data to servlets but i am not able to combine those chunked objects to upload my original data to S3.
  Negative
Is there any alternative way for achieving this?
  Neutral
563e30f161a8013065267452	X	Take a look at https://github.com/minio/minio-java.
  Neutral
It has minimal set of abstracted API's implementing most commonly used S3 calls.
  Negative
Here is an example of streaming upload.
  Neutral
putObject() here is a fully managed single function call for file sizes over 5MB it automatically does multipart internally and once complete Amazon S3 will present it as a single object.
  Negative
You can resume a failed upload as well and it will start from where its left off by verifying previously upload parts.
  Negative
With this putObject() you can upload upto 5TB single large object (5TB is maximum single object size on Amazon S3)
563e30f161a8013065267453	X	TY, wow, that is pretty cool.
  Positive
563e30f161a8013065267454	X	Thanks, exactly what I was looking for.
  Negative
I would give you more votes if I could :)
563e30f161a8013065267455	X	Beautiful!
  Negative
Just Beautiful!
  Very positive
563e30f161a8013065267456	X	I am pretty confused and lost in all the documentation.
  Negative
What I want to do is store a number of objects online (less than 20 Mb).
  Negative
The app that I am developing should be able to retrieve those objects ( preferable JSON but XML would do) and perhaps upload other such objects.
  Negative
From what I see, the Google Cloud storage is not free.
  Negative
Is there anything that does what I need and is also free?
  Neutral
If not, can you point me to the right documentation to achieve what I want?
  Negative
I have been looking into the topic for a couple of days and I am still lost between buckets, the Cloud JSON API, Cloud Storage, all these being new to me.
  Very negative
Any help would be much appreciated!
  Neutral
563e30f261a8013065267457	X	If you have just 20 MB of data then you may use Parse for storing data upto 1Gb for free.
  Negative
Also see https://parse.com/products/data.
  Positive
I heard from those who used it, that the API is also straight forward.
  Neutral
563e30f261a8013065267458	X	Not sure if this is exactly what you are looking fore, but dropbox provides a great android api: http://www.dropbox.com You can upload/download/modify files pretty easily.
  Negative
However, this would not be the best solution onless you want to give user's the ability to sign into their own dropbox and save/modify files on a per-user basis.
  Neutral
It seems like you might be looking for something more like Amazon s3 storage: http://aws.amazon.com/s3/ .
  Negative
It is a very popular cloud storage provider, and you get 5 gigs free (I think), plus it should be simple to integrate with android.
  Positive
see here amazon S3 and android and here: http://aws.amazon.com/pt/sdkforandroid/
563e30f361a8013065267459	X	What's your question then?
  Negative
"Explain everything I mentioned in the previous four paragraphs?"
  Neutral
563e30f361a801306526745a	X	these are 2 task: uploading form mac to server.
  Negative
upload from server to s3.
  Neutral
both have been answered.
  Neutral
563e30f361a801306526745b	X	that's what I was looking for!
  Neutral
thanks!
  Positive
563e30f361a801306526745c	X	I have asked this question before, but it was deleted due too little information.
  Negative
I'll try to be more concrete this time.
  Negative
I have an Objective-C mac application, which should allow users to upload files to S3-storage.
  Negative
The s3 storage is mine, the users don't have an Amazon account.
  Negative
Until now, the files were uploaded directly to the amazon servers.
  Negative
After thinking some more about it, it wasn't really a great concept, regarding security and flexibility.
  Negative
I want to add a server in between.
  Neutral
The user should authenticate with my server, the server would open a session if the authentication was successful, and the file-sharing could begin.
  Negative
Now my question.
  Neutral
I want to upload the files to S3.
  Negative
One option would be to make a POST-request and wait until the server would receive the file.
  Negative
Problems here are, that there would be a delay, when the file is being uploaded from my server to the S3 servers, and it would double the uploading time.
  Negative
Best would be, if I could validate the request, and then redirecting it, so the client uploads it directly to the s3-storage.
  Neutral
Not sure if this is possible somehow.
  Neutral
Uploading directly to S3 doesn't seem to be very smart.
  Negative
After looking into other apps like Droplr and Dropmark, it looks like they don't do this.
  Negative
Btw.
  Neutral
I did this using Little Snitch.
  Neutral
They have their api on their own web-server, and that's it.
  Positive
Could someone clear things up for me?
  Negative
EDIT How should I transmit my files to S3?
  Negative
Is there a way to "forward" it, or do I have to upload it to my server and then upload it from there to S3?
  Neutral
Like I said, other apps can do this efficiently and without the need of communicating with S3 directly.
  Negative
563e30f361a801306526745d	X	If authentication is managed on your server, but you don't want two uploads, then look into AWS/IAM temporary security credentials.
  Negative
http://aws.amazon.com/releasenotes/Java/5316957573949696 In short:
563e30f361a801306526745e	X	you should provide what you have achieved till now so we can give the correct suggestions.
  Negative
563e30f361a801306526745f	X	Apologies if I didn't seem specific enough... it was more that what I had done already made it so all the images locally cached perfectly, but those elsewhere didn't.
  Negative
Someone below pointed out the obvious point I had missed where I cannot control the cache of another site, so I had to do it within S3 itself.
  Negative
563e30f361a8013065267460	X	Thanks a lot.
  Positive
I know it sounds stupid, but I hadn't thought that I'd have to cache it on the S3 server.
  Very negative
Sounds really obvious now!!
  Neutral
563e30f361a8013065267461	X	The author of the question stated that they're using IIS which uses primarily Web.config file for configuration and probably not .
  Negative
htaccess file (unless using something like Helicon Ape).
  Negative
563e30f361a8013065267462	X	Thanks for the reply, but as Tom said, this project is in the Microsoft world.
  Negative
563e30f361a8013065267463	X	I've been able to correctly (I think) enable caching on IIS.
  Negative
The only problem now is that when I run Google's PageSpeed Insights it still says Setting an expiry date or a maximum age in the HTTP headers for static resources instructs the browser to load previously downloaded resources from local disk rather than over the network.
  Negative
But all of the suggestions are external images.
  Neutral
I am using Amazon's S3 to externally host images (linking to direct URLs, as < img src="http://s3.amazon.com......
  Negative
."
  Neutral
/>.
  Neutral
Is there a way I can "leverage browser caching" for these external images?
  Negative
Thanks in advance.
  Neutral
Andy
563e30f361a8013065267464	X	Yes, with Amazon S3 you can still set the Expires header of the objects stored in the bucket.
  Negative
You will have to set this header when storing the object so there are two ways: If you use the API you can do something like http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html For the second case maybe this link will help: http://www.newvem.com/how-to-add-caching-headers-to-your-objects-using-amazon-s3/ Hope this helps.
  Negative
563e30f461a8013065267465	X	Have you tried: in your .
  Neutral
htaccess file, hope this helps.
  Positive
563e30f461a8013065267466	X	It should be noted that the EC2 AMI tools are only for dealing with the older style instance-store (S3 based) AMIs.
  Negative
The tools for the newer, more popular EBS boot AMIs are in the EC2 API tools (ec2-register, ec2-deregister, ec2-create-image).
  Negative
The documentation from Amazon is not clear on this distiction.
  Negative
563e30f461a8013065267467	X	@EricHammond.
  Neutral
.
  Neutral
Thanks for pointing that out!
  Positive
563e30f461a8013065267468	X	The developer tools page here list two set of tools for Amazon EC2 What are the differences between the two set of tools?
  Negative
563e30f461a8013065267469	X	The API tools serve as the client interface to the Amazon EC2 web service.
  Negative
Use these tools to register and launch instances, manipulate security groups, and more The Amazon EC2 AMI Tools are command-line utilities to help bundle an Amazon Machine Image (AMI), create an AMI from an existing machine or installed volume, and upload a bundled AMI to Amazon S3.
  Negative
From the definitions shown at Developer Tools.
  Negative
So, if you want to manage instances, use API tools; if you want to build and upload an AMI (Amazon Machine Image), use AMI tools.
  Negative
563e30f461a801306526746a	X	Here are the lines from Amazon Documentation: The Amazon EC2 command line interface tools (also called the CLI tools) wrap the Amazon EC2 API actions.
  Negative
These tools are written in Java and include shell scripts for both Windows and Linux/UNIX/Mac OSX.
  Negative
Note Alternatively, you can use the AWS Command Line Interface (AWS CLI), which provides commands for a broad set of AWS products, including Amazon EC2.
  Negative
To get started with the AWS CLI, see the AWS Command Line Interface User Guide.
  Negative
For more information about the AWS CLI commands for Amazon EC2, see ec2 in the AWS Command Line Interface Reference.
  Negative
Before you can use the Amazon EC2 CLI tools, you need to download them and configure them to use your AWS account.
  Negative
You can set up the tools on your own computer or on an Amazon EC2 instance.
  Neutral
http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/set-up-ec2-cli-windows.html
563e30f561a801306526746b	X	I feel that this question is too broad as it is presently worded.
  Negative
You can do your own research to find out the pros and cons of both of these easily enough yourself.
  Neutral
If you have a specific question, that good for StackOverflow.
  Positive
563e30f561a801306526746c	X	The default bucket in Cloud Storage is also free up to 5GBs.
  Negative
And above that, as of now, they both cost the same: 0.026$ / GB / Month.
  Positive
563e30f561a801306526746d	X	reads and writes to the BlobStore cost after a point, CloudStore they don't you just pay for storage over the 5GB limit.
  Negative
It is pretty clear that Google wants to deprecate the Blobstore for the CloudStore and they are making the CloudStore the cheaper option to accomplish this.
  Positive
as well as removing that ridiclous upload url call back requirement for the Blobstore.
  Negative
GCS is plain old PUT/POST to a Servlet or writing to a stream from the server side.
  Negative
563e30f561a801306526746e	X	Actually Blobstore is better when it comes to images , because can easily be served resized images to many apps and can be served only one resized image at a time from Cloud Storage.
  Negative
( cloud.google.com/appengine/docs/java/images/… ) Also the fact that when using Blobstore the request is not going through the server but the image is served directly to the client app.
  Negative
These two reasons are making Blobstore better for me.
  Negative
(also Blobstore now is without size limit)
563e30f561a801306526746f	X	Given that they have "extracted" the App Engine Datastore into a self-contained Cloud Service, it seems clear they are moving towards an AWS-like model of discrete services you can use separately or together.
  Negative
Google Cloud Storage already fills that role and is actively promoted; whereas Blobstore is seemingly redundant.
  Negative
563e30f561a8013065267470	X	Blobstore file size limits were completely removed August 2011 code.google.com/p/googleappengine/issues/detail?id=2560
563e30f561a8013065267471	X	We have our application hosted on Google app engine for Java and we have a requirement where we want to store the blood donor appreciation certificates (html files) somewhere.
  Negative
So, we can use either Google blob store or Google cloud storage.
  Negative
I know both of these solutions are feasible to do using gae for Java However, the problem for us is to decide which one to use.
  Negative
What are the pros and cons of these two approaches?
  Neutral
We are a non profit and cannot pay a lot.
  Negative
563e30f561a8013065267472	X	Better use the Blobstore.
  Negative
Is has free 5 GB space (as of March 2012).
  Negative
The cloud storage is paid service.
  Neutral
The App Engine blobstore is like Amazon S3, but less flexible.
  Negative
It has HTTP-based API and Java / Python APIs (see http://code.google.com/appengine/docs/java/blobstore/overview.html).
  Negative
563e30f561a8013065267473	X	If you're starting a new project, I would go with Cloud Storage.
  Negative
It seems that Google is pushing their Cloud Storage platform harder than their blobstore platform.
  Negative
For example, currently programmatically writing files to the blobstore is deprecated but is supported by Cloud Storage.
  Negative
I can't read the future, but I would bet that Google will be deprecating more and more of the blobstore API in favor of the Cloud Storage API, which will lead to headaches down the road.
  Negative
563e30f561a8013065267474	X	First, I'd say that if your HTML file(s) are small (or could be small via gzip compression), then just store it as a BlobProperty in the datastore and add meta-data properties so you can retrieve it appropriately later.
  Negative
If this is not an option, then perhaps consider the future growth of your application.
  Negative
The two big things Cloud Storage has over the Blobstore are 1) accessibility by third-party and 2) no file size restrictions.
  Negative
If, however, you KNOW that these two things will never need to be addressed for your application, then just stick with Blobstore.
  Negative
563e30f661a8013065267475	X	The only problem I see with this is that I'd need some way to check if that directory already existed before making it, otherwise it'd overwrite the directory.
  Negative
563e30f661a8013065267476	X	You can check with ftp_nlist, but I would just try to change into the directory and create it if that fails.
  Negative
563e30f661a8013065267477	X	I am launching a web application soon that will be serving a fair amount of images so I'd like to have a main web server and a static content server and possibly a separate database server later on.
  Negative
I'd like the user to: The problem is I don't know how to have the user instantly upload an image to a separate server.
  Negative
I thought about using amazon s3, but you can't edit filenames before posting them.
  Neutral
(through POST, I'd rather not use the REST api) I could also use php's ftp function to upload to a separate server, but I'd like to dynamically create folders based on the properties of the image (so I don't have all the images in one big folder obviously), but I don't know how this would work if I used ftp... Or I could save them locally and use a CDN, I'm not too familiar with CDN's so I don't know if using them this way would be appropriate or cost-effective.
  Negative
What are my options here?
  Neutral
I'd like the images to be available instantly (no cron jobs/queues) Thanks.
  Negative
563e30f661a8013065267478	X	You can create directories over FTP with PHP, so that should not be a showstopper.
  Negative
563e30f661a8013065267479	X	I thought about using amazon s3, but you can't edit filenames before posting them.
  Negative
(through POST, I'd rather not use the REST api) If you let your PHP server do the uploading to S3 via POST, you can name the files whatever you want.
  Negative
You should do that anyway, letting your users upload to S3 directly, without your PHP code inbetween, sounds like bad for security to me.
  Negative
563e30f661a801306526747a	X	How do you iterate over all albums?
  Negative
563e30f661a801306526747b	X	I am somewhat desperate posting images via the graph api to a user's page.
  Negative
What I want to achieve is: I've got an image in an amazon S3 bucket.
  Positive
I'm using the /userid/photos endpoint to post the image url and the message property like: Now, the API creates an album called: "AppName Photos".
  Negative
When I post multiple time to the same album, it suddenly arranges all images in a view, but I want them to be seperate posts each by each.
  Negative
So, what I want to achieve is: Post an image to a user's page without putting it into an album, as if the user uploaded the image hisself.
  Negative
www.klout.com is able to do this.
  Positive
When I try to share & upload a custom image, the image is being stored in the "Chronic photos".
  Positive
How can I store the image there and not in an app-dependent album?
  Neutral
563e30f661a801306526747c	X	Got it.
  Neutral
Simply iterate over all albums (you need user_photo access) and post to that specific album of type "wall".
  Negative
It is not the most beautiful solution I've ever seen but at least it works and prevents grouping nicely :)
563e30f661a801306526747d	X	Is there anybody here?
  Negative
563e30f661a801306526747e	X	I'm working on a Rails3 project with video encoding through Zencoder, assets are stored on Amazon S3.
  Negative
I'm creating 5 thumbnails for each video encoded, and each PNG (same ratio as the video: 620x465) weight about 600 Ko.
  Negative
Is it possible to preserve ratio but optimize the weight ?
  Neutral
I can set the format to JPG but I can't see any other option in Zencoder API: https://app.zencoder.com/docs/api/encoding/thumbnails Thx !
  Negative
563e30f661a801306526747f	X	Here's the Zencoder support answer: This is something that we're hoping to add in the future, though I don't know of a definite timeline right now.
  Negative
Keep an eye on our blog and newsletter and we'll announce it when the option is available.
  Positive
I hope they will soon :-)
563e30f861a8013065267480	X	This error shouldnt be happening if you followed step #7.
  Neutral
Are you sure it finished successfully?
  Negative
563e30f861a8013065267481	X	I kind of messed up and installed rails before number #7, and then went back to install #7 (and I tried install rails command again, but it didn't really do anything)
563e30f861a8013065267482	X	Finish #7 and then gem install aws-sdk should work.
  Negative
Does rails not work either?
  Neutral
I dont know why you need aws-sdk so I can't answer that question.
  Negative
You certainly dont NEED it for running Rails on EC2.
  Neutral
563e30f861a8013065267483	X	[I just uninstalled it, and reinstalling rails... ]
563e30f861a8013065267484	X	I'm not sure if rails works... i'm totally new at this.
  Negative
I want to get to "hello world" but i'm dying here... I'm not really sure why I need aws-sdk either...
563e30f861a8013065267485	X	I've recently followed this guide for installing rails on my EC2 server.
  Very negative
What I'm trying to figure out now is how do I install the rails aws-sdk (do I even need to?)
  Negative
.
  Neutral
When I run "gem install aws-sdk" I get the following error: gem install aws-sdk I'm not really sure what to do from here....
563e30f861a8013065267486	X	You don't need the aws-sdk gem unless you are making API calls directly to AWS from your Rails application.
  Very negative
This can be common if you want to upload files to Amazon S3, connect to Amazon DynamoDB, or send emails with Amazon Simple Email Service.
  Negative
If you are going to start using the aws-sdk, I would strongly recommend using the version 2 SDK.
  Negative
It has a single dependency on a pure Ruby gem, so you would not be experiencing these gem install failures.
  Positive
563e30f861a8013065267487	X	I'm using Amazon's simple storage service (S3).
  Negative
I noticed that others like Trello were able to configure sub-domain for their S3 links.
  Negative
In the following link they have trello-attachments as sub-domain.
  Negative
https://trello-attachments.s3.amazonaws.com/.../.../..../file.png Where can I configure this?
  Negative
563e30f861a8013065267488	X	You don't have to configure it.
  Negative
All buckets work that way if there are no dots in the bucket name and it's otherwise a hostname made up of valid characters.
  Negative
If your bucket isn't in the "US-Standard" region, you may have to use the correct endpoint instead of ".
  Negative
s3.amazonaws.com" to avoid a redirect (or to make it work at all).
  Negative
An ordinary Amazon S3 REST request specifies a bucket by using the first slash-delimited component of the Request-URI path.
  Negative
Alternatively, you can use Amazon S3 virtual hosting to address a bucket in a REST API call by using the HTTP Host header.
  Negative
In practice, Amazon S3 interprets Host as meaning that most buckets are automatically accessible (for limited types of requests) at http://bucketname.s3.amazonaws.com.
  Negative
— http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html
563e30f961a8013065267489	X	I am afraid the total operation could not be done within 60 seconds... Is it possible to use other libraries to handle this like urllib2?
  Negative
563e30f961a801306526748a	X	No, all HTTP communications in app engine are done via GAE provided libraries, even if you use some other library it's all subject to the 60 second timeout limit.
  Negative
563e30f961a801306526748b	X	when you use urllib2 etc in GAE you are really using Google versions btw.
  Negative
563e30f961a801306526748c	X	OK.
  Neutral
I guess maybe I should separate the file uploading step.
  Negative
563e30f961a801306526748d	X	I had a create a REST API using bottle framework, which receives calls from GAE.
  Negative
Once this REST API is invoked, it did some calculations and sent outputs as a zip file to AMAZON S3 server and return the link to GAE.
  Negative
Everything works fine expect the timeout issue.
  Positive
I tried to adjust the deadline of urlfetch to 60 seconds, which did not solve the problem.
  Negative
I appreciate any suggestions.
  Neutral
GAE side: Broser error info.: REST server: Errors from the REST server:
563e30f961a801306526748e	X	The deadline is a maximum value, once reached it'll fail.
  Negative
And it's failing with Deadline exceeded while waiting for HTTP response So you should try to catch that exception and try again.
  Negative
If the entire operation can't be done in under 60 seconds then there is nothing else to be done, it's a hard limit in GAE that HTTP requests can't exceed 60 seconds.
  Negative
563e30f961a801306526748f	X	This question might be helpful: stackoverflow.com/questions/701545/…
563e30f961a8013065267490	X	That link is dead, by the way.
  Negative
563e30f961a8013065267491	X	Sorry: developer.amazonwebservices.com/connect/…
563e30f961a8013065267492	X	If you're lazy like me, Newvem basically does this on your behalf and aggregates/tracks the results on a per-bucket level across your S3 account.
  Negative
563e30f961a8013065267493	X	@rcoup This newvem.com link is broken.
  Negative
563e30f961a8013065267494	X	doesn't work if files are in sub folders.
  Negative
563e30fa61a8013065267495	X	It counted files in subfolders for me...
563e30fa61a8013065267496	X	The -r in the command is for --recursive, so it should work for sub-folders as well.
  Negative
563e30fa61a8013065267497	X	Why did you resurrect a 5-year-old question to post a poorly formatted copy of an existing answer?
  Negative
563e30fa61a8013065267498	X	The previous answer piped the output into a txt file unnecessarily.
  Negative
563e30fa61a8013065267499	X	IMO this should be a comment on that answer, then.
  Negative
This is a really trivial difference.
  Positive
563e30fa61a801306526749a	X	Seems like a worthy answer- especially since the selected answer for this question starts with 'There is no way...' and @mjsa has provided a one-line answer.
  Negative
563e30fa61a801306526749b	X	For some reason, the ruby libs (right_aws/appoxy_aws) won't list more than the the first 1000 objects in a bucket.
  Negative
Are there others that will list all of the objects?
  Negative
563e30fa61a801306526749c	X	When you request the list, they provide a "NextToken" field, which you can use to send the request again with the token, and it will list more.
  Negative
563e30fa61a801306526749d	X	Unless I'm missing something, it seems that none of the APIs I've looked at will tell you how many objects are in an S3 bucket / folder(prefix).
  Negative
Is there any way to get a count?
  Neutral
563e30fa61a801306526749e	X	There is no way, unless you list them all in batches of 1000 (which can be slow and suck bandwidth - amazon seems to never compress the XML responses), or log into your account on S3, and go Account - Usage.
  Negative
It seems the billing dept knows exactly how many objects you have stored!
  Negative
Simply downloading the list of all your objects will actually take some time and cost some money if you have 50 million objects stored.
  Negative
Also see this thread about StorageObjectCount - which is in the usage data.
  Neutral
An S3 API to get at least the basics, even if it was hours old, would be great.
  Negative
563e30fa61a801306526749f	X	If you use the s3cmd command-line tool, you can get a recursive listing of a particular bucket, outputting it to a text file.
  Negative
Then in linux you can run a wc -l on the file to count the lines (1 line per object).
  Neutral
563e30fa61a80130652674a0	X	Go to AWS Billing, then reports, then AWS Usage reports.
  Negative
Select Amazon Simple Storage Service, then Operation StandardStorage.
  Negative
Then you can download a CSV file that includes a UsageType of StorageObjectCount that lists the item count for each bucket.
  Negative
563e30fa61a80130652674a1	X	In s3cmd, simply run the following command (on a Ubuntu system): s3cmd ls -r s3://mybucket | wc -l
563e30fb61a80130652674a2	X	None of the APIs will give you a count because there really isn't any Amazon specific API to do that.
  Negative
You have to just run a list-contents and count the number of results that are returned.
  Negative
563e30fb61a80130652674a3	X	Old thread, but still relevant as I was looking for the answer until I just figured this out.
  Negative
I wanted a file count using a GUI-based tool (i.e. no code).
  Negative
I happen to already use a tool called 3Hub for drag & drop transfers to and from S3.
  Negative
I wanted to know how many files I had in a particular bucket (I don't think billing breaks it down by buckets).
  Neutral
I had 20521 files in the bucket and did the file count in less than a minute.
  Negative
563e30fb61a80130652674a4	X	The api will return the list in increments of 1000.
  Negative
Check the IsTruncated property to see if there are still more.
  Neutral
If there are, you need to make another call and pass the last key that you got as the Marker property on the next call.
  Negative
You would then continue to loop like this until IsTruncated is false.
  Negative
See this Amazon doc for more info: Iterating Through Multi-Page Results
563e30fb61a80130652674a5	X	3Hub is discontinued.
  Negative
There's a better solution, you can use Transmit (Mac only), then you just connect to your bucket and choose Show Item Count from the View menu.
  Negative
563e30fb61a80130652674a6	X	I used the python script from scalablelogic.com (adding in the count logging).
  Negative
Worked great.
  Positive
563e30fb61a80130652674a7	X	Using new aws CLI, it's possible now.
  Negative
563e30fb61a80130652674a8	X	There is an easy solution with the S3 API now (available in the AWS cli): or for a specific folder:
563e30fb61a80130652674a9	X	The problem with using OFFSET is that it requires the server to iterate over the entire result set before the "LIMIT", so it breaks down for very large data sets.
  Very negative
I've actually used an API that did this and we had major problems getting the data after about 100,000 results.
  Negative
563e30fb61a80130652674aa	X	Following up from my previous question: Using "Cursors" for paging in PostgreSQL What is a good way to provide an API client with 1,000,000 database results?
  Negative
We are currently using PostgreSQL.
  Negative
A few suggested methods: What haven't I thought of that is stupidly simple and way better than any of these options?
  Negative
563e30fb61a80130652674ab	X	The table has a primary key.
  Positive
Make use of it.
  Neutral
Instead of LIMIT and OFFSET, do your paging with a filter on the primary key.
  Neutral
You hinted at this with your comment: Paging using random numbers ( Add "GREATER THAN ORDER BY " to each query ) but there's nothing random about how you should do it.
  Negative
Allow the client to specify both parameters, the last ID it saw and the number of records to fetch.
  Negative
Your API will have to either have a placeholder, extra parameter, or alternate call for "fetch the first n IDs" where it omits the WHERE clause from the query, but that's trivial.
  Negative
This approach will use a fairly efficient index scan to get the records in order, generally avoiding a sort or the need to iterate through all the skipped records.
  Negative
The client can decide how many rows it wants at once.
  Negative
This approach differs from the LIMIT and OFFSET approach in one key way: concurrent modification.
  Negative
If you INSERT into the table with a key lower than a key some client has already seen, this approach will not change its results at all, whereas the OFFSET approach will repeat a row.
  Negative
Similarly, if you DELETE a row with a lower-than-already-seen ID the results of this approach will not change, whereas OFFSET will skip an unseen row.
  Negative
There is no difference for append-only tables with generated keys, though.
  Negative
If you know in advance that the client will want the whole result set, the most efficient thing to do is just send them the whole result set with none of this paging business.
  Negative
That's where I would use a cursor.
  Negative
Read the rows from the DB and send them to the client as fast as the client will accept them.
  Negative
This API would need to set limits on how slow the client was allowed to be to avoid excessive backend load; for a slow client I'd probably switch to paging (as described above) or spool the whole cursor result out to a temporary file and close the DB connection.
  Very negative
Important caveats:
563e30fb61a80130652674ac	X	Have the API accept an offset to start from and the number of records to return.
  Negative
This is a sort of paging where the client can determine how many records to return in one page request.
  Negative
The API should also return the total number of records possible for the query so the client knows how many "pages", or optionally it can derive when it has retrieved the last records when the number of records returned is zero or less than the number of records requested.
  Negative
You can control this in your PostgresSQL query by using the OFFSET clause (which record to start retrieving at) and the LIMIT clause (number of records to return) in your SELECT statement.
  Negative
563e30fb61a80130652674ad	X	I'm writing this application that uses some information from a website and I'm using PhantomJs to extract this information.
  Negative
Now I want the user to be able to run my application without the need of PhantomJs in their system.
  Negative
That way it'll be more like a service call.
  Positive
I have followed the following guide: http://ariya.ofilabs.com/2012/07/cloud-phantomjs-with-ironworker.html To get PhantomJs working and getting the information that I need for some site, now I can queue a worker and get the result in the log using Iron.io's web interface.
  Negative
I would like to know if there is a way to get the result of the execution programmatically.
  Negative
I have taken a look at the API, but I need to authenticate and I also need to provide a different task ID ( which I don't know how to get ).
  Negative
563e30fc61a80130652674ae	X	Queue task => obtain task_id as result of operation.
  Negative
But there are plenty of different ways to get result: read task log via api (you need project_id, token, task_id), Store data to Amazon S3, push information to some kind of queue, touch own api, send info to webhook, write information to database etc
563e30fc61a80130652674af	X	Thanks.
  Negative
I will evaluate this option against proxying a request to s3 (and thus avoiding the latency from the redirect, which I suspect won't be acceptable).
  Negative
Other options always welcome!
  Positive
563e30fc61a80130652674b0	X	So, I'm writing a web application in node.js where users can upload photos, and they can specify some access control settings on every photo (public, private, friends only).
  Negative
I then check the users' session key on every request to ensure that they have access.
  Negative
If they do, I send them the file by opening it using fs and piping it to the response object.
  Negative
However, when I benchmark this with apachebench, I get around 1500 requests per second.
  Neutral
If I remove all the database stuff, it doesn't get much faster.
  Negative
By comparison, Nginx serves 17000 requests per second on the same photo.
  Negative
Obviously this order-of-magnitude difference is going to be a huge cost problem if my service takes off.
  Negative
Is there a better way to control access while preserving static-like performance, apart from making them all public?
  Positive
Edit: realistically, the file is going to be hosted on S3, not in the filesystem.
  Negative
So node will be acting less as a static fileserver and more as an http proxy, which I suspect it will be much better at.
  Negative
563e30fc61a80130652674b1	X	Use an S3 signed URL.
  Negative
A signed URL is a temporary URL for private files that you can send to a single user that references an S3 object.
  Negative
You can also put an expiration time on a signed URL so it doesn't stick around forever.
  Negative
So the flow would look like this: Here's a related blog post: Amazon S3 Signed URLs with NodeJS.
  Negative
563e30fc61a80130652674b2	X	It's deprecated because it has no use case and is pending removal in future releases.
  Negative
Would you please tell me what you need it for?
  Neutral
563e30fc61a80130652674b3	X	Yes, the code uses it to get the amazon server date and time and uses that combination for a sync service whenever the app comes online.
  Negative
563e30fc61a80130652674b4	X	My question is if they are deprecating a method, there should be some replacement for that right??
  Negative
What is it in this case??
  Neutral
Or you are saying like they have decided the method is useless and about to remove it in future release??
  Neutral
563e30fc61a80130652674b5	X	Thanks Yangfan.
  Positive
You really explained it well.
  Positive
As you have said, I am now using the formatRFC822Date method from Dateutils in AWS SDK to get the date and time from the server.
  Negative
Thank you again.
  Positive
563e30fc61a80130652674b6	X	I have been using Android AWS SDK 1.4.6 version to connect with S3object of Amazon web service.
  Negative
I have updated my SDK to 2.2.1 last day and found gethttprequest() method to be deprecated.
  Negative
This method is in com.amazonaws.services.s3.model.S3ObjectInputStream package currently.
  Neutral
However I am not able to find a replacement for that method in documentation too.
  Neutral
Do anyone know, which method to use as a replacement for gethttprequest()?
  Negative
or any useful links could do too.
  Neutral
Thanks in advance.
  Neutral
563e30fc61a80130652674b7	X	There are two reasons that lead to the deprecation: The server date or time from the response isn't part of the Amazon S3 APIs.
  Negative
It should be correct, but I suggest you not rely on it.
  Negative
There are many ways to get the date or time, say NPT time server or from system.
  Neutral
563e30fc61a80130652674b8	X	I am using fineUploader in one of my project on MEAN.IO , I am able to configure fineUploader with angular so as to make a request on server to upload file to S3 bucket, which is working fine .
  Negative
What my requirement is I want to send the bucket owner a zip download link of all the files that uploaded in a particular session on S3.
  Negative
So that bucket owner can just download the zip of those file from the mail only .
  Negative
I go through fineUploader document but could not find anything specific .
  Negative
I also google the solution but don't get any idea on how to approach for this .
  Negative
Any suggestion or link to read will be very helpful
563e30fc61a80130652674b9	X	Here's how you would do this: This is a server-side process, Fine Uploader is out of the picture.
  Negative
563e30fd61a80130652674ba	X	Instead of creating separate files, you can "prepare" a single file by using ftruncate() with the desired size and then write each section inside using fseek() followed by stream_copy_to_stream() :)
563e30fd61a80130652674bb	X	Oh, interesting point!
  Negative
Didn't think about that one.
  Neutral
563e30fd61a80130652674bc	X	Hope that gives you enough ideas to venture into this yourself ;-)
563e30fd61a80130652674bd	X	What would be the best/most efficient way to store the file parts on the backend if I am to write this myself (I'm still thinking there's got to be a library for this, but I'm prepared to do the dirty work if there isn't).
  Negative
Better to use a database or raw files, for example?
  Neutral
I did read the client spec from Amazon so I understand how it works, I'm more interested in a library or else what would be the best way to implement the server.
  Negative
563e30fd61a80130652674be	X	Once the data is transferred you will recombine them and store as a single file as opposed to multiple parts.
  Negative
In terms managing the files transferred to the backend and storing them in either a db or a filesystem would require more context about the data and the requirements.
  Negative
In terms of libraries I have not come across anything specific so far.
  Negative
563e30fd61a80130652674bf	X	You can take a look at the multipart implementation of boto github.com/boto/boto/blob/develop/boto/s3/multipart.py
563e30fd61a80130652674c0	X	Thanks for your help!
  Positive
563e30fd61a80130652674c1	X	Amazon S3 has a very nice feature that allows the upload of files in parts for larger files.
  Positive
This would be very useful to me if I was using S3, but I am not.
  Negative
Here's my problem: I am going to have Android phones uploading reasonably large files (~50MB each of binary data) on a semi-regular basis.
  Negative
Because these phones are using the mobile network to do this, and the coverage is spotty in some of the places where they're being used, strong signal cannot be guaranteed.
  Negative
Therefore, doing a simple PUT with 40MB of data in the content body will not work very well.
  Negative
I need to split up the data somehow (probably into 10MB chunks) and upload them whenever the signal will allow it.
  Negative
Once all of the chunks have been uploaded, they need to be merged into a single file.
  Negative
I have a basic understanding of how the client needs to behave to support this through reading Amazon's S3 Client APIs, but have no idea what the server is doing to allow this.
  Negative
I'm willing to write the server in Python or PHP.
  Neutral
Are there any libraries out there for either language to allow this sort of thing?
  Neutral
I couldn't find anything after about one hour of searching.
  Negative
Basically, I'm looking for anything that can help point me in the right direction.
  Negative
Information on this and what protocols and headers to use to make this as RESTful as possible would be fantastic.
  Negative
Thanks!
  Positive
563e30fd61a80130652674c2	X	From the REST API documentation for multi-part upload it seems that Amazon expects the client to break the large file into smaller multiple parts and upload them individually.
  Negative
Prior to uploading you need to obtain an upload id and on every upload you include the upload id and the a part number for the portion of the file being uploaded.
  Negative
The way you may have to go about structuring is to create a client which can split a huge file into multiple parts and upload them in parallel using the above specified convention.
  Positive
563e30fd61a80130652674c3	X	I'm having the same thoughts currently.
  Negative
Do you have any recommendations, two years later?
  Neutral
563e30fd61a80130652674c4	X	@uval I got a chance to talk to top AWS guys (including Werner Vogels) about this.
  Positive
And the answer is that the Android SDK was something they're not recommending themselves.
  Negative
It is not suited for serious work!
  Negative
563e30fd61a80130652674c5	X	So I understand that you eventually got convinced to drop the idea.
  Negative
It could have been a great advantage indeed.
  Neutral
Thanks!!
  Neutral
{And thanks to StackOverflow for making this kind of interaction possible!}
  Positive
563e30fd61a80130652674c6	X	The Amazon Access Key here would be like an access login for the app, and since there is no server/web service running and all business logic happens through the app, the bullet points 1 & 3 does not have relevance here.
  Very negative
As for threats, I meant by hackers.
  Negative
563e30fd61a80130652674c7	X	Amazon Cloud Services (AWS) has provided the ready to use Library to make calls to SDB, S3, SNS etc right from your Android app.
  Negative
This makes it really easy for a mobile developer who is not familiar with web services and web applications to create a completely scalable cloud based app.
  Negative
We give the Amazon Access Credentials in these API calls to connect to our cloud Account; My question is:
563e30fd61a80130652674c8	X	I talked to the Amazon Advocate for our region and he told that Amazon client library is not designed for such a purpose.
  Very negative
Obviously, I was not very convinced.
  Negative
I think an entire client library to Amazon communication (bypassing the need for a webserver) could be a great advantage for Mobile devs.
  Negative
563e30fe61a80130652674c9	X	Re: Would hard coding the Amazon Access Credentials inside the code (as a field Constant etc) make it vulnerable to extraction?
  Negative
Via decompiling etc.?
  Neutral
Yes, by looking for strings and patterns in the binary.
  Neutral
Also decompiling, but that'd often not be necessary.
  Negative
The first question is, what sort of threats are you trying to protect against?
  Negative
Governments?
  Neutral
Paid hackers?
  Neutral
Or you just want to make it not easy to gain access other than via the app?
  Neutral
563e30fe61a80130652674ca	X	Thak you for your detailed response.
  Positive
This code work for me, when I changing x.Size == 0 on x.Size == 1.
  Negative
But it does not recognize all folder, only a small part of them.
  Negative
I can't understand why that's happening.
  Negative
I'm exactly that my cycle goes over and over again until it reaches the end.
  Negative
But I can't get all of the folders.
  Negative
What can be the reason?
  Neutral
563e30fe61a80130652674cb	X	Maybe related to you changing Size to 1?
  Neutral
'Folders' are presented in this list call as S3 objects with no contents: they should be Size 0.
  Negative
You could simply remove size from the predicate if you're finding the objects you need without it.
  Negative
563e30fe61a80130652674cc	X	If you're still having trouble after that, maybe make sure your ListObjectsRequest isn't limiting your request too much.
  Negative
For example, if you specified a prefix... test your code without it to make sure it isn't limiting your request too much.
  Negative
If that doesn't work, feel free to ask a new question (this one is very clear as-is, we don't want to clutter it up) with an example of your bucket's structure and the request you're trying to make.
  Negative
563e30fe61a80130652674cd	X	If I tried to delete size comparison, I have the same result when I write x.Size == 1.
  Negative
Can you look my code with my results?
  Neutral
codeshare.io/4eBiJ These are how it must look: snag.gy/HbLhK.jpg
563e30fe61a80130652674ce	X	request.Delimiter is unnecessary and may be causing trouble, try it without that.
  Negative
It is interesting that BF/Music/ seems to be getting skipped outright; could you do a sanity check and make sure it exists as you expect in the S3 bucket?
  Negative
As for the subdirectories, if you don't want them you can simply filter your list further after retrieval via regex or matching a single slash, etc.
563e30fe61a80130652674cf	X	All that I found, it's this method GET Bucket But I can't understand how can I get only a list of folders in the current folder.
  Negative
Which prefix and delimiter I need to using?
  Negative
Is that possible at all?
  Neutral
563e30fe61a80130652674d0	X	For the sake of example, assume I have a bucket in the USEast1 region called MyBucketName, with the following keys: Working with folders can be confusing because S3 does not natively support a hierarchy structure -- rather, these are simply keys like any other S3 object.
  Negative
Folders are simply an abstraction available in the S3 web console to make it easier to navigate a bucket.
  Negative
So when we're working programatically, we want to find keys matching the dimensions of a 'folder' (delimiter '/', size = 0) because they will likely be 'folders' as presented to us by the S3 console.
  Negative
Note for both examples: I'm using the AWSSDK.S3 version 3.1 NuGet package.
  Negative
Example 1: All folders in a bucket This code is modified from this basic example in the S3 documentation to list all keys in a bucket.
  Negative
The example below will identify all keys that end with the delimiter character /, and are also empty.
  Negative
Expected output to console: Example 2: Folders matching a specified prefix You could further limit this to only retrieve folders matching a specified Prefix by setting the Prefix property on ListObjectsRequest.
  Negative
When applied to Example 1, we would expect the following output: Further reading:
563e30fe61a80130652674d1	X	Using prefix of the/path/to/read/ (note that there is no leading slash, but there is a trailing slash), and delimiter of /, you'll find all the folders within that folder inside <CommonPrefixes>.
  Negative
CommonPrefixes A response can contain CommonPrefixes only if you specify a delimiter.
  Negative
When you do, CommonPrefixes contains all (if there are any) keys between Prefix and the next occurrence of the string specified by delimiter.
  Negative
In effect, CommonPrefixes lists keys that act like subdirectories in the directory specified by Prefix.
  Negative
For example, if prefix is notes/ and delimiter is a slash (/), in notes/summer/july, the common prefix is notes/summer/.
  Negative
All of the keys rolled up in a common prefix count as a single return when calculating the number of returns.
  Neutral
See MaxKeys.
  Positive
http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e30fe61a80130652674d2	X	Alternatively another simpler approach is using https://github.com/minio/minio-dotnet Minio .
  Negative
Net implements minimal API's to work with Amazon S3 and other compatible storage solutions.
  Negative
Following example shows how you can filter out only directories.
  Negative
Here the CommonPrefix is abstracted as a folder through the ListObjects() API.
  Negative
563e30fe61a80130652674d3	X	I'm having the same problem!
  Negative
I'm using the same set of resources (the blog and model code), and having the exact same result.
  Negative
About half the time, the request gives ERR_CONNECTION_RESET with no other information.
  Negative
563e30fe61a80130652674d4	X	Thanks Joan.
  Neutral
I have an active question in with AWS, but it requires me to replicate my browser code as a stand-alone process for them to debug -- which requires some development.
  Negative
563e30fe61a80130652674d5	X	Let me know if you want help putting something together for AWS.
  Negative
The sooner I get this straightened out, the better.
  Positive
I'd be happy to work with you on github (or alternative), if that would be useful.
  Positive
563e30fe61a80130652674d6	X	Just wanted to add a +1.
  Neutral
I'm experiencing the same issue, having largely followed this guide for downloading from a Angular/coffeescript front end for my Django app.
  Negative
563e30ff61a80130652674d7	X	Update added -- no errors when using an EC2 instance as a client.
  Negative
563e30ff61a80130652674d8	X	I am PUTting files to S3 via ajax requests and about 50% of the time I get ERR_CONNECTION_RESET errors.
  Negative
I know the requests are signed correctly -- any ideas what may be causing this?
  Negative
Again, this is an intermittent problem that I see from multiple locations and machines.
  Negative
Here is the relevant coffeescript code I am using to PUT my files to S3.
  Negative
It is derived from Micah Roberson's and Rok Krulec's work at http://micahroberson.com/upload-files-directly-to-s3-w-backbone-on-heroku/ and http://codeartists.com/post/36892733572/how-to-directly-upload-files-to-amazon-s3-from-your.
  Negative
Update I've been getting very attentive support from Amazon on this issue.
  Negative
Per their suggestion, I created an EC2 Windows instance, loaded the Chrome browser on it, and attempted to upload 5 files 10 times with my code.
  Negative
I did not see the error once.
  Negative
I did see some SignatureDoesNotMatch errors occasionally, but not a single ERR_CONNECTION_RESET error.
  Negative
I am still seeing ERR_CONNECTION_RESET errors though on every non-EC2 client/network location I use.
  Negative
Update Still no solution here.
  Negative
I have moved from using a self-rolled signing algorithm to one provided by boto.
  Positive
No impact on the ERR_CONNECTION_RESET issue though.
  Negative
563e30ff61a80130652674d9	X	I finally gave up on getting this to work.
  Negative
Instead, I am now using Fine Uploader to provide this functionality.
  Negative
563e30ff61a80130652674da	X	I suppose this problem has no decision.
  Negative
Try a POST request: https://aws.amazon.com/articles/1434 Use FormData to append fields, if you prefer AJAX sending To sign a request in nodejs: Amazon S3 POST api, and signing a policy with NodeJS
563e30ff61a80130652674db	X	Isn't it unRESTful that in this case the Location for PUT will not be valid anymore afterwards?
  Negative
Normally if you PUT something, you'd expect to be able to GET it after.
  Neutral
563e30ff61a80130652674dc	X	I think there's no such expectation.
  Negative
At any point in time (also right after the PUT) could another client issue DELETE on the particular resource.
  Negative
The "PUT, GET" sequence you describe does not happen within a transaction or so.
  Negative
563e30ff61a80130652674dd	X	To be more precise, I think it could be said that there's an "expectation" that right after a PUT, a GET would return the entity that has been PUT there, yet there's no requirement for this to be the case.
  Negative
If the expectation of the client isn't met, this indicates that another client has been fussing with the resource.
  Negative
563e30ff61a80130652674de	X	Please note that it might take a while before I try this implementation.
  Negative
REST (pun intended) assured that I will get back to accept the answer if it turns out satisfactory.
  Positive
563e30ff61a80130652674df	X	How about upvoting it at as useful?
  Negative
It's useful, right?
  Neutral
563e30ff61a80130652674e0	X	I'm creating a REST API for a backup service, which in principle is quite simple: Now I'd like to greatly reduce traffic to my server by delegating up- and downloads to a service like Amazon S3.
  Negative
Redirecting downloads is not a problem, since I could just perform a regular redirect (301 or 307?)
  Negative
to some generated expiring URL.
  Neutral
But what about the uploads?
  Neutral
I hope to have something like this: The point is that it all needs to be as transparent as possible to the users.
  Negative
563e30ff61a80130652674e1	X	I don't think the initial POST should include the entire entity.
  Negative
Rather, the POST must explicitly be a request for an "upload bucket" resource to be created.
  Negative
You'd then simply respond to the POST request with 201 Created, with the Location header pointing to the new resource where the file should be uploaded.
  Negative
If the upload bucket chosen must depend on specifics of the file (file size, type), then I'd let the client submit metadata in the POST body.
  Negative
563e30ff61a80130652674e2	X	Please recheck the faq.
  Negative
This is much too broad (and subjective in parts) a question for this site.
  Negative
563e310061a80130652674e3	X	Hey Mat, I know this would come :( But I was hoping someone would send a link to a book/tutorial which covers all of it.
  Negative
I wasn't expecting such a answer though.
  Negative
563e310061a80130652674e4	X	Thx Laykes!
  Neutral
Awesome.
  Positive
This helps me a lot.
  Positive
563e310061a80130652674e5	X	Would you allow users uploading straight to the bucket or would you always go through a server?
  Neutral
563e310061a80130652674e6	X	Can you recommend a book/tutorial for cloud programming with php zend and amazon?
  Negative
563e310061a80130652674e7	X	I go through a server.
  Negative
Uploading directly to a server, while easier, means that you don't get the opportunity to get a lot of information about the files as it is transfered.
  Negative
You can get image dimensions, meta information, mime types etc, which you then use for your Content-Type in Amazon S3.
  Negative
However, what I do, is I let them upload the file to my server, and then I add a job to my Zend_Queue server.
  Negative
I then run a daemon which constantly uploads files to S3.
  Negative
That way the user only needs to upload to my server.
  Negative
I store the file outside of DocRoot, and I rename it to an md5hash for security.
  Negative
563e310061a80130652674e8	X	No, there aren't any books.
  Negative
I started off using Zend_Service_Amazon_S3 component, but then I switched to the actual Amazon S3 SDK/PHP API, (you can find the class by Googling).
  Negative
The component maintainer is doing a very good job with the Zend_*_Amazon_S3 library, but I found it missed out on several key functions which were essential to my use immediately, so I switched while I still found it easy to do so.
  Positive
There are no books available, at all.
  Negative
Just the Amazon API docs, which are actually pretty good.
  Positive
I've only found one mistake which once reported they updated immediately.
  Negative
563e310061a80130652674e9	X	I have never programmed any cloud application so I'm basically researching to get started with it.
  Negative
I'm developing in Zend Framework and want to use the cloud to store media assets.
  Negative
The whole project should be scaleable for the cloud.
  Negative
Thinking about this, more and more questions are coming up in my mind: What I want to do: I would be very thankful for hints on how to tackle this project ;-]
563e310061a80130652674ea	X	I would recommend Amazon S3, it is also what I have been developing on top of.
  Negative
I will also answer your question from an AWS S3 perspective.
  Neutral
How do I handle access rights to the assets?
  Neutral
(Public should only be able to access them if the according article is released. )
  Neutral
When files are uploaded to Amazon S3, you can choose an access policy.
  Negative
You can also set an access policy to every file in the entire "bucket".
  Neutral
A bucket is a unique name used to refer to your "cloud" based storage repository.
  Positive
Each file in the bucket is accessed by a single key.
  Negative
For example, you upload a file called myAwesomeImage.jpg.
  Positive
Now when you transfer that file to S3, you get to choose several options for that file.
  Negative
So you can choose to put your awesome image, in a "fake directory" called some/path/to/file.
  Negative
So you would create a "key" for this Object to be stored under the "key" "some/path/to/file/myAwesomeImage.
  Negative
jpg".
  Neutral
Your buckets can store billions of Objects, and you can chose how you want to store them, you can choose to use the forward slash to create a folder, but it doesn't actually create a folder, it is just a useful mechanism that you can them employ in your application to signifify depth and organisation in your files.
  Negative
Now, ACL So when you upload your Object, you can pick several default access policies, or you can create your own.
  Negative
For example, if you upload an Object as ACL_PUBLIC, that means that anyone can access it.
  Positive
However, if you upload it as ACL_PRIVATE that means that it is a private and only the owner of the file can access it.
  Neutral
a database to assign them to an article?)
  Neutral
So you have a few options here.
  Positive
You can either cache everything to store a local state of your Bucket, or you can constantly check with the Amazon S3 API to find out what files you have.
  Negative
You will know which you need based on your application.
  Neutral
Take a situation that I have... A image is uploaded to our companies file manager, and then three thumbnails are automatically generated, and then watermarks are also applied.
  Negative
This means that each image, could generate at least 3 images, and up to hundreds (depending on how many different watermarks we need to apply).
  Neutral
In our situation, I uploaded 20k images to S3 last week and then imported it into our File Manager.
  Negative
I have to store a local representation of waht we have in S3 because otherwise it takes too long to search and query the repository.
  Negative
I also am not interested in which watermark files and thumbnails we have, but I do need to make sure that they are generated.
  Negative
Storing them locally means you can do all of this.
  Neutral
This is the schema for my files table.
  Neutral
(but I also have another files_dimensions) table which stores all of my dimensions too.
  Negative
How do I refer to them in zend framework?
  Neutral
(Does it make sense to use a cdn like cloudfront?
  Negative
How to create urls?)
  Neutral
You would create a View Helper, and then have something like $view->createUrl( $file ) where $file contains everything that it needs to construct your URL.
  Negative
So you would have your Object path and it's key.
  Negative
Not really.
  Neutral
Zend_Cloud is not fully developed yet.
  Negative
The idea with Zend_Cloud is that it will be interchangeable with any Cloud storage adapter, but it isn't ready.
  Negative
I create different sizes of all of my Objects.
  Positive
I then append it like /123123123/large.
  Negative
jpg /123123123/medium.
  Neutral
jpg http://i.stack.imgur.com/AkT0B.jpg 
563e310061a80130652674eb	X	any luck on this?
  Negative
563e310061a80130652674ec	X	No, I am using backend PHP to do this.
  Negative
563e310061a80130652674ed	X	i think it's possible, just need to use multipart/form-data in the body of request.
  Negative
see here: developers.facebook.com/blog/post/498
563e310061a80130652674ee	X	Since the Facebook JavaScript SDK documentation talks about an URL and not a data-URL, I would say the answer is: No.
  Negative
But how did you managed to do this by PHP SDK?
  Neutral
563e310061a80130652674ef	X	I converted the base64 data-URL string to binary data and created an image file on the server and uploaded the same.
  Negative
563e310061a80130652674f0	X	The following code snippet is used to upload photo using facebook Javascript SDK.
  Negative
Can I use the imgURL to be base64 image URL string which is generated from Canvas?
  Neutral
Something like "data:image/png;base64,iVBORw0KGgoAAAANSUh..."
563e310061a80130652674f1	X	My solution was to upload the base64 data URL to imgur or Amazon S3, then post the returned URL to facebook: http://alipman88.github.io/debt/about/index.html Alternative solution using only JavaScript (haven't tested personally): Upload Base64 Image Facebook Graph API
563e310161a80130652674f2	X	Dropbox has a Java API and is free, have you tried it?
  Negative
dropbox.com/developers/core/start/java
563e310161a80130652674f3	X	My java app generates some .
  Negative
png image which i want to store to any cloud storage through java API.
  Negative
I dont want any paid service if possible in free.
  Neutral
Not getting a good answer for last 4 hours.
  Neutral
Any suggestion?
  Neutral
563e310161a80130652674f4	X	Google Cloud Storage doesn't have a free tier, although for only a handful of PNG images you will likely be spending less than $1 per month.
  Negative
Google Drive, however, is free for the first 15 GB or so (it's shared with your GMail account, your Google+ photos, etc).
  Negative
Google Drive also provides a Java API: https://developers.google.com/drive/quickstart-java Dropbox and S3 also both have APIs and various quantities/time of free storage.
  Positive
If you're just looking to host images, and you're not trying to make money, and you don't need to host that many, you could look into image-specific services.
  Negative
Imgur.com, for example, provides an API.
  Positive
563e310161a80130652674f5	X	Dropbox and Google Drive could meet your needs.
  Negative
You can download/upload files pragmatically to Dropbox and also to Google Drive.
  Negative
They have API to do it.
  Neutral
However, if you are really looking for really complete service where you can scale up your data storage until you want, you really want to take a look at Amazon S3.
  Neutral
In fact, as far as I know, Dropbox works on the top of Amazon S3 to provide their services.
  Negative
If you would like to have an idea about how to upload/download a file to Amazon S3, you can take a look to this application example.
  Positive
If you want the same thing on Dropbox or Google Drive, there are a lot of examples on Internet.
  Negative
However, on these two providers you need a token to upload files, what I don't like.
  Neutral
I prefer the way in which it works for Amazon S3 (just for programming purposes - GUI is better on GD or Dropbox).
  Negative
Amazon S3 is not totally free, but it is really, really cheap.
  Very negative
Be aware of the network latency between your app and your File storage provider if you don't use the same physical infrastructure.
  Negative
563e310261a80130652674f6	X	So basically, you want someone to read the docs for you and provide you with copypaste example for the things you want to do?
  Negative
563e310261a80130652674f7	X	No, what I am saying is that somebody aware of this entire system could help push me in the right direction... that's all, nothing more.
  Negative
563e310361a80130652674f8	X	I want to get product details i.e. their name,desctiption/features,price,all images(small,medium, large, whichever available),parent category, attributes such as weight, color, etc from amazon.com.
  Negative
Is this possible by any means?
  Neutral
I am developing a website in php(wordpress) that will show products from amazon.com I came across amazon aws sdk for php and amazon PA API, but don't know how to use them to get what i want.
  Negative
Also found Amazon-ECS-PHP-Library but don't don't know how to use it.
  Negative
Can anyone tell me about any tutorial or any documentation or any kind of resourec that shows how to get the things that i want.
  Neutral
i am registered at amazon affiliate.I see the services available ( like EC2,S3 and other) but not aware of using them.
  Negative
Please help.
  Neutral
563e310361a80130652674f9	X	The Product Advertising API falls under the Amazon Associates program, which is entirely separate from Amazon Web Services (AWS) which includes Cloud Computing solutions and server architecture.
  Negative
The AWS SDK for PHP is for AWS, and does not include support for the Product Advertising API.
  Negative
Since your question is about PHP and the PAAPI, let me move your question to the appropriate forum where you're more likely to get your question answered.
  Negative
563e310361a80130652674fa	X	I'm dealing with a similar issue (see stackoverflow.com/questions/7796767/…).
  Negative
I figured out that if you add a delay of a few seconds before trying to alter another frame, the "about:blank" resolves to a nice-looking URL.
  Negative
Still got the same bug, though!
  Positive
563e310361a80130652674fb	X	Thanks.
  Neutral
It still throws that error but somehow I can log in.
  Negative
563e310361a80130652674fc	X	I am using google contacts javascript api.
  Negative
I am trying to add contacts to the gmail account of the authenticated users using the code given in the http://code.google.com/apis/contacts/docs/1.0/developers_guide_js.html#Interactive_Samples.
  Negative
I am able to login and logout.
  Neutral
But I try to create a new contact my chrome is given an error.
  Negative
I have hosted the javascript and html file in the amazon s3 bucket and also image.
  Negative
Unsafe JavaScript attempt to access frame with URL about:blank from frame with URL https://s3.amazonaws.com/googlecontacts/google_contacts.html.
  Negative
Domains, protocols and ports must match.
  Negative
And contacts are not created.
  Negative
HTML file javascript file
563e310361a80130652674fd	X	The problem was I was access https server from http server, so the protocol mis matched just changed the feedURi http://www.google.com/m8/feeds/contacts/default/full'; to https://www.google.com/m8/feeds/contacts/default/full';
563e310361a80130652674fe	X	Have you tried putting google.load( 'gdata', '1.
  Very negative
x' ); in the html file?
  Negative
It worked for me.
  Neutral
563e310361a80130652674ff	X	My application requires storage for storing files like images and I dont want to store them in a database (neither RDBMS nor NoSQL) but as plain files.
  Negative
Is this possible?
  Neutral
Cheers Mani
563e310361a8013065267500	X	File system isn't persistent, so content will be lost after a redeploy/restart.
  Negative
A possible workaround is to use local filesystem for quick access to file resources but store them in amazon S3 (or any other storage service).
  Negative
You can use jClouds API to avoid lock-in with a provider API, and be able to run locally for testing purpose switching to FileSystem implementation
563e310461a8013065267501	X	I am wondering if there is a way or process to correctly estimate the number of concurrent users which my app will be able to support with 2 Dynos on a Node.js (Hapi) based API server using MongoDB hosted on MongoLab (on a shared cluster).
  Negative
I think an important point of consideration (bottle-neck) might also be the fact that the users upload images to Amazon S3 using Cloudinary API.
  Negative
I understand that the answer depends and varies depending on the application and the way users interact with the DB during the game loop.
  Negative
But, is it possible to determine an approximate idea about how many concurrent users will be able to play using the present setup (without scaling up any further).
  Negative
Since, most of the services (other than Amazon S3) are not self-scaling, I was thinking it would be valuable to have an estimate to predict the infrastructure cost.
  Negative
563e310461a8013065267502	X	Have you tried adding other sources besides just webcam video?
  Negative
I understand you only want webcam, but I'd be interested in seeing if anything changes.
  Negative
I was able to get it to work in chrome here: jsfiddle.net/Gy7nr
563e310461a8013065267503	X	Hi Brett thanks for the reply.
  Neutral
I tried with 3 services.
  Neutral
filepicker.SERVICES.VIDEO,filepicker.SERVICES.COMPUTER,filepicker.SERVICES.FACEB‌​OOK.
  Negative
Still no luck.
  Negative
This works fine on firefox.
  Positive
Also jsfiddle on chrome is showing this correctly.
  Positive
For some reason chrome & IE are not showing this.
  Negative
563e310461a8013065267504	X	Reasearch : Also I extracted the iframe src dynamically assigned by filepicker code from firefox using firebug and force assigned to iframe src to check if chrome will be able display the widget.
  Negative
[filepicker.io/dialog/open/?
  Negative
m=*%2F*&amp;key=AeE9yA21xTpiOQ3JQ‌​m938z&amp;id=1349789071455&amp;referrer=&amp;iframe=true&amp;s=13,1,3].
  Negative
This works fine on chrome & IE.
  Positive
So on chrome the src for the iframe was never set [atleast with the configuration I have.]
  Negative
563e310461a8013065267505	X	Research : I placed the code in main update in this page, please try opening it on latest version of chrome [22.0.1229.79 m].
  Negative
interviewcup.com/test.php .
  Neutral
563e310461a8013065267506	X	Excellent!!
  Negative
This works fine.
  Positive
I am proceeding with onReady call approach.
  Negative
Thanks for your help Bret !!
  Positive
563e310461a8013065267507	X	I am trying to record video from user webcam using filepicker.io and store it in my Amazon S3 bucket.
  Negative
I am stuck up here.Below code works fine in firefox but not on chrome & IE.
  Negative
The abc iframe src is not set by filepicker api, so the contents remain blank.
  Negative
I verified that 3rd part cookies are enabled on chrome.
  Negative
Any help will be highly appreciated.
  Positive
563e310461a8013065267508	X	It looks like the iframe wasn't included in the DOM yet.
  Negative
It worked for me when I put the javascript code inside the onReady call from jquery: http://www.filepicker.io/api/file/4wW3A6BwQYaPDNBENcau?dl=false
563e310461a8013065267509	X	stackoverflow.com/questions/13390343/…
563e310461a801306526750a	X	I'd like to set up a separate s3 bucket folder for each of my mobile app users for them to store their files.
  Negative
However, I also want to set up size limits so that they don't use up too much storage.
  Neutral
Additionally, if they do go over the limit I'd like to offer them increased space if they sign up for a premium service.
  Negative
Is there a way I can set folder file size limits through s3 configuration or api?
  Negative
If not would I have to use the apis somehow to calculate folder size on every upload?
  Neutral
I know that there is the devpay feature in Amazon but it might be a hassle for users to sign up with Amazon if they want to just use small amount of free space.
  Negative
563e310561a801306526750b	X	There does not appear to be a way to do this, probably at least in part because there is actually no such thing as "folders" in S3.
  Negative
There is only the appearance of folders.
  Negative
Amazon S3 does not have concept of a folder, there are only buckets and objects.
  Negative
The Amazon S3 console supports the folder concept using the object key name prefixes.
  Negative
— http://docs.aws.amazon.com/AmazonS3/latest/UG/FolderOperations.html All of the keys in an S3 bucket are actually in a flat namespace, with the / delimiter used as desired to conceptually divide objects into logical groupings that look like folders, but it's only a convenient illusion.
  Negative
It seems impossible that S3 would have a concept of the size of a folder, when it has no actual concept of "folders" at all.
  Negative
If you don't maintain an authoritative database of what's been stored by clients (which suggests that all uploads should pass through an app server rather than going directly to S3, which is the the only approach that makes sense to me at all) then your only alternative is to poll S3 to discover what's there.
  Negative
An imperfect shortcut would be for your application to read the S3 bucket logs to discover what had been uploaded, but that is only provided on a best-effort basis.
  Negative
It should be reliable but is not guaranteed to be perfect.
  Negative
This service provides a best effort attempt to log all access of objects within a bucket.
  Positive
Please note that it is possible that the actual usage report at the end of a month will slightly vary.
  Negative
Your other option is to develop your own service that sits between users and Amazon S3, that monitors all requests to your buckets/objects.
  Negative
— http://aws.amazon.com/articles/1109#13 Again, having your app server mediate all requests seems to be the logical approach, and would also allow you to detect immediately (as opposed to "discover later") that a user had exceeded a threshold.
  Negative
563e310561a801306526750c	X	Great ideas!
  Very positive
I did not yet successfully implement them but it gives me some thing to try.
  Neutral
I also explored the Javascript-sdk which is a dedicated sdk for sending files via the browser.
  Positive
So far I am having some trouble with the Secret Access Key not matching up.
  Negative
Thanks again!
  Positive
563e310561a801306526750d	X	I am using PhantomJS 1.9.7 to scrape a web page.
  Negative
I need to send the returned page content to S3.
  Neutral
I am currently using the filesystem module included with PhantomJS to save to the local file system and using a php script to scan the directory and ship the files off to S3.
  Negative
I would like to completely bypass the local filesystem and send the files directly from PhantomJS to S3.
  Negative
I could not find a direct way to do this within PhantomJS.
  Negative
I toyed with the idea of using the child_process module and pass in the content as an argument, like so: which would call a php script directly to accomplish the upload.
  Negative
This will require using an additional process to call a CLI command.
  Neutral
I am not comfortable with having another asynchronous process running.
  Negative
What I am looking for is a way to send the content directly to S3 from the PhantomJS script similar to what the filesystem module does with the local filesystem.
  Negative
Any ideas as to how to accomplish this would be appreciated.
  Negative
Thanks!
  Positive
563e310561a801306526750e	X	You could just create and open another page and point it to your S3 service.
  Positive
Amazon S3 has a REST API and a SOAP API and REST seems easier.
  Negative
For SOAP you will have to manually build the request.
  Neutral
The only problem might be the wrong content-type.
  Negative
Though it looks as if it was implemented, but I cannot find a reference in the documentation.
  Negative
You could also create a form in the page context and send the file that way.
  Positive
563e310561a801306526750f	X	have you considered the Same Origin Policy?
  Negative
563e310561a8013065267510	X	Yes, that's taken care of.
  Negative
I'd get a different type of error from a preflighted options request.
  Negative
But that passes, I can see that in the debugging proxy.
  Negative
563e310561a8013065267511	X	I'm trying to upload a file to Amazon AWS from Javascript with a signed URL obtained from a django api where I sign it with the help of Boto.
  Negative
This is my line in python: Using the URL generated from this, I can post to S3 with CURL like so: What does not work though, is using this URL to PUT a file to Amazon AWS via Javascript.
  Negative
Debugging with a proxy reveals a broken pipe error, while CURL would give me the regular Amazon Access Denied XML if something went wrong.
  Negative
This is my javascript code (file is a JS File object): Any ideas where to go from here?
  Negative
Maybe I'm not including all the required headers in the signing process, or FormData is not the right way to go.
  Negative
563e310561a8013065267512	X	I'm working on a Rails app that accepts file uploads and where users can modify these files later.
  Negative
For example, they can change the text file contents or perform basic manipulations on images such as resizing, cropping, rotating etc.
  Negative
At the moment the files are stored on the same server where Apache is running with Passenger to serve all application requests.
  Negative
I need to move user files to dedicated server to distribute the load on my setup.
  Negative
At the moment our users upload around 10GB of files in a week, which is not huge amount but eventually it adds up.
  Neutral
And so i'm going through a different options on how to implement the communication between application server(s) and a file server.
  Negative
I'd like to start out with a simple and fool-proof solution.
  Neutral
If it scales well later across multiple file servers, i'd be more than happy.
  Positive
Here are some different options i've been investigating: So i'm looking for different (and possibly standards-based) approaches how file servers for web applications are implemented and how they have been working in the wild.
  Negative
563e310561a8013065267513	X	Use S3.
  Negative
It is inexpensive, a-la-carte, and if people start downloading their files, your server won't have to get stressed because your download pages can point directly to the S3 URL of the uploaded file.
  Negative
"Pedro" has a nice sample application that works with S3 at github.com.
  Positive
I'm usually terribly incompetent or unlucky at getting these kinds of things working, but with Pedro's little S3 upload application I was successful.
  Negative
Good luck.
  Positive
563e310561a8013065267514	X	you could also try and compile a version of Dropbox (they provide the source) and ln -s that to your public/system directory so paperclip saves to it.
  Negative
this way you can access the files remotely from any desktop as well... I haven't done this yet so i can't attest to how easy/hard/valuable it is but it's on my teux deux list... :)
563e310661a8013065267515	X	I think S3 is your best bet.
  Very negative
With a plugin like Paperclip it's really very easy to add to a Rails application, and not having to worry about scaling it will save on headaches.
  Negative
563e310661a8013065267516	X	I was trying to create kindle fire emulators to test apps for kindle fire tablet, fire phone, amazon TV.
  Negative
I've followed the documentation but i couldn't create emulators for these.
  Negative
Can anybody tell me that whether amazon is giving support for emulators or not.
  Negative
I need it very urgent.
  Negative
Any help would be much appreciated.
  Neutral
Thank you :)
563e310661a8013065267517	X	You need a basic SDK development setup using the Android SDK Manager with the latest versions of the following downloaded: Then you will need to add the various Amazon Add-on Site urls to the Android SDK Manager.
  Negative
To do this open the SDK Manager and go to Tools -> Manage Add-on Sites.
  Negative
.
  Neutral
on the main menu.
  Neutral
Next go to the User Defined Sites and add the following URLs: Now close, and re-open your SDK manager (or use Reload from the Packages menu).
  Negative
Next download all of the Amazon/Kindle related stuff using the SDK manager.
  Negative
This includes: Once all of these have been downloaded, open your SDK folder and browse to \Extras\Amazon\AVDLauncherWindows and run amazonavd.bat.
  Negative
Now create your desired devices from there.
  Neutral
For reference here are the Amazon documentation links: Kindle Fire Fire Phone Fire TV
563e310661a8013065267518	X	What was the solution for this 400 bad request issue?
  Negative
I have the same issue : stackoverflow.com/questions/29623898/…
563e310661a8013065267519	X	I'm trying to upload an image to a WordPress.com blog via their REST API.
  Negative
According to their documentation, I should be able to use the new post endpoint to do so.
  Positive
I'm currently using Rails and rest-client to successfully create text posts using the following code (post_hash contains all the post information): As I mentioned, this works perfectly.
  Positive
Now, I'd like to be able to upload a new image to the blog's Gallery by creating a new "post" with the image attached.
  Positive
I'm trying to use this code to do so (image is a CarrierWave file stored on Amazon S3): This does not work and keeps giving me a 400 Bad Request error.
  Very negative
If anyone has any suggestions as to what I'm doing wrong or advice on a better way of doing this, I'd greatly appreciate it.
  Positive
Thanks!
  Positive
563e310661a801306526751a	X	Questions looking for libraries or off site resources are off topic here.
  Negative
563e310661a801306526751b	X	I intend to build a simple app to send files from a mobile phone to a remote server.
  Negative
File size can vary from 500kb to 10 MB.
  Neutral
Is there any service available from Amazon or Google or any other company that will help with the server end side.
  Neutral
I did some research about Google Cloud messenger, Pushbullet but they support only short messages.
  Negative
563e310661a801306526751c	X	Best way is to upload files from Android (or iOS with similar APIs) to respective cloud storage directly (not streaming through your server process, but direct to the storage) Google Cloud Storage Java Client Library to upload to Google-Cloud-Storage by using GscService.createOrReplace(GcsFilename, GcsFileOptions) Amazon S3 Android TransferManager to upload to Amazon-S3 by using TransferManager.upload(bucketNmae, fileName, file) Both these services are meant for uploading files from a android app to respective cloud directly, without any server code.
  Negative
Then you can do either of Make the file available to download with a web URL (as CMS).
  Neutral
Use the file inside the cloud in your application (as File System).
  Negative
563e310761a801306526751d	X	Are there any functions in the S3 library that can select all files with a particular string in their filename?
  Negative
563e310761a801306526751e	X	Yes via the delete_all_objects method.
  Positive
You can specify a regular expression to use to delete all files in a bucket that match the expression.
  Negative
Docs and examples here: docs.amazonwebservices.com/AWSSDKforPHP/latest/…
563e310761a801306526751f	X	You can accomplish this all with a single call to delete_all_objects instead of multiple calls.
  Positive
It would also be more atomic.
  Neutral
563e310761a8013065267520	X	@nategood There is no such thing as "more atomic".
  Negative
Its either atomic, or not.
  Neutral
And both solutions aren't.
  Negative
563e310761a8013065267521	X	Yes.
  Neutral
Concise would be a better term.
  Negative
This operation is not atomic.
  Negative
563e57b223569e151421e712	X	
563e310761a8013065267522	X	I just started trying out Amazon S3 for hosting my website's images.
  Negative
I'm using the official Amazon AWS PHP SDK library.
  Negative
Problem: How can I delete all files located in a S3 'folder'?
  Negative
For example if I have a file named images/2012/photo.
  Neutral
jpg, I want to delete all files whose filenames start withimages/2012/`.
  Negative
Thanks!
  Positive
563e310761a8013065267523	X	S3 does not have "folders" as you would traditionally think of them on a file system (some S3 clients just do a nice job making S3 appear to have folders).
  Negative
Those / are actually part of the file name.
  Neutral
As such, there is no "delete folder" option in the API.
  Negative
You would just need to delete each individual file that has the images/2012/... prefix.
  Negative
Update: This can be accomplished via the delete_all_objects method in the Amazon S3 PHP Client.
  Positive
Simply specify "/^images\/2012\//" as the regex prefix in the second argument (the first argument being your bucket name).
  Negative
563e310761a8013065267524	X	The best way to delete a folder from S3 with all its files is using the API deleteMatchingObjects()
563e310761a8013065267525	X	Here is a function that will do what you are looking to do.
  Negative
Use: if I want to delete the directory foo
563e310761a8013065267526	X	thank you Prasanna Aarthi
563e310761a8013065267527	X	i have a web page which allows user to choose image file from local disk.
  Negative
.
  Neutral
what i want to do is that get image store it on amazon s 3 bucket and convert local link to amazon link and store it in my db.how do i do this.also please tell, is amazon for free and how to make bucket...please help i am a complete newbie to amazon.
  Negative
here is my code
563e310761a8013065267528	X	Regarding pricing As part of the AWS Free Usage Tier, you can get started with Amazon S3 for free.
  Negative
Upon sign-up, new AWS customers receive 5 GB of Amazon S3 standard storage, 20,000 Get Requests, 2,000 Put Requests, and 15GB of data transfer out each month for one year.
  Negative
This is from their official site, For creating buckets and uploading images.
  Neutral
.
  Neutral
Please go through their documentation ,they are pretty clear and elaborate.You can either use REST API'S to send request or make use of their SDK's in various languages(PHP,.
  Negative
NET etc) Please refer http://docs.aws.amazon.com/AmazonS3/latest/dev/MakingRequests.html
563e310861a8013065267529	X	Thanks for your response....yes ,only way is to traversing through whole addressbook
563e310861a801306526752a	X	I am developing an application to backup the whole address book into amazon s3,but i cant able to find any direct iphone api to get the whole address book into any data structure.
  Negative
I tried the following code,to write the address book array to a file in document directory But the problem is file is not getting created in document directory.... Any suggestions appreciated.... Thanks in advance...
563e310861a801306526752b	X	Im not 100% sure, but Accessing the entire addressbook at once probably is not allowed.
  Very negative
Apple has the AddressBookPeoplePicker for a reason, though picking everyone in the AddressBook one person at a time would be a pain.
  Negative
You may be able to set up a loop to run through the address book programatically.
  Neutral
Then add it to your own NSMutableArray.
  Neutral
563e310861a801306526752c	X	You would require this function: http://developer.apple.com/library/ios/#documentation/AddressBook/Reference/ABPersonRef_iPhoneOS/Reference/reference.html#//apple_ref/doc/uid/TP40007210 An Array of all the people is returned.
  Negative
Read the documentation for more.
  Negative
Best of Luck!
  Positive
563e310961a801306526752d	X	I would ask the author if he could implement it in aws2js.
  Negative
I think it would be very easy to do and he has been recently active in the project.
  Negative
Or if you are able, implement it yourself.
  Neutral
563e310961a801306526752e	X	You can also implement this specific request through their REST API until there is support in one of the libraries.
  Negative
563e310961a801306526752f	X	I can get something this to work only when specifying a MaxKeys value in the listObjects parameters
563e310961a8013065267530	X	Can anyone elaborate on Marker.
  Negative
Looked at the docs and am confused.
  Negative
If I omit it, I just get null for data.
  Negative
563e310961a8013065267531	X	The Marker is a string that specifies the key to start with when listing objects in a bucket.
  Positive
It is optional: if you omit it you will see keys from the beginning (alphanumerically), so it sounds like there might be some other issue you're running into?
  Negative
563e310961a8013065267532	X	As a general tip, I often go back to the HTTP API Reference to verify these things, because the documentation for the javascript SDK is sometimes incomplete or inaccurate: docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e310961a8013065267533	X	Although this is set as the right/selected answer, it should be noted that github.com/SaltwaterC/aws2js has been deprecated.
  Negative
Upon npm install it informs one that "aws2js is deprecated.
  Negative
Please use aws-sdk."
  Negative
563e310961a8013065267534	X	Is there any Amazon S3 client library for Node.js that allows listing of all files in S3 bucket?
  Negative
The most known aws2js and knox don't seem to have this functionality.
  Negative
563e310961a8013065267535	X	Using the official aws-sdk: see s3.listObjects
563e310961a8013065267536	X	In fact aws2js supports listing of objects in a bucket on a low level via s3.get() method call.
  Negative
To do it one has to pass prefix parameter which is documented on Amazon S3 REST API page: The data variable in the above snippet contains a list of all objects in the bucketName bucket.
  Negative
563e310961a8013065267537	X	Published knox-copy when I couldn't find a good existing solution.
  Negative
Wraps all the pagination details of the Rest API into a familiar node stream: If you're listing fewer than 1000 files a single page will work:
563e310961a8013065267538	X	Although @Meekohi's answer does technically work, I've had enough heartache with the S3 portion of the AWS SDK for NodeJS.
  Negative
After all the previous struggling with modules such as aws-sdk, s3, knox, I decided to install s3cmd via the OS package manager and shell-out to it using child_process Something like: (Using the cmd_exec implementation from this question) This approach just works really well - including for other problematic things like file upload.
  Neutral
563e310a61a8013065267539	X	I'd like to say that finally I implemented Tomcat 8 with Spring to do it and I am passing data via Java 8 code which basically streams it from the NAS over the web.
  Negative
It is well scalable - it takes approx 20 cores for 10GBps transfers (downloads).
  Positive
It is not optimized in any way except for the read and write buffers which are somewhat like 8k.
  Negative
563e310a61a801306526753a	X	Thank you very much for your answer.
  Positive
I have implemented some of it in Java and it works pretty much OK.
  Positive
Why I would need any abstraction in API (always asking this myself).
  Negative
I just need API for filesystem, so that it doesnt matter what filesystem is below (ext4, stornext, NTFS, glusterfs), so that I can navigate folders with respect to ACLs on it, and that it works thru web proxy, and it does support fail-over.
  Very negative
563e310a61a801306526753b	X	I finally designed solution which is using multiple storage APIs, so I am having MySQL, Mongo, Stornext and HDS Hitachi over REST as well jCIFS.
  Negative
This way, when storage connectivity has issues, it doesnt destroy the operating system (hangs Nagios checks, kernel panics etc), but instead I can handle these issues myself, e.g. connect to the different head.
  Negative
What I was asking above would be better described as "proxy to the filesystem", but now I can use CIFS/NFS server smbd and Java userspace client jCIFS.
  Negative
563e310a61a801306526753c	X	Here is some more info in this context: unix.stackexchange.com/questions/45899/…
563e310a61a801306526753d	X	WebDav is not working well with large files.
  Negative
Swift is for cheap harddrives while I got already distributed, replicating file system.
  Negative
I just need software layer in form of API so that I can upload and download files, just this "little" part :-)
563e310a61a801306526753e	X	Swift isn't exclusively for "cheap hard drives".
  Very negative
It can support almost any storage backend you want, provided you know how to integrate it.
  Negative
Why waste time re-implementing the wheel?
  Negative
563e310a61a801306526753f	X	I need to build RESTful API for my files on cluster filesystem volumes.
  Negative
I have like 20 servers which do share the same filesystems.
  Negative
All I need is RESTful API services which would allow me to stat(), read(), write(), listFolder(), delete(), setacl() etc.
  Negative
Everything else is handled by cluster filesystem, so I just need to have the above functions.
  Negative
I need something which is pretty much mature so it supports access control lists, it has high performance API (like java ones), the library or project is maintained, and it runs Linux, also locking support would be very useful.
  Positive
I would like to put additional functions myself like getDuration(), so if it's open source that would be advantage.
  Negative
If you are aware of such code which would help me to build something like this I would be very grateful.
  Negative
The purpose of it is to allow BPM system to check if the files are OK on the various Stornext volumes.
  Positive
Since these systems are behind various firewalls and mouting NFS or SMB is not really good because of high availability, the best option seems to be RESTful API as single source to all file operations between firewall zones in some convenient way via HTTP(S) request instead of doing NFS or SSH.
  Negative
563e310a61a8013065267540	X	If you want a very generic web based API to manipulate files Look into design of WebDAV api It's OK if you dont want to use it AS IS, you just look into it as an API inspiration.
  Negative
Look how stat(), listFolders() and setacl() might be just one command.
  Negative
If you looking into something time-tested - this is the one.
  Neutral
This API was designed for web based file access, people put some wrappers around it to have it mountable just like any other file system - see davfs2, to me it's a proof of a solid and complete API.
  Negative
Now presuming you don't want full DAV - but something simpler, then I'd look into some libraries which can help me built a similar API.
  Negative
Check out these: Jackrabbit WebDAV Library, milton.io.
  Negative
There is also of course is Jigsaw project to steel code from.
  Positive
Use them to expose your ad-hoc APU or a selection of StorNext API calls over http.
  Negative
If you want a less generic API to manipulate blobs Check out Amazon S3 API as an inspiration, and a code like littles3 as an implementation example.
  Negative
There are plenty of projects like this, check this search Notice how what you want falls in between what is already available: If you want an API tailored to your domain Typically when faced with a similar challenge, like yours, people leverage their domain knowledge and use-cases.
  Negative
If you need this API for pictures storage and retrieval forget generic file operations and model your API around collection of images.
  Negative
You know up front a lot of information which make API design a much simpler job, for instance:
563e310a61a8013065267541	X	Have you looked at rails-api?
  Neutral
I'm not sure if it supports all the functions you need but is maintained and open-source.
  Negative
https://github.com/rails-api/rails-api You could also include a ruby gem to handle access control lists.
  Negative
https://www.ruby-toolbox.com/projects/acl9
563e310a61a8013065267542	X	I'd recommend looking into a WebDAV implementation -- they're usually integrated into a web server (like Apache) and support most of the standard filesystem operations you require.
  Very negative
If you really want to build it yourself, you could also fire up an object storage platform like OpenStack's "Swift" project, backed by your SAN or NAS appliance over NFS/iSCSI.
  Negative
EDIT: You want to store a large number of photos.
  Neutral
There are various NoSQL databases that would also solve this problem.
  Negative
However you could also solve the problem using the a native network filesystem protocol like NFS.
  Neutral
NFS will perform predictably well (v4.1+ anyway) on the majority of your typical read-and-write filesystem operations.
  Negative
However, you'll also need a way to index and retrieve photo metadata and provide access control mechanisms, and those are where performance can get complicated.
  Neutral
When the file is uploaded to your HTTP API, you should calculate the MD5 hash of it's contents, while storing the original file name, owner UID and other metadata in a relational database.
  Negative
Then write the photo to your NFS mount in a specific "bucket".
  Neutral
For example, assume you have a photo whose content has the MD5 hash: e240a38624f4a370bd2ec65cf771134b.
  Negative
Assuming your NFS mount is at /srv/content you would write the photo to the path /srv/content/e240/a38624f4/a370bd2ec65cf771134b.
  Negative
jpg -- splitting the MD5 hash to create prefixed folders.
  Negative
When your user later wants to retrieve the image, they can request it via the data stored in the relational database, your API can look up the photo's MD5 hash, and then locate it on the filesystem using a similar operation.
  Negative
Please be aware that using MD5 could result in collisions if you have a very large number of differing files, so you may want to use another hashing scheme or a combination of two or more to prevent that from occurring.
  Negative
563e310a61a8013065267543	X	That sounds close to a workable solution, it keeps the credentials out of the visible application.
  Negative
Though what is to stop a request being forged for signing to the backend?
  Negative
IP restrictions?
  Neutral
563e310a61a8013065267544	X	To properly answer on your question, you should add more data about what kind of authorization is used at your service.
  Negative
Security measures that you should take depends on what kind of request signing is used in your service.
  Negative
You gave as example Amazon S3, which should not be accessed from vulnerable code.
  Negative
Frontend code is vulnerable.
  Neutral
If any request to your service requires to be signed with "master" credentials, you should proxy all of them.
  Negative
563e310a61a8013065267545	X	I am using angularjs to make REST requests to an API that requires authorisation.
  Negative
The authorisation requires that the request is signed (similar to Amazon s3) and that this signature is sent in the headers.
  Negative
I am unsure how to do this securely with angularjs.
  Negative
As this is client side the credentials need to be embedded in the js code which exposes a massive security hole.
  Negative
I assume I'm missing something obvious here?
  Negative
Any thoughts would be appreciated.
  Negative
563e310a61a8013065267546	X	No, you aren't missing anything.
  Negative
I think possible solution here is write your own authorization backend that will hold your credentials and will return token to your application.
  Negative
563e310b61a8013065267547	X	Check out aws.amazon.com/lambda
563e310b61a8013065267548	X	chiastic-security: Thanks for your response.
  Negative
It's really helpful.
  Positive
I'll try to implement the polling routine suggested by you.
  Negative
563e310b61a8013065267549	X	@RajeevSingh you're welcome :)
563e310b61a801306526754a	X	I need to monitor a directory on Amazon S3 to check if any new file is added to this directory.
  Negative
I tried using Java NIO Watch Service, however it is not working properly.
  Negative
If I used following syntax with the provided path of S3: Then i get following error: If i remove the file:// prefix from the path then following error is generated: If i modify 'Line2' to Path path=Paths.get("https://abc/dir"); then following trace is generated: Kindly let me know what am i doing wrong here and if it is possible to monitor web resources like these using the Java watch service or if there is any other framework/api.
  Very negative
Thanks
563e310b61a801306526754b	X	You've given it a mixture of a file protocol and an http protocol, which doesn't really make sense.
  Negative
Basically you can't do what you're trying to do, if the only way you have access to the resource is via HTTP.
  Neutral
There's no general mechanism for being notified when an HTTP resource changes, because there's no general mechanism for having push notifications from an HTTP resource, and it's outside the operating system's control so it can't intercept changes as they happen.
  Very negative
With a local file, your operating system can detect changes as they happen because it's ultimately responsible for dealing with writes to local disk, but that doesn't apply in your situation.
  Negative
You'd need something that polls for changes, unless S3 has something bespoke in place to push change notifications (but that's something you'd have to investigate separately).
  Negative
You can't do it with Java NIO.
  Negative
563e310b61a801306526754c	X	And Google has announced that they plan to more tightly integrate Storage for Developers into GAE (code.google.com/appengine/docs/roadmap.html) but it is on the bottom of the roadmap.
  Negative
563e310b61a801306526754d	X	Thanks.
  Neutral
Yes, its a GAE app.
  Negative
Isn't it better to go with CDN storage for assets (which doesn't require updates) in terms of performance (& cost) compared with blobstore?
  Negative
Accessing assets from CDN won't need an instance of GAE app (of-course, tight integration will help on the secure access to assets).
  Negative
563e310b61a801306526754e	X	Unlike datastore objects, blobstore objects can be served directly without invoking your App Engine app.
  Negative
It's designed for exactly this purpose.
  Negative
563e310b61a801306526754f	X	Option A : From GAE database Option B : File System access in GAE Option C : File System access (CDN) outside GAE like Amazon S3?
  Negative
or any other options?
  Neutral
Thanks.
  Neutral
563e310b61a8013065267550	X	If it's an App Engine app, you should use the the Blobstore.
  Negative
If you need stand-alone storage, try Amazon S3 or Google Storage for Developers.
  Negative
The first is tightly coupled to App Engine; the others are platform-agnostic and controlled via public APIs.
  Positive
They're all pretty similar in terms of behavior and pricing.
  Neutral
563e310c61a8013065267551	X	We have a website, which allows users to upload documents (word, pdf, PPT, etc.).
  Negative
We are uploading files to Amazon S3.
  Negative
So, all files will have it's own web URL.
  Negative
For these uploaded documents, we would like to generate thumbnails.
  Negative
This thumbnail needs to be generated based on it's content (like Google document viewer).
  Neutral
Is there any Service/API, which generates thumbnails of documents by it's URL?
  Negative
Thanks and Regards, Ashish Shukla
563e310c61a8013065267552	X	I know Datalogics' PDF WebAPI should be able to do the trick for you.
  Negative
There's a Render Pages service that will take a URL and give you back a thumbnail of that PDF.
  Positive
Please keep in mind that this only works with PDF.
  Neutral
They have a free account you can sign up for to get you started.
  Neutral
(*Disclaimer for Stack Overflow community: Yes, I work for this company.
  Negative
I am posting this as I believe it will benefit the person asking the question.
  Positive
I am providing an explanation of how to use the service.)
  Positive
Here's a screenshot of how to make a request to Render Pages using Postman, something I use for testing out services all the time.
  Negative
Hope this helps!
  Positive
*If anyone feels I posted this incorrectly, please let me know.
  Negative
Still reading up on all the rules.
  Positive
563e310c61a8013065267553	X	You should also take a look at AWS Lambda.
  Positive
In fact, this presentation from the AWS re:Invent 2014 conference shows a live example of using Lambda to generate thumbnail images.
  Negative
This solution will be very reliable, and very cost-effective, but has the downside that you'll be responsible for maintaining the code, or debugging issues.
  Positive
563e310c61a8013065267554	X	I have an API set up to receive (amongst other things) an image byte array from a mobile app.
  Negative
Once received, the image is streamed to Amazon S3 and saved as a png.
  Negative
All good, however I have an issue where some images that are received are different dimensions.
  Negative
Is it possible to determine the width and height of the image using only the byte array (ie, not saving it as an image to the server first), and if necessary crop the image?
  Negative
563e310c61a8013065267555	X	Okay, so I figured it out in the end, and it goes something like this: The general gist of it I found here: http://salman-w.blogspot.co.uk/2009/04/crop-to-fit-image-using-aspphp.html
563e310c61a8013065267556	X	I did the changes you suggested but I unfortunately get errors.
  Negative
Here is the API server log: pastie.org/1805101.
  Negative
563e310c61a8013065267557	X	I think you are on the right track.
  Positive
If I put a !
  Neutral
after @avatar.
  Neutral
save I get this clue: pastie.org/1805120.
  Negative
563e310c61a8013065267558	X	This is a correct output where I use the almost the same code in a webbased Ruby on Rails app.
  Negative
pastie.org/1805171.
  Neutral
563e310c61a8013065267559	X	Did you get it to work?
  Neutral
That last log entry seems like it's working
563e310c61a801306526755a	X	Did you have a solution to this?
  Neutral
I am stuck over here > stackoverflow.com/questions/5903565/….
  Negative
Thanks for any advice!
  Positive
563e310c61a801306526755b	X	I am developing an iPhone app in Appcelerator Titanium.
  Negative
My app i communicating with an API that I build using Rails 3.
  Negative
I want to be able to upload an image from the iPhone app to the API (and Amazon S3).
  Neutral
I am using the gem called Paperclip.
  Positive
Along with the upload request I need to send the name of the file.
  Negative
But if I do it like below, the image_file_name is not recognized.
  Negative
What is wrong with the call?
  Neutral
In the App: http://pastie.org/1805065 In the API model: http://pastie.org/1805071 In the API controller: http://pastie.org/1805073 Output on the API server: http://pastie.org/1805078 Please help!
  Negative
563e310d61a801306526755c	X	Looks like in the API Controller the line: Should read: Explanation: This conclusions was made by looking at the server log output from the API Server, and checking the Parameters hash for the name of the submitted image.
  Negative
Instead of it being named "avatar" as your controller was expecting, it appears to actually be named "image".
  Negative
563e310d61a801306526755d	X	If your proxy does not support the needed methods - the only things you can do is to change proxy to the one that can do that or set up NAT.
  Negative
563e310d61a801306526755e	X	@zerkms: Can you please tell more about NAT setup?
  Negative
563e310e61a801306526755f	X	try to google.
  Negative
It is a lot of articles there.
  Neutral
563e310e61a8013065267560	X	It's very likely that the OP is inside of a corporate firewall, thus making the choice of a caching proxy intentional.
  Negative
Nevertheless, this is a case of "then don't do that."
  Negative
563e310e61a8013065267561	X	@Charles & @Steve-o: Yes my office has proxy server.
  Negative
I am supposed to work from behind Squid.
  Negative
Please suggest.
  Neutral
563e310e61a8013065267562	X	@Amit: updated answer, basically you are forced to proxy to an external server, and you have to implement that proxy protocol yourself.
  Negative
563e310e61a8013065267563	X	What I have found till now is that:  How do I proceed from here ?
  Neutral
563e310e61a8013065267564	X	If the Squid proxy MUST be used AND you cannot fix Squid, then you only have one solution: tunnel the API calls through to a server outside of your network and have that server forward the API calls to Amazon S3 on your behalf.
  Negative
From a basic view you can just replicate all the S3 calls you use on your external server but you must be aware of the security implications, i.e. restricting the usage of the server to say the external IP address of your Squid server, or even API keys much like Amazon use themselves.
  Negative
If more flexibility is available try another proxy preferably non-caching like Pound.
  Negative
563e310e61a8013065267565	X	I really hate patronizing answers that indicate the person asking the question is an idiot.
  Negative
You see it all the time on this site and it's getting annoying.
  Negative
for squid try this configuration directive:
563e310e61a8013065267566	X	Does anyone know if there is a possibility to write data to certain file position that is located on Amazon.S3?
  Negative
For example, how can i write 100 byte chunk of data to file on Amazon.S3 with offset of 1000 bytes?
  Negative
563e310e61a8013065267567	X	No I believe it's not possible to edit any file in-situ on Amazon S3 without downloading it.
  Negative
It's a storage space, just like your hard-disk, where you usually have to fetch the file into memory for editing and then again put it back(save it).
  Negative
There are plenty of APIs available to do that.
  Positive
For java you have AWS-SDK.
  Neutral
563e310f61a8013065267568	X	Is it possible to retrieve the members of a google cloud project and their permissions via an api?
  Neutral
I'm doing this because I want to see if the current signed in user of my app has permission to access the cloud storage section of the project.
  Negative
The user is authenticated via OAuth2.
  Positive
I interact with cloud storage through an Amazon s3 library that has the endpoint changed.
  Negative
563e310f61a8013065267569	X	You can secure the page so that only admins can access it or check programmably.
  Negative
563e310f61a801306526756a	X	If you identify your users, I recommand using a MySQL database and handling the requests with PHP.
  Negative
They are both easy to learn.
  Positive
563e310f61a801306526756b	X	It can be totally anonymous.
  Positive
It is not an account based app.
  Negative
I will look into those though.
  Positive
563e310f61a801306526756c	X	I have an app that I am working on, and I want it to go to my server, get a title, image and description and display it for the user.
  Very negative
I have a good deal of experience with iOS app development, but the server side stuff...well not so much.
  Negative
I was thinking that I might just add a separate XML file on the server for each item that I want to display info for...but I know there are more effective options.
  Negative
Should I use a SQL database?
  Negative
I realize that this is quite a broad question, but I just need a push in the right direction.
  Positive
Thanks.
  Neutral
563e310f61a801306526756d	X	I don't really know how to set up your own server, so i'm using third parties.
  Negative
I'm using amazon s3, it's pretty simple to use, you can store there files that < 1gb.
  Positive
Create iPad, iPhone, and iPod applications that leverage AWS using the AWS SDK for iOS.
  Negative
The SDK helps remove complexity by providing iOS APIs for many AWS services including Amazon S3, Amazon SQS, Amazon SNS, and DynamoDB.
  Negative
The single, downloadable package includes the AWS iOS Library, code samples, and documentation.
  Negative
Here's manual.
  Neutral
But there's a lot other good providers for storing your data, please read this article: Parse.com have good tutorials(photo server) for quick start with it - https://parse.com/tutorials
563e310f61a801306526756e	X	The only issue that I can think of is that it wont display an SSL cert in the browser bar to the user.
  Negative
563e310f61a801306526756f	X	Your login form posts to HTTPS, but you blew it when you loaded it over HTTP
563e310f61a8013065267570	X	@SilverlightFox mind posting that as an answer?
  Negative
563e311061a8013065267571	X	@tau sure, done.
  Negative
563e311061a8013065267572	X	thank you for alerting me to this.
  Positive
here is a related question i started on the security stackexchange: security.stackexchange.com/questions/54744/…
563e311061a8013065267573	X	If I have a completely static frontend at http://domain.com and AJAX in the relevant data from https://api.domain.com, will all of the data transmitted between them be secure (as though both used https)?
  Negative
Is there a potential flaw with this model?
  Negative
The reason I'm asking is because then I can use a service like Amazon S3 to host my static webpage and simply pay for a cert on my API.
  Negative
563e311061a8013065267574	X	Apart from the fact that this model results in only a partially encrypted page so the padlock and https cannot be shown to the user in the address bar, they cannot (rightly) verify that your site is the genuine one, it could also be possible for a MITM attack to be executed on your HTTP content and redirect further communications to the attacker.
  Negative
e.g. your AJAX code loaded over HTTP could be as follows: If the attacker intercepts the response from your server to the victim and modifies it to the user's credentials will be sent to the attacker instead of your website and if your attacker then forwarded them onto your website afterwards the victim would be non the wiser that their details have been stolen.
  Very negative
Tools such as sslstrip can make it very easy for an attacker to accomplish this (assuming they are suitably positioned, which is a requirement of all MITM attacks).
  Negative
563e311061a8013065267575	X	I think this approach is fine.
  Positive
Keep all your heavily accessed and static pages on S3 (even use Cloudfront to speed up further).
  Negative
One good example is the home page of your service, some help pages etc.
  Positive
These pages can be on http as there is nothing private in them.
  Negative
When you then require users to login and access "private" content, send them to another domain like secure.domain.com on https.
  Negative
Then on all the content is secured and users will get confidence to login, as they see the secure green lock in the browser when they are doing login etc.
  Negative
Since your APIs also hopefully be requiring privacy that also should operate on the secure domain.
  Negative
563e311061a8013065267576	X	But is it the best approach when the ec2 instance with the api endpoint will be transferring a big amount of data per nonth?
  Positive
The data needs to be transferred before i can save it.
  Negative
563e311061a8013065267577	X	I need some advise regarding my amazon cloud setup.
  Negative
I have a servicestack api which recieves streams in form of images and videos, and these needs to be saved somewhere.
  Negative
The same api also delivers these streams to clients (website, apps etc.) This means that a lot of data is being transferred over Internet and saved to disk somewhere.
  Negative
What is the best setup here when looking at pricing?
  Positive
I've tried the amazon cloud pricing calculator, but its difficult to figure out all the numbers.
  Negative
I was thinking.... - Normal ec2 instance for hosting api - S3 bukcet for saving images and videos - CloudFront for delivering images and videos Is there a better and cheaper approach?
  Negative
I was thinking that data transfer and disk space would be a huge cost in this setup.
  Negative
563e311061a8013065267578	X	I think amazon s3 is very good for saving images and videos.The pricing based on data transferred "in" to and "out" of Amazon S3.Pay only for what you use.
  Positive
There is no minimum fee.
  Negative
We charge less where our costs are less, and prices are based on the location of your Amazon S3 bucket.
  Negative
563e311061a8013065267579	X	Are you referring to CloudWatch Logs functionality?
  Negative
563e311061a801306526757a	X	Yes, I wish to save then inside my pesonal bucket, instead of the inaccessible amazon's bucket
563e311061a801306526757b	X	Sup guyz, I would like to save my log files from CloudWatch inside my personal bucket and not in the default amazon's bucket, for easily download then.
  Negative
Somebody have already done it or just know if its possible?
  Negative
563e311061a801306526757c	X	It is not possible.
  Negative
CloudWatch Logs stores its data in Streams, which are stored within CloudWatch Logs.
  Negative
It is not possible to point CloudWatch Logs to another data store (eg an Amazon S3 bucket).
  Negative
Logs can be retrieved from CloudWatch Logs via the GetLogEvents() API call.
  Negative
There is also a Retention setting to determine how long data should be retained within CloudWatch Logs.
  Negative
563e311061a801306526757d	X	I think you'll have to change the image sizes on your own... I haven't looked at the API yet, though.
  Negative
563e311161a801306526757e	X	I need from Instagram to serve me images on specific sizes.
  Negative
Is this possible?
  Neutral
563e311161a801306526757f	X	I think you will have to grab the images, say with standard_resolution, and then change the image size yourself... sitepoint.com/image-resizing-php
563e311161a8013065267580	X	okay, that's solution for less then standard_resolution.
  Negative
and is there any way to get bigger size than standard_resolution
563e311161a8013065267581	X	It depends... If you read the information here (help.instagram.com/276722745781769) you will see that Instagram always saves/uploads photos in the best resolution possible.
  Negative
However, that resolution differs for different devices, so you might not always be dealing with the same sized images...
563e311161a8013065267582	X	Thanks for the improvements guys
563e311161a8013065267583	X	I am trying to set custom width for images in Instagram, using Instagram API.
  Negative
I am using hashtag to grab all images related.
  Negative
Can I ask Instagram to serve me custom image sizes?
  Neutral
I know there are 3 sizes: thumbnail, low_resolution, standard_resolution.
  Negative
Is that all what I can get?
  Neutral
Thanks!
  Positive
563e311161a8013065267584	X	Simple do a search and replace on _5 (or _6, ie) to get get your desired size (see below): Image sizes appear to be: http://distilleryimage7.s3.amazonaws.com/xxxxxxxxx_7.jpg 612px × 612px http://distilleryimage7.s3.amazonaws.com/xxxxxxxxx_6.jpg 306px × 306px http://distilleryimage7.s3.amazonaws.com/xxxxxxxxx_5.jpg 150px × 150px It appears _8,_4,_3 are not valid sizes.
  Very negative
563e311161a8013065267585	X	Instagram API returns three standard image resolutions.
  Negative
There is an additional undocumented image size that you could also use however, this might be a bit brittle/liable change in the future and it may also be against their terms of service (not sure).
  Negative
I noticed that this image was available when I saw that the Instagram website was serving up higher resolution images- I'm not really sure why Instagram choose not to make this image available.
  Negative
From the API, you can get these standard image sizes: thumbnail (150 x 150) https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/s150x150/e15/11821182_168255950173048_11130460_n.jpg low_resolution (320 x 320) https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/s320x320/e15/11821182_168255950173048_11130460_n.jpg standard resolution (640 x 640) https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/s640x640/sh0.08/e35/11821182_168255950173048_11130460_n.jpg Then, by looking at their website, I noticed they served up a larger version of the file at 1080 x 1080: undocumented large image (1080 x 1080) https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/s1080x1080/e15/11821182_168255950173048_11130460_n.jpg
563e311161a8013065267586	X	Instagram uses Amazon S3 to save its photo and has a certain naming convention that goes like this: By simply getting the photo URL via the API and then changing the _7, _6, _5 in the image URL to another value (via a search/replace), you can control the size of the image.
  Negative
563e311161a8013065267587	X	Try it?
  Neutral
A 401 with the proper headers will show an authentication popup, regardless how the resource is accessed (direct, img tag, script tag).
  Negative
.
  Neutral
563e311161a8013065267588	X	Good point!
  Positive
So I'll ignore 401's then, and send 400s or 403s instead.... How about the other Http codes... any other weird issues with them?
  Negative
563e311161a8013065267589	X	I'll have to repeat my "try it".
  Negative
;-) You could read the HTTP RFCs, but browsers can have their own quirks.
  Negative
563e311161a801306526758a	X	and that's the reason I ask here - RFCs only get you so far... I'm asking if others have gone down this road and realized browser A has quirk B running on os X ;-) and if they do say so, then I can check that scenario myself and see if it really is a problem, and it'd be documented here for others to find instead of having to try it themselves ;-)
563e311161a801306526758b	X	I don't really understand the question.
  Very negative
The http protocol doesn't know what it is sending/receiving.
  Negative
The error codes are not bounded to the shipped content.
  Negative
It is up to the browser to handle these errors accordingly.
  Neutral
563e311161a801306526758c	X	Still note quite an answer to my question - i want to know what different status's are well supported by browsers, but it appears that maybe the answer is all of them.
  Positive
At least this proves/shows an api sending a 403.
  Negative
Thanks!
  Positive
563e311261a801306526758d	X	EDIT: In an attempt to clarify my question, here's what I'm trying to understand.
  Negative
If a web page embeds an image like so: How do browser handle receiving different HTTP error status codes from the image url?
  Neutral
Is it very consistent across browser, and basically treated the same as if the image wasn't there (404) ?
  Positive
Note that I'm aware that I can "just try it", but I don't have every browser/os/phone around to try it out on, and I'd like to understand how this actually works in reasonably modern desktop and mobile browsers (~IE9 and newer as a fuzzy line).
  Negative
Plus if anyone else is every wondering the same thing, they could come here and see the answer too ;-) ORIGINAL QUESTION: I'm implementing a REST service that returns images and videos securely for a client.
  Negative
I was thinking it'd be nice to send out different HTTP response codes for different types of failures: Will responding with these different error codes work exactly the same as a 404 response for all reasonably modern browsers from both an HTML and Javascript perspective?
  Negative
I know the error code would be different of course, but what I'm trying to ensure is that no strange security errors pop up as a result of using these different HTTP responses.
  Negative
563e311261a801306526758e	X	A 403 is given by lots of public APIs, for example, Amazon's S3 service responds with a 403 when an object with a temporary access time expires.
  Negative
e.g.
  Neutral
An url like so would generate a 403 with the following response (Note: the below isn't a valid S3 url... keys/signature have been changed) https://somebucket.s3.amazonaws.com/test/4a828d8cf5633ab41e2fb8deba599178.jpg?AWSAccessKeyId=ACIAICGLRMBL2PTLALHQ&Expires=1424782953&Signature=sWko4DDDRaBS151iEhEZzfDRbU%3D
563e311261a801306526758f	X	Since you're using REST I assume you're using Ajax or Angular to request your images, right?
  Negative
In that case it is up to javascript to catch the errors.
  Negative
So, your browser will not show any popups when javascript handles it correctly.
  Negative
You could start with something like Note, the 401error can be found in the header of the response, and replaces the 'Content-Type: image/png' header.
  Negative
So the Client (browser) has no way to know that is an image was requested and handles it like any other page.
  Negative
563e311a61a8013065267590	X	Do you have a link we can look at?
  Neutral
563e311a61a8013065267591	X	the file actually now called jqm.css
563e311a61a8013065267592	X	yeah I see it - ok upload manually
563e311a61a8013065267593	X	Have a look.... There shouldn't be any images now, so its ok.
  Negative
But you can click on ask and go to other pages...You see the dirrefence
563e311b61a8013065267594	X	can you put it back the other way now?
  Negative
563e311b61a8013065267595	X	Problem solved!
  Negative
You saved the day dude!
  Positive
563e311b61a8013065267596	X	I'm using com.amazonaws.services.s3.AmazonS3 to upload files from my server to amazon's one.
  Negative
Everything is working perfectly but I'm going crazy because of an issue: I am uploading several files and folders (mostly images, js and css files).
  Negative
The files get uploaded nicely however I have one particular css file (jquery-mobile-1.0.1.
  Negative
css) that gets uploaded however when an html relies on that file, the css is not loaded, until I go and manually upload that one file again and make it public.
  Negative
I literally tried everything (changed file name, location, encoding) but nothing seems to be working.
  Negative
Does anyone have an idea what can cause the problem?
  Neutral
The files are uploaded dynamically, so the way that the particular css file gets uploaded does not differ from other css files.
  Negative
Any help is appreciated.
  Positive
563e311b61a8013065267597	X	The mime type (HTTP content type header) on the files you uploaded is incorrect.
  Negative
S3 does not always set them correctly.
  Negative
Both JS files and CSS files are set to text/html - should be text/css and text/javascript.
  Negative
You need to set them appropriately on the upload API call.
  Neutral
Some upload libraries will do this for you, yours clearly isn't.
  Negative
http://orensol.com/2009/07/14/google-chrome-2-css-content-type-and-amazon-s3/
563e311c61a8013065267598	X	possible duplicate of How to send PUT HTTP Request in Flex
563e311c61a8013065267599	X	Did you mean HTTPService?
  Negative
563e311c61a801306526759a	X	Does Flex 4 support put request?
  Negative
I know that Silverlight 4 support put request using its client http stack.
  Negative
563e311c61a801306526759b	X	In action script 3 we can use put method using below API http://livedocs.adobe.com/flash/9.0/ActionScriptLangRefV3/flash/net/URLRequestMethod.html I have used it on desktop apps to upload images to Amazon S3.
  Negative
Since it need air it can be used only on desktop and not on web apps.
  Negative
563e311c61a801306526759c	X	yes, it does.
  Neutral
You have to set the 'method' attribute to "PUT".
  Neutral
i.e.: For more information have look on the Reference-Lib
563e311c61a801306526759d	X	there is no real way to do this.
  Negative
563e311d61a801306526759e	X	This is a great idea!
  Very positive
Yah.
  Neutral
I'm not looking to make it more secure than like youtube or anything.
  Negative
I just want to defer like 95% of people from downloading the music.
  Negative
563e311d61a801306526759f	X	yup, so come up with some hashing unique ID mechanism that both your server and your jukebox player would understand.
  Negative
maybe use mp3 song hash + some secret data.
  Negative
563e311d61a80130652675a0	X	Do you know how you would implement this?
  Neutral
I'm trying to think.
  Neutral
.
  Neutral
the jukebox would be loaded on the client's side - so he'll see the UID and just be able to type that right in to look like the jukebox.
  Positive
I'm a security novice by the way.
  Neutral
563e311d61a80130652675a1	X	Disregard half of the last statement.
  Negative
.
  Neutral
I didn't see your most recent post.
  Negative
.
  Neutral
563e311d61a80130652675a2	X	Wouldn't the client have access to mp3 song hash + some secret data that he could just send back to the server via play.php?hash=... ??
  Negative
563e311d61a80130652675a3	X	It's harder to do in flash.
  Negative
You can't just look at the source and find the mp3 file in the URL.
  Negative
563e311d61a80130652675a4	X	You can still record it, though.
  Positive
As long as anything DRM'ed can be played back it can be saved without the restrictions too.
  Negative
Fact of life (and of technology).
  Neutral
563e311d61a80130652675a5	X	Yah my goal is to defer like 95% of people from trying it.
  Negative
.
  Neutral
that's all.
  Neutral
563e311d61a80130652675a6	X	I don't think it's any harder in Flash.
  Negative
Switch to program that records audio being played back (there's a million free ones), hit "record" button.
  Negative
Actually I think that's even easier than digging through an HTML for a URL.
  Negative
:-)
563e311d61a80130652675a7	X	I'm looking to build a jukebox and I am wondering how one would secure songs that are in <audio> tags with HTML 5.
  Very negative
I don't want people to be able to download the song, but I'd like to stream it via those <audio> tags.
  Negative
Any suggestions?
  Neutral
563e311e61a80130652675a8	X	It's possible using Amazon S3 (similar to the way Soundcloud does it) to generate secure mp3 links for use in your HTML5 player.
  Negative
You generate a secure mp3 on S3 that is valid for a very short time (seconds or minutes), there by prevent someone from copying and sharing the link.
  Negative
You will need to generate the link dynamically using SDK/API.
  Negative
See example of how to use PHP to generate secure links.
  Negative
563e311e61a80130652675a9	X	you could check referer, use some hashing mechanism (unique ID) to verify the streaming player is your jukebox, not the stream saver etc.
  Negative
BUT: whatever you do, some people will figure it out (or using the last resort - catching the whole stream, following on what kind of data your jukebox sends etc.)
563e311e61a80130652675aa	X	Whatever you give people to listen via a stream can be saved to disk too.
  Negative
563e311e61a80130652675ab	X	This is not possible.
  Negative
In order for the client computer to be able to play the song, the song has to be transferred there.
  Negative
Period.
  Neutral
563e311e61a80130652675ac	X	Hi and thank you for your quick answer but when do you mean "storing on the filesystem in my app-root/data directory" do you mean localy on the smartphone?
  Neutral
563e311e61a80130652675ad	X	No, i mean storing it on your openshift gear.
  Negative
563e311e61a80130652675ae	X	I would like to develop a mobile app which publish pictures online inside a mongodb on openshift.com.
  Negative
At the same time I would like to host website where some customers can see a "wall" with all the publications.
  Negative
My question is: Openshift.com is a good choice when you need to store datas (pictures, text) from smartphones and in the same time to host a website?
  Positive
p.s. the pictures will be stored directly in the mongodb, is it a good way to do?
  Positive
Thank you in advance
563e311e61a80130652675af	X	Yes, you would be best off writing a simple API that your mobile application will connect to (in your language of choice) to get the data into mongo.
  Negative
I personally would either store the images on the filesystem in your app-root/data directory, or store them on Amazon S3.
  Negative
563e311f61a80130652675b0	X	This looks like it will do the trick.
  Negative
The process is already handled in a waterfall type workflow, so if any part of it fails before the last step (which would be updating this status) it terminates early, so the status will only get updated if everything else is successful.
  Very negative
Thanks for the help!
  Positive
563e311f61a80130652675b1	X	A year and a half later, I get an upvote for this, but I feel guilty: DynamoDB has released secondary indexes, which are made for this purpose without the complexity of you having to manage another table.
  Negative
Check them out!
  Neutral
563e311f61a80130652675b2	X	Lol, I noticed that as well and was also considering updating the post.
  Positive
But yeah, secondary index are nice... though there are still instances where your original answer does still apply.
  Positive
Anyone interested in this post, be sure to read the documentation when deciding if they are best for you, especially the "Use Indexes Sparingly" section of the documentation if you're wondering why (documention available here: docs.aws.amazon.com/amazondynamodb/latest/developerguide/…).
  Negative
563e311f61a80130652675b3	X	I'm fairly new to Amazon's AWS and its API for Java, so I'm not exactly sure what the most efficient method for what I'm trying to do would be.
  Negative
Basically, I'm trying to setup a database that will store a project's ID, it's status, as well as the bucket and location when uploaded to an S3 bucket by a user.
  Negative
What I'm having trouble with is getting a list of all the project IDs that have a status of "ready" under the status attribute.
  Negative
Any projects that are of status "ready" need to have their ID numbers loaded to an array or arraylist for later reference.
  Negative
Any recommendations?
  Neutral
563e311f61a80130652675b4	X	The way to do this is to use the scan API.
  Negative
However, this means dynamo will need to look at every item in your table, and check if its attribute "status" is equal to "ready".
  Neutral
The cost of this operation will be large, and will charge you for reading every item in your table.
  Positive
The code would look something like this: There is a way to make this better, though it requires denormalizing your data.
  Negative
Try keeping a second table with a hash key of "status", and a range key of "project ID".
  Positive
This is in addition to your existing table.
  Neutral
This would allow you to use the Query API (scan's much cheaper cousin), and ask it for all items with a hash key of "ready".
  Negative
This will get you a list of the project IDs you need, and you can then get them from the project ID table you already have.
  Negative
The code for this would look something like: The downside to this approach is you have to update two tables whenever you update the status field, and you have to make sure that you keep them in sync.
  Positive
Dynamo doesn't offer transactionality, so you have to be ready for the case where the update to the master project table succeeds, but your secondary status table doesn't.
  Negative
Or vice-versa.
  Neutral
For further reference: http://docs.amazonwebservices.com/amazondynamodb/latest/developerguide/QueryAndScan.html
563e311f61a80130652675b5	X	I'm getting a pdf stream as a result of an api call.
  Negative
I would like to use this stream as a pdf template, modify it with mPDF and save it to amazon s3 bucket.
  Negative
Is it possible to do that without saving the stream locally as tmp file and reading it with setSourceFile()?
  Neutral
Is there any way to pass the stream to mPDF?
  Neutral
I would really like to avoid saving the file locally, therefore I already tried to upload the stream directly to s3 and then pass the url to setSourceFile, but this results in an error: fseek(): stream does not support seeking when filesize() is being called in pdf_parser.php Thanks
563e311f61a80130652675b6	X	Ok thanks, that makes sense and puts me on the right path I think
563e312061a80130652675b7	X	I seem to be having trouble from Amazon's documentation (both general AWS and the .
  Very negative
Net API references) how to accomplish getting a cloudfront url for a file uploaded to S3.
  Negative
I already have C# ready to upload files into our company's S3 into specific buckets and folders.
  Negative
However, I have been told that due to costs we shouldn't be serving out of S3, we should be serving from the cloudfront.
  Neutral
So after I upload files (via code) to S3, how do I then generate/retrieve a cloudfront url for it?
  Negative
563e312061a80130652675b8	X	I don't know about the .
  Negative
net specific code, but basically you create a distribution that points to your s3 bucket.
  Positive
The URL you use will be the domain you get back from creating the distribution + the s3 object's name that you want to download.
  Negative
The documentation on this is here
563e312061a80130652675b9	X	just wondering if you have a solution to simailr problem stackoverflow.com/questions/24431130/…
563e312061a80130652675ba	X	i have a web application running on tomcat7 and mySql, now i want to deploy it to aws.
  Negative
.
  Neutral
the application need to write file on disk (such as images uploaded by users) some one can help me pointing out how to configure a good infrastructure in aws for my need?
  Negative
i read this: http://aws.amazon.com/elasticbeanstalk/ , i think that my needs are an EC2 instance for running tomcat and an Amazon RDS whit mySql... i need something else for R/W file ?
  Negative
i need to change my code in some way in order to make it work on aws?
  Neutral
thanks in advance, Loris
563e312061a80130652675bb	X	Elasticbeanstalk is a good way to get started with an application deployment at AWS.
  Negative
For persistent file storage you can use S3 or an EBS volume.
  Negative
S3 allows you to read and write using amazon's SDK/API.
  Positive
I am using this on a java application running at AWS and it works pretty smoothly.
  Positive
http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Client.html It is also possible to mount S3 over NFS, you can read some interesting points in this answer: How stable is s3fs to mount an Amazon S3 bucket as a local directory With EBS you can create a persistent storage volume attached to your EC2 node.
  Negative
Please note that EBS is a block level storage device so you'll need to format it before its usable as a filesystem.
  Negative
EBS allows you to help protect yourself from data loss by configuring EBS snapshot backups to S3.
  Negative
http://aws.amazon.com/ebs/details/ -fred
563e312061a80130652675bc	X	See if this post[stackoverflow.com/questions/12586147/… helps.
  Negative
563e312161a80130652675bd	X	I'm building a new asp.net web api application using straight html 5 and a web api restful service.
  Negative
I'm already using forms authentication and the [Authorize] attribute to secure my web api calls.
  Negative
I'm trying to stay true as much as I can to the restful principles.
  Negative
I'm mimicking what an existing app does that uses two-factor authentication and uses asp.net web forms.
  Positive
The two-factor authentication is not used for logging in, but for an additional task of remoting into another machine through the site and a plugin.
  Negative
The existing web application uses session state to store a pin that is generated and emailed to the user.
  Negative
Then when the user enters in the pin it is checked against the pin in session state.
  Negative
So it seems like my options are... So what out of these options is the best option?
  Negative
Are there other possibilities?
  Neutral
563e312161a80130652675be	X	I'm not 100% sure I understand your architecture, but it seems very relevant that your security should at no point rely on the client.
  Negative
Assume the user has a javascript debugger in their browser (which most people actually do without realizing), and a custom build of your plugin.
  Negative
So the secondary PIN challenge should be embedded on the server side into the "remoting" protocol.
  Negative
If it is something based on RDP or VNC, it should be possible to change the connection password of the given user on the fly to the one-time generated PIN.
  Negative
563e312161a80130652675bf	X	I bilieve, you should not pay too much attention to RESTful principles (wait, did I say that?
  Negative
:))... You see, the one side is the theory and the other side is the practice.
  Very positive
In practice, you often need to break RESTful principles for security!
  Negative
Let's take the nonce - it's a big random number generated for each request, which gets then checked on the server, if the same nonce has not been sent before.
  Negative
This involves storing state (nonces) - i.e., it's not stateless, but it's sometimes critical for security.
  Negative
Also, by the way, OAuth, which is often used, is not RESTful.
  Negative
For encypting the PIN you should not use JavaScript client!
  Negative
You can implement some kind of encryption endpoint on the domain JavaScript client is used, though: Your coworker gave a good recommendation!
  Negative
Amazon is one of the best REST API implementations.
  Positive
Also check my idea: Looking for feedback on my REST-style API authentication design and two-factor authentication.
  Negative
563e312161a80130652675c0	X	I suggest hiring someone since you've chosen not attempt anything yourself.
  Negative
563e312161a80130652675c1	X	Yeah, try something yourself and show us your code - other than that we won't solve your troubles for you.
  Negative
Although I can assure you it is possible.
  Positive
The best way in my opinion would be uploading the file to your server and then move it to another (with some sort of remote service) and then delete it from your server.
  Negative
563e312161a80130652675c2	X	You might find some info about uploading APIs here: developers.google.com/+/domains/api/media/insert
563e312161a80130652675c3	X	I have a website and I would like to allow visitors to upload images.
  Negative
I want these images to be stored somewhere so that I can check them out and maybe put them on the website.
  Negative
The 'problem' is that the server doesn't have much space, so if possible I don't want to keep the images on the server and instead have the website right away upload the images to for example a Google+ album.
  Negative
(Doesn't have to be Google+, just an example) So, what I want: * Have users upload images.
  Negative
* Store the images somewhere else so that it doesn't fill up the server * Be able to check out these images myself What would be the best way to achieve this?
  Negative
(If it's possible to begin with)
563e312161a80130652675c4	X	From what I learn that you can run an image hosting service for your website, and upload images via REST api.
  Negative
Take a look at ImageS3, https://github.com/images3/images3-play, with this, you can upload you images to Amazon S3.
  Positive
and manage these images via ImageS3 web admin console.
  Neutral
Hope this answer can help you a little.
  Neutral
563e312261a80130652675c5	X	USE real time streaming from server side.
  Negative
563e312261a80130652675c6	X	@VedantTerkar: how does that help?
  Neutral
563e312261a80130652675c7	X	@Qantas94Heavy look at this: therealtimeweb.com/index.cfm/2005/11/2/…
563e312261a80130652675c8	X	@VedantTerkar: the questioner wants to prevent users from being able to keep the music on their computer -- they can always get the information from the stream or record it off stereo mix.
  Negative
563e312261a80130652675c9	X	if you're using php then it is .
  Neutral
htaccess trick: stackoverflow.com/questions/14024877/…
563e312261a80130652675ca	X	So what about soundcloud for example ?
  Negative
Sometimes the Download button is disabled, but we still can listen.
  Neutral
563e312261a80130652675cb	X	My point is that, with all any form of DRM, if you are able to listen to or view the final content, the consumer can record the content.
  Negative
The best you can hope for is to obfuscate the link to the content.
  Positive
The SoundCloud example you gave certainly allows the listener to record the content, just not through the officially-supported download mechanisms.
  Neutral
If the user can see/hear the content, they can record the content.
  Neutral
Search for "the analog hole" for obvious examples.
  Negative
563e312261a80130652675cc	X	Ok, I will add jingles to it.
  Negative
Thank's.
  Neutral
563e312261a80130652675cd	X	works fine!
  Very positive
http://www.my-website.com/myaudio.mp3 redirects to the main page.
  Neutral
thank you.
  Positive
563e312361a80130652675ce	X	Your welcome!
  Positive
:) glad to help
563e312361a80130652675cf	X	hum... sorry but it seems <audio src="myaudio.mp3" /> does not work now.
  Negative
Direct download is blocked, but my page cannot get the file.
  Negative
563e312361a80130652675d0	X	Try putting the song in a folder and not the main directory
563e312361a80130652675d1	X	I think (tested) that this is the best solution : stackoverflow.com/questions/10236717/….
  Negative
It works as well
563e312361a80130652675d2	X	I mean, I want to share an audio file without letting users to download it.
  Negative
Here are my tries : Is it possible ?
  Neutral
If so, how ?
  Neutral
563e312361a80130652675d3	X	The correct answer is that, if you allow your users to listen to the music, they will, by definition, be able to record said music.
  Positive
There is a very clear distinction between security, which you are requesting, and obfuscation, which you actually want.
  Negative
563e312361a80130652675d4	X	Add a .
  Neutral
httacess function to block users from downloading files in a path or a whole entire folder.
  Negative
You can also make a password for files also so they wont be able to be deleted without a password This goes in your .
  Negative
htacess file The last RewriteRule change the download-file.php to your actually php or html file with the download.
  Negative
563e312361a80130652675d5	X	It's possible using Amazon S3 (similar to the way Soundcloud does it) to generate secure mp3 links for use in your HTML5 player.
  Negative
You generate a secure mp3 on S3 that is valid for a very short time (seconds or minutes), there by prevent someone from copying and sharing the link.
  Negative
You will need to generate the link dynamically using SDK/API.
  Negative
See example of how to use PHP to generate secure links.
  Negative
563e312361a80130652675d6	X	@PhilippeBossu No idea.
  Negative
Did you try to ask Nomadesk support about this?
  Neutral
Asking them would be more effective for both you and them.
  Negative
563e312461a80130652675d7	X	Propose something like google docs or dropbox, with secured storage for enterprise
563e312461a80130652675d8	X	Documents are stored for offline access = they are stored on your local computer, or on any of your varied remote destinations for access.
  Negative
563e312461a80130652675d9	X	Documents are encrypted = for transmission and if you pay the additional amount for their super secured service.
  Negative
563e312461a80130652675da	X	Solution work on iOS, Android and if possible Windows / Mac OS
563e312461a80130652675db	X	So, in short.
  Negative
My solution ticks all the boxes.
  Neutral
Stop voting me down!
  Neutral
If you're voting me down, at least comment on WHY you think my answer deserves a down-vote.
  Negative
563e312461a80130652675dc	X	Are there any software solutions or SDKs parts that handle the following: I know that google docs will propose offline access and distribute an application on IPad but I would like to know if there are alternatives ?
  Negative
Thank you Regards
563e312461a80130652675dd	X	See Nomadesk offerings - they do exactly what you need.
  Positive
563e312461a80130652675de	X	You can try one of the following cloud services:
563e312461a80130652675df	X	Try crashplan http://www.crashplan.com/ .
  Negative
563e312461a80130652675e0	X	I'd like to copy a file that's already on S3 to another bucket using the Zend Framework and without having to copy the file to the local filesystem first.
  Negative
How can this be achieved?
  Neutral
If copying to another bucket is not possible, is it possible to just make a copy of an object (without having to copy the file to the local filesystem)?
  Negative
563e312561a80130652675e1	X	Amazon S3 allows you to copy an object between buckets and to create a copy of the object in the same bucket using the PUT copy call - http://docs.amazonwebservices.com/AmazonS3/latest/API/index.html?RESTObjectCOPY.html To create the object copy using Zend Framework you can use the copyObject method in Zend_Service_Amazon_S3
563e312561a80130652675e2	X	Thanks for the help jterrace.
  Negative
The Service Account Authentication example seems to be close to what I need but I don't know what to include or what to do to get the Cloud Storage to work.
  Negative
Unfortunately I'm not using the App Engine but I'll use it if it makes it easier.
  Negative
.
  Neutral
563e312561a80130652675e3	X	I have a private bucket on Google Cloud storage and I am having trouble figuring out how to read/write data to it.
  Negative
Right now I am using Amazon S3 and they make it VERY easy to do.
  Negative
Literally: But with Google Cloud Storage, I am very confused.
  Negative
I've come to the conclusion that I need a signed URL but I really don't want to have to use gsutil everytime I want to grab data (which is a lot).
  Negative
All of this is happening server-side so I can't have the customer do the whole browser pop up and sign into google sort of thing.
  Negative
Is there not a way to just use an API/Secret combination like Amazon S3?
  Negative
Could anyone help me figure out the easiest way to get this to work for my PHP server side application?
  Negative
563e312561a80130652675e4	X	From App Engine, you can use the Google Cloud Storage PHP stream wrapper to write to GCS.
  Negative
If you want your PHP script to work outside of App Engine, there is also the Google APIs Client Library for PHP.
  Negative
There is a GCS PHP example you might want to take a look at.
  Positive
Specifically, in your case, you'd probably want to use Service Account Authentication, for which there is an example of here.
  Negative
563e312561a80130652675e5	X	I need a recommendation for some image hosting with a fancy web-interface for uploading pics and an API (or even better — an iOS SDK) to get those photos in a mobile app.
  Negative
I'm aware of that question: Need recommendation for image hosting with API access.
  Positive
However, I don't need any image-uploading API, what I need is a way to populate galleries of photos to my iOS app users: upload the pics through a web interface, and then be able to retrieve all of them in the app.
  Neutral
So there shouldn't be any user authentication in the mobile app, and the photos are meant to be public.
  Negative
I'm looking into several flickr iOS API libraries, but I'm not sure if any of them can be used without user authentication — e.g. by anonymously pulling the galleries of some specific app-hardcoded user (me).
  Negative
Or maybe I'm overthinking and there's another suitable solution that I'm missing?
  Neutral
Any help is highly appreciated!
  Positive
563e312561a80130652675e6	X	Ok, it turned out that Flickr is more or less the thing I needed — it obviously has a nice web-interface for managing your photos, and there are a bunch of components to access these galleries on iOS device (for instance, https://github.com/devedup/FlickrKit and https://github.com/lukhnos/objectiveflickr).
  Negative
Moreover, you don't need to log in to access publicly available photo sets and collections, so I decided to go for this option.
  Negative
563e312561a80130652675e7	X	I am not sure if you would like to host this image hosting service by yourself.
  Negative
if you don't, then you can take a look at ImageS3, https://github.com/images3/images3-play.
  Negative
An open and free image hosting service run on top of Amazon S3.
  Positive
It is like other cloud image hosting service which provides REST api, web interface admin console.
  Negative
563e312661a80130652675e8	X	Yes, you can sercurely let your visitors upload to S3, without giving away your Amazon AWS credentials, by creating a HMAC signature signature on your server, which then is used by the visitor's browser to upload directly to S3.
  Negative
Have a look here: docs.amazonwebservices.com/AmazonS3/latest/dev/… However, I do not know if that is possible to combine with pause/resume-upload.
  Negative
563e312661a80130652675e9	X	But would it be possible to port this multipart stuff into javascript (or flash/actionscript) and do it in the browser, without giving away aws credentials?
  Negative
563e312661a80130652675ea	X	Thank you Steffen, but my understanding is that the low level doesn't allow to pass from client to S3 directly (without web server), at least that's what the PHP example shows if I'm correct...am I missing something here?
  Negative
563e312661a80130652675eb	X	@style-sheets: There is no way to avoid this other than exploring a client side JavaScript solution using the S3 REST API directly; I don't think it is much of a problem cost/performance wise, insofar EC2 to S3 connections are rather fast and free within one region.
  Very negative
Obviously this approach offloads the pause/resume problem to HTML forms though, which again requires JavaScript as well as modern browsers supporting the File API - maybe How to resume a paused or broken file upload can get you started in case.
  Negative
563e312661a80130652675ec	X	@style-sheets I believe you can accomplish this using a browser plugin like flash, silverlight or java and directly using the REST API.
  Negative
I currently use a silverlight plugin to upload large files (up to 5GB) directly to S3.
  Negative
I haven't implement pause/resume don't use the S3 large file support but it should be possible.
  Neutral
Using a plugin is the only way to achieve broad browser coverage.
  Negative
Checkout this SO thread stackoverflow.com/questions/478799/….
  Neutral
There are lots of links there to various free and not free plugins.
  Negative
563e312661a80130652675ed	X	There is an official JS SDK today.
  Positive
Also, there is an intelligent multipart upload API available.
  Positive
563e312661a80130652675ee	X	(I'm new to Amazon AWS/S3, so please bear with me) My ultimate goal is to allow my users to upload files to S3 using their web browser, my requirements are: My two-part question is: Is it even possible to do this for large files?
  Negative
If so how?
  Neutral
If it's possible to upload directly to S3, how can I handle pause/resume?
  Negative
PS.
  Neutral
I'm using PHP 5.2+
563e312661a80130652675ef	X	The meanwhile available AWS SDK for JavaScript (in the Browser) supports Amazon S3, including a class ManagedUpload to support the multipart upload aspects of the use case at hand (see preceding update for more on this).
  Negative
It might now be the best solution for your scenario accordingly, see e.g. Uploading a local file using the File API for a concise example that uses the HTML5 File API in turn - the introductory blog post Announcing the Amazon S3 Managed Uploader in the AWS SDK for JavaScript provides more details about this SDK feature.
  Negative
My initial answer apparently missed the main point, so to clarify: If you want to do browser based upload via simple HTML forms, you are constrained to using the POST Object operation, which adds an object to a specified bucket using HTML forms: POST is an alternate form of PUT that enables browser-based uploads as a way of putting objects in buckets.
  Very negative
Parameters that are passed to PUT via HTTP Headers are instead passed as form fields to POST in the multipart/form-data encoded message body.
  Negative
[...] The upload is handled in a single operation here, thus doesn't support pause/resume and limits you to the original maximum object size of 5 gigabytes (GB) or less.
  Negative
You can only overcome both limitations by Using the REST API for Multipart Upload instead, which is in turn used by SDKs like the AWS SDK for PHP to implement this functionality.
  Negative
This obviously requires a server (e.g. on EC2) to handle the operation initiated via the browser (which allows you to facilitate S3 Bucket Policies and/or IAM Policies for access control easily as well).
  Negative
The one alternative might be using a JavaScript library and performing this client side, see e.g. jQuery Upload Progress and AJAX file upload for an initial pointer.
  Negative
Unfortunately there is no canonical JavaScript SDK for AWS available (aws-lib surprisingly doesn't even support S3 yet) - apparently some forks of knox have added multipart upload, see e.g. slakis's fork, I haven't used either of these for the use case at hand though.
  Very negative
If it's possible to upload [large files] directly to S3, how can I handle pause/resume?
  Negative
The AWS SDK for PHP supports uploading large files to Amazon S3 by means of the Low-Level PHP API for Multipart Upload: The AWS SDK for PHP exposes a low-level API that closely resembles the Amazon S3 REST API for multipart upload (see Using the REST API for Multipart Upload ).
  Very negative
Use the low-level API when you need to pause and resume multipart uploads, vary part sizes during the upload, or do not know the size of the data in advance.
  Negative
Use the high-level API (see Using the High-Level PHP API for Multipart Upload) whenever you don't have these requirements.
  Negative
[emphasis mine] Amazon S3 can handle objects from 1 byte all the way to 5 terabytes (TB), see the respective introductory post Amazon S3 - Object Size Limit Now 5 TB: [...] Now customers can store extremely large files as single objects, which greatly simplifies their storage experience.
  Negative
Amazon S3 does the bookkeeping behind the scenes for our customers, so you can now GET that large object just like you would any other Amazon S3 object.
  Negative
In order to store larger objects you would use the new Multipart Upload API that I blogged about last month to upload the object in parts.
  Negative
[...]
563e312661a80130652675f0	X	I don't know of any flash uploaders offering anything more powerful than standard HTTP Post but you could develop your own flash software for the client with co-ordinating software on the server.
  Negative
563e312761a80130652675f1	X	stackoverflow.com/a/14916525/6309 might now help, maybe as a zipped p2 update?
  Negative
I mention that in my edited answer below.
  Negative
563e312761a80130652675f2	X	The problem I have is that github serves directories and files as HTML pages.
  Negative
To access the contents of a file there is a "raw" link.
  Positive
However, my update site is made up of several files and folders so I think I need to find a way to get a "raw" directory from github.
  Neutral
Alternatively, can an update site be created in a zipped file where all the files are stored in a single archive?
  Negative
563e312761a80130652675f3	X	@mchr: I don't think an update site can be one single zip file.
  Negative
As for the raw directory listing, that is a good question to support.github.com: support.github.com/discussions/site/… might help: replace 'tree' by 'raw' in your GitHub address.
  Positive
563e312761a80130652675f4	X	It appears that if you use the root of the repository, and use "raw" in the url, as @BrunoMedeiros suggests in his answer, the update site works fine.
  Negative
563e312761a80130652675f5	X	A note on big repositories and Github usage.
  Negative
This links describes what Github expects the limits to be: help.github.com/articles/what-is-my-disk-quota - "Rule of thumb: 1GB per repository, 100MB per file".
  Negative
That should be fine for most Eclipse update sites.
  Positive
If it gets too big, you can reset the underlying Git repo.
  Negative
563e312761a80130652675f6	X	Thanks for sharing about the raw links to serve github folder publicly.
  Negative
563e312861a80130652675f7	X	I tried this (here is my update site) but it seems, oddly enough, that this works for Eclipse on Linux, but from Windows computers I can neither install nor update.
  Negative
Is anyone else experiencing this?
  Neutral
563e312861a80130652675f8	X	I just tried to install your feature (Pig Latin) in Windows, from github.com/eyala/pig-eclipse-update-site/raw/master, and it installed fine.
  Negative
563e312861a80130652675f9	X	Github pages are not a proper place for an update site.
  Negative
See my answer below.
  Positive
It may be fine if your jars are small but overall they advise against placing binaries there.
  Neutral
563e312861a80130652675fa	X	I am using github to develop an eclipse plugin.
  Negative
I would like to have a public eclipse update site for my plugin and wondered if I can use github for this?
  Neutral
I know that github can be used for hosting individual files by using the "raw" links provided on the file information pages.
  Negative
563e312861a80130652675fb	X	Github pages are not a proper place for an update site.
  Negative
Github pages may not properly serve large binary files as explained in this issue.
  Negative
It may be fine if your jars are small but overall they advise against placing binaries there.
  Neutral
Instead they recommend placing binaries in the download section of the repository.
  Positive
I'd be happy if this situation changes because it would be very convenient to publish an update site by pushing to github.
  Positive
For now one would have to use their API to programatically upload files in the download section.
  Negative
Answers to this other question points to some libraries and scripts that uses this API for use within java/maven, perl, ruby, etc.
563e312861a80130652675fc	X	You may now try it in a release page (July 2013).
  Negative
See "Publish a project release (binary/source packages) on Github?"
  Negative
Original answer (January 2013) I have not tested it, but technically, a p2 repository can be defined in any shared path (either filesystem-shared or web-based-shared) You should only need to:
563e312861a80130652675fd	X	Forget the Github project releases feature, that won't work as a true update site (see notes at the end).
  Negative
To achieve what you want, you can create a Github repo, commit/push your p2 repository there and then serve it as an update site, using raw links.
  Negative
So for example, for the repository: https://github.com/some-user/some-repository/ you can serve it as an update site using the link: https://github.com/some-user/some-repository/raw/master/ Notes: Yes, if you open the update site link in a browser, github will give you no file listings, but rather a 404.
  Negative
But that's fine.
  Positive
The Eclipse update site mechanism doesn't need the parent link to be valid.
  Negative
Instead Eclipse will directly look for <update-site URL>/artifacts.jar (or .
  Neutral
xml) and from the information in artifacts.jar, it will itself discover the URLs of the other artifacts stored in the update site.
  Negative
AFAIK, at no point does the Eclipse update mechanism need the web server to do file listings of a directory.
  Negative
Note2: if you use Github project releases, you can only attach a zipped p2 repository to it.
  Negative
That is not a proper update site because it is a static repository: there is no URL to which new releases can be uploaded to.
  Negative
Eclipse won't be able to automatically discover new updates, rather the user will need to download the zip for each new release he/she wants to update to.
  Negative
(Also with a proper update site, only the necessary artifacts for installation/update/query will be downloaded - a minor advantage)
563e312861a80130652675fe	X	http://pages.github.com/ The Github Pages feature allows you to host arbitrary folders of files without git turning each file into a github page.
  Negative
563e312861a80130652675ff	X	No it is not possible anymore, the Downloads API has officially been deprecated.
  Negative
From the GitHub blog: However, some projects need to host and distribute large binary files in addition to source archives.
  Negative
If this applies to you, we recommend using one of the many fantastic services that exist exactly for this purpose such as Amazon S3 / Amazon CloudFront or SourceForge.
  Positive
Check out our help article on distributing large binaries.
  Neutral
See this help article on distributing large binaries.
  Positive
563e312861a8013065267600	X	Note that if the file is public, there's no need for the authorization token: curl -H 'Accept: application/vnd.
  Negative
github.v3.raw' https://api.github.com/repos/owner/repo/contents/path will return the raw file.
  Positive
563e312961a8013065267601	X	Is the -H 'Accept: application/vnd.
  Neutral
github.v3.raw' necessary?
  Neutral
I was able to access a private file without that part.
  Positive
563e312961a8013065267602	X	@NickChammas: without that header I get a JSON response with metadata and the actual file contents base64 encoded, rather than the file as plain text.
  Very negative
563e312961a8013065267603	X	What if the above command gives error as curl: (56) Proxy CONNECT aborted what does that mean.
  Negative
563e312961a8013065267604	X	This does no longer work, github has changed the mechanism.
  Negative
.
  Neutral
563e312961a8013065267605	X	On the CI server, I want to fetch a config file that we maintain on Github so it can be shared between many jobs.
  Negative
I'm trying to get this file via curl, but these approaches both fail (I get a 404):
563e312961a8013065267606	X	The previous answers don't work (or don't work anymore).
  Negative
You can use the V3 API to get a raw file like this (you'll need an OAuth token): curl -H 'Authorization: token INSERTACCESSTOKENHERE' -H 'Accept: application/vnd.
  Negative
github.v3.raw' -O -L https://api.github.com/repos/owner/repo/contents/path All of this has to go on one line.
  Neutral
The -O option saves the file in the current directory.
  Neutral
You can use -o filename to specify a different filename.
  Neutral
To get the OAuth token follow the instructions here: https://help.github.com/articles/creating-an-access-token-for-command-line-use I've written this up as a gist as well: https://gist.github.com/madrobby/9476733 EDIT: API references for the solution are as follows:
563e312961a8013065267607	X	Or, if you don't have a token:
563e312961a8013065267608	X	I was struggling with this for a few minutes until I realized all that is needed is to wrap the url in quotes to escape the ampersand.
  Very negative
That worked for me in my private repo.
  Negative
563e312961a8013065267609	X	(I don't have rep points enough to comment on the "You can use the V3 API to get a raw file like this (you'll need an OAuth token):..." answer, so here's my comment: I ran into an authentication error when the url was redirected to Amazon S3: Only one auth mechanism allowed; only the X-Amz-Algorithm query parameter... Changing from the Authorization header to the ?
  Very negative
access_token= param worked for me.
  Negative
563e312a61a801306526760a	X	My website is hosted in yahoo small business, The database server is MySQL, I want to automate db backup, My main requirement is backup files must go to amazon bucket.
  Very negative
Yahoo small business does not provide access to putty, which could be used to run the backup script.
  Negative
So, I have scheduled a task locally(windows machine) which calls the php backup script on the server and saves the backup files to folder in the server, Now i want to save these files to amazon bucket instead of folder, How to do this.
  Negative
563e312a61a801306526760b	X	Don't quote me entirely on this but, Yahoo in general is very very very limiting in what they allow there clients to do.
  Negative
Best you can do that I can think of off the top of my head is run another scheduled task on your windows machine that will physically download the file then pass it to the bucket.
  Negative
Though pending yahoo doesn't limit it severely like they used to you may even be able to get away with the concept of another scheduled item on your windows machine.
  Negative
Where it just pings another script on your yahoo that will use the Amazon S3 api's to drop the file in a bucket.
  Negative
All in all though your best bet is get off of yahoo, get a cheap VPS where you are open to do a lot more with your server than a standard shared system especially one that limits its customers 10 fold.
  Neutral
I host many of my sites through myhosting.com I have a vps account, i got something like 200 gigs of storage a terabyte of transfer and so many more perks for 36 dollars a month.
  Negative
And I can do virtually anything I want with the server as if it were my own linux based machine to do such with.
  Negative
Anyway thats off topic.
  Negative
The point I am ultimately trying to make is getting on an account like that you can run CRON jobs that is the same thing as the scheduler on your machine but on the same server as your site you want to back up, so no worries about a disconnect between server and home brew machine.
  Very negative
You will also be able to install other 3rd party concepts that may secure your transfer to Amazon every night or do many of other things as well.
  Neutral
.
  Neutral
Just food for thought.
  Neutral
All in all staying on Yahoo is going to really limit your ability.
  Negative
563e312a61a801306526760c	X	I think a big reason to go with S3 would be object durability.
  Negative
How much engineering and management will need to go into a CouchDB deployment that has eleven 9's of durability and near infinite scalability?
  Negative
563e312a61a801306526760d	X	I'm writing a simple web API that revolves entirely around file uploads.
  Negative
Users can upload files to the service via the HTTP-based API, and the service will generate files for users to access and will also need to store them along with the uploaded file.
  Positive
So there will be a lot of files at play.
  Positive
Basically, I'm trying to decide between storing these in CouchDB and storing these in something like Amazon's S3.
  Negative
With CouchDB, I'd probably have a single document for the initial uploaded file, by the user, with the attachment data inline in the _attachments collection.
  Negative
Additional files made by the system would be added to that document.
  Negative
(The service does document conversion, so they upload an Excel XLS and the system generates a PDF, TXT, etc.) I think this would be nice, because one delete on the uploaded document record will also delete the generated PDFs, TXTs, or any other attachments.
  Negative
With S3, I feel the security it knowing that I'm using a hosted solution dedicated entirely to individual file storage.
  Negative
It also dedicates that bandwidth exclusively to those files, and it wouldn't be coming from my API web server.
  Negative
The downsides are that it adds a lot of additional logic to my API code, and now I have to keep a lot of remote files in sync with what my local CouchDB database knows of them.
  Positive
Also, I'd have to deal with request signing and stuff if I wanted end-users to access the files directly off S3.
  Negative
Documents are all stored individually, so deleting the user's uploaded attachment from CouchDB would require me to make several delete queries to S3 for the other files, as well.
  Negative
I'm familiar with S3, and use it in a current project, but CouchDB looks really awesome in how it allows attachments.
  Positive
I'd love to use it, but are there any gotchas or downsides?
  Neutral
Does CouchDB attachments make more sense than S3 in the scenario I described above, with a lot of uploaded files being stored?
  Negative
Thanks
563e312a61a801306526760e	X	I've used couchdb successfully for many projects and several a similar project.
  Negative
You get so much in the box of box with couchdb.
  Negative
The questions I have are what's the average size of your files, and how big do you think your db will go?
  Negative
563e312a61a801306526760f	X	Both solutions are quite sensible: there are pros and cons.
  Negative
One advantage you didn't mention for storing files as CouchDB attachments is that they will be replicated with the data.
  Neutral
It makes it easier for continuous backup, and in your snapshots your data will be coherent with your files.
  Negative
563e312b61a8013065267610	X	Salesforce attachment file size is officially limited to 5MB (doc) but if requested they can increase this limit on a one to one cases.
  Negative
My question: Can I retrieve this newly allowed file size limit using the API.
  Negative
Context: Non-Profits are applying for grants via a web portal (.
  Negative
NET), all data is stored in Salesforce.
  Negative
They are asked to attach files.
  Neutral
We read the file size they try to upload and send an error message if it exceeds 5MB as it will not be accepted by Salesforce.
  Negative
This is to avoid having them wait for few minutes to upload to only be told that the file size is too large.
  Negative
We would like to update our code so that it allows files bigger than 5MB if Salesforce allows it.
  Negative
Can we retrieve this information via the API?
  Neutral
Thank you!
  Positive
563e312b61a8013065267611	X	You can call the getUserInfo() function in the SOAP API, part of the returned data includes the field orgAttachmentFileSizeLimit (this appears to be missing from the docs, but is in the WSDL)
563e312b61a8013065267612	X	I'll recommend going away from Salesforce to store files, more if you'r expecting to hit limits, also there is a limit on the space for storing organization wide, a useful service like Amazon S3 would be very useful, you can then attach the S3 url to your record in case it is needed, it'll be also available for external applications without having to load your org's api consumption.
  Very negative
563e312b61a8013065267613	X	Why don't you use Google Drive?
  Negative
or Dropbox?
  Neutral
563e312b61a8013065267614	X	I want more like google cloud or amazon cloud.
  Negative
.
  Neutral
is there any alternative
563e312b61a8013065267615	X	If your total storage requirements are very small, you can probably afford Google Cloud Storage.
  Negative
Storing a gigabyte over a month will cost you about 8 cents, and downloading that gigabyte every single day will cost you about 12 cents per day.
  Negative
That's about $4 for the whole month.
  Positive
Or you can just use Google Drive for free.
  Neutral
It also has an API: developers.google.com/drive/v2/reference
563e312b61a8013065267616	X	I am a Mtech student and currently doing my project in cloud environment.
  Negative
I am looking for a free cloud storage with small memory to upload and access text files on to cloud.
  Negative
Is there any cloud storage which can help in my requirements.
  Neutral
Thanking you in anticipation
563e312b61a8013065267617	X	Amazon S3 going to be one of the best solution, But you are talking about free.
  Positive
I would suggest you to go for dropbox it's free upto some extent.
  Negative
I already integrated with my rails application, they are providing API and Secret Key for app configuration.
  Negative
Have a look to below link 12 free cloud storage options Five Best Cloud Storage Providers
563e312c61a8013065267618	X	Several possibilities: Amazon S3, Dropbox, GoogleDrive.
  Negative
.
  Neutral
Be aware of network latency if your app is deployed outside the same physical infrastructure.
  Negative
You can find a complete application example working on CloudBees + Amazon S3 (here) However, you can get a similar approach through the runtime and file system storage you want.
  Negative
563e312c61a8013065267619	X	Thanks for you suggestions.
  Negative
Looks like multi-iteration hashing with sufficiently long salts will be secure enough (even when salt values are stored with hashed passwords).
  Negative
563e312c61a801306526761a	X	That sounds fine as a storage system for all but the most stringent requirements.
  Positive
Remember to be careful with how passwords are sent across the wire, to expire people's session identifiers whenever they log in, and all of the other equally important security considerations that plague web authentication.
  Negative
563e312c61a801306526761b	X	Thanks, I am looking at AWS docs now.
  Negative
563e312c61a801306526761c	X	I'm building a non-browser client-server (XULRunner-CherryPy) application using HTTP for communication.
  Negative
The area I'm pondering now is user authentication.
  Negative
Since I don't have substantial knowledge in security I would much prefer using tried-and-tested approaches and ready-made libraries over trying to invent and/or build something myself.
  Negative
I've been reading a lot of articles lately and I can say all I have been left with is a lot of frustration, most of which contributed by this and this blog posts.
  Very negative
What I think I need is: So the question is: what are the modern (headache-free preferrably) techniques and/or libraries that implement this?
  Negative
(No sensitive information, like credit card numbers, will be stored).
  Negative
I've been looking at OAuth and they have a new revision which they strongly recommend to use.
  Negative
The problem is the docs are still in development and there are no libraries implementing the new revision (?)
  Negative
.
  Neutral
563e312c61a801306526761d	X	This may not be a complete answer, but I would like to offer some reassuring news about rainbow tables and the web.
  Negative
I wouldn't worry too much about Rainbow Tables with regards to the web for the following reasons: (1) Rainbow table cracks work by examining the hashed password.
  Negative
On the web, the hashed password is stored on your database so to even consider using rainbow tables one would first need to hack your entire database.
  Negative
(2) If you use a salt as most password storage systems do, then rainbow tables rapidly become unfeasible.
  Negative
Basically a salt adds a series of extra bits to the end of a given password.
  Positive
In order to use a rainbow table, it would need to accommodate the extra bits in each plaintext password.
  Negative
For example the first link you showed us had a rainbow table implementation that could crack up to 14 characters in a password.
  Positive
Therefore if you had more than 14 bytes of a salt that system would be useless.
  Negative
563e312c61a801306526761e	X	Amazon Web Services, OpenID, and OAuth have examples of request signing.
  Negative
Amazon Web Services is an easy example to follow because there isn't a more complex protocol around the interactions.
  Negative
They basically involve having the client or server sign a request by hashing all of its fields with a previously set up key (or keypair), and having the other end verify the signature by doing the same.
  Negative
Replaying the hash is prevented by including a nonce or timestamp in the fields.
  Negative
Setting up keys or other credentials to allow this can be done over SSL, and it should be noted that one of the motivation of OAuth WRAP is to replace some or all of this request signing with SSL, for ease of implementation.
  Very negative
563e312c61a801306526761f	X	After a lot of poking around and trying to write my own prototype based on Amazon S3 design which (I thought) was pretty secure, I found this excellent website which has answers to all my questions, an Enterprise Security API Toolkit, and much, much more: OWASP.
  Negative
563e312c61a8013065267620	X	My Spark applications take a folder as the input which contains lots of text file.
  Negative
How can I get filename of each input split programatically?
  Neutral
563e312d61a8013065267621	X	Normally you cannot retrieve the filename from which input originated.
  Negative
However if you use Hadoop HDFS FileSystem api you can list the content of the directory.
  Negative
And iterate over all files.
  Negative
But this is not pure spark program anymore.
  Negative
And it is dependent on the storage layer used (HDFS, amazon s3, etc).
  Negative
563e312d61a8013065267622	X	try this.it worked for me.
  Negative
Hope it will help you.
  Neutral
563e313061a8013065267623	X	So, you're basically asking how to do asymmetric crypto?
  Negative
563e313061a8013065267624	X	I am usure about the file formats that should be used to feed the App with the keys.
  Negative
563e313061a8013065267625	X	You tell me.
  Negative
Which encryption toolset are you using?
  Negative
PGP/GPG, for example, uses keys that look like this y3xz.com/yuvadm.pgp
563e313061a8013065267626	X	Thanks.
  Negative
What about the file format for the keys?
  Neutral
563e313061a8013065267627	X	That completely depends on the encryption/toolset you are using.
  Neutral
But I think PublicKey even implements Serializable, so you can just dump it into a file.
  Negative
Also, you can read through the official Java security doc.
  Neutral
563e313161a8013065267628	X	You cannot just accept every public key within an automated message.
  Negative
Your key needs some kind of verification, otherwise anybody can perform a man in the middle attack (among others).
  Negative
You can either do this out of band, or using a PKI method of verification (certificate chain).
  Negative
563e313161a8013065267629	X	My Java app needs to handle encrypted files.
  Negative
This is the workflow: Amazon S3 provides the Java classes for download/upload, decryption/encryption.
  Negative
This API takes as input java.security.KeyPair.
  Negative
I am unsure how the customer should supply the key to My Java app, so that the app can get the key as java.security.KeyPair?
  Negative
What would be the proper way to exchange the keys between Customer and App?
  Neutral
Which key file format could be used?
  Neutral
563e313161a801306526762a	X	Ussually, assymmetric encryption/decryption works like this: Now you have got a file from the customer.
  Negative
To be able to send back encrypted messages, you need another public/private key pair.
  Negative
This time, the customer must be the only one knowing the private key.
  Negative
He can - for instance - put the public key in his file that he has sent to you.
  Positive
Anyway, somehow you need to get a public key from him.
  Neutral
With that key, you encrypt your files and send them to Amazon S3.
  Negative
The user picks them up and decrypts them with his private key.
  Negative
So, the customer must not give you a java.security.KeyPair, because those contain the private key.
  Negative
It's unsafe to send the private key.
  Neutral
But he can send you the public key as a java.security.PublicKey.
  Neutral
I think the best way would be to send it to you either within the file he supplies anyway, or within a separate file that he uploads at the same time and besides the supplied file.
  Positive
563e313161a801306526762b	X	The problem is that you don't have a method of distributing trust yet.
  Negative
Fortunately there is one that works reasonably well: TLS.
  Positive
TLS certificates are stored within the browser (and in the JRE, if you require a thick client instead).
  Negative
Your key pair should be generated locally (or on a secured machine and imported).
  Negative
The private key should be kept safe the whole time.
  Negative
The customer connects to your site using TLS, and downloads your public key.
  Negative
Then the customer uploads the public key of his key pair.
  Neutral
This can be performed during some setup/configuration phase.
  Positive
Now the customer can encrypt files for you, and you can encrypt files for the customer.
  Positive
Note that TLS already provides encryption (confidentiality).
  Negative
So what you have gained is that files are protected during storage, after they have been transported.
  Negative
Once you have trust in the public key (and a trustworthy system) you could send files over plain HTTP.
  Negative
Adding a signature is pretty important, otherwise anybody can replace the files in storage.
  Positive
Some audit logging is probably required as well, otherwise files may be removed.
  Negative
Other schemes are possible (I prefer a PGP scheme for file encryption/decryption), but they require out of band communication of the keys.
  Negative
Note that this is just the basic scheme, there are a lot of pitfalls, but working out a specific security architecture for you application is clearly off topic.
  Negative
563e313161a801306526762c	X	Thanks a lot!
  Positive
Few weeks ago I tried to launch instance of opengeo and inserted this script link in user text and I was able to stop instance but I don't understand this script
563e313161a801306526762d	X	I'm a newbie and my knowledge is very low, so sorry if I'm not making myself clear.
  Very negative
I would like to launch an EC2 instance of the OpenGeo AMI, but I noticed that I can't stop this instance because it doesn't use EBS.
  Negative
I tried to launch an instance of OpenGeo and then attach an EBS volume, but there is still no stop function available.
  Negative
I think I should create an EBS volume with commands inserted in user text when I'm launching the instance, but that is only a theory.
  Negative
Can you provide me a solution, thanks.
  Positive
563e313161a801306526762e	X	Just realized that there appears to be a free OpenGeo Suite Community Edition on Amazon Web Services as well indeed.
  Negative
Consequently you might be able to succeed by means of the articles referenced in my initial answer below.
  Negative
Good luck!
  Positive
Given this appears to be a paid AMI, I'm afraid that it isn't possible, see the FAQ Can I create paid AMIs backed by Amazon EBS snapshots?
  Negative
: No.
  Neutral
Currently Amazon DevPay only supports AMIs that are backed by Amazon S3.
  Negative
This means that your customers cannot use Amazon EC2 instances that leverage Amazon EBS backed root devices yet.
  Negative
Otherwise I'd recommend to first ask the AMI creator to provide EBS backed ones instead or as well, see Eric Hammond's excellent summary You Should Use EBS Boot Instances on Amazon EC2 for why this is almost always preferable.
  Negative
Finally, section Converting Amazon EC2 instance store-backed AMIs to EBS-Backed AMIs within Creating Amazon EBS-Backed AMIs answers your question, which is no easy process though: There's no simple API or button in the AWS Management Console that converts an existing Amazon EC2 instance store-backed AMI to an Amazon EBS-backed AMI.
  Very negative
[...] The required steps are only briefly outlined in the following paragraphs there accordingly.
  Negative
There are quite some other posts around going into more detail though, e.g. Creating an EBS-backed AMI from an S3-backed AMI, Amazon EC2 – Boot from EBS and AMI conversion or Amazon EC2 - Swap root instance store device with EBS device.
  Negative
563e313261a801306526762f	X	the only way to save more than one file at once is to zip them together.
  Negative
563e313261a8013065267630	X	I want to know if they are a way to download multiple files with javascript and package it,i knew that there is the file system api but i can have only 5mb per object and it support is only for modern browsers.
  Negative
Now i have a Store server in amazon s3(Just for music and photos) and a Dedicated server in an other company, the user have the way to download multiple songs but its a slow process because the server have to request to amazon s3 download each song a zipped and then fired to the user to download so how i can do this with out download two times, i thought in javascript but im not sure how.
  Negative
Thanks
563e313261a8013065267631	X	In terms of s3 urls, are there really 2 kinds?
  Negative
And why?
  Neutral
What are the different syntaxes?
  Neutral
and Is this it?
  Neutral
Why are there 2?
  Negative
Are there more?
  Neutral
Are these correct?
  Neutral
563e313261a8013065267632	X	The additional functionality of providing multiple URL patterns for an object in the S3 is due to the Virtual Hosts and Website Hosting and publishing the data from the root directory.
  Negative
I got this info from In the Bucket starting URL style - bucket.s3.amazonaws.com/key you can simple add the files like favicon, robots.txt etc where as in the other URL pattern - s3.amazonaws.com/bucket/key - there is no notion of root directory where you can put those files.
  Negative
Content Snippet from AWS S3 Page - Virtual Hosting of Buckets : In general, virtual hosting is the practice of serving multiple web sites from a single web server.
  Negative
One way to differentiate sites is by using the apparent host name of the request instead of just the path name part of the URI.
  Negative
An ordinary Amazon S3 REST request specifies a bucket by using the first slash-delimited component of the Request-URI path.
  Negative
Alternatively, you can use Amazon S3 virtual hosting to address a bucket in a REST API call by using the HTTP Host header.
  Negative
In practice, Amazon S3 interprets Host as meaning that most buckets are automatically accessible (for limited types of requests) at http://bucketname.s3.amazonaws.com.
  Negative
Furthermore, by naming your bucket after your registered domain name and by making that name a DNS alias for Amazon S3, you can completely customize the URL of your Amazon S3 resources, for example, http://my.bucketname.com/.
  Negative
Besides the attractiveness of customized URLs, a second benefit of virtual hosting is the ability to publish to the "root directory" of your bucket's virtual server.
  Negative
This ability can be important because many existing applications search for files in this standard location.
  Positive
For example, favicon.ico, robots.txt, crossdomain.xml are all expected to be found at the root.
  Negative
563e313261a8013065267633	X	Probably not possible, since CloudTrail streams data to CloudWatch Logs.
  Negative
CloudWatchLogs then generates a metric in CloudWatch.
  Negative
This can then trigger an Alarm, which sends a notification to SNS, which sends a message to SQS.
  Negative
Unfortunately, the instance information is not stored in the CloudWatch metric, so it can't be passed to the next process.
  Negative
Closest option is to configure Auto Scaling to send a notification when an instance is terminated, but that probably doesn't match your use-case.
  Negative
563e313261a8013065267634	X	@john can you thinknof anyother way of doing it ?
  Negative
563e313261a8013065267635	X	I basically need to know the information of nodes terminated automatically
563e313361a8013065267636	X	I have configured an Alarm on CloudTrail events.
  Negative
The metric of the alarm is to trigger it when it finds the information in the logs that an instance is terminated.
  Negative
The information sends a message to an SNS topic which in turn calls SQS.
  Positive
It is all working as of now.
  Neutral
However, when I read SQS I can only see the information of the alarm, but I would like to obtain details of the instance that got terminated.
  Negative
For example, below is what I see: But I instead I want to see the instance id information which was there in the CloudTrail logs :
563e313361a8013065267637	X	AWS CloudTrail delivers log files to your Amazon S3 bucket approximately every 5 minutes.
  Negative
The delivery of these files can then be used to 'trigger' some code that checks whether a certain activity has occurred.
  Negative
And a good way to run this code is AWS Lambda.
  Positive
The basic flow is:  Here are two articles that describe such a setup:
563e313361a8013065267638	X	Why are you using S3 for file delivery.
  Negative
You should use CLoudFront.
  Negative
.
  Neutral
which does.
  Neutral
563e313361a8013065267639	X	+1 That's a good trick, but CloudFront doesn't gzip content.
  Negative
It will serve gzipped content only if the origin serves gzipped content.
  Negative
563e313361a801306526763a	X	Here's CloudFront's instructions for using gzipped content - and they don't automatically serve gzipped content, the URL for the gzipped version needs to be generated on the client side if you're using S3.
  Negative
docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/…
563e313361a801306526763b	X	Can I use JavaScript to detect if the user's browser supports gzipped content (client side, not node.js or similar)?
  Negative
I am trying to support the following edge case: There are a lot of possible files that can load on a particular web app and it would be better to load them on demand as necessary as the application runs rather than load them all initially.
  Negative
I want to serve these files off of S3 with a far-future cache expiration date.
  Negative
Since S3 does not support gzipping files to clients that support it, I want to host two versions of each file -- one normal, and one gzipped with content-type set to application/gzip.
  Negative
The browser of course needs to know which files to request.
  Negative
If JavaScript is able to detect if the browser supports gzipped content, then the browser will be able to request the correct files.
  Negative
Is this possible?
  Neutral
563e313361a801306526763c	X	Javascript can't, but you can use Javascript to detect wether or not the browser supports gzipped content.
  Negative
I commented above and would just like to reiterrate, you should use CloudFront anyway, which does gzip content.
  Negative
If you are using S3, then there is zero reason why you would not want to use CloudFront, however, for the purposes of answering your question... This blog post perfectly addresses how you would detect if the browser supports Gzip.
  Negative
http://blog.kenweiner.com/2009/08/serving-gzipped-javascript-files-from.html Here is a quick summary: 1) Create a small gzipped file, gzipcheck.js.jgz, and make it available in CloudFront.
  Negative
This file should contain one line of code: 2) Use the following code to attempt to load and run this file.
  Negative
You'll probably want to put it in the HTML HEAD section before any other Javascript code.
  Negative
If the file loads, it sets a flag, gzipEnabled, that indicates whether or not the browser supports gzip.
  Negative
563e313361a801306526763d	X	Well cloudfront does not gzip content automatically.
  Negative
Till the time Amazon decides to do automatic gzip compression in S3 and Cloudfront one has to use the below workaround.
  Negative
563e313361a801306526763e	X	I am trying to pass an image created by Cordova's camera plugin to Amazon Web Service's S3.
  Negative
In the past, I have used the HTML File API to create my S3 params, and been able to pass the file object.
  Negative
I can't directly link to how you do this, but there is an example on this page under the section 'example uses the HTML5 File API to upload a file on disk to S3'.
  Neutral
But this file has not been inserted by an input element, so I can't access anything like files[0] - the file is returned by Cordova either as a base64 or an available file location.
  Negative
So I am trying to figure out how I would replicate that action using Cordova information.
  Negative
The solution I have worked off of is found here which results in the code below: This process works, but: What I would like to do is send the actual file object instead of messing with this base64.
  Negative
Alternatively, I would take a better/smarter/faster way to handle the base64 to S3 process.
  Negative
563e313361a801306526763f	X	I was able to use the 'JavaScript Canvas to Blob' polyfill to create a blob from my Base64, and then send the returned Blob on to S3.
  Negative
Because it is now a blob instead of a coded string, I can use getSignedURL in the s3 APK to reference the image.
  Negative
The only change from above is: var theBody = btoa(evt.target.
  Negative
_result); Becomes var theBody = window.dataURLtoBlob(evt.target.
  Negative
_result); https://github.com/blueimp/JavaScript-Canvas-to-Blob
563e313461a8013065267640	X	What why would someone down vote this?
  Negative
Can you explain?
  Neutral
This works for me!
  Positive
563e313461a8013065267641	X	Voted down because this post specifically asks about S3 and S3 is not a normal ec2 instance so it is not running an SSH server.
  Negative
You need to use an HTTP protocol to talk to it.
  Negative
563e313461a8013065267642	X	Is it possible to upload a file to S3 from a remote server?
  Neutral
The remote server is basically a URL based file server.
  Negative
Example, using http://example.com/1.jpg, it serves the image.
  Negative
It doesn't do anything else and can't run code on this server.
  Negative
It is possible to have another server telling S3 to upload a file from http://example.com/1.jpg
563e313461a8013065267643	X	If you can't run code on the server or execute requests then, no, you can't do this.
  Negative
You will have to download the file to a server or computer that you own and upload from there.
  Negative
You can see the operations you can perform on amazon S3 at http://docs.amazonwebservices.com/AmazonS3/latest/API/APIRest.html Checking the operations for both the REST and SOAP APIs you'll see there's no way to give Amazon S3 a remote URL and have it grab the object for you.
  Negative
All of the PUT requests require the object's data to be provided as part of the request.
  Negative
Meaning the server or computer that is initiating the web request needs to have the data.
  Negative
I have had a similar problem in the past where I wanted to download my users' Facebook Thumbnails and upload them to S3 for use on my site.
  Negative
The way I did it was to download the image from Facebook into Memory on my server, then upload to Amazon S3 - the full thing took under 2 seconds.
  Negative
After the upload to S3 was complete, write the bucket/key to a database.
  Negative
Unfortunately there's no other way to do it.
  Negative
563e313461a8013065267644	X	I think the suggestion provided is quite good, you can SCP the file to S3 Bucket.
  Positive
Giving the pem file will be a password less authentication, via PHP file you can validate the extensions.
  Negative
PHP file can pass the file, as argument to SCP command.
  Negative
The only problem with this solution is, you must have your instance in AWS.
  Negative
You can't use this solution if your website is hosted in other Hosting Providers and you are trying to upload files straight to S3 Bucket.
  Negative
563e313461a8013065267645	X	Technically it's possible, using AWS Signature Version 4, Assuming your remote server is the customer in the image below, you could prepare a form in the main server, and send the form fields to the remote server, for it to curl it.
  Very negative
Detailed example here.
  Neutral
563e313461a8013065267646	X	you can use scp command from Terminal.
  Negative
1)using terminal, go to the place where there is that file you want to transfer to the server 2) type this: N.B. Add "ec2-user@" before your ec2blablbla stuffs that you got from the Ec2 website!!
  Negative
This is such a picky error!
  Negative
3) your file will be uploaded and the progress will be shown.
  Negative
When it is 100%, you are done!
  Positive
563e313461a8013065267647	X	I'm using the LitS3 library to help upload photos from my ASP.NET MVC application to Amazon S3.
  Negative
I've read through all the documentation, googled around, and i can't figure out how to set the cache-control header for the photos when i upload them.
  Negative
I know you can do it with the REST API, but as i'm using the LitS3 library, that's not an option (unless i scrap the library altogether).
  Negative
Has anyone figured out how to do it?
  Negative
I see the documentation there is a section for "want more flexibility" - which seemingly gives access to nearly 100% of the API, but can't see how i can apply that to my situation.
  Negative
Here's how i'm currently uploading: Where inputStream is a Stream, that i get from the HttpPostedFileBase.InputStream in my MVC action.
  Negative
None of the AddObject overloads support setting any other header apart from content-type.
  Negative
So it looks like i need to dig deeper and use a lower-level method, but as i said - just can't find out how.
  Negative
Can anyone help?
  Neutral
563e313461a8013065267648	X	Got it!
  Positive
Thanks to this thread, which doesn't concern cache-control, but it shows how to use AddObjectRequest with a given Stream.
  Negative
Here's the working code, if anyone else is interested:
563e313561a8013065267649	X	I wanted to go the S3 route initially, but the volume of uploads is fairly low plus it's tough to sell this sort of thing to our non-technical customers who are super concerned about security.
  Negative
I would think it would be relatively simple to access files on the filesystem of a server on the same local network.
  Neutral
563e313561a801306526764a	X	In my Rails app, for HIPAA reasons, I need to keep all my data stored on a separate server from my web application.
  Negative
This is simple to do with the database, but what's the best way to allow my rails app (on the web server) to access uploads on the filesystem of the database server?
  Neutral
Or should I just store the uploads in the database (mysql)?
  Neutral
I'm using Rails 3.2 and Paperclip, but could switch to Carrierwave or another solution Thanks!
  Negative
563e313561a801306526764b	X	Or should I just store the uploads in the database (mysql) No.
  Negative
I will point you here for a better explanation Storing Images in DB - Yea or Nay?
  Negative
what's the best way to allow my rails app (on the web server) to access uploads on the filesystem of the database server I would probably create another web server with a separate rails app that was responsible solely for serving up files from it's local filesystem through some authenticated API.
  Negative
It looks like people do use Amazon S3 successfully with HIPAA-compliant websites for file storage.
  Negative
563e313561a801306526764c	X	you can configure paper clip to store your documents on amazon s3 https://github.com/thoughtbot/paperclip#storage http://rubydoc.info/gems/paperclip/Paperclip/Storage/S3
563e313661a801306526764d	X	Is it possible to have growing files on amazon s3?
  Negative
That is, can i upload a file that i when the upload starts don't know the final size of.
  Negative
So that I can start writing more data to the file with at an specified offset.
  Negative
for example write 1000 bytes in one go, and then in the next call continue to write to the file with offset 1001, so that the next bytes being written is the 1001 byte of the file.
  Negative
563e313661a801306526764e	X	Amazon S3 indeed allows you to do that by Uploading Objects Using Multipart Upload API: Multipart upload allows you to upload a single object as a set of parts.
  Positive
Each part is a contiguous portion of the object's data.
  Negative
You can upload these object parts independently and in any order.
  Positive
If transmission of any part fails, you can retransmit that part without affecting other parts.
  Negative
After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object.
  Positive
[...] One of the listed advantages precisely addresses your use case, namely to Begin an upload before you know the final object size - You can upload an object as you are creating it.
  Positive
This functionality is available by Using the REST API for Multipart Upload and all AWS SDKs as well as 3rd party libraries like boto (a Python package that provides interfaces to Amazon Web Services) do offer multipart upload support based on this API as well.
  Negative
563e313661a801306526764f	X	Is it possible to expose Amazon S3 account bucket (shared by ACL setings) to the users setup using new Amazon AIM API under different account?
  Negative
I'm able to create working IAM policy when related to the users and objects belonging to a single account.
  Positive
But as it seems this no longer works when two different accounts are involved - despite account 2 being able to access account 1's bucket directly.
  Negative
Sample policy is: In this case AIM user is able to list test.doom bucket (owned by the same AWS account) and not 'test1234.doom' bucket (owned by the different AWS account).
  Negative
This is despite one account having correct ACL permissions to access the other bucket.
  Neutral
563e313661a8013065267650	X	It looks like this can't be done.
  Negative
http://aws.amazon.com/iam/faqs/#Will_users_be_able_to_access_data_controlled_by_AWS_Accounts_other_than_the_account_under_which_they_are_defined Although it looks like in the future they might be allowed to create data under another account.
  Negative
http://aws.amazon.com/iam/faqs/#Will_users_be_able_to_create_data_under_AWS_Accounts_other_than_the_account_under_which_they_are_defined
563e313661a8013065267651	X	The more obvious way to avoid the potential problem seems like it would be to use the virtual url format for an S3 bucket, https://bucket-name.s3-us-west-2.amazonaws.com/key (substituting the appropriate region, or just "s3" for US-Standard) unless your bucket name has a dot in it... in which case, create a bucket name without a dot... no?
  Negative
563e313661a8013065267652	X	Good idea.
  Neutral
That does work exactly how I had hoped S3 would and avoids any possible same-origin problems.
  Negative
Thank you!
  Positive
563e313661a8013065267653	X	I have a web page that uses JavaScript to access users' Gmail accounts through the Gmail API using OAuth 2.0, and I'd like to host the HTML and JS files on Amazon's S3 service.
  Very negative
For this reason, I've authorized https://s3-us-west-2.amazonaws.com/ as one of the origin URIs in the Google Developer Console's credential page.
  Negative
My page works great, but it has me wondering: If my page shares the same origin URI as every other web page hosted on S3, wouldn't this mean that another page would satisfy the same-origin policy and be allowed access to my page's access tokens?
  Negative
For example, say a user just granted my app access to his Gmail account.
  Negative
Google sees an approved origin URI and sends back an access token.
  Negative
Now imagine some other page hosted on S3 had copied my app's Client ID (publicly stored as a variable in JS) and was repeatedly requesting for an access token from Google.
  Negative
Once my user grants that access to my page, won't the other page also receive the token from Google since it satisfies the same origin URI?
  Negative
I realize one solution to this potential problem is to host my web page on a domain that I have control over, but I'm curious if that is the only way.
  Negative
563e313661a8013065267654	X	I also couldn't figure this out.
  Negative
I'm not sure if Google's docs are to blame, but it's hard to construct such a solution.
  Negative
I ended up using the Datastore's Image API instead.
  Negative
563e313661a8013065267655	X	Brandon, just to clarify, if I have an App Engine app, and I want users to upload to the Google Cloud Storage, I use Blobstore Python API?
  Negative
563e313661a8013065267656	X	You certainly can, yes.
  Positive
563e313761a8013065267657	X	I'm trying to set up a basic python-based google app engine site that allows users to upload files to google cloud storage (mostly images) I've been going through the documentation for the JSON API and the GCS client library overview (as well as blobstore etc) and still don't have a good handle on which is the best method and how they are related.
  Negative
Would be great if someone could give an overview of this or point me to some resources I can check out Also, any sample code that's relevant would be really helpful.
  Negative
I've been able to run the upload samples here but not sure if they're useful for an app engine setup: https://github.com/GoogleCloudPlatform/storage-file-transfer-json-python Thanks!!
  Negative
563e313761a8013065267658	X	Google Cloud Storage has two APIs -- the XML API and the JSON API.
  Negative
The XML API is XML based and very like the Amazon S3 API.
  Negative
The JSON API is similar to many other Google APIs, and it works with the standard Google API client libraries (for example, the Google API Python library).
  Negative
Both of these APIs can be used from anywhere, with or without App Engine, and are based on RESTful HTTP calls.
  Negative
App Engine provides a couple of standard ways to access Google Cloud Storage.
  Negative
The first is built into App Engine's API as a feature called the "Google Cloud Storage Python API".
  Negative
This does not directly use either the XML or the JSON API.
  Negative
It's deprecated and no longer recommended.
  Positive
The second App Engine library is called the "Google Cloud Storage Python Client Library" and is not part of the core App Engine API.
  Negative
Instead, it's a Python library put out by Google that you can download and add to your application like any other library.
  Negative
This library happens to be implemented using the XML API.
  Negative
It provides a few extra features that are useful for App Engine users, such as the ability to serialize an upload while it's in progress.
  Neutral
There's an example of using this library included as part of the download, in the python/demo directory.
  Negative
You can also see it online.
  Positive
Equivalents of these tools also exist in Java and Go.
  Negative
There's no need for users to use the App Engine-specific libraries unless they find them to be useful.
  Negative
The standard Python library or even just hand-written HTTP calls using urlfetch will work just as well.
  Negative
The App Engine library merely provides some useful extras for App Engine users.
  Negative
App Engine also have a "Blobstore Python API".
  Positive
This is a feature specific to App Engine and distinct from Google Cloud Storage, except that it provides a few hooks into Google Cloud Storage, such as the ability to store files in Google Cloud Storage using the Blobstore API.
  Negative
563e313761a8013065267659	X	interesting plugin for the S3 you got there.
  Positive
563e313761a801306526765a	X	I can't agree more.
  Negative
563e313761a801306526765b	X	philsturgeon is quite the programmer, I really trust his work.
  Positive
563e313761a801306526765c	X	@IamJohnGalt I can't agree with you more I found it fairly annoying ones I opened up cake template files for the actually pages and what did i see?
  Negative
__d(code code) this kind of syntax is just plain stupid and in a way will make huge mess as this is fairly large project with allot of code.
  Very negative
563e313761a801306526765d	X	@burzum actually Yii seemed to have had a bigger file size I also considered Laravel Framework.
  Negative
But as I said Yii seemed-ed to be bigger of the two aw 23.
  Negative
xx MB, and from I read people are not very found of CakePHP.
  Negative
So I will probably go with CI or Laravel in the long term.
  Negative
563e313761a801306526765e	X	@IamJohnGalt to be honest John, Cake reminds me of PERL ones it becomes a total hacked together mess ones the code becomes larger and larger.
  Negative
Also would you recommend Laravel or Symphony?
  Negative
If not I will just go with CI or Laravel as I do have a developer on my team familiar with LARAVEL and he said its pretty amazing
563e313761a801306526765f	X	@burzum OOP can become a mess on its own, not mentioning that this project got 3 developers of different skill level and the way they write code in general.
  Neutral
Having odd syntax which I never seen before trying out CakePHP was annoying.
  Negative
It took me seconds to create new pages with CI and I did not have such luck with CakePHP.
  Negative
563e313761a8013065267660	X	@IamJohnGalt actually I seen more and more articles pooping up about FuelPHP played with it today also seems like a great framework.
  Positive
563e313761a8013065267661	X	I am building a web app designed to provide users with file-sharing ability, this is a safe app as the service is paid and will be using Stripe Payment API However I can't decide which php framework would be the best for this task.
  Very negative
I am very clear that this can be done with both, but I want the framework to be light on the server and not eat allot of server resources like RAM and CPU as I am limited at this moment, but should the app grow higher then resources will be increased.
  Neutral
I looked at Code Igniter and CakePHP.
  Negative
In my view CakePHP seems much more lighter then CodeIngiter but I never used either one so I can't judge.
  Negative
All I need the framework to do is Provide up-most security possible against XSS, Injections Able to work with file system and amazon s3 Play nice with amazon s3 and provide easy upload method to s3 Provide simple template scheme Provide fastest possible database and general response time Work with Stripe Payment API Work with PHP 5.3.x and MySQL 5.3.x as efficiently This is really it, if you can provide some pro feedback on which one would do the best or just do great for the task of file-sharing and dealing with files in s3 please let me know.
  Negative
Thanks in advance.
  Neutral
563e313861a8013065267662	X	If you want something small go for Yii if it is still small at all or any other micro framework.
  Negative
The questions is if it offers you everything you need to get it done in time and is solid.
  Positive
Cake had just two security issues in the last few years AFAIR, the latest one (1 year ago) has just hit ruby on rails.
  Negative
It is solid and has thousands of unit tests for the core.
  Positive
What is a template scheme for you?
  Neutral
Something like twig?
  Neutral
There are plugins for virtually any template engine.
  Negative
But template engines are a waste of resources.
  Negative
Go for plain php/html views.
  Negative
I know that some template engines support caching to increase performance but thats pointless because cake already offers caching techniques and also full page caching.
  Negative
So you can cache any output.
  Negative
If you want to go for OOP views checkout the Ctk plugin for Cake.
  Negative
I've written a plugin for storing files in any kind of storage system.
  Negative
It supports more storage engines than the few you list.
  Negative
And it was exactly written for the task you ask for.
  Negative
I'm storing files first locally and move them to S3 later.
  Negative
I don't know if there is any free available plugin for the stripe API, I think you'll have to do a little coding own your own... ;) If you go for cake you can use this plugin as a base and maybe you would like to make it open source, in this case I would help you with it.
  Negative
But I'm sure that any php framework can made to work with this API.
  Negative
563e313861a8013065267663	X	There are not much new improvement done in CodeIgniter.
  Negative
To take advantage of the new PHP 5.3, I would recommend you to use CakePHP instead.
  Negative
They are actively working in version 3 which will fully support 5.3 .
  Neutral
The following post might help you making a good decision.
  Positive
http://philsturgeon.co.uk/blog/2012/05/laravel-is-awesome
563e313861a8013065267664	X	I would suggest codeigniter.
  Negative
I have used both heavily and I really have a lot of issues with Cake.
  Negative
Cake is not the lighter of the two, I found that Cake wants you to do things cakes way otherwise you are in for a really sore time.
  Negative
This is not really how a framework should work.
  Negative
Codeigniter on the other hand hs a framework to support you but allows you to write unobstructed php all day as you need it.
  Negative
Cake is really good at getting in the way and has a much steeper learning curve.
  Very positive
Codeigniter will let you do all of those things you need, I know for instance that there are handy functions like $this->security->xss_clean($your_data); that return data cleaned of xss issues.
  Negative
So its support for stuff like that is super handy.
  Positive
Templating is less robust in codeigniter, but the reason I like codeigniter is that it stays out of your way and lets you code PHP.
  Negative
This is good, because you know how to code in PHP, whereas when you code in Cake, you are writing weird cake code.
  Positive
This has the nasty side effect of making the code hard to manage should more people come on to the team and its impossible to ask people who do not know cake for help with cake stuff.
  Very negative
Also cake does not guarantee backwards compatibility, your installation is your forever, if you use say version 1.6 then it might not be possible for you to move to 2.0 without serious rewrites (which seems short sighted on their part.)
  Negative
Cake will offer you more features though, so if your in a hurry it will help you along with large parts of the setup, provided your always cool with doing everything its way.
  Negative
If your not the best at database stuff cake totally abstracts that away.
  Positive
I personally hate that, because I'm serious about db fine-tuning and lots of things I want to do to make better use of the db cake just flat-out forbids.
  Negative
Codeigniter's freedom why it is the one I would pick.
  Positive
Its docs are great two.
  Positive
Oh yeah the docs.
  Positive
Codeignite's docs are so much better than cake, so that is a good reason to pick it too.
  Positive
563e313861a8013065267665	X	Why do you need to talk to the cloud storage directly - why's that better than talking to a cloud-hosted web service acting as an interface / gatekeeper to the storage?
  Negative
563e313861a8013065267666	X	It is an issue of performance, however hypothetical.
  Negative
I just feel like it would be a waste of resources.
  Negative
563e313861a8013065267667	X	Am researching same general topic.
  Negative
AWS has introduced IAM that alleges to "Manage access for federated users" but have not had time to fully delve into it.
  Negative
Similarly AWS appears to be waiting-and-seeing attitude before implementing CORS (en.wikipedia.org/wiki/Cross-origin_resource_sharing) which would greatly contribute to this area.
  Negative
563e313861a8013065267668	X	@tillda Curious: why are you trying to do everything via JavaScript?
  Negative
Is it cost-related (don't want to pay for a web or worker role - though they are now as cheap as $0.02/hour) - or something else?
  Neutral
563e313861a8013065267669	X	cloud storage is supposed to be for the users, under their account, or for you, under your secret account.
  Negative
just like being able to read js file's source, you can't have both secrecy and client-side-only code.
  Negative
563e313861a801306526766a	X	Remember that JSONP is confined to GETS ... so it's only 1/2 a loaf.
  Negative
(en.wikipedia.org/wiki/…)
563e313861a801306526766b	X	That Azure thing looks like a good approach, but JSONP is in my opinion absolutely deprecated technique.
  Negative
563e313861a801306526766c	X	@codingoutloud JSONP works and we'll need it in either way.
  Positive
This is a great article by Gaurav on how to use JavaScript to upload large files directly from client browser to Azure Storage.
  Positive
the JS files absolutely need to be hosted on the same storage account where you target the upload, because we cannot use JSONP to upload to the Storage.
  Negative
But we can use JSONP to get the SAS from the Server we control.
  Negative
563e313861a801306526766d	X	I'm researching a possibility of using some cloud storage directly from client-side JavaScript.
  Negative
However, I ran into two problems: Security - the architecture is usually build on per cloud client basis, so there is one API key (for example).
  Neutral
This is problematic, since I need a security per my user.
  Negative
I can't give the same API key to all my users.
  Negative
Cross-domain AJAX.
  Neutral
There are HTTP headers that browsers can use to be able to do cross domain requests, but this means that I would have to be able to set them on the cloud-side.
  Negative
But, the only thing I need for this to work is to be able to add a custom HTTP response header: Access-Control-Allow-Origin: otherdomain.com.
  Negative
My scenario involves a lots of simple queue messages from JS client and I thought I would use cloud to get rid of this traffic from my main hosting provider.
  Negative
Windows Azure has this Queue Service part, which seems quite near to what I need, except that I don't know if these problems can be solved.
  Negative
Any thoughts?
  Neutral
It seems to me that JavaScript clients for cloud services are unavoidable scenarios in the near future.
  Negative
So, is there some cloud storage with REST API that offers management of clients' authentication and does not give the API key to them?
  Negative
563e313861a801306526766e	X	Windows Azure Blob Storage has the notion of a Shared Access Signature (SAS) which could be issued on the server-side and is essentially a special URL that a client could write to without having direct access to the storage account API key.
  Negative
This is the only mechanism in Windows Azure Storage that allows writing data without access to the storage account key.
  Negative
A SAS can be expired (e.g., give user 10 minutes to use the SAS URL for an upload) and can be set up to allow for canceling access even after issue.
  Negative
Further, a SAS can be useful for time-limited read access (e.g., give user 1 day to watch this video).
  Negative
If your JavaScript client is also running in a browser, you may indeed have cross-domain issues.
  Negative
I have two thoughts - neither tested!
  Negative
One thought is JSONP-style approach (though this will be limited to HTTP GET calls).
  Negative
The other (more promising) thought is to host the .
  Positive
js files in blob storage along with your data files so they are on same domain (hopefully making your web browser happy).
  Negative
The "real" solution might be Cross-Origin Resource Sharing (CORS) support, but that is not available in Windows Azure Blob Storage, and still emerging (along with other HTML 5 goodness) in browsers.
  Negative
563e313961a801306526766f	X	Yes you can do this but you wouldn't want your azure key available on the client side for the javascript to be able to access the queue directly.
  Negative
I would have the javascript talking to a web service which could check access rights for the user and allow/disallow the posting of a message to the queue.
  Negative
So the javascript would only ever talk to the web services and leave the web services to handle talking to the queues.
  Negative
Its a little too big a subject to post sample code but hopefully this is enough to get you started.
  Negative
563e313961a8013065267670	X	I think that the existing service providers do not allow you to query storage directly from the client.
  Negative
So in order to resolve the issues: Update: Looks like Google already solves your problem.
  Negative
Check this out.
  Neutral
On https://developers.google.com/storage/docs/json_api/v1/libraries check the Google Cloud Storage JSON API client libraries section.
  Negative
563e313961a8013065267671	X	This can be done with Amazon S3, but not Azure at the moment I think.
  Negative
The reason for this is that S3 supports CORS.
  Negative
http://aws.amazon.com/about-aws/whats-new/2012/08/31/amazon-s3-announces-cross-origin-resource-sharing-CORS-support/ but Azure does not (yet).
  Negative
Also, from your question it sounds like a queuing solution is what you want which suggests Amazon SQS, but SQS does not support CORS either.
  Negative
If you need any complex queue semantics (like message expiry or long polling) then S3 is probably not the solution for you.
  Negative
However, if your queuing requirements are simple then S3 could be suitable.
  Neutral
You would have to have a web service called from the browser with the desired S3 object URL as a parameter.
  Negative
The role of the service is to authenticate and authorize the request, and if successful, generate and return a URL that gives temporary access to the S3 object using query string authentication.
  Positive
http://docs.aws.amazon.com/AmazonS3/latest/dev/S3_QSAuth.html A neat way might be have the service just redirect to the query string authentication URL.
  Negative
For those wondering why this is a Good Thing, it means that you don't have to stream all the S3 object content through your compute tier.
  Negative
You just generate a query string authenticated URL (essentially just a signed string) which is a very cheap operation and then rely on the massive scalability provided by S3 for the actual upload/download.
  Negative
Update: As of November this year, Azure now supports CORS on table, queue and blob storage http://msdn.microsoft.com/en-us/library/windowsazure/dn535601.aspx
563e313961a8013065267672	X	With Amazon S3 and Amazon IAM you can generate very fine grained API keys for users (not only clients!)
  Negative
; however the full would be PITA to use from Javascript, even if possible.
  Negative
However, with CORS headers and little server scripting, you can make uploads directly to the S3 from HTML5 forms; this works by generating an upload link on the server side; the link will have an embedded policy document on, that tells what the upload form is allowed to upload and with which kind of prefix ("directories"), content-type and so forth.
  Negative
563e313961a8013065267673	X	In order to have contents you're going to need to upload it in some fashion (iframe, ajax, flash, or traditional form).
  Negative
563e313961a8013065267674	X	The file must be uploaded to the server first.
  Negative
563e313961a8013065267675	X	Not necessarily, if the browser supports the new File API (see html5rocks.com/en/tutorials/file/dndfiles)
563e313a61a8013065267676	X	I'm actually using this plugin jasny.github.com/bootstrap/javascript.html#fileupload and I can get a preview of the file so the data are somewhere.
  Very negative
563e313a61a8013065267677	X	in that case the "data" will be on the server.
  Negative
You'll have to output the data to the client (browser) before you can access it via Javascript/jQuery
563e313a61a8013065267678	X	Hey, I tried your solution and that works.
  Negative
I retrieved the information but it's not well encoded.
  Negative
For a file I get \xc3\xbf\xc3 instead of getting this encode \xff\xd8\xff for the 3 first characters of my picture.
  Negative
What should I use to get the second encode for my picture?
  Neutral
563e313a61a8013065267679	X	@kschaeffler try fr.readAsDataURL(file); this is function reading Base64 datas, but i haven't tested it yet.
  Negative
563e313a61a801306526767a	X	actually this function give me not enough data.
  Negative
that's already what I have from the preview that I use in the plugin jasny.github.com/bootstrap/javascript.html#fileupload.
  Negative
I think that the data I retrieve are not Base64 encoded and this is the problem, but I'm not sure
563e313a61a801306526767b	X	I actually need the same encode data that I retrieve when I post through html form with the "multipart/form-data" encode type
563e313a61a801306526767c	X	You don't get the actual content with this...
563e313a61a801306526767d	X	I actually have a file input and I would like to retrieve the Base64 data of the file.
  Very negative
I tried: to retrieve the data.
  Neutral
But it only provides the name, the length, the content type but not the data itself.
  Negative
I actually need these data to send them to Amazon S3 I already test the API and when I send the data through html form with encode type "multipart/form-data" it works.
  Negative
I use this plugin : http://jasny.github.com/bootstrap/javascript.html#fileupload And this plugins gives me a preview of the picture and I retrieve data in the src attribute of the image preview.
  Negative
But when I send these data to S3 it does not work.
  Negative
I maybe need to encode the data like "multipart/form-data" but I don't know why.
  Negative
Is there a way to retrieve these data without using an html form?
  Neutral
563e313a61a801306526767e	X	you can try FileReader API something like this.
  Negative
563e313a61a801306526767f	X	input file element: get file :
563e313a61a8013065267680	X	I created a form data object and appended the file: and i got: in the headers sent.
  Negative
I can confirm this works because my file was sent and stored in a folder on my server.
  Negative
If you don't know how to use the FormData object there is some documentation online, but not much.
  Negative
Form Data Object Explination by Mozilla
563e313b61a8013065267681	X	I'm storing images in Amazon S3 using Fog and Carrierwave.
  Negative
It returns a url like bucket.s3.amazonaws.com/my_image.jpg.
  Negative
DNS entries have been set up so that images.mysite.com points to bucket.s3.amazonaws.com.
  Neutral
I want to adjust my views an APIs to use the images.mysite.com/my_image.jpg URL.
  Negative
Carrierwave, however, only spits out the Amazon based one.
  Negative
Is there a simple way to tell Carrierwave and/or Fog to use a different host for its URLs than normal?
  Neutral
If not, how would I modify the uploader to spit it out?
  Negative
563e313b61a8013065267682	X	Come to find out that, as of June 6th, 2012, Amazon AWS does not support custom SSL certs, which makes this a moot point.
  Negative
563e313b61a8013065267683	X	How you plans store the secret key in your clients?
  Negative
563e313b61a8013065267684	X	Ismael, thanks for your interest.
  Positive
That's one of our main concerns.
  Positive
563e313b61a8013065267685	X	My company is developing a SaaS application using PHP/Zend Framework.
  Negative
In order to provide access to different mobile devices, we have created a RESTful API and right now we are dealing with the authentication mechanism.
  Positive
After several meetings, we have agreed to use an "Amazon S3 REST API like" implementation.
  Negative
I mean to include in every request an APIKey (public) and a signature generated from a secretKey only known by client and server.
  Negative
All request are made using HTTPS protocol.
  Negative
My questions are: Thank you very much in advance.
  Positive
Best regards
563e313b61a8013065267686	X	I'm pretty new to Map/Reduce world and trying to evaluate the best option to figure if I can leverage it to create index in Solr.
  Neutral
Currently, I'm using a regular crawl to fetch data and index it in Solr directly.
  Negative
This is working without any issues.
  Neutral
But going forward, we need to access a sizable data residing in Amazon S3.
  Negative
There are around 5 million data presently stored in S3 , which needs to be indexed.
  Negative
I'm thinking of using Amazon Elastic Map/Reduce (EMR) to directly access the content from S3 and subsequently create the index in Solr.
  Negative
The data structure is simple, the url (which is unique) is the S3 key, the value is a XML file.
  Positive
The url will be used as the doc id in Solr while relevant portion of the XML data will be stored as fields in Solr index.
  Negative
My question is whether EMR is the right approach?
  Neutral
The task is to access the data from S3, extract certain elements from XML, do some processing and then call Solr API to generate the index.
  Negative
The processing part requires few classes, possibly a chain of command pattern, before indexing the data.
  Negative
Is it something achievable?
  Neutral
Doo I require a reducer or can use a mapper to do the process?
  Negative
If reducer is need, what will the scope of it?
  Neutral
Currently, I've a single index which is storing the data.
  Negative
Any pointers on this will be highly appreciated.
  Neutral
Thanks
563e313b61a8013065267687	X	You can try using MapReduceIndexer Tool.
  Negative
You can download it from apache-sole.
  Neutral
It is part of contrib module.
  Neutral
563e313b61a8013065267688	X	did you manage to do this yet?
  Neutral
563e313b61a8013065267689	X	Im working on it, i think i will have to write a library that will handle it, extend the current api using decorator design pattern.
  Negative
Still have dificulties to think about the system design.
  Positive
563e313c61a801306526768a	X	Thats awesome when i assume that all my files are public, but its not the case.
  Negative
563e313c61a801306526768b	X	Ths problem I see here is the use of absolute url, which is a pain to handle in a prod env when switching servers
563e313c61a801306526768c	X	I edited my answer with a possible solution: add a flag to the SavedFile object.
  Negative
563e313c61a801306526768d	X	As you may know Laravel uses Flysystem PHP package to provide filesystem abstraction.
  Negative
Ive started to use this feature in my project, just for fun uploaded some images to my Amazon s3 bucket, I have also instance of Cloudfront linked to this bucket.
  Negative
My problem is when im trying to display those images in my html page, i need a url.
  Negative
I could not find any "clean" way to do it, as flysystem is generic library i throught that i will be able to do something like this: for 'public' files and its easy to determine if file is "public" or not because its included in their api, so if im using s3 bucket as my driver for the disk i should get: "https://s3.amazonaws.com/my_awesome_bucket/path/image.png" or alternatively: for 'private' files - i should get a temporary url that will expire after some time.
  Negative
Is this something i can achieve only with specific implementation?
  Neutral
for example if im using Amazon S3 i can easily run: But i dont want to do ugly "switch cases" to determine what driver im using.
  Negative
And how can i get a url from a cdn (like cloudfront) through the FileSystem interface?
  Negative
Any suggestion?
  Neutral
563e313c61a801306526768e	X	I think of Flysystem as an interface to a disk (or other storage mechanism) and nothing more.
  Negative
Just as I would not ask my local filesystem to calculate a public URI, I would not ask Flysystem to do it either.
  Negative
I create objects that correspond to the files that I save via Flysystem.
  Neutral
Depending on my needs, I might save the public URI directly in the database record, or I might create a custom getter that builds the public URI based on runtime circumstances.
  Negative
With Flysystem, I know the path to the file when I write the file.
  Negative
In order to keep track of these files I'll typically create an object that represents a saved file: When I save the file, I create a record of it in the database: Whenever I need the public URI, I can just grab it off the SavedFile model.
  Neutral
This is handy for trivial applications, but it breaks down if I ever need to switch storage providers.
  Negative
Another neat trick is to define a method that will resolve the public URI based on a variable defined in the child of an abstract SavedFile model.
  Negative
That way I'm not hard-coding the URI in the database, and I can create new classes for other storage services with just a couple of variable definitions: Now if I have a bunch of files stored on my local filesystem and one day I move them to Amazon S3, I can simply copy the files and swap out the dependencies in my IoC binding definitions and I'm done.
  Very negative
No need to do a time consuming and potentially hazardous find-and-replace on a massive database table, since the calculation of the URI is done by the model: Edit: Just add a flag to the object:
563e313d61a801306526768f	X	Nice @theblackbenzkid I'll take a look at this
563e313d61a8013065267690	X	@ReynierPM please let me know how you get on.
  Negative
Would be interesting to see.
  Positive
Multi Channel Commerce is a huge entity within the eCommerce/Commerce sector as it migrates and creates the bridge of Non Web Sales to Online sales - OpenCart Database Layer can offer this with customisation.
  Negative
563e313d61a8013065267691	X	well my client buy a extension in OpenCart site at 200 USD and this only can synchronize from OpenCart to Amazon but not from Amazon to OpenCart.
  Negative
Then I get a report as a flat file from Amazon, read that file, insert in a temporary table at my DB and then lookup at WorldCat because Google Books change their API and can't get books by ISBN as far as I know and all this is a module for OpenCart that I'm planning to sell in the near future when it's done and tested because it give me a lot of headaches.
  Very negative
This is what I'm doing right now.
  Neutral
563e313d61a8013065267692	X	opencart.com/index.php?route=extension/extension/… opencart.com/index.php?route=extension/extension/…
563e313d61a8013065267693	X	Two new mods I have seen to help and aid in code bases.
  Negative
The only issue with external and commercial mods within Opencart IMO is trust factor.
  Negative
vBulletin and Magento have commercial companies that use the platform and environment therefore commercial and copyright licences and full commercial developers in the community - Opencart community is to closed and bloated with enthusiasts rather than pros.
  Negative
.
  Neutral
you cannot even sign up as the developer or contribute to their forum.
  Negative
563e313d61a8013065267694	X	I'm a book seller and have a storefront in Amazon and also have another store outside Amazon.
  Negative
I'll like to show my Amazon books in my other store with proper link to buy directly trough Amazon, is this possible using Amazon MWS?
  Negative
Any sample using PHP?
  Neutral
563e313d61a8013065267695	X	It is possible.
  Positive
Using Amazon MWS - you need to use the Amazon API with PHP - It would be not entirely complex to do; but making a plugin for Amazon, eBay and Opencart direct management of uploading and sync for multi commerce has always been on community pipelines.
  Negative
No plugin exists at the moment; free or commercial; you have to do this yourself.
  Negative
Alternative is to create a native app for opencart.
  Neutral
Someone did this via a freelancer site; worth speaking to them: Google Cache of Mayoousa Amazon Store and Amazon Product Feed Integration PDF Or edit this (or uses code bases from this): eBay for OpenCart eBay for OpenCart Demo ebay for Opencart Module on Opencart Amazon S3 module for opencart Amazon Payments module for Opencart
563e313d61a8013065267696	X	My implementation is scalable, I have multiple servers, and each has a Sidekiq, running workers.
  Very negative
I just need to run the Sidekiq job on the same server that received the file.
  Negative
This might lead to over use of some server, but I think that the load balancer will sort that out.
  Negative
563e313d61a8013065267697	X	If the load balancer works well, then the solution 2 should have better performance as it only requires one upload.
  Negative
563e313d61a8013065267698	X	So I'm faced with a choice of whether to implement a direct upload to S3 or to proxy images through my servers and then to S3.
  Negative
I do need to process images once they are uploaded, creating thumbs and different versions.
  Negative
So, in short what I would like to know is which approach is better, from those that might have done this before.
  Negative
Choice 1.
  Neutral
Direct upload to S3 Choice 2.
  Neutral
Upload through servers Looking at this, it would seem that uploading directly to S3 is actually slower than through my servers first.
  Negative
However, does the load that initial upload places on server outweigh the extra download step when going directly to S3?
  Neutral
Also are there additional factors involved, such as network performance in each case and similar?
  Negative
563e313d61a8013065267699	X	Regards to performance, uploading directly to S3 is a better solution.
  Negative
The reason is that S3 is highly scalable, which means: if you use one thread to upload files to S3, the speed is 1M/s, if you use 10 threads, the speed will be about 10M/s.
  Negative
For you problem, if you have a lot of users uploading files at the same time, S3 can handle these requests with less performance personality.As uploading files directly to the server, it requires your server can handle many requests at the same time, if your implementation is salable, it may have a chance to beat S3 on performance.
  Negative
But solution 1 has more costs as it has more transactions, and also data transfer out.
  Positive
563e313d61a801306526769a	X	One issue with uploading directly to S3 is that you're uploading to the location of the bucket, whereas your servers may be distributed across multiple locations throughout the world.
  Negative
(For example, if you're running your servers in multiple EC 2 regions).
  Negative
In that case, you find you have lower latency (higher transfer rates) going to your server than going to S3 directly.
  Negative
Also, there may not be transfer out charges from S3 if you're transferring data to a server running on EC2 in the same region.
  Negative
Otherwise Matt's right - you'll pay anywhere from $0.02 - $0.25 / GB for transferring data from S3.
  Negative
Mike
563e313d61a801306526769b	X	I was a member of a big project using S3 storage.
  Negative
S3 upload is not 100% perfect officially.
  Negative
It is very important that whether you choose one of them you mentioned.
  Positive
Because each one has different benefits and drawbacks.
  Positive
You should choose first one if you want to make overload in client side instead of server side because of some reason like 1.
  Negative
Server is low spec or busy 2.
  Neutral
There are huge upload files in huge clients side.
  Positive
3.
  Neutral
Customer want to know the result of upload immediately and want to do next action with uploaded files.
  Negative
But you should make strong error handling logins in upload program.
  Negative
Otherwise, you might be suffered from customer's complain.
  Negative
.
  Neutral
You should choose second one if you want to have higher reliability of uploading process in terms of structure or development environment.
  Negative
Because S3 uploading is HTTP protocol so it is very slow and sometime it may make overload in upload side.
  Negative
If there are some abnormal cases in client side like stopping upload or network problem, you might get into problem handling the situation.
  Negative
So it is stable to upload files in server side again because error handling is essential as amazon mentioned below link.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/ErrorBestPractices.html The URL mentioned "When designing an application for use with Amazon S3, it is important to handle Amazon S3 errors appropriately ..." Also I recommend you use just only official APIs from Amazon to handle errors instead of plug-in or S3 application.
  Very negative
:)
563e313e61a801306526769c	X	Thanks Norm for your answer, I guess I have to rework my code here.
  Negative
563e313e61a801306526769d	X	I'm trying to upload files to Amazon S3.
  Negative
Works well, as long as the files don't exceed 1GB in size.
  Negative
If so, Powershell runs into some Memory Loop when it processes the 1st byte after 1 GB and never makes it to my write-host line I just can't figure out why there's a limit and how to handle that.
  Very negative
Any ideas?
  Neutral
(Code snippet below opens a file stream Reader + writer.
  Negative
The real code for this function on my home computer contains an additional cryptostream writer, which I removed here for the sake of readability of the code, just in case someone's wondering what the code should be good for.
  Negative
Result however is the same -> Memory Loop)
563e313e61a801306526769e	X	Using the S3FileInfo to write to a stream does not work well for large files.
  Negative
All the data is needed for computing the signature to S3 so the S3FileInfo buffers the data written to its streams until the stream is closed.
  Negative
I would suggest using the TransferUtility and use the Upload(stream, bucketName, key) method that takes in your stream, see Using the High-Level .
  Negative
NET API for Multipart Upload for details and example snippets.
  Negative
Since you are using a Crypto stream you might want to take a look at the S3 encryption support added to version 2 of the SDK, see Client Side Data Encryption with AWS SDK for .
  Negative
NET and Amazon S3.
  Neutral
563e313e61a801306526769f	X	I would like to decrypt a CSV dump of an Amazon Redshift table locally.
  Negative
I m using the unload command and client side encryption since the data contains sensitive information.
  Negative
The command i am using is like this: to generate a master_key i used the follwing command: This outputs: I used the key as the `master_symmetric_key.
  Negative
I copy the s3 data locally and try to decrypt like this: But get: How do I decrypt an Amazon Redshift CSV dump?
  Negative
563e313e61a80130652676a0	X	The Key is stored as metadata and is available in x-amz-meta-x-amz-key, and The IV is stored as metadata and is available in x-amz-meta-x-amz-iv.
  Negative
From the Redshift documentation: ... UNLOAD then stores the encrypted data files in Amazon S3 and stores the encrypted envelope key and IV as object metadata with each file.
  Negative
The encrypted envelope key is stored as object metadata x-amz-meta-x-amz-key and the IV is stored as object metadata x-amz-meta-x-amz-iv.
  Negative
When you get the S3 object you will also get these meta-data fields.
  Negative
Here are some example of S3 GET-Object example: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html
563e313f61a80130652676a1	X	streisand effect in 3... 2... 1...
563e313f61a80130652676a2	X	@Charlie "The Streisand effect is a primarily online phenomenon in which an attempt to censor or remove a piece of information that has the unintended consequence of causing the information to be publicized widely and to a greater extent than would have occurred if no censorship had been attempted..." Sorry, I dont get it?
  Negative
563e313f61a80130652676a3	X	A better question would be "What are the characteristics of good Web APIs?"
  Neutral
.
  Neutral
I think that's what you're trying to get at.
  Neutral
Or that's what you should be trying to get at.
  Neutral
My attempt to answer, below.
  Neutral
563e313f61a80130652676a4	X	@Pongus eh, close enough.
  Negative
I was talking about Jeff posting this question on Twitter, causing an influx of upvotes to him.
  Negative
It's not exactly the Streisand Effect, but it's got the same jist
563e313f61a80130652676a5	X	@Charlie Somerville, with you!
  Negative
Probably more the Katie Price effect... :o)
563e313f61a80130652676a6	X	+5.
  Negative
Now what was I saying about the Streisand effect?
  Neutral
:P
563e313f61a80130652676a7	X	-1 Whilst those may be "popular" API's, many of them are definately not "gold standards".
  Negative
The Flickr API is particularly hilarious, for example.
  Very positive
It advertises itself as RESTful and yet is nothing of the sort!
  Negative
563e313f61a80130652676a8	X	@nathan see where it says "please feel free to edit"?
  Negative
Well.
  Neutral
.
  Neutral
please feel free!
  Positive
Just click the edit link!
  Positive
563e313f61a80130652676a9	X	I'm interested in the topic as well.
  Positive
I recently found out about Mashery - they seem to add a layer on top of your public facing API which helps with the permissions and user's request throttling.
  Negative
563e313f61a80130652676aa	X	The Sun Cloud API is by far the closest thing to a REST style API.
  Negative
563e313f61a80130652676ab	X	A comment on the Twitter having "Good REST practice".
  Negative
Twitter does not use hypermedia to discover links and due to its use of generic media types the responses are not self-descriptive.
  Negative
It is far from being RESTful.
  Neutral
563e313f61a80130652676ac	X	@Darrel - agreed but a pure RESTFUL service is only necessary if its going to be consumed and updated by robots - otherwise you spend too much time worrying about being RESTful instead of delivering a product IMO
563e314061a80130652676ad	X	@schmoopy Most static HTML websites are pure RESTful services.
  Negative
Are those only consumed by robots?
  Negative
563e314061a80130652676ae	X	@Darrel - yes, only google reads them, we read google ;-) -- my only point was that being purely restful may not be ideal since your API is used by developers and not robots -- typically - if websites required users to read them with a decoder ring then I am sure the web would be heading in a different direction.
  Very negative
563e314061a80130652676af	X	took the words right out of my mouth :)
563e314061a80130652676b0	X	Doesn't look RESTful whatsoever though.
  Negative
So loses points big time there.
  Negative
563e314061a80130652676b1	X	Yea was just checking it out for reference, but I cant take it serious since I can pass in an invalid method route and still get back a 200 ResponseCode - ws.audioscrobbler.com/2.0/… returns an HttpStatusCode of 200 - should be 404 since its not a found 'resource' IMO
563e314061a80130652676b2	X	SOAP?
  Very negative
Excuse me while i go over here and throw up...
563e314061a80130652676b3	X	+1 for strict and safe!
  Negative
563e314061a80130652676b4	X	Seems like there are two categories of APIs for websites today.
  Negative
APIs which allow the functionality of the site to be extended like Facebook, Myspace, etc.
  Negative
These APIs seem to be very diverse.
  Negative
APIs which allow interaction with the existing site functionality like Twitter, Flickr, etc.
  Negative
These all claim to be REST based, but are in reality simply "data over HTTP".
  Negative
If you were creating a website that allowed both functional extension and outside interaction, what existing APIs would you use as a reference model?
  Negative
563e314061a80130652676b5	X	We're doing some research in this area ourselves.
  Negative
Not a lot out there in terms of "gold standard" for website API references.
  Negative
The most common website APIs referenced are: Another list here: http://www.pingable.org/the-top-15-web-apis-for-your-site/ Someone recommended the book Restful Web Services as a good reference on this.
  Negative
(please feel free to edit the above list to add other high profile websites with APIs)
563e314061a80130652676b6	X	How To Design A Good API and Why it Matters, a 60 minute Google tech talk by Joshua Bloch, is relevant.
  Negative
563e314061a80130652676b7	X	Having worked with a few, I'll get right down to it Facebook Myspace Photobucket (disclaimer: I wrote the server side of the photobucket api) Twitter Ideal characteristics So that being said... something between Facebook and Twitter.
  Negative
Of course I'm partial to some of the stuff we have on Photobucket, but I also hate some of it.
  Negative
563e314061a80130652676b8	X	It would seem to me that the documentation of the API is just as (or more) important than the actual design of the API.
  Negative
Well written, simple documentation will make up for any design flaws.
  Negative
That's what I've learned after looking at the various links posted already.
  Negative
Specifically, Last.fm's documentation seems very good: easy to navigate and easy to understand.
  Positive
563e314061a80130652676b9	X	Last.fm's api continues to be one of the most well maintained apis on the net.
  Positive
It's also been around longer than most, because it started out basically as just that.
  Neutral
http://www.last.fm/api
563e314061a80130652676ba	X	Regarding Jeff's list of big APIs: I am pretty sure that common does not mean "Gold Standard".
  Negative
No need to keep a manual list of widespread API.
  Negative
To get a list check out http://www.programmableweb.com/apis/directory/1?sort=mashups .
  Negative
Since I like REST as loose standard, I'd say that an API is "Golden" when it makes sense and is intuitive.
  Negative
… all make the most sense for me and are well thought out (as Brian already pointed out).
  Positive
In my current daily work I also work a lot with OpenSocial, where URIs feel very natural but also extend the REST standard in its own way.
  Negative
If you like it strict and safe, use SOAP.
  Negative
563e314061a80130652676bb	X	I would check out OpenSocial, a movement to create an API Standard for social network sices.
  Negative
They use REST for this and have a 'user' centric approach.
  Negative
But this is a very well documented approach which might help for even a site that is not totally Social based.
  Positive
If you are looking for some internal code implementations look at Drupals hook system and Wordpress.
  Negative
http://code.google.com/apis/opensocial/
563e314061a80130652676bc	X	I think the best way to answer is to list the characteristics of good web APIs instead of citing examples.
  Negative
If you like Twitter/Facebook/etc APIs, what aspect of these APIs do you find attractive?
  Negative
I'll take a first stab: Please add more to comments.
  Negative
563e314161a80130652676bd	X	I don't have any experience with the others, but even as it has evolved over the years, the Facebook API is still awful.
  Negative
It doesn't come anywhere near being a "gold standard."
  Negative
Rather, it is something people struggle through and grit their teeth because once they finally get it right, it can add a lot of value.
  Negative
563e314161a80130652676be	X	Some APIs which are notably good:
563e314161a80130652676bf	X	It will depend on what your target audience is.
  Negative
If it is .
  Neutral
net shops then soap is probably okay other wise focus on REST since it has a much lower bar of entry.
  Negative
From there look at website APIs that target the same people you would like to.
  Negative
This way your api will feel familiar.
  Positive
563e314161a80130652676c0	X	Force (previously known as SalesForce) API: http://www.salesforce.com/us/developer/docs/api/index.htm
563e314161a80130652676c1	X	AtomPub is the gold standard because it was designed by some of the brightest minds on the internet.
  Negative
You can't go too far wrong using iit as a basis.
  Negative
That is what google and msft does.
  Neutral
563e314161a80130652676c2	X	Had a similar question to this which didn't get much action, but thought it would be good to link to it.
  Negative
http://stackoverflow.com/questions/775988/what-web-apis-would-you-most-want-to-replicate-or-are-the-most-popular
563e314161a80130652676c3	X	If I were designing a web api today for an existing web site, assuming the web site was well designed with respect to its proper usage of HTTP, I would use the existing web site as the design guideline.
  Negative
Take Stack Overflow as an example, it has the entire URI space already mapped out.
  Negative
It has a complete set of interconnections defined between the different representations.
  Negative
Users of the site are already familiar with the site structure and therefore the API structure would already be familiar.
  Negative
The only part that needs to change is the content of the representations, to eliminate all unnecessary markup.
  Negative
It would be necessary to add in a few extra templated links to allow for searches that are currently only accessible via javascript.
  Neutral
For example, searching for a user is not easily discoverable via navigating because currently the link is built via javascript.
  Negative
The really tricky decision is what media-type to use.
  Neutral
You could use bare bones html with RDFa style metadata markup, or go wild and use the new Microdata format in Html5.
  Negative
Or, you could return a custom media type based on xml or Json.
  Neutral
Something like application/vnd.
  Neutral
stackoverflow.question+xml, etc.
  Negative
The custom media type makes versioning really easy, but it is less accessible to clients that were not designed to access StackOverflow directly.
  Negative
The custom types could be used in combination with Atom feeds which are mostly already there in StackOverflow, Designing a web api is really no different than designing a website, other than the fact that you are delivering content that will be consumed by a program that is not a web browser.
  Negative
What you don't want to do is create an Http based data access layer.
  Positive
That is just like showing your underwear to the world.
  Negative
The existing web site is optimized for all the common usage scenarios, many of the api access patterns are going to be similar, so reuse the "views" that have already been created.
  Negative
It may be necessary to add a few extra links here and there to make it a bit quicker for programs to get the data they want, but those can be added incrementally as the need arises.
  Negative
Well written web sites are already very effective APIs for web browser clients, it is really not necessary to go back to the drawing board to support any other type of client.
  Negative
The API structure does not need to change, just the delivered content.
  Negative
563e314161a80130652676c4	X	But how can I know ulrs for files at CloudFront?
  Neutral
For example I have an image /images/test.
  Positive
png at my EC2 server.
  Neutral
And CloudFront was configurated with custom origin option.
  Negative
So how can I know url to access this image at CloudFront?
  Neutral
563e314161a80130652676c5	X	http://*.cloudfront.net/images/test.png * = your CloudFront ID
563e314161a80130652676c6	X	if you set up a CNAME record say cdn.example.com to point to DISTRIBUTION_ID.
  Negative
cloudfront.net , then your will be able to access your file by going to cdn.example.com/images/test.png
563e314161a80130652676c7	X	Just for understanding.
  Negative
How to configurate CF?
  Neutral
Is this an EC2 instance?
  Neutral
I should to upload files to server and load balancing work will be done by Amazon?
  Negative
(I mean that I have only one CF instance and if load will grow Amazon will add another instance?
  Negative
How can I upload files?
  Neutral
Can I make git repository and commit files?
  Negative
If I have another EC2 instance with web application is there any way to get access to my files at Cloud Front through file system?
  Negative
I mean if I can to use something like php: file_get_contents('/cloud_front/images/some_image.png') What links should I use to get, for example, images at Clod Front server?
  Negative
I mean that may be there are urls like http://distilleryimage10.s3.amazonaws.com/d2e89af606cc11e3b81c22000a1fbca3_6.jpg to get file access.
  Negative
So how can I generate this url?
  Negative
Any API?
  Neutral
Thanks for your time.
  Positive
563e314161a80130652676c8	X	CloudFront is a Content Delivery Netwerk.
  Negative
You can not upload (single) files.
  Negative
Instead, it makes a copy of your existing files (that you host on S3 or something similar) when that file is requested.
  Negative
This is overkill when a particular file is only accessed a few times.
  Negative
However, if a file is accessed over a thousand times, it can be cheaper and faster to store a copy on CloudFront (this is done automatically, you cannot select individual files) because the roundtrip a computer makes when connecting to the CloudFront datacenter (e.g. AMS) is much shorter than when connecting to your S3 files (e.g. in the worst case US west).
  Neutral
Do note that CloudFront is not one datacenter, so it's also not really suitable if your files are accessed often, but all from different locations (so that after all, every file is only accessed once or so at a CF datacenter).
  Negative
563e314161a80130652676c9	X	i've got an old debian root server, that is hosted at an provider in germany.
  Negative
The maschine is now about 6 years old and i want to move it to a cloud.
  Positive
the question is if there is an easy way to clone the root server (dd or something else) and use this an amazon (or something else) cloud server.
  Negative
are there any way's to this or do i have to migrate the whole server to a new instance in the cloud.
  Negative
thx
563e314161a80130652676ca	X	Look into this tool http://aws.amazon.com/ec2/vmimport/ it should do what you're asking.
  Negative
The first method is to import your VM image using the Amazon EC2 API tools.
  Negative
To get started, simply:
563e314261a80130652676cb	X	Make asynchronous versions of your web API methods and use those methods.
  Negative
If that's not possible, call the methods from separate tasks/threads.
  Negative
563e314261a80130652676cc	X	Have you looked at the AWS documentation for the asynchronous APIs using C#?
  Negative
563e314261a80130652676cd	X	Don't create async void methods unless you're sure that you have to, and that it's the correct option at the time.
  Negative
Usually it's not.
  Negative
You should almost always be returning a Task instead.
  Negative
563e314261a80130652676ce	X	First off I apologize for terrible wording of that question...here's the scenario: I built a WEB API method that receives a ProductID and then uploads that products images to Amazon S3.
  Negative
This part is working just fine.
  Positive
I am now trying to get a console app running that will grab a range of ProductIDs and loop through them, calling the API method, and not wait for the results... Can anyone point me in the right direction?
  Negative
I suppose another caveat would be to not eat up all the resources on the machine running the console app...so maybe a thread cap?
  Negative
UPDATE (This still seems to be synchronous):
563e314261a80130652676cf	X	There are a couple easy ways to do this.
  Positive
I'd recommend using Parallels.
  Positive
It makes the most optimized use of your environments many threads/cores.
  Neutral
For your example, you'd simply do something like this: The other method would be to use Tasks, and send each call as a separate task.
  Negative
Be careful with this approach though because you could overload the system with pending tasks if you have too many iterations.
  Negative
563e314261a80130652676d0	X	I do not recommend using Parallel.For.
  Negative
It does not give an satisfactory control of parallelism (you probably don't want to hammer away hundrades of requests which will start to timeout) also it requires unnecessary context switching.
  Negative
Threads/cores isn't the limiting factor in case http requests.
  Negative
In the example change to and when using real web api calls also remember to wait in Main otherwise the program will terminate before the calls have been made.
  Negative
Then to control the level of parallelism I think the easiest solution is to use a Semaphore to count the number of outstanding calls, waiting in the main loop for the semaphore to be signaled again before issuing new requests.
  Neutral
563e314261a80130652676d1	X	This could explain the strange issues we were seeing.
  Neutral
Thanks.
  Neutral
My theory is that while uploading two files to the same s3 key concurrently, Amazon chooses one to "win".
  Negative
Sometimes, it chooses "incorrectly", meaning it chooses the one which ends up getting killed (incomplete file), which causes the S3 key to remain empty (no file ends up getting saved).
  Negative
563e314261a80130652676d2	X	I think I'm having a problem with concurrent s3 writes.
  Negative
Two (or more) processes are writing almost the same content to the same s3 location at the same time.
  Negative
I'd like to determine the concurrency rules that govern how this situation will play out.
  Neutral
By design, all of the processes but one will get killed while writing to s3.
  Negative
(I had said they are writing "almost" the same content because all but one of the processes are getting killed.
  Negative
If all processes were allowed to live, they would end up writing the same exact content.)
  Negative
My theory is that the process getting killed is leaving an incomplete file on s3, and the other file (which presumably was written fully) is not being chosen as the one that gets to live on s3.
  Negative
I'd like to prove or disprove this theory.
  Neutral
(I'm trying to find out if the issues are caused by concurrency issues during write to s3, or some other time).
  Negative
From the FAQ at http://aws.amazon.com/s3/faqs/ : Q: What data consistency model does Amazon S3 employ?
  Neutral
Amazon S3 buckets in the US West (Oregon), US West (Northern California), EU (Ireland), Asia Pacific (Singapore), Asia Pacific (Tokyo), Asia Pacific (Sydney) and South America (Sao Paulo) Regions provide read-after-write consistency for PUTS of new objects and eventual consistency for overwrite PUTS and DELETES.
  Very negative
Amazon S3 buckets in the US Standard Region provide eventual consistency.
  Negative
I'm using the US Standard Region.
  Negative
563e314261a80130652676d3	X	I don't think that the consistency statements in that FAQ entry say anything about what will happen during concurrent writes to the same key.
  Negative
However, it is not possible to have an incomplete file in S3: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html says Amazon S3 never adds partial objects; if you receive a success response, Amazon S3 added the entire object to the bucket.
  Negative
This implies that only the file that is completely uploaded will exist at the specified key, but I suppose it's possible that such concurrent writes might tickle some error condition that result in no file being successfully uploaded.
  Very negative
I'd do some testing to be sure; you might also wish to try using object versioning while you're at it and see if that behaves differently.
  Negative
563e314361a80130652676d4	X	In the default US Standard region, S3 provides eventually consistent writes.
  Negative
Making a GetObjectMetadata() call immediately following a PutObject() may give a 404, because the object has not been copied to all of the datacenters yet.
  Negative
Your code above may throw an exception, even when the put was successful.
  Positive
The AmazonS3 client will throw an exception if there was a failure, either on the client side or the server side.
  Negative
563e314361a80130652676d5	X	That seems to work fine, I'll deploy it and see how it holds up in production.
  Positive
563e314361a80130652676d6	X	Calculate the MD5 against the key or really the data sent?
  Negative
563e314361a80130652676d7	X	Approximately once\week a file upload fails when saving to Amazon S3 (1\300).
  Negative
The following code works well enough to confirm that the file saved correctly, but I can't help but think there's a better way.
  Positive
When a file does fail, no exception is thrown so I'm never really certain where the problem lies.
  Negative
Any suggestions for better confirmation?
  Negative
563e314461a80130652676d8	X	According to the API documentation it recommends you check the ETag value against the a calculated MD5 hash of the data you sent.
  Negative
They obviously should match.
  Neutral
"To ensure an object is not corrupted over the network, you can calculate the MD5 of an object, PUT it to Amazon S3, and compare the returned Etag to the calculated MD5 value."
  Negative
http://docs.amazonwebservices.com/AmazonS3/latest/API/SOAPPutObject.html Hope that helps
563e314461a80130652676d9	X	Well, if you can compress the data, that might reduce it and therefore it would be better.
  Negative
But apart from that, POST looks like the valid HTTP method to send the data.
  Negative
There is also PUT but it should be much similar.
  Negative
563e314461a80130652676da	X	I, personally, would use an AJAX-based method such as jsonp as opposed to CURL.
  Negative
Curl is exceedingly slow, while AJAX is virtually instant.
  Very negative
563e314561a80130652676db	X	@Austin: I bet that's not the case here.
  Negative
563e314561a80130652676dc	X	True, it depends on his goal and how he plans to use the data.
  Negative
563e314561a80130652676dd	X	@Austin i am sending data between 2 servers , no browsers , no javascript to use ajax .
  Negative
and even json requires some sort of connection , so what would it be ?
  Negative
563e314561a80130652676de	X	FTP would be slow , i am not sending FILES i am sending DATA which has to be processed , and for sure i won't use a 3rd party !!
  Very negative
563e314561a80130652676df	X	Used curl .
  Negative
divided the file to many pieces and sent it
563e314561a80130652676e0	X	I would advise against using MySQL.
  Negative
It's not really designed for transporting files (esp.
  Negative
large ones).
  Neutral
563e314561a80130652676e1	X	FTP would be slow , because i am sending DATA to be processed and not files !
  Negative
.
  Neutral
563e314561a80130652676e2	X	MySQL would be a performance loss
563e314561a80130652676e3	X	I am using CURL to send large amounts of data between servers , am using POST , is this OK or is there any better/standard way to send large serialized data with curl ?
  Negative
the problem is in the max-post-size in the php settings , i have to change it (default 2MB) .
  Negative
i didn't encounter any problems with this yet , but when the system will be online it is possible that data larger than 50MB will be sent each time !
  Negative
Any ideas ?
  Neutral
Thank you .
  Positive
EDIT : I am sending DATA and not FILES , a data that once received should be processed by the second server and saved to database/file/do some action and might need to send a response after processing the data .
  Negative
I just would like to know , will i face any other problem except max-post-size ?
  Negative
(forget about timeouts of both curl and php) , and is there anyway to make the server not look at max_post_size ?
  Negative
maybe by using PUSH ?
  Neutral
or PUT ?
  Neutral
does that post_size affect the PUSH or PUT ??
  Neutral
and how to use it via curl ?
  Neutral
so many questions !
  Neutral
563e314661a80130652676e4	X	Using cURL is perfectly fine.
  Positive
Personally, I would prefer to not having to do it through web server (eg.
  Negative
Apache) as there can be too many potential faults along the way, eg.
  Negative
PHP timeout, web server timeout, memory limit, no write privileges, limited to web root, etc.
  Very negative
I would prefer to do it through mechanisms designed for file transfers:
563e314661a80130652676e5	X	The way is ok.
  Negative
Two more ideas for you:
563e314661a80130652676e6	X	Does anyone know how I can store large binary values in Riak?
  Negative
563e314661a80130652676e7	X	For now, they don't recommend storing files larger than 50MB in size without splitting them.
  Negative
See: FAQ - Riak Wiki If your files are smaller than 50MB, than proceed as you would with storing non binary data in Riak.
  Negative
Another reason one might pick Riak is for flexibility in modeling your data.
  Negative
Riak will store any data you tell it to in a content-agnostic way — it does not enforce tables, columns, or referential integrity.
  Negative
This means you can store binary files right alongside more programmer-transparent formats like JSON or XML.
  Neutral
Using Riak as a sort of “document database” (semi-structured, mostly de-normalized data) and “attachment storage” will have different needs than the key/value-style scheme — namely, the need for efficient online-queries, conflict resolution, increased internal semantics, and robust expressions of relationships.Schema Design in Riak - Introduction
563e314661a80130652676e8	X	@Brian Mansell's answer is on the right track - you don't really want to store large binary values (over 50 MB) as a single object, in Riak (the cluster becomes unusably slow, after a while).
  Very negative
You have 2 options, instead: 1) If a binary object is small enough, store it directly.
  Negative
If it's over a certain threshold (50 MB is a decent arbitrary value to start with, but really, run some performance tests to see what the average object size is, for your cluster, after which it starts to crawl) -- break up the file into several chunks, and store the chunks separately.
  Negative
(In fact, most people that I've seen go this route, use chunks of 1 MB in size).
  Negative
This means, of course, that you have to keep track of the "manifest" -- which chunks got stored where, and in what order.
  Positive
And then, to retrieve the file, you would first have to fetch the object tracking the chunks, then fetch the individual file chunks and reassemble them back into the original file.
  Negative
Take a look at a project like https://github.com/podados/python-riakfs to see how they did it.
  Positive
2) Alternatively, you can just use Riak CS (Riak Cloud Storage), to do all of the above, but the code is written for you.
  Negative
That's exactly how RiakCS works -- it breaks an incoming file into chunks, stores and tracks them individually in plain Riak, and reassembles them when it comes time to fetch it back.
  Negative
And provides an Amazon S3 API for file storage, for your convenience.
  Positive
I highly recommend this route (so as not to reinvent the wheel -- chunking and tracking files is hard enough).
  Negative
Yes, CS is a paid product, but check out the free Developer Trial, if you're curious.
  Negative
563e314661a80130652676e9	X	Just like every other value.
  Neutral
Why would it be different?
  Negative
563e314661a80130652676ea	X	Use either the Erlang interface ( http://hg.basho.com/riak/src/461421125af9/doc/basic-client.txt ) or the "raw" HTTP interface ( http://hg.basho.com/riak/src/tip/doc/raw-http-howto.txt ).
  Negative
It should "just work."
  Neutral
Also, you'll generally find a better response on the riak-users mailing list than you will here.
  Negative
http://lists.basho.com/mailman/listinfo/riak-users_lists.basho.com (No offense to z8000, who seems to also have answers.)
  Negative
563e314661a80130652676eb	X	I'm not entirely clear on your requirements: Do you (1) want the user to login to your own backend authentication service?
  Negative
Or (2) do you want the user to not need to login, but still be able to upload to S3 (without embedding your AWS credentials)?
  Neutral
563e314661a80130652676ec	X	Could I use STS without cognito?
  Negative
Would that be a simpler solution?
  Neutral
563e314761a80130652676ed	X	Well, you could, but again Amazon Cognito provides a super-set of the functionality of what web identity federation provides.
  Negative
Some benefits include, allowing users to have guest access and later link a social login to the same user.
  Negative
Cognito also provides data synchronization capabilities across devices.
  Positive
You can read this article(blogs.aws.amazon.com/security/post/Tx3SYCORF5EKRC0/…) to further understand the benefits of using Amazon Cognito.
  Neutral
563e314761a80130652676ee	X	So I'm a bit confused by the Amazon documentation on Cognito concerning one of their stated use cases: "use your own identity system... allowing your apps to save data to the AWS cloud".
  Negative
In my case I want to give them aws tokens to upload directly to s3 from the mobile client without putting my aws keys on the client device.
  Negative
In order to implement this on the server side - how do I generate the proper credentials so that the client can use this identity on the client app to upload to s3?
  Negative
Do I first call getId() (what values do I pass if I'm using my own login - since I'm not providing a facebook or twitter ID?
  Negative
How do I pass in my own db's generated user ids?
  Neutral
AWS.CognitoIdentity.getCredentialsForIdentity() method from the congito API... or maybe I have to new up an AWS.CognitoIdentity?
  Negative
Any links to a good example?
  Neutral
I couldn't find any full examples in the documentation itself.
  Negative
For example in their documentation amazon says that var identityId = AWS.config.credentials.identityId; retrieves an identityid for your end user immediately, however looking at it, it seems to be a property and not an id factory.
  Negative
How does it generate unique ids, or is one identity id shared by all of my users?
  Neutral
Are there credentials of some sort that I can derive from this that I can then pass on to my mobile client to get upload privileges to s3?
  Negative
I also read something about AWS STS service - is that an alternative to using Cognito?
  Neutral
563e314761a80130652676ef	X	You can find an example in this AWS Mobile blog post and the differences between developer authenticated identities and regular identities in this other blog post.
  Negative
Basically, the flow is that your app will authenticate against your backend, then your backend will call GetOpenIdTokenIdTokenForDeveloperIdentity and send the resulting token and Identity ID to the user's app.
  Neutral
The user's app can use this token to obtain Cognito credentials using the SDK, and with this credentials make calls to S3 or other AWS services.
  Negative
Each user will have it's own credentials, so they only have access to their own resources in S3.
  Negative
About STS, that's what the SDK will internally use to obtain the credentials, but as long as you use the SDK you don't need to worry about it.
  Negative
It's not an alternative to Cognito, but they both work together.
  Neutral
563e314761a80130652676f0	X	It is possible to the .
  Positive
JAR to run the code of httpclient from the jar inside my jar only ?
  Neutral
563e314761a80130652676f1	X	Yes, to make the code clear, you could merge all HttpClient invocation in to one Class (let's call it HttpClientHelper); Then create a class loader which has 4.1.1 in its class path, and use this class loader to load HttpClientHelper.
  Negative
If you need some sample codes, give me your email address.
  Negative
563e314761a80130652676f2	X	Is it possible to use the same lib with different versions ?
  Neutral
The thing is: i have the httpclient-4.0.1 into my application in the WEB-INF/lib directory.
  Negative
I made an API for the Amazon S3 service, which use the httpclient-4.1.1.
  Negative
But i don't want to update my application library to use the newer version, because i don't have enough time to test and garantee that the application will run properly.
  Negative
So, is there a way, that my API i`ve made (actually a jar) to use the httpclient-4.1.1 without need to upgrade the library of my application (4.0.1)?
  Negative
563e314761a80130652676f3	X	you cannot use 2 versions of same Lib in the same class loader; but you can use different class loader to load th different versions.
  Negative
For example, you can use a sub classloader to loader httpclient-4.0.1, and shield 4.1.0 in the super class loader.
  Negative
563e314a61a80130652676f4	X	Okay, first off the ecommerce API was renamed to the associates API.
  Negative
Here's a link to the latest: developer.amazonwebservices.com/connect/…
563e314a61a80130652676f5	X	apisigning.com claims: "As of August 17th, 2009, Amazon now requires that all requests to their Product Advertising API be signed."
  Negative
563e314b61a80130652676f6	X	Register for Amazon Web Services.
  Negative
Then try the following URL: http://ecs.amazonaws.com/onca/xml?Service=AWSECommerceService&Operation=ItemLookup&AWSAccessKeyId=YOURKEY If you have my experience you'll see: What is this trying to tell me?
  Negative
What are "Signature" and "Timestamp" parameters?
  Neutral
This is a n00b problem of some sort, but finding the answer is not obvious.
  Negative
I Googled for "MinimumParameterRequirement".
  Neutral
I looked at http://docs.amazonwebservices.com/AWSEcommerceService/2007-01-17 and http://s3.amazonaws.com/awsdocs/ECS/20080819/QRC-AAWS-2008-08-19.pdf and http://docs.amazonwebservices.com/AWSEcommerceService/2007-01-17/ApiReference/ErrorCodesAndMessages.html.
  Negative
In fact, even a link to the latest API doc for ecommerce would be nice.
  Negative
Is 2008-08-19 the latest?
  Neutral
563e314b61a80130652676f7	X	Well here's the signature parameter: http://docs.amazonwebservices.com/AmazonFPS/latest/FPSAdvancedGuide/index.html?APPNDX%5FGeneratingaSignature.html It's a hash of the other parameters.
  Negative
Strange that they give lots of examples without it and claim that those examples work.
  Neutral
563e314b61a80130652676f8	X	anyone can grab the client side code from your page and modify it to circumvent the xml check.
  Negative
563e314b61a80130652676f9	X	Any thoughts on how this should be tackled?
  Neutral
563e314b61a80130652676fa	X	unless S3 allows you to run your code on their side, where you can check the contents of the files, you could post to your own server and do the check there and then post to S3 from your server.
  Negative
563e314b61a80130652676fb	X	Yes I can do that, but i do not think one can run code on s3 other than html; as for the server side processing I thought since i can directly upload to s3 I would save the server side processing time and resources for something else
563e314c61a80130652676fc	X	Will read on the info you posted; will get back to you ASAP...thank you
563e314c61a80130652676fd	X	I allow users to upload to s3 directly without the need to go through the server ; Everything works perfectly; my only worry is the security.
  Negative
In my javascript code I check for file extensions .
  Negative
However I know in javascript code Users can manipulate script- in my case to allow upload of xml files - (since client side upload) and thus would'nt they be able to replace the crossdomain.xml in my bucket and accordingly be able to control my bucket?
  Neutral
Note: I am using the bucket owner access key and secret key.
  Negative
Update: Any possible approaches to overcome this issue...?
  Positive
563e314c61a80130652676fe	X	If you are not adverse to running additional resources, you can accomplish this by running a Token Vending Machine.
  Positive
Here's the gist: A simple example of creating a "dropbox"-like service using per-user access is detailed here.
  Positive
563e314c61a80130652676ff	X	That's right.
  Positive
Would definitely be a problem since I am using the CurrentUser to protect the data.
  Negative
563e314d61a8013065267700	X	Consider using this, then: stackoverflow.com/questions/3681493/…
563e314d61a8013065267701	X	I need to store user credentials in my app.
  Negative
I can store and retrieve the password with protectdata.
  Neutral
But as soon as I push a new revision of my app I loose the credentials.
  Negative
It seems like appharbor cleans the ProtectedData Store.
  Negative
Is this behavior on purpose?
  Neutral
Is there a better way to store user credentials on appharbor.
  Neutral
OAuth is not an options since it's a ftp account.
  Negative
563e314d61a8013065267702	X	Changes to the local instance file system are not persisted across deploys on AppHarbor, so you'll have to store the data someplace else.
  Negative
We generally recommend using Amazon S3.
  Positive
You can use System.Security.Cryptography.ProtectedData.Protect() from the same API to encrypt data written there.
  Negative
Here's some sample code: protect bytes data .
  Neutral
net I'm not super familiar with this API, but you might get into trouble if you app us running on multiple AppHarbor instances using different user accounts.
  Negative
563e314d61a8013065267703	X	Here is the files schema github.com/eknowles/mongoose-crate-example/blob/master/app/… and the api github.com/eknowles/mongoose-crate-example/blob/master/app/…
563e314d61a8013065267704	X	Amazing!
  Negative
Thanks so much for the effort you put into answering this.
  Negative
This goes a long way towards helping me understand express routing in general.
  Neutral
I owe you one.
  Neutral
Stack says I have to wait 20hrs before rewarding the bounty, but rest assured it's yours!
  Neutral
563e314d61a8013065267705	X	No worries!
  Neutral
Express is an awesome framework, loads of really great stuff out there!
  Very positive
563e314d61a8013065267706	X	Any chance you could modify your example for Express 4?
  Negative
express.multipart is removed from the Express 4.x library so I can't use the built in parser like your example does.
  Negative
I've been using body-parser, but maybe I will need to use multiparty
563e314d61a8013065267707	X	You can use github.com/expressjs/multer for Express4, it's very simple to setup
563e314d61a8013065267708	X	I am building a RESTful API using Node + Express 4 + MongoDB + Mongoose.
  Very negative
One thing my API needs to do is store and retrieve files.
  Negative
Which I will store in Amazon S3.
  Negative
Mongoose has a specific plugin for attaching files to Mongo documents called Mongoose-Crate, which in turn has a storage provider Mongoose-Crate-S3 that uploads files to S3.
  Negative
I've done my best to adapt the example code from the Mongoose-Crate-S3 npm page to work as an express route, but so far I've not gotten an image to successfully upload to my S3 storage.
  Neutral
Documents of my 'file' model are being created in my mongo database, but the only have an '_id' and '__v' fields.
  Negative
No 'title', no 'description', nothing to indicate that the .
  Negative
post endpoint is actually receiving the files I try to post.
  Neutral
I keep making slight adjustments to my code but I am generally getting some variation on "Could not get any response".
  Negative
Here is my mongoose schema file.js (with my S4 credentials removed of course) And here is the relevant snippet of my api routes file.
  Negative
I'm fairly certain the code I need to fix goes in here.
  Negative
I fully expect I am missing something obvious, but MEAN stack programming is new to me, and I've scoured stackoverflow & the web at large looking for more examples or anything to hint at what I am missing.
  Negative
Please help!
  Neutral
563e314e61a8013065267709	X	First of all you should set up your location path of where you want the file to be saved on S3, currently in that example it's using the same path as the origin file (which could be /var/tmp/y7sday... or C:/Users/SomeGuy/Pictures/.
  Negative
.)
  Neutral
so we need to sort that out first.
  Neutral
In my code I'm using the same name as the file they gave, in production you might want to sort these by date and add a random uuid to them.
  Negative
full example Next in your API endpoint you want to include any form information in the body and add those to the object, these will be in the req.body part.
  Negative
The most important thing to note here is that I've set the attachment as file, this needs to match the field you have declared in your mongoose schema or it will die.
  Negative
Next include req.files.file as the second argument.
  Neutral
full example I've uploaded my workings to GitHub, so please do clone that and try it out.
  Positive
All you need to do is POST to /file/ Make sure that you set the field in the form to be file like the image below  Tip: If you ever run into any problems it's always good to do a console.log(req) and see where the bits are that you need.
  Negative
563e314e61a801306526770a	X	I have refered to the amazon site -'http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html'to upload a audio file directly to the S3 server.
  Negative
The file gets uploaded successfully, but I am unable to handle the return status.
  Negative
I used the option 'success_action_status' and set it to 200 to make sure that it is returned in the success block.
  Positive
But it always gets returned in the error block.
  Negative
I am using Dot Net 3.5 with MVC.
  Negative
I have used It always ends up in the error block.
  Negative
I want to process the request further based on the return status (success/error).
  Negative
Any help will be highly appreciated.
  Positive
563e314e61a801306526770b	X	If you know the name of the bucket you want to use, why do you need to list them?
  Negative
putObject to the bucket directly.
  Neutral
563e314e61a801306526770c	X	putObject can get bucketName as String: (docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/…, java.lang.String, java.io.File)
563e314e61a801306526770d	X	@Guy what if I'm using Bucket reference for some other purposes?
  Negative
So please learn the difference between a text and object reference.
  Positive
563e314e61a801306526770e	X	Some concepts are probably too hard for me to understand... I like simple life with API calls that are using text... but this is only me.
  Negative
563e314e61a801306526770f	X	Thank you for the answer.
  Positive
+1.
  Neutral
563e314e61a8013065267710	X	Can you tell me how can I check with AmazonS3Client.getS3AccountOwner() whether I'm the owner?
  Negative
563e314e61a8013065267711	X	AmazonS3Client.getS3AccountOwner() gets the owner of the current account the code is using - that is, yourself.
  Negative
AmazonS3Client.getBucketAcl(bucketId).
  Neutral
getOwner() gets the owner of the bucket.
  Neutral
Comparing the two should show whether you are the owner of the bucket.
  Negative
563e314e61a8013065267712	X	thank you for the explanation :)
563e314f61a8013065267713	X	You are referring to the method createBucket() of AmazonS3 interface, and I'm using an implementation of that method and I get an exception when I use that method.
  Negative
Firstly, I was thinking that the method should return the bucket if it already exists, but it does not.
  Negative
Exception message: com.amazonaws.services.s3.model.AmazonS3Exception: Status Code: 409, AWS Service: Amazon S3, AWS Request ID: ****, AWS Error Code: BucketAlreadyOwnedByYou, AWS Error Message: Your previous request to create the named bucket succeeded and you already own it., S3 Extended Request ID: ****
563e314f61a8013065267714	X	So basically, your answer doesn't help.
  Very negative
Thanks, anyway.
  Negative
563e314f61a8013065267715	X	Thank you for the answer, but I'm using Java as specified above.
  Negative
563e314f61a8013065267716	X	Also, the "no request made" suggests that you'll get a valid bucket object even if the bucket does not actually exist, which is not quite what the question's code does.
  Negative
563e314f61a8013065267717	X	While I was trying to upload objects to a specific bucket in Amazon S3 storage via Amazon Web Services API, I obviously needed a reference to that existing bucket.
  Negative
I was using AmazonS3Client to perform that, however I couldn't see any method such as getBucket(String bucketID).
  Negative
Please check here: AmazonS3Client I applied a brute-force search to get a reference to the desired bucket: Obviously, this works fine.
  Positive
But I wonder why the API doesn't provide such a trivial method or might I be missing something or do you know any other method(s) from the same API which will replace the lines above with one-line code?
  Negative
EDIT: For a simple task like uploading a file, the bucket name seems to be sufficient: However, my question is as the title implies, why isn't there a method returning a reference to a specific Bucket instance?
  Negative
563e314f61a8013065267718	X	Your brute force method gets a Bucket object if the bucket exists and you own it, and otherwise results in null.
  Negative
To get the same result without listing the buckets: If you already know you own a bucket with the relevant name, you can skip all but the last step.
  Negative
Not exactly a one line result, but it does avoiding listing the buckets, which could be good if you have a large number of buckets, and I think it's the API's intended way to do what you want.
  Negative
563e314f61a8013065267719	X	Quoting my own article: The createBucket call is idempotent: It if exists, no exception is thrown.
  Negative
Hope it helps
563e314f61a801306526771a	X	I don't think you're missing anything.
  Negative
There are no APIs in that list that return a single bucket.
  Negative
The Ruby SDK seems to do what you want.
  Negative
http://docs.aws.amazon.com/AWSRubySDK/latest/frames.html Before you can upload files to S3, you need to create a bucket.
  Negative
s3 = AWS::S3.new bucket = s3.buckets.create('my-bucket') If a bucket already exists, you can get a reference to the bucket.
  Negative
bucket = s3.buckets['my-bucket'] # no request made
563e314f61a801306526771b	X	I'll give it a stab.
  Negative
According to the Java Doc and the REST API doc (v 1.7.1), there is simply nothing that you can pass in to get anything less than a full list of buckets.
  Negative
If I had to guess, I would say that its probably because Amazon cautions in their docs to create/delete buckets sparingly and thus pushes the burden onto the dev of caching that response or of searching through it themselves.
  Negative
Theres also a limit on the number of buckets per account, so the number of buckets in the response is reasonably small.
  Negative
One of the comments on the question mentioned that most of the calls use a String and not a Bucket, which caused me to take a look at the Bucket class.
  Negative
The only fields that it contains other than its name are the owner and creation date.
  Negative
If that's what you're trying to get at, then its probably in your best interest keep the full response somewhere.
  Positive
The bucket owner and dates aren't going to change very often, if at all.
  Negative
Perhaps I'm assuming too much here, but it might be best if you just poll the service at app startup or on a regular basis to see if anything changed, updating your cache when needed and to use this cache instead of hitting S3 every time you need this info.
  Negative
Its also worth mentioning that if you don't need an owner and creation date, there's nothing stopping you from instantiating a Bucket yourself and setting the name on it.
  Negative
563e314f61a801306526771c	X	I have a senario where we have many clients uploading to s3.
  Negative
My question is what is the best approach to knowing that there is a new file.
  Positive
Is it realistic/good idea, for me to poll the bucket ever few seconds?
  Negative
563e314f61a801306526771d	X	UPDATE: Since November 2014, S3 supports the following event notifications: These notifications can be issued to Amazon SNS, SQS or Lambda.
  Negative
Check out the blog post that's linked in Alan's answer for more information on these new notifications.
  Negative
Original Answer: Although Amazon S3 has a bucket notifications system in place it does not support notifications for anything but the s3:ReducedRedundancyLostObject event (see the GET Bucket notification section in their API).
  Negative
Currently the only way to check for new objects is to poll the bucket at a preset time interval or build your own notification logic in the upload clients (possibly based on Amazon SNS).
  Negative
563e314f61a801306526771e	X	Push notifications are now built into S3: http://aws.amazon.com/blogs/aws/s3-event-notification/ You can send notifications to SQS or SNS when an object is created via PUT or POST or a multi-part upload is finished.
  Negative
563e314f61a801306526771f	X	Your best option nowadays is using the AWS Lambda service.
  Positive
You can write a Lambda using either node.js javascript, java or Python (probably more options will be added in time).
  Negative
The lambda service allows you to write functions that respond to events from S3 such as file upload.
  Positive
Cost effective, scalable and easy to use.
  Positive
563e315061a8013065267720	X	You must configure a shared folder where you can store user generated content.
  Negative
Check here for same example about shared folder.
  Neutral
Hope this help
563e315061a8013065267721	X	I want to deploy a php application from a git repository to AWS Opsworks service.
  Negative
I've setup an App and configured chef cookbooks so it runs the database schema creation, dumping assets etc..
  Negative
.
  Neutral
But my application has some user generated files in a sub folder under web root.
  Negative
git repository has a .
  Neutral
gitignore file in that folder so an empty folder is there when i run deploy command.
  Negative
My problem is : after generating some files (by using the site) in that folder, if I run 'deploy' command again 'Opsworks' adds a new release under 'site_name/releases/xxxx' folder and symlink to it from 'site_name/current' folder.
  Negative
So it makes my previous 'user generated stuff' inaccessible.
  Negative
What is the best solution for this kind of situation?
  Neutral
Thanks in advance for your kind answers.
  Positive
563e315061a8013065267722	X	You have a few different options.
  Neutral
Listed below in order of personal preference: When using OpsWorks think of replicable/disposable servers.
  Neutral
What I mean by this is that if you can create one server (call it server A) and then switch to a different one in the same stack (call it server B), the result of using server A or server B should not impact how your application works.
  Negative
While it may seem like a good idea to save your user generated files in a directory that is common between different versions of your app (every time you deploy a new release directory is generated) when you destroy your server, you run the risk of destroying your files.
  Negative
Benefits and downsides of using S3?
  Negative
Benefits: Downsides: Benefits and downsides of using EBS?
  Negative
Benefits: Downsides: Downside of using a database?
  Negative
My preferred choice would be to use S3, but ultimately this is your decision.
  Negative
Good luck!
  Positive
EDIT: Take a look at this repository opsworks-chef-cookbooks it contains some recipes to deploy Symfony2 application on OpsWorks.
  Negative
I have been using it for over a year and works quite well.
  Positive
563e315061a8013065267723	X	Use Chef templates, and use them in a recipe in the opsworks deploy lifecycle event.
  Negative
563e315061a8013065267724	X	Presumably your server is an EC2 instance, because there is no such thing as a server 'in S3' - right?
  Negative
563e315061a8013065267725	X	check it before.
  Negative
Couldn't find any valuable information for this
563e315061a8013065267726	X	Did you check the Code Samples?
  Negative
They have an example of downloading a file from S3 in there: $response = $s3->get_object('aws-sdk-for-php', 'demo/big-buck-bunny.
  Negative
mp4', array( 'fileDownload' => '.
  Negative
/downloads/big-buck-bunny.
  Neutral
mp4' ));
563e315061a8013065267727	X	It is ok if I can download the particular file in the particular account to my account.
  Negative
What do u mean by " download from the source account and upload to the target account".
  Neutral
The target account is my account and my server is running their.
  Neutral
May be I didn't get the exact architect of the S3.
  Negative
Can't I just download that file to my account?
  Negative
563e315061a8013065267728	X	There might be some confusion regarding the different AWS services: Amazon S3 is storage for the Internet - it is really just that, storage, so you can't run a server on it etc.; on the other hand, Amazon EC2 provides resizable compute capacity in the cloud - slightly simplified this boils down to running servers on demand, and your server is probably such an EC2 instance accordingly.
  Very negative
Both EC2 and S3 (and in fact all AWS services) are usually accessed via a single AWS account.
  Negative
563e315061a8013065267729	X	So the question is whether you want to download from an S3 bucket stored in your AWS account to an EC2 instance running in your AWS account, or want to copy an object (file) from one S3 bucket in your AWS account to another S3 bucket in your AWS account.
  Negative
563e315061a801306526772a	X	Yes the requirement is this.
  Negative
Users have videos on their S3.
  Neutral
I want to download a copy of that video from their S3 to my EC2 directory.
  Negative
563e315161a801306526772b	X	Hi I'm really new to amazon s3.
  Negative
I want to download a file to my server (which is in s3) from a given another s3 location (credentials will be provided).
  Negative
This must be done using a php (cakephp 1.2) method and there is no user interface for this.
  Negative
(this could be a cron job probably).
  Negative
I couldn't find any good article regarding this by googling.
  Negative
Any sample code that I can do this work?
  Neutral
563e315161a801306526772c	X	Check out AWS SDK for PHP.
  Negative
563e315161a801306526772d	X	If I understand you correctly, you want to copy an Amazon S3 object (file) from one AWS account to a different AWS account without downloading and uploading it to a separate system (apparently an Amazon EC2 instance)?!
  Negative
It is possible to copy an object within a single account by means of copyObject(), but cross account operations aren't supported by this API (and neither for any other AWS resources, as far as I know, which is likely a deliberate decision to ease and streamline the security architecture and process).
  Negative
So while your use case is sound, there is no other solution than channeling this process through your server, i.e. download from the source account and upload to the target account.
  Negative
This shouldn't be much of a problem cost or performance wise though, because There is no Data Transfer charge between Amazon EC2 and other Amazon Web Services within the same region (i.e. between Amazon EC2 US West and Amazon S3 in US West) (see section Data Transfer in Amazon EC2 Pricing) and these operations will facilitate Amazon's decent internal network infrastructure (rather than crossing the public internet).
  Negative
563e315161a801306526772e	X	You'll need to provide a lot more information.
  Negative
For example, what does the request look like?
  Neutral
What version of Fine Uploader are you using?
  Neutral
What sort of parameters are being associated with the file?
  Neutral
563e315161a801306526772f	X	I am using FineUploader to upload to S3.
  Negative
I have everything working including deletes.
  Neutral
However, when I upload larger files that get broken into multi-part uploads, I get the following error in the console (debugging turned on): Can someone point me in the right direction as what I should check for settings, or what additional info you might need?
  Negative
563e315161a8013065267730	X	Since you haven't included anything really specific to your setup, code, or the failing request, my best guess is that your server isn't returning a proper signature response for uploads made to the S3 REST API (which is used for larger files).
  Negative
You'll need to review that procedure for generating a response to this type of signature request.
  Neutral
Here's the relevant section from Fine Uploader's S3 documentation: Fine Uploader S3 uses Amazon S3’s REST API to initiate, upload, complete, and abort multipart uploads.
  Negative
The REST API handles authentication by signing canonically formatted headers.
  Negative
This signing is something you need to implement server-side.
  Negative
All your server needs to do to authenticate and supported chunked uploads direct to Amazon S3 is sign a string representing the headers of the request that Fine Uploader sends to S3.
  Negative
This string is found in the payload of the signature request: { "headers": /* string to sign */ } The presence of this property indicates to your sever that this is, in fact, a request to sign a REST/multipart request and not a policy document.
  Negative
This signature for the headers string differs slightly from the policy document signature.
  Neutral
You should NOT base64 encode the headers string before signing it.
  Negative
All you must do, server-side, is generate an HMAC SHA1 signature of the string using your AWS secret key and then base64 encode the result.
  Negative
Your server should respond with the following in the body of an ‘application/json’ response: { "signature": /* signed headers string */ }
563e315161a8013065267731	X	Cyberduck is spelled in one word with no camel case.
  Negative
563e315261a8013065267732	X	Thanks .
  Neutral
It's working fine for me .
  Positive
563e315261a8013065267733	X	worked fine with standard S3
563e315361a8013065267734	X	I am trying to use Cyberduck CLI to connect to an S3 compatible S3-compatible CEPH API by UKFast (https://www.ukfast.co.uk/cloud-storage.html).
  Negative
It has the same function as Amazon but uses a different url/ server obviously.
  Neutral
The connection is via secret key and pass phrase the same as S3.
  Neutral
Cyberduck CLI protocols are listed here: https://trac.cyberduck.io/wiki/help/en/howto/cli I have tried using the below command the windows command prompt.
  Negative
The problem is that Cyberduck auto adds amazon AWS URL.
  Negative
So how do I use all the S3 options with a custom end point?
  Negative
563e315361a8013065267735	X	The s3:// scheme is reserved for AWS in Cyberduck CLI.
  Negative
If you want to connect to a third party services compatible with the S3 protocol you will need to create a custom connection profile.
  Positive
A connection is a XML property list .
  Negative
cyberduckprofile file that you install, providing another connection scheme.
  Positive
An example of such a profile is the Rackspace profile shipped within the application bundle in Profiles/Rackspace US.cyberduckprofile adding the rackspace:// scheme to connect to OpenStack Swift compatible Rackspace Cloud.
  Negative
You can download one of the other S3 profiles available and use it as a template.
  Neutral
Make sure to change at least the Vendor key to the protocol scheme you want to use such as ukfast and put in the service endpoint of UKFast as the value for the Default Hostname key (Which corresponds to s3.amazonaws.com; I cannot find any documentation for the S3 endpoint of UKFast.
  Negative
When done, verify the new protocol is listed in duck --help.
  Negative
You can then use the command duck --list ukfast://bucket/ --username <AccessKey> --password <Secret Key> to list files in a bucket.
  Negative
You might also want to request UKFast to provide such a profile file for you and other users to make setup simpler.
  Negative
The same connection profile can also be used with Cyberduck.
  Negative
563e315361a8013065267736	X	+1 because I would also like to know, but I suspect the answer will be an emphatic no.
563e315461a8013065267737	X	Thanks David - I suggest making this an RFE.
  Negative
I’ve used MLJAM in the past.
  Negative
My preference is to host the Java code in a Tomcat cluster and communicate with MarkLogic via RESTful APIs.
  Positive
In either case, it complicates the deployment footprint and should be avoided.
  Negative
563e315461a8013065267738	X	In general - in most companies I have worked with - the more customers explicitly and directly making it known to their support or sales or account reps what future features are important and valuable to them, especially if given a priority, urgency and rating comparison to other important and valuable features they may be asking for is the most productive way to influence the the prioritization of new features.
  Negative
Associatiating and communicating a tangible 'value' of a feature to the company is very helpful vs 'would love to have xxx'.
  Negative
563e315461a8013065267739	X	I'd like to use MarkLogic's JVM to run some custom Java code.
  Negative
Using the MarkLogic JVM would dramatically reduce the infrastructure/deployment footprint.
  Negative
The custom Java code that I want to run within the JVM will extract data from legacy excel spreadsheets.
  Negative
I will be using the Apache POI Java APIs to do this.
  Negative
Apache POI Spreadsheet API => http://poi.apache.org/spreadsheet/index.html I know that there's some undocumented ways to call the MarkLogic JVM directly.
  Negative
I assume this is being done for the HDFS and Amazon S3 features.
  Negative
Is there a way to use the MarkLogic Java VM and if so how?
  Negative
563e315461a801306526773a	X	"Empathetic No" Where "No" means that if there was such a feature that was safe, efficient, tested, and supported and guaranteed to work with with the same enterprise quality and reliability as documented features, it would be documented.
  Positive
Since its not - don't do it, no matter how tempting.
  Negative
Alternatively: There is the excellent and well used public domain "MLJAM" library http://developer.marklogic.com/learn/2006-05-mljam And this uses 100% documented features.
  Positive
You can also go the other way and start from java, query and push to ML ... the overhead can actually be minimal if you design it right.
  Positive
Sometimes even more efficient by splitting out the workload then by doing it all in one process or system.
  Negative
Take a look at the Java Client API, this can run on the same host as MarkLogic or on a different host.
  Negative
-David
563e315461a801306526773b	X	This is such a good idea, thank you!
  Very positive
563e315461a801306526773c	X	Excellent answer.
  Neutral
Thanks!
  Positive
563e315461a801306526773d	X	I'm running a website that handles multimedia uploads for one of its primary uses.
  Negative
I'm wondering what are the best practices or industry standard for organizing alot of user uploaded files on a server.
  Negative
563e315461a801306526773e	X	I don't think you are going get any concrete answers unless you give more context and describe what the use-case are for the files.
  Negative
Like any other technology decision, the 'best practice' is always going to be a compromise between the different functional and non-functional requirements, and as such the question needs a lot more context to yield answers that you can go and act upon.
  Positive
Having said that, here are some of the strategies I would consider sound options: 1) Use the conventions dictated by the consumer of the files.
  Negative
For instance, if the files are going to be used by a CMS/publishing solution, that system probably has some standardized solution for handling files.
  Negative
2) Use a third party upload solution.
  Neutral
There are a bunch of tools that can help guide you to a solution that solves your specific problem.
  Negative
Tools like Transloadit, Zencoder and Encoding all have different options for handling uploads.
  Negative
Having a look at those options should give you and idea of what could be considered "industry standard".
  Positive
3) Look at proved solutions, and mimic the parts that fit your use-case.
  Neutral
There are open-source solutions that handles the sort of things you are describing here.
  Negative
Have a look at the different plugins to for example paperclip, to learn how they organize files, or more importantly, what abstractions do they provide that lets you change your mind when the requirements change.
  Neutral
4) Design your own solution.
  Neutral
Do a spike, it's one of the most efficient ways of exposing requirements you haven't thought about.
  Neutral
Try integrating one of the tools mentioned above, and see how it goes.
  Positive
Software is soft, so no decision is final.
  Negative
Maybe the best solution is to just try something, and change it when it doesn't fit anymore.
  Neutral
This is probably not the concrete answer you were looking for, but like I mentioned in the beginning, design decisions are always a trade-off, "best-practice" in one context could be the worst solution in another context :) Best off luck!
  Negative
563e315561a801306526773f	X	Your question is exceptionally broad, but I'll assume you are talking about storage/organisation/hierarchy of the files (rather than platform/infrastructure).
  Negative
A typical approach for organisation is to upload files to a 3 level hierarchical structure based on the filename itself.
  Negative
Eg.
  Neutral
Filename = "My_Video_12.
  Neutral
mpg" Which would then be stored in, Or another example, "a9usfkj_0001.
  Negative
jpg" This way, you end up with a manageable structure that makes it easy to locate a file's location simply based on its name.
  Negative
It also ensures that directories do not grow to a huge scale and become incredibly slow to access.
  Positive
Just an idea, but it might be worth being more explicit as to what your question is actually about.
  Negative
563e315561a8013065267740	X	From what I understand you want a suggestion on how to store the files.
  Negative
If is that what you want, I would suggest you to have 2 different storage systems for your files.
  Negative
The first storage would be a place to store the physical file, like a directory on your server (w/o FTP enabled, accessible or not to browsers, ...) or go for Amazon s3 (aws.amazon.com/en/s3/), Rackspace CloudFiles (www.rackspace.com/cloud/cloud_hosting_products/files/) or any other storage solution (you can even choose dropbox, if you want).
  Negative
All of these options offers APIs to save/retrieve the files.
  Positive
The second storage would be a database, to index and control the files.
  Negative
On the DB, that could be MySQL, MSSQL or a non-relational database, like Amazon DynamoDB or SimpleSQL, you set the link to you file (http link, the path to the file or anything like this).
  Negative
Also, on the DB you can control and store any metadata of the file you want and choose one or many @ebaxt's solutions to get it.
  Negative
The metadata can be older versions of the file, the words of a text file, the camera-model and geo-location of a picture, etc.
  Negative
Of course it depends on your needs and how it will be really used.
  Neutral
You have a very large number of options, but without more info of what you pretend to do is hard to suggest you a solution.
  Negative
On Amazon tutorials area (http://aws.amazon.com/articles/Amazon-S3?browse=1) you can find many papers about it, like Netflix's Transition to High-Availability Storage Systems, Using the Java Persistence API with Amazon SimpleDB and Petboard: An ASP.NET Sample Using Amazon S3 and Amazon SimpleDB Regards.
  Negative
563e315561a8013065267741	X	See similar question stackoverflow.com/questions/1312087/…
563e315561a8013065267742	X	Your answer is correct in spirit.
  Negative
A REST interface should be discoverable as you describe.
  Negative
However, all media-types used by representations should have formal definitions.
  Neutral
REST is not an excuse avoid documenting your API.
  Negative
The difference is what you are documenting.
  Neutral
Do document the data structures that flow between the client and the server, just not the endpoints (ie.
  Negative
URLS).
  Neutral
563e315561a8013065267743	X	Machine readable definitions are useful for defining types and other support code in various programming languages.
  Negative
563e315561a8013065267744	X	Most REST interfaces I see are described with a simple web page describing the URL, the method, accepted input and returned result.
  Negative
For example the Amazon S3 or the Twitter API documentation.
  Negative
But why should I settle with what is apparently good enough for Amazon or Twitter... So, is it worth describing a REST API in a machine readable format?
  Negative
And if yes, which one?
  Neutral
WSDL 2.0 claims is capable of describing REST.
  Negative
WADL is explicitly created for describing REST services.
  Negative
Both WSDL 2.0 and WADL seem to have a rather small following atm and it seem to be little return for the effort of creating and maintaining the description documents.
  Negative
My uestion is basically to validate or negate my assumption.
  Neutral
Do you use WSDL/WADL to describe your services?
  Negative
Do you rely on WSDL/WADL to consume others' services?
  Neutral
Does your tool of choice support either one at the moment?
  Neutral
563e315561a8013065267745	X	Yes, you should.
  Neutral
You will be able to generate your client code, tests and documentation using a set of tools supporting WADL.
  Neutral
Some examples can be found here.
  Neutral
Also, I think you should stick with WADL, rather than WSDL 2.0 because it is less verbose and way simpler (IMHO).
  Negative
In fact, in WADL you describe exactly what the user sees on the documentation page, just using WADL XML syntax.
  Negative
BTW, that is why it's so easy to write XSLT-based documentation generators for WADL.
  Negative
563e315561a8013065267746	X	The following is just my personal opinion: I think WADL is similar to site maps for html pages.
  Negative
Site maps are considered theoretically a good practice, but rarely implemented and even more rarely used by people.
  Negative
I think the reason is simple - wandering around a site and pushing strategically placed buttons is often significantly more rewarding than browsing a complex map.
  Positive
REST API methods should not require a formal description.
  Negative
So if API is created thoughtfully it is pretty easy to discover all the resources just by following strategically placed uri links of a 'home' RESTful resource.
  Positive
563e315561a8013065267747	X	There's a chicken/egg phenonenon here.
  Negative
WADL is useless without tools that produce or consume it.
  Negative
The tools are useless unless sites publish WADL.
  Negative
etc.
  Neutral
For me, The Amazon model works fine.
  Positive
Depending on your audience you will get more return on an effort to produce samples, including snips od sample dialogs (what does request1 look like on the wire, same for response 1, then request 2, response 2, etc), and code in vvarious languages that are important to you.
  Negative
If you want to go to a machine-readable definition, you can use XSD if it is an XML message format.
  Negative
Obviously this is not WADL but coupled with your english description, it may provide a little extra utility for developers.
  Negative
563e315661a8013065267748	X	What is the benefit of a machine-readable REST API definition?
  Negative
The point of REST is for the API to be relatively simple and easy-to-understand.
  Negative
Natural language works well for this.
  Positive
If you mean "API Type Definitions" when you say "API Definition" then there may be some value in providing metadata.
  Negative
This, however, is only one piece of an API definition.
  Negative
Having "machine readable" API can easily repeat the API source code, violating the DRY principle.
  Negative
It's often simpler to write English descriptions of what the REST verbs do and what the URI's are.
  Positive
Send the type's which are marshalled through JSON (or YAML or JAXB) as source code.
  Negative
That's the perfect machine-readable API -- actual working source for the message object class.
  Positive
563e315661a8013065267749	X	The most popular usage of WSDL (and WADL in the same way) is code generation.
  Positive
It sure helps speed up development, but nothing can replace plain old documentation.
  Negative
For humans and not for machines.
  Neutral
563e315661a801306526774a	X	Thank you for the detailed answer!
  Positive
I've actually started implementing the protocol you detailed before, but haven't gotten around to actually finishing it yet: github.com/keichan34/s3uploader/tree/2.0-wip
563e315661a801306526774b	X	Great, I've noticed you are using Ruby in your project and also found another Ruby project which I have added to the answer for your reference.
  Negative
563e315661a801306526774c	X	Here is another example in PHP https://github.com/ienzam/s3-multipart-upload-browser
563e315661a801306526774d	X	@BausTheBig this is/was the first project linked in the answer.
  Negative
Obviously it wasn't clear so thanks to your comment I have updated the answer to make the link more clear.
  Positive
563e315661a801306526774e	X	@BausTheBig actually, the bulk of the project is written in Javascript, it only needs a small server-side endpoint, which can be written in any language easily.
  Negative
The "canonical" endpoint is written in Python though (Flask).
  Negative
563e315661a801306526774f	X	Not supported by S3?
  Negative
docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
563e315661a8013065267750	X	It certainly is supported by S3
563e315661a8013065267751	X	There is NO support for browser-based multi-part uploads, folks... which is what the question was about.
  Negative
Browser-based single-part uploads are supported and multi-part uploads via the web service API are supported.
  Negative
But not both together.
  Negative
563e315661a8013065267752	X	You CAN initiate multipart uploads from a browser via the S3 REST API.
  Negative
using XMLHttpRequest level 2.
  Neutral
Not sure what you are talking about when you say there is no support for "browser-based" multi-part uploads.
  Neutral
563e315661a8013065267753	X	Is it possible to use the HTML 5 File API (for example, this library: https://github.com/23/resumable.js ) in conjunction with the S3 multi-part upload feature?
  Neutral
http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html
563e315661a8013065267754	X	Yes, but you will need some kind of server backend to handle the Amazon API keys in a more secure way if you are going to make it part of a public website.
  Negative
You can find what looks like a complete example implementation of this these projects: Please note that I have not used, tested or reviewed these projects.
  Negative
A rough description of the sequence is as follows: Notes:
563e315761a8013065267755	X	Browser-based POST?
  Negative
Yes.
  Neutral
Multi-part uploads?
  Neutral
No, it isn't currently supported by S3.
  Neutral
:(
563e315761a8013065267756	X	Ok, that makes sense.
  Negative
permanent_url is the property to use, of course :-) Missed that one.
  Negative
It is not mentioned in the documentation either, maybe it should be added.
  Negative
563e315761a8013065267757	X	We are trying to download attachments using the Asana REST API.
  Negative
For attachments in Google Drive and Dropbox the view_url property gives a permanent, non-expiring link, but for attachments in Asana itself (which uses Amazon S3) this is not the case.
  Negative
For S3 the view_url looks like this: https://s3.amazonaws.com:443/prod_object_assets/assets/19422864231098/Time_travelling_-_How_does_the_world_look_like_in_10_years.docx?AWSAccessKeyId=AKIAI7NUHQYARXR2GGCQ&Expires=1415262240&Signature=TCpmP6kKbxl5YQQ554P0MlMw6%2BY%3D#= Notice the "Expires" section in the link.
  Negative
We would very much like to have a permanent link to attachments in S3, is this possible with the Asana REST API?
  Negative
When reading the API documentation it seems like this should be possible: https://asana.com/developers/api-reference/attachments.
  Negative
There's a distinction between download_url and view_url, where it is clearly stated that the download_url may only be valid for one hour.
  Negative
But for view_url there is no such warning, indicating that view_url is a permanent, non-expiring link.
  Negative
But this is not consistent with what we see when using the API (we use the /attachments/attachment-id endpoint).
  Negative
Does anyone know if this is a bug, or is the documentation incorrect?
  Negative
563e315761a8013065267758	X	I think the documentation is incomplete - you're right we should call out that both view_url and download_url should be treated as temporary.
  Negative
If you want a permanent url, use permanent_url.
  Negative
563e315761a8013065267759	X	There is now Amazon WorkMail aws.amazon.com/workmail
563e315761a801306526775a	X	Was that available in November of 2013?
  Negative
Just wondering; thanks.
  Positive
563e315761a801306526775b	X	No, it wasn't.
  Negative
It's new to AWS this year.
  Positive
563e315761a801306526775c	X	I'm relatively new to AWS, but I am trying to figure out how to get AWS to receive emails.
  Negative
According this post How to configure email accounts like support@xyz.com or feedback@xyz.com on AWS SES only handles outbound email.
  Negative
What I am hoping to achieve is the ability to filter aliases.
  Positive
For example, if the alias is "xyz12alias", then any email sent to "xyz12alias@mydomain.co", can see the email and process the content appropriately.
  Negative
Which in my case will be storing it in account associated with the filter.
  Neutral
Can anybody direct me to a strategy or service within AWS that would allow me to implement inbound email on Amazon AWS?
  Negative
https://postmarkapp.com/inbound appears to give me what I want, but is there anything within the AWS framework itself?
  Negative
Are there alternate services to postmarkapp?
  Negative
Thanks.
  Neutral
563e315761a801306526775d	X	You'd have to set up your own server; that's the way to handle it using AWS.
  Negative
They don't provide anything other than their bulk email delivery service.
  Negative
A few links below: http://jeffreifman.com/how-to-install-your-own-private-e-mail-server-in-the-amazon-cloud-aws/ http://cerebellumstrategies.com/2012/04/15/amazon-linux-postfix-dovecot/ Update: there is now a solution available in AWS, as referenced in the comments below.
  Negative
563e315861a801306526775e	X	Amazon Simple Email Service just introduced incoming e-mail support: https://aws.amazon.com/about-aws/whats-new/2015/09/amazon-ses-now-supports-inbound-email/ In addition to offering a scalable, cost-effective email-sending platform, Amazon SES can now accept your incoming emails.
  Negative
You can configure Amazon SES to deliver your messages to an Amazon S3 bucket, call your custom code via an AWS Lambda function, or publish notifications to Amazon SNS.
  Negative
You can also configure Amazon SES to drop or bounce messages you do not want to receive.
  Negative
If you choose to store your messages in Amazon S3, Amazon SES can encrypt your mail using AWS Key Management Service (KMS) before writing it to the bucket.
  Negative
You configure all of these actions by defining receipt rules, which you set up by using the Amazon SES console or the Amazon SES API.
  Negative
Receipt rules enable a single message to trigger multiple actions.
  Neutral
Your rules can be as broad or as specific as you choose because you can configure them to apply to specific email addresses or entire domains.
  Neutral
You can also use receipt rules to control which messages Amazon SES can accept on your behalf.
  Negative
Another filtering method is to set up custom IP address block lists and allow lists.
  Positive
If you know that you don’t want to receive mail originating from a particular IP address range, simply add it to your account's IP address block list.
  Negative
You can also override block lists by adding IP address ranges to your allow list, which provides fine-grained control over your inbound email traffic.
  Negative
563e315861a801306526775f	X	Still doesn't appear to be possible on SES.
  Negative
I'd recommend looking at Mandrill and Sendgrid though.
  Negative
http://mandrill.com/features/ https://sendgrid.com/docs/API_Reference/Webhooks/parse.html
563e315861a8013065267760	X	This really did seem like my best bet, but I'm still consistently hitting my host's limits.
  Negative
Back to the drawing board.
  Neutral
Thanks!
  Positive
563e315861a8013065267761	X	I'm running a backup script using AWS CLI to perform an S3 sync command every night on my MediaTemple server.
  Negative
This has run without fail for months, but I updated my Plesk installation and now every night, when the backup script runs, MediaTemple disables my server due to excessive usage.
  Negative
The limits I seem to be crossing are as follows: They also include a networking snapshot at the time they take the server offline which includes many open connections to Amazon IP addresses (9 at time of the snapshot).
  Negative
Is there anything I can do to throttle the connections to AWS?
  Negative
Preferably I'm looking for an option within the AWS API (though I haven't seen anything useful in the documentation), but barring that, is there something I can do on my end to manage the connections at the network level?
  Negative
563e315861a8013065267762	X	The AWS CLI S3 transfer commands (which includes sync) have the following relevant configuration options: This isn't so granular as throttling packets per second, but it seems like setting a lower concurrent request value and lowering both multipart threshold and chunksize will help.
  Negative
If the values you pasted are close to average, I would start with these values and tweak until you're reliably not exceeding the limits anymore:
563e315861a8013065267763	X	I ended up using Trickle and capping download & upload speeds at 20,000 kb/s.
  Negative
This let me use my existing script without much modification (all I had to do was add the trickle call to the beginning of the command).
  Negative
Also, it looks like bandwidth throttling has been added as an issue to AWS CLI, so hopefully this will all be a non-issue for folks if that gets implemented.
  Negative
563e315861a8013065267764	X	Thanks, it looks really good, but what about licensing issues?
  Negative
Can I use Temboo freely in my projects?
  Neutral
563e315961a8013065267765	X	Yes you can - there are no licensing issues.
  Negative
We simply wrap access to publicly available APIs, making them appear consistent and easier to use.
  Negative
We adhere to API terms of service.
  Negative
563e315961a8013065267766	X	Alright, so I'm only subject to APIs terms, right?I'll take a deep look, but it seems like you did a good work... you can't tell me about the compatibility with Google App Engine, can you?
  Positive
563e315961a8013065267767	X	Exactly, you're subject to the API terms.
  Negative
Also, our free tier limits you to a certain number of calls (10,000) and data transfer (512MB) per month.
  Negative
As regards Google App Engine, we have used Temboo in that context before, but unfortunately it's not something we have documented.
  Negative
563e315961a8013065267768	X	I need to create a Java web app that uses the API of at least two different cloud storage providers (Google Drive, Dropbox, SkyDrive, Mega, ...).
  Negative
I'm wondering if there's someone with experience using these APIs who can tell which are the easiest to use and which are the most difficult...
563e315a61a8013065267769	X	Temboo supports a number of cloud storage APIs, and can generate the Java source code you need to access them.
  Very negative
See here: https://live.temboo.com/library/keyword/storage/ By normalizing API access, Temboo makes talking to one API as easy as talking to the next, so it sounds like something that you'll find useful for this project.
  Positive
Full disclosure: I work at Temboo.
  Neutral
563e320e61a801306526776a	X	Kloudless provides a common API to several different cloud storage APIs (Dropbox, Box, GDrive, OneDrive, etc.).
  Negative
Kloudless also provides SDKs in popular languages and UI widgets to handle authentication and other user interactions.
  Positive
You can find more information and sign up here: https://developers.kloudless.com/ Full disclosure: I work at Kloudless.
  Negative
563e320f61a801306526776b	X	The Amazon S3 service is very simple, and I've had great experiences working with it for large files and large numbers of files in the context of web services.
  Positive
Once you've signed up for the service, you can use the RESTful API to create buckets and upload objects to them.
  Negative
The Java library is the reference library for interfacing with the services, although there are ports to other languages as well (such as boto for Python).
  Negative
563e320f61a801306526776c	X	After a very little research on this subject, I've found out that probably the cloud storage provider with the simplest API is MediaFire, which offers really simple interaction through RESTful services.
  Negative
You can see the API documentation.
  Neutral
I've not yet started working deeply with this API, but it seems to provided all the basic functionalities.
  Negative
The API served by Box seems to be also quite simple.
  Negative
It uses OAuth 2.0, which makes it more secure.
  Negative
See developers website.
  Positive
The SOAP API from 4sync is also really simple.
  Positive
There's no much documentation and the samples in the website seem to be from an older version, but anyway it is very easy to use.
  Negative
See documentation here.
  Positive
563e321061a801306526776d	X	Do you want to send an E-Mail to your own G-Mail Account with a file (attachment)?
  Negative
563e321061a801306526776e	X	i want to create a tool like this viksoe.dk/code/gmail.
  Negative
htm
563e321161a801306526776f	X	Which is fully based around sending an e-mail to your G-Mail account with a file attachment...
563e321161a8013065267770	X	A much better approach than using GMail.
  Negative
Plus Google Docs offers more than enough space for most users as well and if you already have a GMail account, all you need to do is activate it (sign up with the same e-mail address).
  Negative
563e321161a8013065267771	X	I am creating an application which uploads file to gmail account avilable space can any one please tell me the best way to do it ?
  Negative
i read somewhere about using IMAP protocol is the best way to upload files or is there any other good way around ?
  Neutral
regards
563e321261a8013065267772	X	GMail is not suitable for keeping generic files.
  Negative
Google offers Documents and Data API mechanisms for storing files.
  Negative
You might want to look at them.
  Neutral
BTW we have products, which combined let you create a virtual drive with Google backend storage in a couple of hours.
  Negative
Callback File System offers a virtual drive, and CloudBlackbox lets you store data on Amazon S3, MS Azure and on GMail storages.
  Negative
563e321261a8013065267773	X	see gist.github.com/Offbeatmammal/3718414 for a working sample
563e321261a8013065267774	X	I am attempting to play an HTML5 video within my WebView app.
  Negative
It works as expected on every device I have tested that is running Android 5.x, but does not work on any device running 4.x, meaning it essentially doesn't work at all.
  Very negative
I have turned on hardware acceleration and I have set a WebChromeClient as the docs say to do, but the video still will not play.
  Negative
In order to support inline HTML5 video in your application, you need to have hardware acceleration turned on, and set a WebChromeClient.
  Negative
AndroidManifest.xml MyFragment.java Is there something else I need to do that is not documented in the developer reference?
  Negative
563e321261a8013065267775	X	The problem is WebKit poorly handles redirects for videos.
  Negative
There are videos within webpages from my company's proprietary API.
  Positive
When a video is clicked, the call goes to our API, then redirects to Amazon S3 to get the actual video file.
  Negative
WebKit then tries to "chunk" the video (instead of pre-loading the entire thing) as you would expect.
  Negative
However, S3 has already done that, which causes the playback to be completely broken.
  Neutral
Android 5.x works fine because WebView is based upon Chromium starting in 4.4, which handles the redirect appropriately.
  Positive
563e321361a8013065267776	X	Why can't you catch SocketException directly?
  Negative
The fact that it isn't a checked exception doesn't mean that you cannot catch it.
  Negative
563e321361a8013065267777	X	I think that's only true for exceptions that inherit from RuntimeException.
  Negative
Trying to catch SocketException here results in a compilation error because that exception itself is not thrown.
  Negative
563e321361a8013065267778	X	Ok I see what you mean now.
  Positive
SocketException is a checked exception but the AmazonS3Client is not passing it up, right?
  Negative
563e321361a8013065267779	X	The javadoc says that putObject() throws AmazonClientException when this kind of error occurs.
  Negative
AmazonClientException has a method called isRetryable(), maybe you can try with that.
  Negative
563e321361a801306526777a	X	@DavidLevesque You're right and that basically answers my question.
  Positive
Write it up as an answer and I will accept it.
  Positive
563e321461a801306526777b	X	So basically crank up the timeouts and retries to avoid the exception being thrown in the first place?
  Negative
It would seem to greatly decrease the likelihood of the exception being thrown, but not eliminate it.
  Negative
563e321461a801306526777c	X	I have a ThreadPoolExecutorService to which I'm submitting runnable jobs that are uploading large (1-2 GB) files to Amazon's S3 file system, using the AWS Java SDK.
  Negative
Occasionally one of my worker threads will report a java.net.SocketException with "Connection reset" as the cause and then die.
  Negative
AWS doesn't use checked exceptions so I actually can't catch SocketException directly---it must be wrapped somehow.
  Negative
My question is how I should deal with this problem so I can retry any problematic uploads and increase the reliability of my program.
  Negative
Would the Multipart Upload API be more reliable?
  Neutral
Is there some exception I can reliably catch to enable retries?
  Negative
Here's the stack trace.
  Neutral
The com.example.
  Neutral
* code is mine.
  Neutral
Basically what the DataProcessorAWS call does is call putObject(String bucketName, String key, File file) on an instance of AmazonS3Client that's shared across threads.
  Negative
563e321461a801306526777d	X	The javadoc says that putObject() throws AmazonClientException when this kind of error occurs.
  Negative
AmazonClientException has a method called isRetryable(), you can try with that.
  Positive
563e321561a801306526777e	X	'Connection reset' means the connection is hosed.
  Neutral
Close the socket, or whatever higher-level construct you're using.
  Neutral
Probably the server has decided the upload is too large, or it's overloaded, or something.
  Negative
Whether you can retry the operation is something only you can know.
  Neutral
563e321561a801306526777f	X	
563e321561a8013065267780	X	Did you manage to fix this problem?
  Negative
If so, please, share your experience
563e321561a8013065267781	X	Hi, Did you manage to get this solved?
  Negative
563e321561a8013065267782	X	I'm not using S3 to set cookies, I'm using a django backend to set cookies on the S3 domain but for some reason that's not happening.
  Negative
.
  Neutral
563e321661a8013065267783	X	S3 is ignoring sent cookies and not sending back any other cookies.
  Negative
Doesn't matter on how you achieved that the browser is accepting a cookie on the "S3 domain", the S3 service is not working like this.
  Negative
563e321661a8013065267784	X	I have an api endpoint for static S3 hosted site.
  Negative
The S3 site lives on the domain name: www.mysite.com My api (django) runs on the site the domain name: api.mysite.com When I use my login button on my site and sign in using proper username/password django sends back response with a Set-Cookie but the browser doesn't set any cookies.
  Negative
You can see the full response below, note the line Set-Cookie:sessionid=3kn2hovtweeofalf00ld3lowb6yvete; Domain=.
  Negative
mysite.com; expires=Mon, 27-May-2013 22:21:54 GMT; Max-Age=1209600; Path=/ In Django I have the SESSION_COOKIE_DOMAIN = '.
  Very negative
mysite.com' but I've tried changing it to 'mysite.com' and '' neither of which has allowed my browser to set this returned cookie.
  Negative
www.msyite.com is a static site hosted on Amazon S3 but I'm using Django as my api/backend for data.
  Negative
When I render my login pages using Django they work just fine (login/logout cookies and sessions all work fine, so I know it's not my django code) but when using S3 or even a python SimpleHTTPServer the browser doesn't set the returned cookie.
  Negative
Thanks in advance!
  Neutral
563e321661a8013065267785	X	AFAIK the purpose of a static S3 site is to not set/accept any cookies at all?
  Negative
We're using that for static content like images in order to get rid of all that cookie stuff and not have the overload in the request during image or CSS requests.
  Negative
And: S3 is not a usual web server, it behaves very different, RTM is highly recommended.
  Negative
It's weired, true, but the answer I distilled from this thread Can S3 set a user cookie?
  Negative
is just "No."
  Negative
563e321661a8013065267786	X	+1 Thanks for your fast answer!
  Neutral
The client is concerned about privacy, and I was trying to not pay some 3rd parties for upload services...
563e321661a8013065267787	X	All I can say is find a better server :/ 10MB is an extremely small amount of space.
  Negative
563e321761a8013065267788	X	Without any extra information, I'm not sure.
  Negative
Amazon EC2 offers a free 612MB RAM server free (not sure how many gigs of storage) for one year.
  Negative
If you have experience with Linux servers, you may want to give it a try.
  Negative
563e321761a8013065267789	X	Thank you!
  Positive
I'll leave this topic opened a bit before accepting.
  Positive
Thanks!
  Positive
I'm really out of imagination how to solve this.
  Negative
563e321761a801306526778a	X	I have a classic form and an upload image input that allows visitors to enter details and one image.
  Positive
currently I store images on the client server and send just the image URL to the client email, but recently the folder reached the limit of 10MB and no further images uploads were possible and further emails failed submission too.
  Negative
I tried for a long time to somehow send the form with an uploaded image the the client email, without storing it server side but with no success.
  Negative
Googled all around and tried lot of suggestions - I was just able to send the form but with no image attached to it.
  Negative
What can I do?
  Neutral
563e321761a801306526778b	X	You should be able to get the binary contents of the image with file_get_contents($image) using the tmp location when you submit the form and then use that data in the e-mail.
  Negative
The approach to display in the e-mail will vary depending on if you are using html e-mails or not.
  Neutral
This link may help php: recreate and display an image from binary data
563e321761a801306526778c	X	It is not possible to attach a file to an e-mail without first storing it on the server.
  Negative
You may consider using a service such as Amazon S3 to host the files.
  Negative
That way, you don't have to handle the files and you can still e-mail the URLs, which makes everyone happier.
  Negative
Alternatively, you may be able to find a service such as ImageShack that provides an API to upload pictures to.
  Positive
Good luck!
  Positive
563e321961a801306526778d	X	Thanks Peter, Is there any example or a tutorial to show the method of accessing Google Storage with java?
  Negative
That will be a great help for me.
  Positive
thanks again.
  Positive
563e321961a801306526778e	X	I am developing an application on google app engine with gwt.
  Negative
There is a requirement for this application to store files(eg: pdf, msword,.
  Negative
zip ect).
  Negative
I tried to use amazon S3 in Google app engine.
  Negative
But it fails because app enigine does not allow me to write file handling code if the app is running on Google app engine.
  Negative
One another option I tried was using Blobstore which limit to store files lesser than 1Mb per API call.
  Negative
Is there any other option to store large files at least 10mb with Google app engine?
  Negative
563e321c61a801306526778f	X	Possible solutions: Blobstore API is limited to 1Mb per API call.
  Negative
You could chunk up your file into multiple calls: 1MB quota limit for a blobstore object in Google App Engine?
  Negative
Use Google Storage and it's API.
  Negative
563e321d61a8013065267790	X	When I store files with GAE I use next aproach: 1 In my JSP page create a form with file input: 2 Write a servlet/controller that read binary data from request 3 Write this data to DataStore (limited with 1MB) or into BlobStore (one call to API is limited with 1MB, but total file size can be up to 2GB).
  Negative
For files with size > 1MB you have to do few API calls.
  Positive
563e321d61a8013065267791	X	Ok sounds great and I will definitely do it like this.
  Positive
How would you go ahead if you wanted to store information about the file on GAE Data Store?
  Neutral
Like the user who stored it and the mime-type for example?
  Neutral
563e321d61a8013065267792	X	Store that information when the user requests the 'redirect' page.
  Negative
You can do a HEAD request on the newly-uploaded file to fetch the metadata, if necessary.
  Negative
563e321d61a8013065267793	X	What about security issues?
  Neutral
I mean there is no way to validate the data (except AJAX) in the form before submitting it to S3 right?
  Negative
So basically if I set the max.
  Neutral
file-size within the form like Amazons suggests it you can just write your own form and upload to my bucket?
  Positive
And the meta-data I would add within the form can also easily be modified...
563e321d61a8013065267794	X	Never mind.
  Negative
I found the encrypted policy file ;)!
  Neutral
Thanks so much for your help guys!
  Positive
I am new to Stack Overflow but this is amazing!
  Positive
563e321d61a8013065267795	X	Okay, so far so good.
  Positive
But if I have a 20MB file and I use the Amazon S3 Python Library to send that file to S3...won't GAE kill the process because it takes longer than 30 seconds?
  Negative
563e321d61a8013065267796	X	To be honest, I dont really know GAE's limitations, I just looked at it briefly and its flaws were way to apparent and limiting for my particular uses.
  Very negative
To be honest, outside the fact they have a free edition available, I see very little to recommend it.
  Positive
563e321d61a8013065267797	X	I'm pretty sure he already knew all this - and it's not what he was asking.
  Negative
563e321e61a8013065267798	X	That link states maximum object size: 2 gigabytes.
  Negative
I don't know when it changed, but it's still good news :)
563e321e61a8013065267799	X	Thanks for the shout-out ;)
563e321e61a801306526779a	X	Also, I should note, that there's a decent chance it will work with S3 as well.
  Positive
Give it a try just by removing the filter from the build.xml file and testing it out.
  Negative
I'd be curious what the results are.
  Neutral
563e321e61a801306526779b	X	I know this has been asked before but there is really not a clear answer.
  Negative
My problem is I built a file upload script for GAE and only found out after, that you can only store files up to aprox.
  Negative
1MB in the data store.
  Neutral
I can stop you right here if you can tell me that if I enable billing the 1MB limit is history but I doubt it.
  Negative
I need to be able to upload up to 20mb per file so I thought maybe I can use Amazon's S3.
  Neutral
Any ideas on how to accomplish this?
  Neutral
I was told to use a combination of GAE + Ec2 and S3 but I have no idea how this would work.
  Negative
Thanks, Max
563e321e61a801306526779c	X	From the Amazon S3 documentation: The user opens a web browser and accesses your web page.
  Negative
Your web page contains an HTTP form that contains all the information necessary for the user to upload content to Amazon S3.
  Neutral
The user uploads content directly to Amazon S3.
  Negative
GAE prepares and serves the web page, a speedy operation.
  Negative
You user uploads to S3, a lengthy operation, but that is between your user's browser and Amazon; GAE is not involved.
  Negative
Part of the S3 protocol is a *success_action_redirect*, that lets you tell S3 where to aim the browser in the event of a successful upload.
  Negative
That redirect can be to GAE.
  Neutral
563e321e61a801306526779d	X	Google App Engine and EC2 are competitors.
  Negative
They do the same thing, although GAE provides an environment for your app to run in with strict language restrictions, while EC2 provides you a virtual machine ( think VMWare ) on which to host your application.
  Negative
S3 on the other hand is a raw storage api.
  Positive
You can use a SOAP or REST api to access it.
  Neutral
If you want to stick with GAE, you can simply use the Amazon S3 Python Library to make REST calls from Python to S3.
  Negative
You will, of course, have to pay for usage on S3.
  Negative
Its amazing how granular their billing is.
  Positive
When getting started I was literally charged 4 cents one month.
  Positive
563e321e61a801306526779e	X	For future reference, Google added support for large file upload (up to 50 MB): The new feature was released as part of the Blobstore API and is discussed here.
  Negative
563e321e61a801306526779f	X	Thomas L Holaday's answer is the correct answer, I suppose.
  Negative
Anyway, just in case, here's a link to Amazon Web Services SDK for App Engine (Java), which you can use e.g. to upload files from App Engine to Amazon S3.
  Negative
(Edit: Oh, just noticed -- excepting S3) http://apetresc.wordpress.com/2010/06/22/introducing-the-gae-aws-sdk-for-java/ Written by Adrian Petrescu.
  Negative
From his web site: [It is] a version of the Amazon Web Services SDK for Java that will run from inside of Google App Engine.
  Negative
This wouldn’t work if you simply included the JAR that AWS provides directly into GAE’s WAR, because GAE’s security model doesn’t allow the Apache Commons HTTP Client to create the sockets and low-level networking primitives it requires to establish an HTTP connection; instead, Google requires you to make all connections through its URLFetch utility
563e321e61a80130652677a0	X	Some Google App Engine + S3 links: Previous related post... 10mb limit.
  Very negative
This link demonstrates small file uploads.
  Negative
I haven't found an example of large uploads yet... This link shows a different approach, (with a fix for a known issue)
563e321e61a80130652677a1	X	I'm working on integrating an existing app with File Picker.
  Negative
In our existing setup we are relying on md5 checksums to ensure data integrity.
  Negative
As far as I can see File Picker does not provide any md5 when they respond to an upload against the REST API (nor using JavaScript client).
  Negative
We are using S3 for storage, and as far as I know you may provide S3 with an md5 checksum when storing files so that Amazon may verify and reject storing request if data seems to be wrong.
  Very negative
To ensure that data is not corrupted traversing the network, use the Content-MD5 header.
  Negative
When you use this header, Amazon S3 checks the object against the provided MD5 value and, if they do not match, returns an error.
  Negative
Additionally, you can calculate the MD5 while putting an object to Amazon S3 and compare the returned ETag to the calculated MD5 value.
  Negative
I have investigated the etag header which Amazon returns a bit, and found that it isn't clear what actually is returned as etag.
  Negative
The Java documentation states: Gets the hex encoded 128-bit MD5 hash of this object's contents as computed by Amazon S3.
  Negative
The Ruby documentation states: Generally the ETAG is the MD5 of the object.
  Negative
If the object was uploaded using multipart upload then this is the MD5 all of the upload-part-md5s Another place in their documentation I found this: The entity tag is a hash of the object.
  Negative
The ETag only reflects changes to the contents of an object, not its metadata.
  Negative
The ETag is determined when an object is created.
  Neutral
For objects created by the PUT Object operation and the POST Object operation, the ETag is a quoted, 32-digit hexadecimal string representing the MD5 digest of the object data.
  Negative
For other objects, the ETag may or may not be an MD5 digest of the object data.
  Negative
If the ETag is not an MD5 digest of the object data, it will contain one or more non-hexadecimal characters and/or will consist of less than 32 or more than 32 hexadecimal digits.
  Negative
This seems to describe how etag is actually calculated on S3, and this stack overflow post seems to imply the same thing: Etag cannot be trusted to always be equal to the file MD5.
  Negative
563e321e61a80130652677a2	X	
563e321f61a80130652677a3	X	That layout looks a lot like Shoji: aminus.org/rbre/shoji/shoji-draft-02.txt
563e321f61a80130652677a4	X	I'm designing a REST API.
  Negative
The part I'm working on now involves simply reading objects in the form of JSON data off of the server and modifying them.
  Negative
The resource that I am thinking of using for this looks like: /data/{table name}/{row key} I would like to allow GET and PUT operations on this resource.
  Negative
The question that I am wrestling with is that I would like to return other data along with the JSON object such as customer messages, the amount of time it took for the round trip to the data base, etc..
  Negative
.
  Neutral
I would also like to allow for sending query arguments with the payload in cases where the URL would be too long if they were included there.
  Negative
So the resources would work like this: GET Server returns: PUT Client sends as payload: I'm afraid this might violate the rules for proper RESTful GET and PUT resources because what you are sending to the server is not exactly what you are getting back out since other information is being included in the payloads.
  Very negative
I'd rather not have every operation be a POST as a remedy.
  Negative
Am I being too much of a stickler with this?
  Negative
Is there some other way I should be structuring this?
  Neutral
Thanks!
  Positive
I should note that in the resource: /data/{table name}/{row key}, I used 'table name' and 'row key' for simplicity.
  Negative
This is for use with a noSQL database.
  Neutral
This resource is intended to work similar to Amazon S3.
  Negative
"uuid" would actually be a better description than 'row key'.
  Neutral
563e321f61a80130652677a5	X	I see nothing wrong with your approach.
  Negative
But if I was implementing this scenario, I would have asked myself the following questions: Let's take as an example "response time".
  Negative
If it's part of your resource, your approach is perfect and nothing else should be done.
  Positive
However, if it's not part of the resource, return it as a HTTP header.
  Neutral
Fair enough.
  Positive
563e321f61a80130652677a6	X	As for me it just depends on how additional info is going to be used.
  Negative
For my customers responseTime is not a question (or at least I think so :), they just need that response.
  Negative
For me as developer it can help debugging.
  Positive
So when customer gives me slow request I can test it easy, and that extra information could help.
  Positive
Here I mean that it is possible to create simple url as you specified /data/{table name}/{row key} and send just response according to that request, and you can make one more url /data/{table name}/{row key}/debug or whatever else to get the same data with additional info like "reponseTime".
  Negative
Just an idea ;) UPDATE: Ah yes, forgot: do not use table-name as part of your url, at least modify its name.
  Negative
I don't like to tell anybody how my tables are called, if somebody is going to hack my DB injecting extra code I would like her to spend more time looking for any information, instead of giving her info on a plate :)
563e321f61a80130652677a7	X	I don't see anything wrong with this, looks pretty standard to me.
  Negative
I'm not sure what you are planning to pass in queryArguments, is this where you would specify a callback to execute for JSON-P clients?
  Negative
Only thing I'd recommend you keep in mind is that REST deals with resources, and that does not necessarily map 1-to-1 with tables.
  Negative
Instead of using a row key you might want to have some type of GUID or UUID that you can map to that resource.
  Negative
563e321f61a80130652677a8	X	I doesn't appear in the meta data--whether an object is a folder or not.
  Negative
Is there a specific method you guys on SO know of?
  Neutral
I can't find anything of worth in Google search.
  Negative
563e321f61a80130652677a9	X	Objects are not folders.
  Negative
From the docs: An Amazon S3 bucket has no directory hierarchy such as you would find in a typical computer file system.
  Negative
You can, however, create a logical hierarchy by using object key names that imply a folder structure.
  Positive
The best you can do is use GET Bucket (list objects) to get some info about the contents of the bucket: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e321f61a80130652677aa	X	There is no folder concept on S3.
  Negative
All objects have a key (like name) and keys can contain special characters like / (slash).
  Negative
This gives us a feel of folders.
  Positive
When you list the bucket contents it returns the list of all the objects (and the keys).
  Negative
Then you can see if the key string contains slash (/).
  Neutral
If it contains, then understand the object is in a "folder like" structure.
  Negative
That way you get the full details.
  Neutral
"A response can contain CommonPrefixes only if you specify a delimiter.
  Negative
When you do, CommonPrefixes contains all (if there are any) keys between Prefix and the next occurrence of the string specified by delimiter.
  Negative
In effect, CommonPrefixes lists keys that act like subdirectories in the directory specified by Prefix.
  Negative
For example, if prefix is notes/ and delimiter is a slash (/), in notes/summer/july, the common prefix is notes/summer/.
  Negative
All of the keys rolled up in a common prefix count as a single return when calculating the number of returns.
  Neutral
See MaxKeys."
  Positive
http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html
563e322061a80130652677ab	X	using AWS-CLI, this should work: aws s3 rm s3://bucket/folder1/folder2/ --recursive.
  Negative
This will remove everything from folder2 and below.
  Negative
563e322061a80130652677ac	X	will this delete all versions permanently and not leave delete markers in the objects' place?
  Negative
563e322061a80130652677ad	X	No, above command will leave the marker behind.
  Negative
For deleting versioned objects, the version number has to be specified in the delete command.
  Negative
You may have to write a quick hack to get the object IDs along with versions and then run a delete command in the loop them
563e322061a80130652677ae	X	OK you're confirming my suspicions in the original post... thanks.
  Negative
563e322261a80130652677af	X	Thanks John, I've read about that as well but we need to target specific objects in our bucket on an ad hoc basis so a lifecycle rule won't apply here.
  Negative
563e322361a80130652677b0	X	According to the documentation objects can only be deleted permanently by also supplying their version number.
  Negative
I had a look at Python's Boto and it seems simple enough for small sets of objects.
  Positive
But if I have a folder that contains 100 000 objects, it would have to delete them one by one and that would take some time.
  Negative
Is there a better way to go about this?
  Neutral
563e322361a80130652677b1	X	An easy way to delete versioned objects in an Amazon S3 bucket is to create a lifecycle rule.
  Negative
The rule activates on a batch basis (Midnight UTC?)
  Negative
and can delete objects within specified paths and it knows how to handle versioned objects.
  Negative
See: Such deletions do not count towards the API call usage count, so it can be cheaper, too!
  Negative
563e322361a80130652677b2	X	dupe: stackoverflow.com/questions/651171/usermode-file-system
563e322361a80130652677b3	X	not a dupe, I am not looking for a library to do this for me, I am looking to do it myself.
  Negative
563e322361a80130652677b4	X	although I might end up having to use that ;)
563e322361a80130652677b5	X	By the way although I hardly care about whether it is going to be user mode or not (I'd even prefer system-mode actually so that the data would be available to system services without need for a user to log-in) I am really curious about writing a file system driver in C# and could find nothing interesting on this subject so far...
563e322461a80130652677b6	X	+1 this worked for me....
563e322461a80130652677b7	X	To the best of my understanding Dokan needs to be installed as a kernel-mode driver (Dokan.sys), and only after this can you implement your own user-mode code (optionally in .
  Very negative
Net) layered on top...
563e322461a80130652677b8	X	@user424257 LGPL allows use in commercial projects (GPL does too in fact).
  Positive
Dokan has it's own shortcomings, though, such as a huge tail of bugs and lack of support and maintenance.
  Negative
563e322461a80130652677b9	X	how is this pure usermode?
  Negative
It's a kernel driver, isn't it??
  Negative
563e322461a80130652677ba	X	Corection - you don't need Microsoft signing for the driver to be loaded.
  Negative
The driver must be signed and countersignature must be included, but it's a different matter.
  Negative
Microsoft signing (WHQL certification, to be correct) is a different story.
  Negative
563e322461a80130652677bb	X	Rather than talking to local hardware, I am actually looking from the other end of it .
  Negative
.
  Neutral
how to make my abstracted file system appear as a drive in Windows.
  Neutral
563e322461a80130652677bc	X	I appreciate it, although it is your product and you can charge whatever you want for it, but $4K for a 'discounted' private business vendor license is a little too steep for me
563e322461a80130652677bd	X	@esac writing drivers never was cheap, and maintaining them is quite expensive as well.
  Very negative
"Free" Dokan is full of bugs with no release cycle.
  Negative
So you pay with either your money or your disappointed customers.
  Negative
563e322461a80130652677be	X	I agree fully, and you obviously should charge for your product.
  Negative
As a developer who writes a lot of tools in his free time, most of them which only I use personally and they never see the light of day, I can't justify 4K.
  Negative
I would be glad to pay a MUCH smaller fee for a personal use, non-redistributable copy, and if for some reason I ever wanted to release something, I would be far more willing to shell out the 4K required.
  Negative
563e322461a80130652677bf	X	Have an evaluation key that works for 6 months.
  Neutral
After 6 months, the user would need to request a further evaluation key.
  Negative
If you see a copy of a program out in the wild using one of these evaluation keys, you don't give them out to that user anymore, and their software stops working.
  Negative
563e322461a80130652677c0	X	@MatBee we have private business licenses and free non-commercial licenses.
  Negative
The only restriction is no open-source.
  Negative
563e322461a80130652677c1	X	Is it possible to write a filesystem for Windows in pure usermode, or more specifically purely in managed code?
  Neutral
I am thinking of something very similar to GMAILFS.
  Negative
Excluding what it is doing under the covers (GMAIL, Amazon, etc..)
  Negative
the main goal would be to provide a drive letter and support all of the basic file operations, and possibly even adding my own structures for storing metadata, etc..
  Negative
563e322461a80130652677c2	X	It's difficult.
  Negative
I'd take a look at some projects which have done some of the hard work for you, e.g. Dokan.
  Neutral
563e322561a80130652677c3	X	Yes.
  Neutral
It's possible and has been successfully done for the ext2 filesystem.
  Positive
Note that you will need to write your own driver which will require Microsoft signing to be run on some OSes.
  Negative
563e322561a80130652677c4	X	Sure, you can abstract the regular file operations and have them running in the cloud (see Google Apps, Amazon S3, Microsoft Azure etc.).
  Negative
But if you'd like to talk to local devices - including the local HD - you'll have to use system APIs and those use drivers (system/kernel mode).
  Negative
As long as all you want is a storage service -no problem.
  Negative
If you want a real OS, you'll need to talk to real hardware and that means drivers.
  Positive
563e322561a80130652677c5	X	Just as a reference - our Callback File System is a maintained and supported solution for creation of filesystems in user-mode.
  Negative
563e322561a80130652677c6	X	How about an URL so we can have a look at it?
  Neutral
563e322561a80130652677c7	X	thanks, i didn't know it was called that.
  Negative
I will get jquery from google.
  Negative
563e322561a80130652677c8	X	i have to try moving my scripts to the bottom of the page.
  Negative
Does this include css scripts as well?
  Negative
563e322561a80130652677c9	X	No, CSS style sheets go at the top of the page.
  Negative
For more info and other best practices, see developer.yahoo.com/performance/rules.html
563e322561a80130652677ca	X	re: free web account idea: If I move my images to a Google Sites account, do you know if Google Sites' servers are just as fast as Google.com?
  Negative
563e322561a80130652677cb	X	They should be served using the same infrastructure, so I reckon that it will be just as fast.
  Negative
They do have the option for additional bandwidth at cost, but you can just use the free bandwidth until you get an idea of the bandwidth needed.
  Negative
563e322561a80130652677cc	X	I am thinking to save server load, i could load common javascript files (jquery src) and maybe certain images from websites like Google (which are almost always never down, and always pretty fast, maybe faster than my server).
  Negative
Will it save much load?
  Neutral
Thanks!
  Positive
UPDATE: I am not so much worried about saving bandwidth, as I am reducing Server Load because my server has difficulty when there are a lot of users online, and I think this is because there are too many images/files it loads from my single server.
  Very negative
563e322561a80130652677cd	X	This is known as a content delivery network, and it will help, although you should probably make sure you need one before you go about setting it all up.
  Negative
I have heard okay things about Amazon S3 for this (which Twitter, among other sites, use to host their images and such).
  Neutral
Also, you should consider Google's API cloud if you are using any popular javascript libraries.
  Negative
563e322661a80130652677ce	X	You might consider putting up another server that does nothing but serve your static files using an ultra efficient web server such as lighttpd
563e322661a80130652677cf	X	Well, there are a couple things in principle: So if you are truly not experiencing any bandwidth problems, I don't think offloading your images, etc will do much for you.
  Very negative
However, as you move stuff off to Google, then it frees your server's bandwidth up for more concurrent requests and faster transfer on the existing ones.
  Neutral
The only tradeoff here is that clients will experience a slight (most likely unnoticable) initial delay while DNS looks up the other servers and initiates the connection to them.
  Negative
563e322661a80130652677d0	X	It really depends on what your server load is like now.
  Negative
Are there lots of small web pages and lots of users?
  Negative
If so, then the 50K taken up by jQuery could mean a lot.
  Negative
If all of your pages are fairly large, and/or you have a small user base, caching jQuery with Google might not help much.
  Negative
Same with the pictures.
  Neutral
That said, I have heard anecdotal reports (here on SO) that loading your scripts from Google does indeed provide noticeable performance improvement.
  Negative
I have also heard that Google is not necessarily 100% uptime (though it is close), and when it is down it is damned inconvenient.
  Negative
If you're suffering from speed problems, putting your scripts at the bottom of the web page can help a lot.
  Negative
563e322861a80130652677d1	X	I'm assuming you want to save costs by offloading commonly used resources to the web at large.
  Negative
What you're suggesting is called Hotlinking.
  Negative
.
  Neutral
that means directly linking to other people's content.
  Neutral
While it can work in most cases, you do lose control of the content, that means your website may change without your input.
  Negative
Since image hosted on google are scoured from other websites, the images may be copyrighted, causing some (potential) concern, or they may have anti-hotlinking measures that may block the images from your webpage.
  Negative
If you're just working on a hobby website, you can consider hosting your resources on a free web account to save bandwidth.
  Negative
563e322961a80130652677d2	X	I would remove your personal comments about Perl and Java
563e322961a80130652677d3	X	I need to create RESTful API for uploading media data.
  Negative
I need to be able to handle hundreds (thousands) of simultaneous requests.
  Neutral
Once data is uploaded to my server, we are going to store it on Amazon S3 and populate some meta data into database.
  Negative
Could you advice in a few questions: 1) Which language is better for these kind of tasks ?
  Negative
(I'm familiar with PHP and Perl) 2) What about server?
  Negative
(nginx ?)
  Neutral
3) We need to be able to scale easily in case there are a lot of requests 4) Anything else you could point out and advice ?
  Negative
Thank you
563e322a61a80130652677d4	X	See stackoverflow.com/a/16537396/165673
563e322a61a80130652677d5	X	did you just do this or it is from your working code?
  Negative
563e322a61a80130652677d6	X	Approximately 2-3 days of work some months ago.
  Negative
But happy to share it (I also submitted a patch to plupload because of issues I had during implementation).
  Positive
563e322a61a80130652677d7	X	Does anyone have any sample code (preferrably in rails) that uploads to s3, using s3's servers.
  Negative
Again, uploading directly to s3, where the actual upload/streaming is also preformed on amazon's servers.
  Negative
563e322a61a80130652677d8	X	Requirements: Idea: I posted the code as a gist at https://gist.github.com/759939, it misses commments and you might run into some issues due to missing methods (had to rip it from our codebase).
  Negative
stored_file.
  Neutral
rb contains a model for your DB.
  Positive
Has many of paperclips helper methods inlined (which we used before we switched to direct upload to S3).
  Negative
I hope you can use it as a sample to get your stuff running.
  Neutral
563e322b61a80130652677d9	X	If you are using Rails 3, please check out my sample projects: Sample project using Rails 3, Flash and MooTools-based FancyUploader to upload directly to S3: https://github.com/iwasrobbed/Rails3-S3-Uploader-FancyUploader Sample project using Rails 3, Flash/Silverlight/GoogleGears/BrowserPlus and jQuery-based Plupload to upload directly to S3: https://github.com/iwasrobbed/Rails3-S3-Uploader-Plupload
563e322b61a80130652677da	X	To simply copy files, this is easy to use: Smart Copy Script into S3
563e322b61a80130652677db	X	Amazon wrote a Ruby library for the S3 REST API.
  Negative
I haven't used it yet.
  Negative
http://amazon.rubyforge.org/
563e322b61a80130652677dc	X	No, it doesn't work because it requires a String in the parameter.
  Negative
My problem is that I couldn't get the path of each file in my bucket.
  Negative
563e322c61a80130652677dd	X	If it requires a string then it must expect that string to be a path to a local file.
  Negative
In that case, the only option is to copy the file from S3 to your local filesystem (using key.get_contents_to_filename) and then pass the path to the local file to the Mp3AudioFile class.
  Negative
563e322c61a80130652677de	X	Well, I need to get the paths + mp3 metadata of these files to store it in a MySQL database for my Django app, so later I'll be able to play them in on my website.
  Negative
Is there any other library (other than eyeD3) that allows me to read tags from a non-local file?
  Negative
563e322c61a80130652677df	X	I don't, sorry.
  Negative
If you want the full URL for the MP3 files as stored in S3 you can call key.generate_url(1, query_auth=False, force_http=True) for each key.
  Negative
This will give you a URL pointing to the object in S3.
  Positive
Hope that helps.
  Neutral
563e322c61a80130652677e0	X	That means the files are not publicly readable.
  Negative
Do you want them to be?
  Neutral
Or are you intended these to be accessible only through your application?
  Neutral
You can create signed URL's that expire in a period of time.
  Positive
For example, key.generate_url(60, force_http=True) would generate a signed URL that will expire (i.e. become unusable) in 60 seconds.
  Negative
I don't really know what your application is so I'm not sure what to suggest.
  Negative
563e322c61a80130652677e1	X	I'll be running the update (meta data extraction from mp3 files in AS3 and upload the data to MySQL database) from the Django admin panel.
  Negative
So I should download the files temporarely into my server and then get the metadata from them?
  Negative
563e322c61a80130652677e2	X	@MohamedTurki - Is this a one-off task?
  Neutral
If yes, then I'd write a completely separate script in whatever language you prefer to do the indexing?
  Negative
I wouldn't want to download the files to my web server even if temporarily.
  Negative
563e322c61a80130652677e3	X	emm, I'm not sure how frequently I'll be doing this, perhaps once in three months.
  Negative
Problem is, i'll need to fill the database from different sources, AS3 to get the url of each mp3 file, and from local drive to get metatags of the same mp3 file.
  Negative
Maybe I'm not giving this task as much attention as it deserves, as I thought it wouldn't be this complicated, maybe I should write a script that uploads files and extract metal data at the same time?
  Negative
Is that feasible?
  Neutral
563e322c61a80130652677e4	X	@MohamedTurki I would absolutely go down the route of 'indexing' the files during the upload process.
  Negative
S3 buckets can be somewhat difficult to manage with lots of files if you don't already have a firm grip of what's in there.
  Negative
This should be easy to implement.
  Neutral
Upload to your server, extract the tags and add to DB along with any other info about file (incuding details of S3 location eg bucket subfolder etc in case you want to move some files in time), move to S3.
  Negative
If your server is in EC2 then this is even better as transfers between EC2 and S3 are free and very fast.
  Positive
563e322c61a80130652677e5	X	I'm trying to get mp3 tags from my files that stored in Amazon S3 using Boto.
  Negative
Here is my script : However, I could list all the files in my bucket and so on.
  Negative
The error i'm getting is Is there any problem with my code?
  Negative
563e322c61a80130652677e6	X	You are passing key.name to the eyeD3 functions but I think you want a file-like object for the call to eyeD3.Mp3AudioFile.
  Negative
I haven't used eyeD3 and it doesn't seem to want to install via pip so I can't try this but something like this should work:
563e322c61a80130652677e7	X	There is no way to get the tags from the files without downloading them from S3.
  Very negative
You might consider using EC2 to process the files or Amazons Elastic MapReduce but you're still going to be downloading the file to read the tags.
  Negative
563e322c61a80130652677e8	X	I had to write a script that the meta data of the mp3 files from my local drive, uploads the songs to Amazon S3 (Using Boto API) and set privileges to "public", generates a URL, then store the URL and metal data into a MySQL database.
  Negative
So just in case some runs into same problem, this solved my issue as I now don't need to upload the songs and then run an update for my database.
  Very negative
563e322c61a80130652677e9	X	Thanks, I already thought so.
  Negative
Amazon seems a pretty good bet, thanks for the hint!
  Very positive
563e322c61a80130652677ea	X	We are implementing a leveleditor for our game and want to enable submitting the user-generated levels.
  Negative
All other users of the game should be able to play these levels.
  Positive
Is it possible to store the levels in GameCenter or is the only way to achieve this to set up a dedicated server?
  Negative
In case we have to go with our own server, are there any preconfigured services for this scenario?
  Negative
Something like dropbox, with a nice API instead of having to code everything ourselves from the ground up.
  Positive
Thanks a lot!
  Positive
563e322d61a80130652677eb	X	GameCenter does not currently have support for downloadable content.
  Negative
You could peer-to-peer share small payloads of data between players using the matching API, but that's pretty clearly not what you want.
  Negative
In thinking about other services, the first/easiest thing that comes to mind for me would be to use Amazon S3.
  Negative
It's super-simple, reasonably cheap, has good content distribution, availability, etc. (Sure beats running your own server, anyway.)
  Negative
563e322d61a80130652677ec	X	Great, thank a lot!
  Positive
563e322d61a80130652677ed	X	how you have used the Security token service in AWS
563e322d61a80130652677ee	X	I have a server, which should provide temporary AWS credentials to the client.
  Negative
The credentials will be transmitted using HTTPS.
  Neutral
The client should be able to upload S3 files, as well as download them.
  Positive
The concern I have is the following: I have multiple users accessing ONLY their own directory: /Users/someUser/myfile.
  Negative
png You can set policies to allow or deny S3 in general, but you can't grant only the access to a specific path.
  Negative
What should I do about this?
  Neutral
Will the HTTPS transmission be enough?
  Neutral
Then my second question.
  Neutral
If I hear "temporary credentials", I have a key in mind, that is valid for a couple of hours and then expires.
  Negative
But I'm not sure if IAM is really built for that.
  Negative
Should I provide the same credentials for all users?
  Negative
Or do I generate a key-pair for each client?
  Neutral
The server runs with PHP, the client with Objective-C.
  Negative
563e322d61a80130652677ef	X	You can specify permissions on a path in Amazon S3.
  Negative
For more details see the following: Using IAM Policies Also, if you want to create "temporary credentials" you can use the AWS Security Token Service.
  Negative
This service allows you to create credentials that last from 1 - 36 hours and you can put a policy on those credentials to limit their access.
  Positive
For more details about the service see: Security Token Service API Reference Finally, there is an article written for the AWS Mobile SDKs that does something similar.
  Negative
It has a server to issue temporary credentials to users that use an Amazon S3 bucket.
  Negative
It limits the users to a "sub-folder" of the bucket and also limits their actions.
  Neutral
You can read and this sample here: Credential Management for Mobile Applications Hope this helps you get to the information you need.
  Negative
563e322d61a80130652677f0	X	So what is the financial cost of this operation?
  Negative
As well, is this algorithm efficient?
  Negative
Say I have 10000000 objects, since S3 is flat does it search for all objects?
  Negative
563e322d61a80130652677f1	X	It's almost free (in terms of cost); 1/2 cent per 10k requests.
  Negative
It's generally advisable to not do this operation as a standard part of your code- there are typically much better way to structure things so this isn't something you need to do often.
  Negative
563e322d61a80130652677f2	X	Thanks for the information.
  Neutral
What would be the better practice then?
  Neutral
563e322d61a80130652677f3	X	It depends what you are trying to do.
  Neutral
If you post your use case as a new question, link it here.
  Positive
You can use SNS/SQS and possibly Lambda.
  Neutral
563e322d61a80130652677f4	X	I would like to get all the URLs of objects stored in a folder.
  Negative
I will only have one level of folders so I am not concerned with nested folders.
  Negative
I have read the PHP client API (http://docs.aws.amazon.com/aws-sdk-php/v2/api/class-Aws.S3.S3Client.html) for S3 but can't seem to find a way to accomplish this.
  Negative
I found this code from StackOverflow to get the size of contents: Which is close to something I want, except I do not want all the items in a bucket, I want all the items in a certain bucket's folder.
  Negative
My second question is how much would I need to spend to accomplish this as there doesn't seem to be an get/put commands being used so I am not sure how much Amazon charges to for this operation?
  Negative
563e322d61a80130652677f5	X	ListObjects takes a prefix argument.
  Negative
That prefix is the "directory" inside a bucket.
  Positive
563e322d61a80130652677f6	X	I doubt there's a straightforward way.
  Negative
Do the folder names follow a pattern?
  Negative
563e322e61a80130652677f7	X	TJ, there is no pattern on the folder names, they are all UUIDs.
  Negative
I am trying to use "file-named-a" as a flag to indicate another processor that the folder is ready for processing.
  Negative
563e322e61a80130652677f8	X	Thank you John.
  Positive
I think that is the best I can do for now.
  Positive
563e322e61a80130652677f9	X	I understand that s3 does not have "folder" but I will still use the term to illustrate what I am looking for.
  Negative
I have this folder structure in s3: my-bucket/folder-1/file-named-a my-bucket/folder-2/... my-bucket/folder-3/file-named-a my-bucket/folder-4/... I would like to find all folders containing "file-named-a", so folder-1 and folder-3 in above example will be returned.
  Very negative
I only need to search the "top level" folders under my-bucket.
  Negative
There could be tens of thousands of folders to search.
  Negative
How to construct the ListObjectsRequest to do that?
  Neutral
Thanks, Sam
563e322e61a80130652677fa	X	An Amazon S3 bucket can be listed (ListBucket()) to view its contents, and this API call can be limited by a Prefix.
  Negative
However, it is not possible to put a wildcard within the prefix.
  Neutral
Therefore, you would need to retrieve the entire bucket listing, looking for these files.
  Negative
This would require repeated calls if there are a large number of objects.
  Neutral
Example: Listing Keys Using the AWS SDK for Java
563e322e61a80130652677fb	X	may I ask how you resolved this problem?
  Negative
I know its more than a year old but I am facing the same issue and any help would be appreciated.
  Negative
563e322e61a80130652677fc	X	... or the server certificate isn't trusted (see this question).
  Negative
563e322e61a80130652677fd	X	Thanks @Bruno.
  Neutral
I was looking for that earlier and could not find it.
  Negative
563e322e61a80130652677fe	X	I'm using Simpl3r, a simple high level Android API for file uploads using the Amazon S3 service, to upload media files to my bucket.
  Negative
On some uploads, I'm getting a SSLException error.
  Negative
Here's the code where the exception is thrown: When it is come the app is stuck in upload state and this exception is not caught in my exception block.When i am searching for this problem, i found somewhere that your JVM is obsoleted you have to update your JVM.
  Negative
So how to resolve this problem, any ideas?
  Negative
563e322e61a80130652677ff	X	javax.net.ssl.SSLPeerUnverifiedException: No peer certificate Either you are using Anonymous Diffie-Hellman (ADH), the certificate is not installed on the server, SSL/TLS is not enabled for the server, or there's a trust issue with the server.
  Negative
You should verify that SSL/TLS is available with OpenSSL (or another tool).
  Negative
For example: Here's the OpenSSL docs on s_client(1).
  Negative
If there is a SSL/TLS server available, then see Bruno's answer at Apache HTTPClient SSLPeerUnverifiedException.
  Negative
There's another potential problem: no client certificate.
  Negative
But I don't believe that's a problem here.
  Negative
It does not look like you configured for mutual authentication, and I don't see the TLS alert in the back trace.
  Negative
563e322e61a8013065267800	X	If the base64'd data is not an image, then imagecreatefromstring will return FALSE.
  Negative
That's probably all the "validation" you need?
  Neutral
563e322e61a8013065267801	X	Why are you base64 encoding the images in the app?
  Negative
Why not just send the binary data in the payload?
  Negative
Or post it like form data so you can get the image via $_FILES?
  Neutral
563e322e61a8013065267802	X	I'm building a RESTful JSON API to serve as the backend for an iPhone app with some camera/photo functionality.
  Negative
I'm trying to determine what are useful and reliable ways to handle the image uploads.
  Negative
We are going to be base64 encoding the images within the app and POSTing with the rest of the payload to the API, and then on the server we want to resize the image to multiple different dimensions (thumbnail, etc.) and push all of the files to Amazon S3.
  Very negative
Some of the resizing may be done asynchronously.
  Negative
What useful ways to do this, taking into account any file format conversion (we want all images to be JPEG, but they may not come directly from the camera), and that we do not want to permanently store the files on the server?
  Negative
I have done this much: But I am mostly unsure about how do I validate that the base64 string represents a JPEG, or do I need to?
  Negative
I create the image using imagecreatefromstring(), but what should be the next steps after that?
  Neutral
563e322f61a8013065267803	X	Thanks for responding.
  Negative
I am using the s3handler.js that you linked to already.
  Negative
It works, except when I enable the chunking option browser-side.
  Positive
Any other suggestions for me to continue troubleshooting this?
  Negative
563e322f61a8013065267804	X	Your next step should be to verify that your endpoint is returning the correct signature to fine uploader, and then that this signature is being sent to s3 properly without any additional headers not accounted for when generating the signature.
  Negative
563e322f61a8013065267805	X	Not sure if it is relevant, but I added two lines of code to enable CORS in your s3handler.js file.
  Negative
563e322f61a8013065267806	X	The nodeJS endpoint is returning the signature generated by the code provided in s3hanlder.js for multi-part uploads.
  Negative
It is provided this by the fileUploaderS3 object in the browser.
  Neutral
I verified the response to the post matches in the browser and console.log() output from node.
  Negative
563e322f61a8013065267807	X	This is all F.U. code involved, nothing I wrote.
  Negative
Since the client access key works when chunking is turned off I'm not sure how to troubleshoot this.
  Negative
It's either a bug in the example code, or perhpas something is not configured correctly on the S3 CORS?
  Negative
563e322f61a8013065267808	X	For files less than 5mb my configuration is working to upload.
  Negative
For files larger than 5mb they always fail with Post response from AWS: I am using NodeJS, with the relevant functions from the demo s3handler.js unchanged eg: the signRestRequest() function appears to be working as expected per this documentation: http://blog.fineuploader.com/2013/08/16/fine-uploader-s3-upload-directly-to-amazon-s3-from-your-browser/#support-chunking For files less than 5mb with this configuration, resume works if the connection is closed and then re-open automatically.
  Very negative
I am hosting the page and NodeJS in the same local Ubuntu VM.
  Negative
My browser side configuration: This is the response before it is SHA1 HMac encoded, then base64 encoded: My temporary test S3 Bucket CORS configuration: I can provide any additional information as required and have purchased a license.
  Negative
Thanks!!
  Neutral
This is the header sent to the AWS post for a multi-part upload:
563e322f61a8013065267809	X	If AWS reports that your signature is incorrect, then the issue is with your signature generation code.
  Negative
As documented, chunked requests (by default files greater than 5 MB) use S3's multipart upload API, and the signature requirements are different.
  Negative
The following is from Fine Uploader's documentation, specific to chunked uploads: Fine Uploader S3 uses Amazon S3's REST API to initiate, upload, complete, and abort multipart uploads.
  Negative
The REST API handles authentication by signing canonically formatted headers.
  Negative
This signing is something you need to implement server-side.
  Negative
All your server needs to do to authenticate and supported chunked uploads direct to Amazon S3 is sign a string representing the headers of the request that Fine Uploader sends to S3.
  Negative
This string is found in the payload of the signature request: { "headers": /* string to sign */ } The precense of this property indicates to your sever that this is, in fact, a request to sign a REST/multipart request and not a policy document.
  Negative
This signature for the headers string differs slightly from the policy document signature.
  Neutral
You should NOT base64 encode the headers string before signing it.
  Negative
All you must do, server-side, is generate an HMAC SHA1 signature of the string using your AWS secret key and then base64 encode the result.
  Negative
Your server should respond with the following in the body of an 'application/json' response: { "signature": /* signed headers string */ } Fine Uploader's server-examples repo even includes a full node.js example that handles chunked and non-chunked signing logic.
  Negative
563e322f61a801306526780a	X	Obviously you can use Facebook for this purpose.
  Negative
P.S.: I don't think you can upload photos using Javascript.
  Neutral
563e322f61a801306526780b	X	well i am referring to this - webdevhub.net/facebook-api/picture-upload-facebook-api and looks like it's possible.
  Negative
563e322f61a801306526780c	X	Here you are submitting to Facebook end-point directly!
  Negative
The example described in the article is NOT recommended as it'll move your visitors away from your website/app!
  Positive
563e322f61a801306526780d	X	you are right.
  Positive
in fact, I just realized that this is not the right approach... would you suggest something better ?
  Negative
or will it solve my problem if i use ajax submit ?
  Neutral
563e322f61a801306526780e	X	since I am using socket.io in node.js, I can certainly send these information to the server and process it there.
  Negative
.
  Neutral
Please suggest if there is a better approach.
  Negative
563e322f61a801306526780f	X	edited my post after "EDIT"...
563e322f61a8013065267810	X	I am building a site with nodejs and mongodb and using facebook for authentication.
  Negative
.
  Neutral
my users will be required to upload certain photo and others will be able to view them.
  Positive
Since I am using facebook, I am wondering if I should allow users to upload their photos to their facebook profile and I'll save the links into the database so that later on other users can view this.
  Negative
Is this the right approach ?
  Neutral
or should I use flickr or picasa or something else ?
  Neutral
I have to do this in javascript and I know facebook has support for this.
  Negative
Please let me know what you think.
  Neutral
EDIT: Hi, Finally I found the module connect-form to upload the file to the server and then uploading the file to facebook using the module facebook-js.
  Negative
I found fb.api for "/me/feed" works perfectly in node.js server.
  Positive
But when I tried to use the graph api for "/me/photos" as mentioned in http://developers.facebook.com/docs/reference/api/album/, I got the error.
  Negative
That is because it expects the source to be in "multipart/form-data".
  Negative
In my html, it is already "multipart/form-data", otherwise I'll not get the file in the server side.
  Negative
In the server side, however, I am not very sure about how to embed this in FB.api... what should be the "source" parameter in FB.api "/me/ photos" ?
  Negative
I tried this with "source:files.source" as mentioned in my example.
  Positive
But it does not work.
  Negative
may be i am missing something very silly...
563e322f61a8013065267811	X	You have the option to upload photos to Facebook using the url, read the following article: https://developers.facebook.com/blog/post/526/ I suggest to use some CDN for photos like Amazon S3 which is very reliable and scalable.
  Very negative
Then you can optionally have it on Facebook too with a simple API call (don't forget about extended permissions to upload photos in this case) hope this helps
563e323061a8013065267812	X	I need to share this app with a few friends for testing.
  Neutral
I need an online solution.
  Neutral
563e323061a8013065267813	X	Well I found this site and it is good for my purpose: hostinghood.com
563e323061a8013065267814	X	For specific reasons I need to store a small files (up to 1MB) in a web server so that my application can read data from it.
  Negative
I don't need any server side computing.
  Negative
I know there are commercial solutions for this but since this is a test app I need to manage costs down.
  Negative
Does any one know a free solution to store files like a server?
  Neutral
Thanks in advance.
  Neutral
563e323061a8013065267815	X	If you have Python installed on the computer, just go to the directory where you have the files and type python -m SimpleHTTPServer - it will start a HTTP server on port 8000.
  Negative
If you want this to be hosted online, then you can use any number of free services: Dropbox/Google Drive/OneDrive - upload the file there an use their API or a simple direct link to it.
  Negative
Use Amazon S3 - its available for free for 12 months if you are a new subscriber.
  Negative
563e323061a8013065267816	X	If you need that to speed up your website, you can use CDN at: https://www.cloudflare.com/ It stores automatucly your website's static data.
  Negative
563e323161a8013065267817	X	Unfortunately btoa is not available from within Deployd event scripts.
  Negative
ReferenceError: btoa is not defined
563e323161a8013065267818	X	hmm, the, the only way is that you write your own base64 coding/decoding function...
563e323161a8013065267819	X	I'm using Deployd to build an API to be consumed by an AngularJS app.
  Negative
I'm attempting to integrate the ng-s3upload module to save some images on Amazon S3.
  Negative
The ng-s3upload module requires the backend server, in this case deployd, to generate a Base64 encoded policy.
  Negative
I created a GET event to generate the policy but haven't figured out how I can Base64 encode it form within the Deployd event script.
  Neutral
Any help or ideas is appreciated.
  Positive
I tried to use the NodeJS Buffer function, Deployd is based on Node, but it is not available form the event script environment.
  Negative
563e323161a801306526781a	X	You can use the btoa() function to encode strings to base64 format.
  Negative
EDITED As you say you can't use btoa, I wrote an implementation of a base64 encoding function.
  Negative
You use it just like this: Here is the code, you can see it in action in this jsfiddle.
  Negative
563e323161a801306526781b	X	Try toString('base64');
563e323161a801306526781c	X	sounds like a good idea, is it easy to integrate with an uploader... like plupload?
  Negative
563e323161a801306526781d	X	it is easier to use the plain api, and it all depends how you would like to use, it, only from frontend, choose js plain api, from backend, with java and python its very easy, with go its easy and with php i dont know, if you choose from backend then you should be able to easy integrate it with uploaders like plpload
563e323161a801306526781e	X	I'm looking for a cloud based service which will allow my customers to upload very high resolution and print pdfs (sometimes about 60mb), store the images and create low resolution images very quickly I've started looking at Amazon S3 but know this doesn't do anything with the files uploaded and started looking at google app engine.
  Very negative
I did think about using dropbox core api but i think this is really for 1 to 1 users rather than hundreds of users daily.
  Neutral
Any suggestions for services would be great Thanks David
563e323161a801306526781f	X	have a look at google cloud storage: https://cloud.google.com/products/cloud-storage there you can upload files up to 5 tb, and as many as you can pay.
  Negative
it works perfectly with lots of users.
  Positive
you can use buckets or folders per user, its up to you.
  Negative
also its possible to reach that files with an own domain, apis are available for many languages as well
563e323261a8013065267820	X	Are there third party tools that you would recommend?
  Negative
Thanks
563e323261a8013065267821	X	Not really.
  Negative
I think just hitting Google and seeing what's out there would be your best bet.
  Negative
563e323261a8013065267822	X	Which cloud management consoles?
  Negative
Thank you very much
563e323261a8013065267823	X	Thanks datasage.
  Negative
Do you know good scripts that would allow me to define a retention period/number of versions I want to keep and that would also send an email allowing me to quickly know if a backup went wrong?
  Negative
563e323261a8013065267824	X	You could quote some fragments or provide some context to this link.
  Negative
Please read stackoverflow.com/questions/how-to-answer
563e323261a8013065267825	X	Thanks!
  Neutral
Added some more context.
  Neutral
563e323261a8013065267826	X	Skeddly works fine and is pretty much free if you do not backup too often.
  Positive
563e323261a8013065267827	X	Does it mean you have to have an instance running Scalr?
  Negative
563e323261a8013065267828	X	You can have an instance running on AWS or on a machine running on prem which is my configuration.
  Negative
I'm using it for automated snap shots right now but plan to create scripts to save the snapshots in S3 or glacier
563e323261a8013065267829	X	It is not readable.
  Negative
563e323261a801306526782a	X	@Andy Check now ?
  Neutral
is it fine ?
  Positive
563e323261a801306526782b	X	Yes thanks!
  Positive
...
563e323261a801306526782c	X	I'm looking for a backup solution for Amazon EC2 instances.
  Negative
I found this: http://www.n2ws.com and I wanted to know if there were other ones.
  Positive
Thank you PS: It's possible to automatically backup RDS databases using Amazon solution but there isn't anything for EC2 instances... Is there?
  Negative
563e323261a801306526782d	X	I've been using Skeddly for several months now to automatically backup the EBS volumes attached to my EC2 instances.
  Negative
I'm really happy with it so far.
  Very positive
I liked the way I could define which instances to backup: only instances with a specific tag are backed up.
  Negative
I just have to add this tag to the instances I want to back up.
  Negative
No need to do any change in Skeddly each time I add an instance.
  Negative
I had to define 2 actions in Skeddly: one to backup the instances and one to delete the old snapshots.
  Negative
And I receive emails to inform me the actions (backup and expiration) have been successful or not.
  Negative
563e323261a801306526782e	X	If by "EC2 Instances" you really mean "EC2 Instances with EBS Drives" then the Snapshot features of EBS, available through the AWS Console and the AWS API, are what you're looking for.
  Negative
From the EBS Docs: Amazon EBS also provides the ability to create point-in-time snapshots of volumes, which are persisted to Amazon S3.
  Positive
These snapshots can be used as the starting point for new Amazon EBS volumes, and protect data for long-term durability.
  Negative
The same snapshot can be used to instantiate as many volumes as you wish.
  Negative
These snapshots can be copied across AWS regions, making it easier to leverage multiple AWS regions for geographical expansion, data center migration and disaster recovery.
  Negative
Amazon doesn't offer any scheduling or retention type policies around snapshots, but there are some third party tools that leverage the AWS API's.
  Negative
563e323261a801306526782f	X	The company I work for has been using Amazon's S3, EBS, and EC2 almost since their inception.
  Negative
It became painfully obvious, after losing 2 (1 development and 1 production) virtual servers 4 days after they were completed and scheduled to be let loose on EC2 the next night.
  Very negative
To make a long story short, we did not find a standalone application that was very small, lightweight, and nearly configurable to any situation.
  Negative
Using AWS .
  Neutral
NET SDK, we were able to write the above application in less than a day and then using the Task Scheduler on our in-house Windows Server 2008 R2 server.
  Negative
We have gone through a number of scenarios and settled on the following schedule: EC2 instances images are created weekly, EBS snapshots are created daily.
  Negative
EC2 instances older than 31 days are dropped and EBS snapshots are dropped after 60 days, per our contract we entered in a contract with a client who had been burned previously with a standalone application that was supposed to run the backups on its own internal scheduling code/mechanism.
  Negative
It never ran, and no one looked at it after they set it up.
  Negative
As the application matures we plan on having Simple E-Mail Service (SES) for backup summary/log e-mail to our developers, and Simple Queuing Service (SQS) to record the process.
  Negative
Hope this helps.
  Positive
563e323261a8013065267830	X	The cloud protection manager product you found (www.n2ws.com) does support automoated backups of full EC2 instances, beyond backing-up EBS volumes individually, as well as RDS snapshots.
  Negative
It also has the scheduling, data retention policies and automated alerts options you were looking for and other backup related features for AWS.
  Negative
Couldn't find other 3rd party products providing comparable automated backups for EC2 instances, but some of the cloud management consoles allow snapshot scheduding & creation of data retention policies.
  Negative
563e323361a8013065267831	X	Sort of.
  Neutral
You can snapshot EBS volumes on a regular interval.
  Positive
While there isn't anything in the UI to do this for you automatically, the API will allow you to do it.
  Negative
You can either roll your own backup script, or search for one that has been publicly released.
  Negative
563e323361a8013065267832	X	For critical applications, a backup solution should be more than just scheduling snapshots.
  Negative
You'd expect features like application-support, backup policies and powerful recovery options and more.
  Neutral
You can ream about it in my post: http://www.n2ws.com/blog/tier-1-application-backup-on-amazon-cloud.html It's from the n2ws site and also references the CPM product.
  Negative
563e323361a8013065267833	X	For easy management with a GUI, there is also Skeddly.
  Negative
It is pay as you go with CAD 0.15 for most actions.
  Negative
It is also possible to do all this things free.
  Neutral
A good script to start from is this.
  Positive
563e323561a8013065267834	X	There is an opensource project called Scalr that I just started using for about a week and it has features that enable you to scheduled automated snapshots/backups of your EBS volumes.
  Negative
Scalr is actually a cloud management solution and has many fabulous features that I've yet to play with but I'm looking forward to it.
  Positive
There is a pay version but I'm just kickn the tires on the free open source version for now.
  Negative
The Scalr installer is available on Github: https://github.com/Scalr/installer-ng The Scalr source code is on Gitub too: https://github.com/Scalr/scalr Installation instructions are on the Scalr wiki: https://scalr-wiki.atlassian.net/wiki/x/0Q8b
563e323561a8013065267835	X	You can use AutomatiCloud to backup your EC2 volumes and RDS instances.
  Very negative
AutomatiCloud allows you to define schedules for backups and cleans up after a retention period you can configure.
  Negative
It also sends out email notifications in case of success/failure.
  Negative
And it is free!
  Positive
www.automaticloud.net Disclaimer: I am the author
563e323561a8013065267836	X	Here is script Script to Automate AMI backup !
  Negative
It will find instance-id of all instance in your VPC n create AMI backup !
  Neutral
563e323561a8013065267837	X	I can probably add to this, I've used skeddly for about 12 months it works, kinda, but it has a pretty bad UI and I can never reach their guys if I've needed support or have feature improvement ideas.
  Negative
I've worked with the team over at www.cloudmgr.com for a fair while too and have been much more impressed.
  Negative
CloudMGR's UI is better both from a UI design and responsiveness perspective, its easier for non techs.
  Positive
It handles elements beyond backups such as cost optimisation and scheduling and more.
  Negative
The support team is also stellar which is v. valuable.
  Positive
Give them a try.
  Neutral
(full disclosure - not related to Skeddly/cloudMGR or any other vendor, I work for a company who has used all of the above).
  Negative
563e323561a8013065267838	X	What specific request is failing?
  Negative
563e323561a8013065267839	X	Sorry I am not sure exactly what you are asking for but: whenever I drag and drop a JPG file on the browser it generates the thumbnail but fails to upload.
  Very negative
it just returns the error message: Reason: XHR returned response code 0 and fails to upload the image to my S3 bucket.
  Very negative
I am not sure what part is failing, if there is a way to review my JSON policy and signed data or failing CORS configs/values somewhere that is what I am looking for advice on.
  Negative
563e323561a801306526783a	X	I guess whenever I have something partially working/failing I can enable verbose logging somewhere to figure out whats broken (or at least google error messages).
  Negative
For this, the only error I see the javascript alert with UPLOAD FAILED Reason: XHR returned response code 0.
  Negative
Just trying to figure out what else I can test/inspect/review.
  Negative
563e323561a801306526783b	X	The first step is to figure out which specific request is failing.
  Negative
If you look at the network tab of your browser's dev tools, you will be able to make this determination.
  Positive
You will also see more useful information in the console.
  Neutral
563e323561a801306526783c	X	Gotcha thanks.
  Neutral
So digging around I see the following error: XMLHttpRequest cannot load s3.amazonaws.com/dev-pre-content.
  Very negative
No 'Access-Control-Allow-Origin' header is present on the requested resource.
  Negative
Origin '192.168.1.215'; is therefore not allowed access.
  Negative
The test box I am running this off is a VM guest on my laptop.
  Negative
Its a full mirror of my production server.
  Positive
From this laptop IP 192.168.1.215 it goes directly to my S3 bucket named 'dev-pre-content' which is on US-WEST-2 AWS.
  Negative
563e323561a801306526783d	X	Editing the server side code s3demo-cors.php and changing header('Access-Control-Allow-Origin: {$_SERVER['SERVER_NAME']}'); rather than * seemed to move me a step further.
  Negative
Now Im hitting the following errors: [Fine Uploader 5.2.0] Error attempting to parse signature response: SyntaxError: Unexpected token < [Fine Uploader 5.2.0] Received an empty or invalid response from the server!
  Negative
[Fine Uploader 5.2.0] Policy signing failed.
  Negative
Received an empty or invalid response from the server!
  Negative
.
  Neutral
563e323561a801306526783e	X	Im stepping through the policy signing code for POST method and not sure what exactly is required.
  Negative
It has my AWS credentials already, looks like it constructs the JSON policy (in code) but cant seem to gain visibility into what the JSON output looks like or how to validate what is being generated by the s3demo-cors.php example you provide.
  Negative
563e323561a801306526783f	X	I see this documentation here: blog.fineuploader.com/2013/08/16/… which explains the json policy values but not sure if this is to be posted to s3demo-cors.php OR where/how this is supposed to be implemented.
  Negative
563e323661a8013065267840	X	The PHP example handles the signing for you.
  Negative
563e323661a8013065267841	X	SUCCESS: Verdict was my request endpoint.
  Negative
Originally it was: "s3.amazonaws.com/dev-pre-content"; which I thought I read was supposed to be a valid format.
  Negative
I had to change it to "dev-pre-content.
  Neutral
s3.amazonaws.com"; in order for it to work over HTTPS and recognize it.
  Negative
563e323661a8013065267842	X	Nothing in your answer relates to CORS.
  Negative
Perhaps you are confusing terms?
  Neutral
563e323661a8013065267843	X	so I ended up putting corsag to what I meant is the policy,
563e323661a8013065267844	X	The issue, as described by the question poster, was an invalid S3 bucket endpoint over HTTPS.
  Negative
Not CORS or policy-related.
  Neutral
stackoverflow.com/questions/29779346/…
563e323661a8013065267845	X	I have been kicking around trying to implement the S3 uploader into my application and getting closer but no cigar.
  Negative
Here is my setup in a nutshell: I have followed the blog post here: http://blog.fineuploader.com/2013/08/16/fine-uploader-s3-upload-directly-to-amazon-s3-from-your-browser/ multiple times (hoping I am not missing something).
  Negative
I know my IAM permissions are valid because other PHP tests allow various PutObject and list commands successfully.
  Negative
I have verified my CORS config is setup as follows for testing:  I have a few files I am using for this: s3.php = my test page with the fineuploader instance My PHP server side code is from your examples s3demo-cors.php (sorry code formatting got a little garbled when pasting in here)
563e323661a8013065267846	X	Based on your follow-up comment explaining the specific issue: So digging around I see the following error: XMLHttpRequest cannot load s3.amazonaws.com/dev-pre-content.
  Very negative
No 'Access-Control-Allow-Origin' header is present on the requested resource.
  Negative
Origin '192.168.1.215'; is therefore not allowed access.
  Negative
The problem is with your bucket's CORS configuration.
  Negative
You'll need to be sure you have appropriate CORS rules associated with the specific bucket you are uploading to.
  Neutral
563e323661a8013065267847	X	It was exactly what you said, I believe that should implement the policy signature.
  Negative
When you send the file directly to the S3 server, you must first inform the política assignature, so your javascript will pass the credentials that will be on your php file, so that after you send the files you want.
  Negative
Here is a working example of política signature 
563e323661a8013065267848	X	I'd add price -- most storage providers charge for both space and traffic, which can lead to enormous bills.
  Positive
563e323661a8013065267849	X	Simple question, doesn't seem to have been directly asked yet.
  Negative
What are the benefits to storing files (images/videos/documents/etc) on Amazon S3-and-related vs. on the File System your app is deployed on?
  Neutral
It seems that the general consensus is to store files on the File System over the Database (here and here), but where does the Cloud fit in?
  Negative
Should all files just be stored on the File System?
  Negative
I really would like to use the Cloud, so there's more of an API and I could get the docs from multiple apps, but I'm unsure about the performance tradeoff.
  Very negative
563e323661a801306526784a	X	The pros and cons of storing files in "the cloud" IMHO: The pros: The cons: So, bearing this in mind, you decide what's best for your particular project.
  Negative
563e323661a801306526784b	X	Konamiman's answer is a good one, I just wanted to add one thing to the pros: some cloud providers have Content Delivery Network integration, like Rackspace with Limelight, which can give end users very fast content delivery.
  Negative
563e323661a801306526784c	X	So, you have a private key that you would like to use directly from the client side browser?
  Negative
563e323761a801306526784d	X	On every request I am encrypting the URL before making the CORS request and sending on a header.
  Negative
The requirement is that I need to send an encrypted header on the request.
  Negative
At this moment I have no way to hide the private key that I am using to generate the encrypted header.
  Negative
If you know a better way of doing this let me know.
  Negative
563e323761a801306526784e	X	Ask your server for the header.
  Negative
Keep the private key on the server.
  Neutral
AFAIK, there is no secure way to store a private key in the browser.
  Negative
563e323761a801306526784f	X	I don't have a server right now.
  Negative
I think that I will have a node.js server.
  Negative
Is it the right way?
  Neutral
Even if I ask the key to the server, it will be available in memory, how can I fix that?
  Negative
563e323761a8013065267850	X	No, I didn't say ask the server for the key.
  Negative
I said ask the server for the header (already encrypted by the key).
  Neutral
Again, to my knowledge, this is the only way to protect the key.
  Negative
563e323761a8013065267851	X	I want to hash on client side.
  Negative
But in order to hash I need a private key.
  Negative
I am using HMAC-SHA1.
  Negative
Any help?
  Neutral
563e323761a8013065267852	X	@Tony you could encrypt either with public or private key.
  Negative
Take a look on the Crypto-JS library
563e323961a8013065267853	X	I have a static AngularJS file.
  Negative
It's deployed on Amazon S3.
  Neutral
I am accessing a API which has a hash authentication mechanism.
  Negative
I have to use a private key in order to create a hash of the URL and send on the header.
  Neutral
It works great, but the problem is that I have to find a way to keep the private key out of the reach of hackers.
  Neutral
I would like to know if anyone knows a way to keep the key secure.
  Negative
I have thought about running my site on a node.js server.
  Neutral
The only requirement is that I should be able to deploy it on Amazon web services.
  Negative
Does it make sense?
  Neutral
563e323a61a8013065267854	X	There isn't a secure way to store secrets on client side.
  Negative
Private keys should always stay in the server.
  Neutral
The common authentication mechanism for APIs is to hash (not encrypt) authentication on client side.
  Negative
But if you need encrypt something, you should use a asymmetric encryption algorithm with a public key on client.
  Negative
http://searchsecurity.techtarget.com/definition/asymmetric-cryptography
563e323a61a8013065267855	X	I haven't tried this feature myself, but I've found CloudFormation in general to lag behind the API on the order of months.
  Negative
If there was a change some time back as to how this works, it's likely it just hasn't migrated to CF..
  Negative
.
  Neutral
yet?
  Neutral
563e323a61a8013065267856	X	Kerim, thanks for the update.
  Negative
I noticed this was added just a few days back.
  Negative
Anyone working on same problem should be able to leverage this solution.
  Neutral
563e323a61a8013065267857	X	I am trying to set up my S3 to notify my SQS Queue for a "PUT" Object Creation Event.
  Negative
I am able to achieve this using CLI by: Also able to do the same using Java: However when I tried to something similar using CloudFormation template, I cannot find any way to trigger a notification to SQS.
  Negative
The only option I see that works and is documented is to trigger notification to SNS.
  Negative
I have referred the Cloud Formation Documentation: I tried doing something like this: But this as expected threw an error: "Encountered unsupported property QueueConfiguration" from amazon.
  Negative
Looked at this API documentation I would like to know if someone has been able to do this using CloudFormation Templates as thats how I am maintaining all the other AWS resources and do not want to do anything special for this particular feature.
  Negative
Any help is appreciated.
  Positive
563e323a61a8013065267858	X	There is no need "Id" in Cloudformation Template ( You can check from QueueConfiguration Doc ) and your second mistake, that is not "QueueConfiguration", it's "QueueConfigurations".
  Very negative
Because of that you get an error that says "Encountered unsupported property QueueConfiguration" It must be something like that.
  Negative
While you are reading cloudformation template documents, you must be careful about "Required:" sections.
  Negative
If it is not required, you don't need to fill it, just remove that line from your template if you don't use it( Like S3 Tags ).
  Negative
Other Docs about it: S3BucketDocs NotificationConfigurationDocs
563e323a61a8013065267859	X	the best solution is the simpliest :) Thank you
563e323b61a801306526785a	X	I'm researching this for a project and I'm wondering what other people are doing to prevent stale CSS and JavaScript files from being served with each new release.
  Negative
I don't want to append a timestamp or something similar which may prevent caching on every request.
  Negative
I'm working with the Spring 2.5 MVC framework and I'm already using the google api's to serve prototype and scriptaculous.
  Negative
I'm also considering using Amazon S3 and the new Cloudfront offering to minimize network latency.
  Negative
563e323b61a801306526785b	X	I add a parameter to the request with the revision number, something like: The 'ver' parameter is updated automatically with each build (read from file, which the build updates).
  Negative
This makes sure the scripts are cached only for the current revision.
  Negative
563e323b61a801306526785c	X	Like @eran-galperin, I use a parameter in the reference to the JS file, but I include a server-generated reference to the file's "last modified" date.
  Negative
@stein-g-strindhaug suggests this approach.
  Neutral
It would look something like this: The server ignores the parameter for the static file and the client may cache the script until the date code changes.
  Negative
If (and only if) you modify the JS file on the server, the date code will change automatically.
  Negative
For instance, in PHP, my script to create this code looks like this: So then when your PHP file includes a reference to a CSS file, it might look like this: ... which will create ...
563e323b61a801306526785d	X	With regards to cached files, I have yet to run into any issues of bugs related to stale cached files by using the querystring method.
  Negative
However, with regards to performance, and echoing Todd B's mention of revving by filename, please check out Steve Souders' work for more on the topic: "Squid, a popular proxy, doesn’t cache resources with a querystring.
  Negative
This hurts performance when multiple users behind a proxy cache request the same file - rather than using the cached version everybody would have to send a request to the origin server."
  Negative
"Proxy administrators can change the configuration to support caching resources with a querystring, when the caching headers indicate that is appropriate.
  Negative
But the default configuration is what web developers should expect to encounter most frequently."
  Negative
http://www.stevesouders.com/blog/2008/08/23/revving-filenames-dont-use-querystring/
563e323b61a801306526785e	X	Use a conditional get request with an If-Modified-Since header
563e323b61a801306526785f	X	This is actually a very hard issue, and something that you can spend a while engineering the correct solution for.
  Negative
I would recommend publishing your files using a timestamp and/or version built into the url, so instead of: /media/js/my.
  Negative
js you end up with: /media/js/v12/my.js or something similar.
  Neutral
You can automate the versioning/timestamping with any tool.
  Negative
This has the added benefit of NOT breaking the site as you roll out new versions, and lets you do real side-by-side testing (unlike a rewrite rule that just strips the version and sends back the newest file).
  Negative
One thing to watch out for with JS or CSS is when you include dependent urls inside of them (background images, etc) you need to make sure the JS/CSS timestamp/version changes if a resource inside does (as well as rewrite them, but that is possible with a very simple regex and a resource manifest).
  Negative
No matter what you do make sure not to toss a ?
  Neutral
vblah on the end, as you are basically throwing caching out the window when you do that (which is unfortunate, as it is by far the easiest way to handle this)
563e323b61a8013065267860	X	If you get the "modified time" of the file as a timestamp it will be cached until the file is modified.
  Negative
Just use a helper function (or whatever it is called in other frameworks) to add script/css/image tags that get the timestamp from the file.
  Negative
On a unix like system (wich most survers are) you could simply touch the files to force the modified time to change if necessary.
  Negative
Ruby on Rails uses this strategy in production mode (by default I beleave), and uses a normal timestamp in development mode (to be really sure something isn't cached).
  Negative
563e323b61a8013065267861	X	If you use MAVEN, you can use this, ADD on you pom.xml: With this you can acess ${timestamp} in your view.
  Negative
Like this sample:
563e323b61a8013065267862	X	I was wondering if there is a similar method in PHP to combine multiple zip files into one new zip-file without recompressing the contents - like this zip file library: http://www.example-code.com/vb/zip_appendFilesToExistingZip.asp Reason I ask is because of this article on fast file zipping for Amazon S3: http://www.w2lessons.com/2012/01/fast-zipping-in-amazon-s3.html See why: Upon inspecting the Chilkat API, I noticed the existence of a QuickAppend method which serves to append one zip to another.
  Very negative
I began wondering how the compression time would be affected if we pre-zipped each file in S3, in its destination directory structure, and then simply appended them all together to form the final zip.
  Negative
To my dismay, the difference in compression time was astonishing.
  Negative
Small zip files in the 100kb-300kb range saw a 2x-3x speed improvement, while those larger than 10mb saw a 10x – 15x improvement.
  Negative
For example, a 14mb zip with 25 files varying in size from 100kb to 8mb took a mere 120ms to compress into the final zip, while building the zip from scratch took over 1.5 seconds.
  Negative
Anybody know a similar technique in PHP?
  Neutral
563e323c61a8013065267863	X	Ok, I didn't look to deep into the Chillkat extensions, they seem to have a PHP extension as well.
  Negative
See: http://www.example-code.com/phpExt/zip_appendFilesToExistingZip.asp However, my question now would be - is there an open-source PHP library that does the same thing?
  Negative
563e323c61a8013065267864	X	I'm concerned about the undocumented restrictions on object (key) names.
  Negative
Amazon claims any unicode works, but clearly '.
  Neutral
.
  Neutral
/.
  Neutral
.
  Neutral
/word' does not.
  Negative
I'm wondering what else isn't supported...
563e323c61a8013065267865	X	Looks like the answer is "No, there is not a document".
  Negative
I would recommend asking your question on the AWS forums.
  Negative
On a side note, here is a similar question (and answer :) ) : stackoverflow.com/questions/3146380/…
563e323c61a8013065267866	X	@Downvoter: it would be good to have feedback as why you think the answer does not address the question.
  Negative
Or even better, an edit to the answer.
  Neutral
563e323c61a8013065267867	X	docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html
563e323c61a8013065267868	X	From the AWS docs, I understand that: However, these rules seem too permissive.
  Negative
For instance, if I make a key called '.
  Neutral
.
  Neutral
/.
  Neutral
.
  Neutral
/d', a 400 ERROR occurs when I attempt to access it with the GET OBJECT API.
  Negative
Interestingly, I have no problem accessing '.
  Negative
.
  Neutral
/d'.
  Neutral
Is there a document specifying what is and is not legal?
  Neutral
563e323c61a8013065267869	X	The only restrictions provided by Amazon is (as found on their Technical FAQ): What characters are allowed in a bucket or object name?
  Negative
A key is a sequence of Unicode characters whose UTF-8 encoding is at most 1024 bytes long.
  Negative
Additional restrictions apply for Buckets (as found on the Rules for Bucket Naming section of their Bucket Restrictions and Limitations FAQ): In all regions except for the US Standard region a bucket name must comply with the following rules.
  Negative
These result in a DNS compliant bucket name.
  Neutral
Less permissive restrictions apply to the US standard region.
  Negative
Please see the FAQs for additional information and some examples.
  Neutral
Hope it helps!
  Positive
563e323c61a801306526786a	X	According to AWS S3 documentation: Although you can use any UTF-8 characters in an object key name, the following key naming best practices help ensure maximum compatibility with other applications.
  Negative
Each application may parse special characters differently.
  Neutral
The following guidelines help you maximize compliance with DNS, web safe characters, XML parsers, and other APIs.
  Negative
Please find below the The following character sets are generally safe for use in key names: NOTE ABOUT THE DELIMITER ("/") The following are examples of valid object key names: 4my-organization my.great_photos-2014/jan/myvacation.
  Negative
jpg videos/2014/birthday/video1.wmv Note that the Amazon S3 data model is a flat structure: you create a bucket, and the bucket stores objects.
  Negative
There is no hierarchy of subbuckets or subfolders; however, you can infer logical hierarchy using keyname prefixes and delimiters as the Amazon S3 console does.
  Negative
e.g if you use Private/taxdocument.
  Neutral
pdf as a key, it will create the Private folder, with taxdocument.pdf in it.
  Positive
Amazon S3 supports buckets and objects, there is no hierarchy in Amazon S3.
  Negative
However, the prefixes and delimiters in an object key name, enables the Amazon S3 console and the AWS SDKs to infer hierarchy and introduce concept of folders.
  Neutral
The following characters in a key name may require additional code handling and will likely need to be URL encoded or referenced as HEX.
  Negative
Some of these are non-printable characters and your browser may not handle them, which will also require special handling: You should avoid the following characters in a key name because of significant special handling for consistency across all applications.
  Very negative
563e323c61a801306526786b	X	when I follow those examples I get errors like: undefined method `write' for #<Aws::S3::Object bucket_name="my_bucket", key="image.png"> (NoMethodError)
563e323c61a801306526786c	X	I ended up using this answer (stackoverflow.com/questions/130948/ruby-convert-file-to-string) then used object = bucket.object('image.png') object.put(body: contents)
563e323c61a801306526786d	X	@EldadMor You linked to the v1 documentation.
  Negative
The v2 documentation is found here: docs.aws.amazon.com/sdkforruby/api/index.html
563e323c61a801306526786e	X	You're my freaking hero!
  Negative
Spent the last 3-4 hours trying to make this work to no avail.
  Negative
Doing this in v1 of the aws-sdk was so simple but v2 seems unnecessarily complicated.
  Negative
563e323d61a801306526786f	X	@mmichael I'm curious in what way v2 uploads are more complicated than v1?
  Negative
The syntax is very similar, no?
  Negative
563e323d61a8013065267870	X	Well, with v1 you didn't have to deal with the Client or Resource classes, you could just run s3 = AWS::S3.new and then s3.buckets['bucket_name'].
  Negative
objects['key'].
  Neutral
write(file: '/path/to/file').
  Neutral
I actually misspoke, the v2 version isn't more complicated.
  Negative
I meant to say that the v2 docs are just more confusing.
  Neutral
Nowhere in the new docs does it show you how to do the steps in your answer.
  Positive
And if they do, then it's not as clear as the instructions in v1 where it shows you everything you need on one page.
  Negative
563e323d61a8013065267871	X	I agree, they have raised the knowledge level above beginner in the v2 documentation
563e323d61a8013065267872	X	yeah, the docs are a nightmare.
  Very negative
would be great to have a better overview of common use cases for things like Client and Resource
563e323d61a8013065267873	X	I'm having a hell of a time working with the aws-sdk documentation, all of the links I follow seem outdated and unusable.
  Negative
I'm looking for a straight forward implementation example of uploading an image file to an S3 bucket in Ruby.
  Negative
Any advice is much appreciated.
  Neutral
563e323d61a8013065267874	X	Here is how you can upload a file from disk to the named bucket and key: That is the simplest method.
  Negative
You should replace 'key' with the key you want it stored with in Amazon S3.
  Neutral
This will automatically upload large files for you using the multipart upload APIs and will retry failed parts.
  Negative
If you prefer to upload always using PUT object, you can call #put or use an Aws::S3::Client: Also, the API reference documentation for the v2 SDK is here: http://docs.aws.amazon.com/sdkforruby/api/index.html
563e323d61a8013065267875	X	this is what AWS states:"To verify an email address, make an API call with the email address as a parameter.
  Negative
This API call will trigger a verification email, which will contain a link that you can click on to complete the verification process. "
  Negative
...need to know how to make that API call.
  Negative
thanks
563e323d61a8013065267876	X	I am signed up for AWS SES (with instance, S3 and my website running nicely).
  Negative
I also have rec'd approval for sending out email without receiver verification and "mass production" OK.
  Negative
The only thing I'm left with is having my 3 "from" email addresses verified.
  Negative
Started to download Perl, as was suggested to run email-verification scripts -- but got no where with the installation.
  Negative
Do have my credentials ready to use.
  Negative
There is an AWS SES API to use for verification which I can't find... suspect that it has something to do with AWS's sdk which I could figure out how to install.
  Negative
So my question: is there a simple, straight forward way to get my email addresses to Amazon for verification via a response email they send?
  Negative
Their documentation is somewhat confusing.
  Negative
563e323d61a8013065267877	X	You have to go validate your email address through their web service (like the perl script is doing).
  Negative
You can also use their SDK's that they publish, which are wrappers around their web service.
  Negative
For example, if you have Visual Studio, you can use the AWS SDK for .
  Negative
NET (also available as a Nuget package: PM> Install-Package AWSSDKForNET) and set up a simple console application to do something like this: They also have SDK's available in PHP and Java that work pretty much the same.
  Positive
563e323e61a8013065267878	X	Assuming you've already looked through the AWS SDK for PHP, what have you got so far?
  Negative
As for resizing and cropping, I believe your application's server should be taking care of that and then send to your S3 bucket.
  Negative
563e323e61a8013065267879	X	1st and 3rd answer are fine.
  Positive
2nd > resize/crop is for remote image (hosted in asw ).
  Negative
eg ... asw.com/abc.jpg?h=200&w=200&q=100
563e323e61a801306526787a	X	You can use s3 library to get object S3::getObject($bucketName, $uploadName) then use the image library
563e323e61a801306526787b	X	For my website i want to store all image using AWS S3 service using API.
  Very negative
How to do that thing via API/SDK 1) How to upload image/file in different folder using API (from my website).
  Negative
2) How to resize/crop image on the fly.
  Negative
eg 50x50 px, 250x250 px.
  Negative
3) Force download.
  Neutral
Thanks
563e323e61a801306526787c	X	Your question is a bit vague, so the answer cannot be precise.
  Negative
I'll give some advice or library to start: This library should be a nice place to start working with S3 and files: https://github.com/tpyo/amazon-s3-php-class This should do the trick: https://github.com/eventviva/php-image-resize Or you can try PHP Imagick: http://php.net/manual/en/book.imagick.php Stackoverflow already has the answer for you: How to force file download with PHP Hope this helps get you started.
  Negative
563e323e61a801306526787d	X	I don't think AWS has a built-in feature to resize any stored image in S3.
  Negative
As eldblz mentioned, you have to do the resizing on your own.
  Neutral
You can use S3 stream wrapper.
  Neutral
The S3 stream wrapper is pretty amazing: http://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-s3-stream-wrapper.html It will allow you to use built-in PHP functions like file_get_contents and file_put_contents.
  Positive
Get details of the original file: Get the image data with file_get_contents(or fopen): create an image resource from the data and resize: Output: Hope this helps!
  Negative
563e323e61a801306526787e	X	We have an archive stored in Amazon s3.
  Negative
For one of our application I need to get the list of folders under a given URL.
  Negative
Instead of parsing the output from the web query to the URL, Is there a way to get the list of folder names in JSON or XML format using AWS REST APi's?
  Negative
If its not possible with REST API's is there any alternative better than parsing the web request return?
  Negative
563e323e61a801306526787f	X	Can anyone with experience using cloud encoding services like Encoding.com with asp.net MVC point me in the right direction workflow wise?
  Negative
I will be storing my videos in Amazon S3 so I would like to avoid forcing the user to upload to my web server only to immediately be pushed to S3.
  Negative
Encoding.com has made this uploader script available here I am wondering if this is the proper start to the workflow and where would I go from here?
  Negative
I would imagine I could take this result and call their API to begin the processing.
  Negative
Just looking for general info/help on the workflow as a whole and what to show the user while all of this is taking place.
  Positive
Any insight would be greatly appreciated.
  Positive
563e323e61a8013065267880	X	@GeogeGreen i need to upload large videos to s3 bucket, most likely it would be 5GB , can i do it using NSURLSession coz what i have read is that background sessions wont be execute long time
563e323f61a8013065267881	X	Thanks.
  Negative
This works very well
563e323f61a8013065267882	X	How would you do this using pre-singed authentication?
  Positive
The client only has accessKey and signature, this assumes you have key and secret.
  Negative
563e323f61a8013065267883	X	I am actually using FederationToken to generate temporary credentials before (docs.aws.amazon.com/STS/latest/APIReference/…;.
  Negative
I have never used pre signed url, but as far as I understand the documentation it is a simple url request to the generated url.
  Negative
No need to use AWS SDK methods for that, just create an NSUrlSessionUploadTask without any AWS integration at all.
  Negative
563e323f61a8013065267884	X	Good that it works.
  Positive
.
  Neutral
Strangely ive been unable to add prefixes to the file uploaded- i.e s3.amazonaws.com/NSURLessionUploadTest/image.
  Negative
jpg works fine but s3.amazonaws.com/NSURLessionUploadTest/Prefix/image.
  Positive
jpg fails.
  Negative
.
  Neutral
anyone faced this?
  Neutral
563e323f61a8013065267885	X	Since this is any other string for S3, results should be the same with or without prefix.
  Negative
Maybe it is the bucket policy that is preventing you from uploading.
  Positive
What exactly is the error?
  Negative
A second idea: Please print out the url of request2 just before starting the upload task, maybe the slash is being changed somehow.
  Negative
563e323f61a8013065267886	X	i want to upload videos to S3 bucket using V2 api and it should support pause and resume capability , Can use objc version of this code snippet to upload
563e323f61a8013065267887	X	Awesome, that really helped.
  Negative
Apple were adding an extra header field after I had signed the request!
  Positive
563e323f61a8013065267888	X	@GeorgeGreen can you provide some more information?
  Negative
How did you overcome this eventually?
  Neutral
563e323f61a8013065267889	X	@GeorgeGreen I am very interested too.
  Negative
563e323f61a801306526788a	X	@GeorgeGreen can you please expand on this?
  Positive
563e323f61a801306526788b	X	From the docs for "Uploading Body Content using a file": "The session object computes the Content-Length header based on the size of the data object.
  Negative
If your app does not provide a value for the Content-Type header, the session also provides one."
  Negative
This can mess up your s3 signature.
  Negative
563e323f61a801306526788c	X	Link only answers are discouraged.
  Negative
Please include the salient aspects of your solution in your answer, or delete this answer and just leave a comment.
  Negative
563e323f61a801306526788d	X	@Genady is thr anyway that i can resume uploading if the internet connection drops ??
  Negative
563e324061a801306526788e	X	Interesting question, I didn't try it yet.
  Negative
563e324061a801306526788f	X	@Genady Okrain currently if the connection drops the upload process will stop , do u think is thr any chance to resume upload from where it stopped ??
  Negative
563e324061a8013065267890	X	I have an app which is currently uploading images to amazon S3.
  Negative
I have been trying to switch it from using NSURLConnection to NSURLSession so that the uploads can continue while the app is in the background!
  Negative
I seem to be hitting a bit of an issue.
  Negative
The NSURLRequest is created and passed to the NSURLSession but amazon sends back a 403 - forbidden response, if I pass the same request to a NSURLConnection it uploads the file perfectly.
  Negative
Here is the code that creates the response: And then this signs the response (I think this came from another SO answer): Then if I use this line of code: It works and uploads the file, but if I use: I get the forbidden error.
  Negative
.
  Neutral
!?
  Neutral
Has anyone tried uploading to S3 with this and hit similar issues?
  Negative
I wonder if it is to do with the way the session pauses and resumes uploads, or it is doing something funny to the request.
  Neutral
.
  Neutral
?
  Neutral
One possible solution would be to upload the file to an interim server that I control and have that forward it to S3 when it is complete... but this is clearly not an ideal solution!
  Negative
Any help is much appreciated!!
  Neutral
Thanks!
  Positive
563e324061a8013065267891	X	I made it work based on Zeev Vax answer.
  Positive
I want to provide some insight on problems I ran into and offer minor improvements.
  Negative
Build a normal PutRequest, for instance Now we need to do some work the S3Client usually does for us Now copy all of that to a new request.
  Negative
Amazon use their own NSUrlRequest class which would cause an exception Now we can start the actual transfer This is the code that creates the background session: It took me a while to figure out that the session / task delegate needs to handle an auth challenge (we are in fact authentication to s3).
  Negative
So just implement
563e324061a8013065267892	X	The answers here are slightly outdated, spent a great deal of my day trying to get this work in Swift and the new AWS SDK.
  Negative
So here's how to do it in Swift by using the new AWSS3PreSignedURLBuilder (available in version 2.0.7+):
563e324061a8013065267893	X	I don't know NSURLSessionUploadTask very well yet but I can tell you how I would debug this.
  Negative
I would use a tool like Charles to be able to see HTTP(S) requests that my application makes.
  Negative
The problem is likely that the NSURLSessionUploadTask ignores a header that you set or it uses a different HTTP method than Amazon's S3 expects for the file upload.
  Negative
This can be easily verified with an intercepting proxy.
  Positive
Also, when Amazon S3 returns an error like 403, it actually sends back an XML document that has some more information about the error.
  Negative
Maybe there is a delegate method for NSURLSession that can retrieve the response body?
  Neutral
If not then Charles will certainly give you more insight.
  Positive
563e324061a8013065267894	X	Here is my code to run the task: I open sourced my S3 background uploaded https://github.com/genadyo/S3Uploader/
563e324061a8013065267895	X	For background uploading/downloading you need to use NSURLSession with background configuration.
  Negative
Since AWS SDK 2.0.7 you can use pre signed requests: PreSigned URL Builder** - The SDK now includes support for pre-signed Amazon Simple Storage Service (S3) URLs.
  Negative
You can use these URLS to perform background transfers using the NSURLSession class.
  Neutral
Init background NSURLSession and AWS Services Implement upload file function NSURLSession Delegate Function:
563e324061a8013065267896	X	I just spent sometime on that, and finally succeeded.
  Negative
The best way is to use AWS library to create the request with the signed headers and than copy the request.
  Negative
It is critical to copy the request since NSURLSessionTask would fail other wise.
  Negative
In the code example below I used AFNetworking and sub-classed AFHTTPSessionManager, but this code also works with NSURLSession.
  Negative
Another good resource is the apple sample code hereand look for "Simple Background Transfer"
563e324061a8013065267897	X	Recently Amazon has updated there AWS api to 2.2.4.
  Negative
speciality of this update is that , it supports background uploading , you don't have to use NSURLSession to upload videos its pretty simple , you can use following source block to test it, i have tested against with my older version , it is 30 - 40 % faster than the previous version in AppDelegate.m didFinishLaunchingWithOptions method // ~GM~ setup cognito for AWS V2 configuraitons in handleEventsForBackgroundURLSession method in upload class More references - : http://mobile.awsblog.com/post/Tx283AGGIL76PKP/Amazon-S3-Transfer-Utility-for-iOS
563e324061a8013065267898	X	thanks for answer
563e324061a8013065267899	X	can I use CDN with images ?
  Negative
and if can then how to use it with upload from website to CDN server
563e324161a801306526789a	X	Yes, and you can check with your CDN provider on the methods they allow for uploading, such as pull (CDN server download the files from your website/server) or push (sent from your website/server to the CDN server) Example : automatic push to CDN deployment strategy
563e324161a801306526789b	X	Seems like there are a few options to accomplish this.
  Negative
The first one would be using the CDN as Origin.
  Negative
In which case, there is already an answer with some advice.
  Negative
The second option would be using your current website as Origin for the images.
  Negative
In which case you will need to do some DNS work that would look something like this: Published URL -> CDN -> Public Origin Step 1 - images.yoursite.com IN CNAME images.yoursite.com.edgesuite.net --- This entry will send all traffic requests for the images subdomain to Akamai's CDN edge network.
  Negative
Step 2 - origin-images.
  Neutral
yoursite.com IN A or IN CNAME Public front end for the images So the way it works is that in step one you get a request for one of your images, which will be then sent via DNS to the edge network in the CDN (in this case Akamai HTTP only).
  Negative
If the CDN does not already have the image in cache or if its cache TTL is expired, it will then forward the request to the public origin you have setup to pull the file, apply any custom behavior rules (rewrites, cache controls override, etc), cache the content if marked as cacheable and then serve the file to the client.
  Negative
There is a lot of customization that can be done when serving static content via CDN.
  Positive
The example above is very superficial and it is that way to easily illustrate the logic at a very high level.
  Positive
563e324161a801306526789c	X	In common CDN setups you actually don't upload images to the CDN.
  Negative
Instead, you access your images via a CDN, quite like accessing resources via an online Proxy.
  Negative
The CDN, in turn, will cache your images according to your HTTP cache headers and make sure that subsequent calls for the same image will be returned from the closest CDN edge.
  Negative
Some recommended CDNs - AWS CloudFront, Edgecast, MaxCDN, Akamai.
  Negative
Specifically for images, you might want to take a look at Cloudinary, http://cloudinary.com (the company I work at).
  Negative
We do all of this for you - you upload images to Cloudinary, request Cloudinary for on-the-fly image transformations, and get the results delivered via Akamai's high-end CDN.
  Positive
563e324161a801306526789d	X	Do you mean you want to use a CDN to host images?
  Neutral
And you want to upload images from your website to the CDN or use the website run by the company hosting the CDN to upload the images?
  Negative
Ok, firstly yes you can use a CDN with images.
  Neutral
In fact it's advised to do so.
  Negative
Amazon CloudFront and RackspaceCloud's Cloudfiles are the two that immediately spring to mind.
  Negative
Cloudfiles you can upload either by their API or through their website and CloudFront you upload to Amazon's S3 storage which then hooks into the CloudFront CDN.
  Negative
563e324161a801306526789e	X	Iam using AWS to upload images, css and some zip files for my site and they are fine to upload them.But now I want like I first upload a zip on localhost and I will extract them into one folder and I want to upload that entire folder into aws.Can anyone help me to do it.Thanks in advance.
  Negative
Iam using function to upload files like It is working fine for single images.But I dont know how to do it for entore folder.
  Negative
563e324161a801306526789f	X	There is no API call for Amazon S3 that can upload an entire folder.
  Negative
You can loop through your list of local files and then upload each individually to S3.
  Negative
If you're capable, doing it in parallel can greatly speed the upload, too.
  Negative
You could also cheat by calling out to the AWS Command Line Interface (CLI).
  Negative
The CLI can upload/download a recursive list of files and can also do multi-part upload for large files.
  Negative
There is also an aws s3 sync command that can intelligently upload only new/modified files.
  Negative
563e324161a80130652678a0	X	what about the same domain but on a subdomain.
  Neutral
.
  Neutral
like assets.mysite.com...?
  Neutral
563e324161a80130652678a1	X	Domains/Subdomains have no bearing on the physical server or its filesystem.
  Negative
563e324161a80130652678a2	X	I'm looking for some quick info about best practices on storing user's uploaded files on different servers or sub domains... For example, photos on facebook of course arent on facebook.com/files/users/453485 etc..
  Negative
.
  Neutral
but rather on photos.ak.fbcdn.net or whatever... I'm wondering how with php i can upload to a different server whilst maintaining a mysql connection to my original... is it possible?
  Negative
563e324161a80130652678a3	X	Facebook uses a content delivery network (cdn, hence fbcdn or facebook content delivery network) and probably uses webservices to pass binary data (photos) from server to server.
  Negative
Rackspace Cloud offers a similar service.
  Neutral
Here is an example application of their PHP library to access their webservice api: http://cloudfiles.rackspacecloud.com/index.php/Sample_PHP_Application
563e324161a80130652678a4	X	I'm going to make the assumption that you have multiple webservers, and want to be able to access the same set of files on each one.
  Negative
In that case, some sort of shared storage that each machine can access might be a good place to start.
  Negative
Here are a couple options I've used: If you don't have control over the hardware or aren't able to install extra software, I'd suggest Amazon S3.
  Negative
There is an api that you can use to shuttle files back and forth.
  Negative
The only downside is that you don't get to use storage that you may already use, and it will cost you some money.
  Negative
If you do have access to the hardware and software, MogileFS is somewhat like S3, in that you have an api to access the files.
  Negative
But is different in that you get to use your existing storage and get to do so for no additional cost.
  Negative
NFS is a typical place where people will start, because it's the simplest way to get started.
  Negative
The downside is that you'll have to be able to configure servers, and setup a NFS volume for them to mount.
  Positive
But if I were starting a high-volume photo hosting service, I'd use S3, and I'd put a CDN like Akamai in front of it.
  Negative
563e324261a80130652678a5	X	I've seen the recently Google Drive pricing changes and they are amazing.
  Positive
This changes everything !
  Neutral
We have a SaaS website in which we keep customer's files.
  Negative
Does anyone know if Google Drive can be used to keep this kind of files/service or it's just for personal use?
  Neutral
Does it have a robust API for uploading, downloading, and create public URL's to access files as S3 have ?
  Positive
Edit: I saw the SDK here (https://developers.google.com/drive/v2/reference/).
  Negative
The main concern is if this service can be used for keeping customer's files, I mean, a SaaS website offering a service and keeping files there.
  Negative
563e324261a80130652678a6	X	This doesn't really change anything.
  Negative
“Google Drive storage is for users and Google Cloud Storage is for developers.”
  Negative
— https://support.google.com/a/answer/2490100?hl=en The analogous service with comparable functionality to S3 is Google Cloud Storage, which is remarkably similar to S3 in pricing.
  Negative
https://developers.google.com/storage/pricing
563e324461a80130652678a7	X	Does anyone know if Google Drive can be used to keep this kind of files/service or it's just for personal use?
  Negative
Yes you can.
  Positive
That's exactly why the Drive SDK exists.
  Negative
You can either store files under the user's own account, or under an "app" account called a Service Account.
  Negative
Does it have a robust API for uploading, downloading, and create public URL's to access files as S3 have ?
  Positive
"Robust" is a bit subjective, but there is certainly an API.
  Positive
There are a number of techniques you can use to access the stored files.
  Positive
Look at https://developers.google.com/drive/v2/reference/files to see the various URLs which are provided.
  Neutral
Por true public access, you will probably need to have the files under a public directory.
  Negative
See https://support.google.com/drive/answer/2881970?hl=en NB.
  Positive
If you are in the TB space, be very aware that Drive has a bunch of quotas, some of which are unpublished.
  Negative
Make sure you test any proof of concept at full scale.
  Neutral
563e324461a80130652678a8	X	Sorry to spoil your party, before you get too excited, look at this issue.
  Negative
It is in Google's own product, and has been active since November 2013 (i.e.4 months).
  Negative
Now imagine re-syncing a few hundred GB of files once a while.
  Negative
Or better, ask your customers to do it with their files after you recommended Drive to them.
  Neutral
563e324461a80130652678a9	X	A Service.
  Neutral
Definitely.
  Positive
If you manage your AsyncTask inside an Activity, it may get killed when the activity gets in background
563e324461a80130652678aa	X	stackoverflow.com/questions/6957775/…
563e324461a80130652678ab	X	you said it.
  Negative
"the OS kills the app to free memory."
  Negative
.
  Neutral
That's a serious disadvantage.
  Neutral
563e324561a80130652678ac	X	@fiddler Thanks for your suggestion.
  Negative
So i think i shall use IntentService instead of AsyncTask.
  Negative
563e324561a80130652678ad	X	Thank you for your answer.
  Positive
I shall better use IntentService.
  Positive
563e324561a80130652678ae	X	Good Android citizen.
  Negative
.
  Neutral
Key words.
  Neutral
.
  Neutral
563e324561a80130652678af	X	I have a requirement where a user is able to upload a video to Amazon S3.
  Negative
I have achieved this using the java high-level api in amazon sdk.
  Negative
During the upload process if the user clicks the home button the upload must continue in the background.
  Neutral
What would be a better approach :?
  Neutral
*1 Using AsyncTask: I have tried using AsyncTask and it works fine.
  Positive
But if the uploading process continues for long interval at the background, the OS kills the app to free memory.
  Negative
Is there any way that i can handle this situation, and let my app complete the upload process.
  Negative
*2 Using Service: Someone suggested me to use a Service + ui notification.
  Negative
I feel like using AsyncTask, because it works fine for me.
  Very positive
Is there any advantage of using a Service over AsyncTask.
  Neutral
563e324561a80130652678b0	X	Most of the time an AsyncTask is coupled to your UI, the Activity that started it etc.
  Negative
Those will stay in memory until the task is finished.
  Neutral
This upload scenario of yours begs for implementation through an IntentService.
  Negative
This will decouple the uploading from a specific activity and make your App a good Android citizen in regard to the Android life cycle.
  Positive
You can now create a Notification that is periodically updated from the Service that shows the status of the upload and lets the user cancel the upload from his status bar.
  Positive
563e324561a80130652678b1	X	An upload library I maintain, Fine Uploader handles uploads directly to S3 in all browsers, including IE7.
  Negative
It also supports chunking, auto-resume, retry, and a bunch of other features.
  Very negative
A live demo of the upload-to-s3 function can be found on the page I just linked to.
  Positive
563e324561a80130652678b2	X	This is not a browser-based solution.
  Negative
It's a Microsoft Windows software.
  Negative
563e324561a80130652678b3	X	I'm looking for a front-end solution for uploading files to amazon s3 (that is, not passing them through my server.
  Negative
The solution I have found is https://code.google.com/p/swfupload/ It might do the job, but it requier flash and this is the first sentence of the project description is: SWFUpload has not been under active development for several years.
  Neutral
Here are my desired features, though none of them are nessesary
563e324661a80130652678b4	X	You could start by using this tutorial as a baseline , if you are asking about uploading from your web app - http://aws.amazon.com/articles/1434
563e324661a80130652678b5	X	kgu87 is correct, this article pretty much explains the entire process to upload files directly to S3 without passing them trough your own server.
  Negative
You can also check out the AWS docs related to this on: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html If you're looking for an upload tool that supports HTML5 uploads directly to S3, check out Plupload They have a great article that explains how to set it up: https://github.com/moxiecode/plupload/wiki/Upload-to-Amazon-S3 The documentation describes a PHP service that's used to generate a policy and signature (both are required for S3 to accept your download) but you can use any language to generate those.
  Negative
Also, in certain use cases, you can just generate a one-time policy with a very high expiration time and hard code it into your upload form.
  Positive
563e324661a80130652678b6	X	You could use this tool: http://aws.amazon.com/customerapps/Amazon-S3/Consumers/2069 free, and works in most browsers; doesn't require a server.
  Negative
S3 Browser is a free client interface for users of the Amazon S3 service.
  Negative
It provides an easy to use GUI to manage your storage buckets and allows you to: - Browse, create, delete Amazon S3 buckets - Upload and download files to and from Amazon S3 - Create public URLs to share the files.
  Neutral
- Keep your files backed up on a multiple data centers.
  Neutral
- Set Access Control on buckets and files.
  Negative
S3 Browser is free for non-commercial use.
  Negative
563e324661a80130652678b7	X	I would say that REST and WSDL/SOAP-like services have different usage / areas where they differ in strength.
  Negative
563e324661a80130652678b8	X	Very much agreed.
  Positive
563e324661a80130652678b9	X	I'm having trouble understanding why a WSDL would be so beneficial, when the truth is that the service definition is not that human-readable, and most developers will use automated tools to consume it to generate objects in the application.
  Very negative
Why isn't it easier for a service provider to define a simple XML schema, and just tell me in the documentation what I can get and how I can get it?
  Negative
I understand the whole "universal definition" aspect, but it all just seems so overcomplicated to me.
  Neutral
Most APIs for social sites use a basic RESTful architecture, and all they do is give you a schema representation of the data you will get back.
  Negative
Seriously, I must be missing something here.
  Negative
563e324661a80130652678ba	X	At one time, WSDL was popular and I'm sure for internal tools for many companies (and large SOA systems), WSDL is still in use.
  Negative
But you're correct, the adoption of REST has taken WSDL off the map a bit once it "hit the scene".
  Negative
Take for example Amazon S3.
  Neutral
They offer a WSDL along with the REST API.
  Neutral
I had read somewhere that 98% of S3 users are using the REST API and Amazon is considering dropping the WSDL support.
  Negative
REST is clean.
  Positive
WSDL often depends on other tools to parse it out, or to automatically build functions for your application to benefit from the services offered by the WSDL.
  Negative
REST also has the benefit of being much more natural by taking advantage of HTTP and not really relying on anything more.
  Negative
One you get SOAP into the mix and the many other acronyms that go along with WSDL, you end up with a lot on your hands....
563e324761a80130652678bb	X	So what "simple XML schema" would you propose that would let the tools give the same level of code generation support that they do now?
  Negative
I think the WSDL designers would argue that they're already giving the simplest schema they could which exposed everything they needed to.
  Negative
I'm not saying I'd necessarily agree, but being able to autogenerate clients is very powerful.
  Positive
563e324761a80130652678bc	X	To me WSDL seems like another example of over-engineered "onion architecture", like original Java EJBs.
  Negative
Lots of layers and tears.
  Positive
563e324761a80130652678bd	X	This link should help you out.
  Neutral
http://www.prescod.net/rest/rest_vs_soap_overview/ This is a great resource to help those who do not understand the SOAP vs REST contention.
  Positive
They are different tools.
  Neutral
Use them so that you solve your problem in the most efficient way.
  Negative
563e324761a80130652678be	X	For one thing, WSDL is what the automated tools use to generate objects.
  Negative
And it already is a pretty simple XML format, but I'm beginning to believe that a tool that makes it easy to write XML won't ever exist.
  Negative
I don't think anybody is saying it's a better protocol than REST, but it came out first, has great tool support (Visual Studio completely it abstracts away when creating and consuming services), and it's a standard so it will probably stay popular for a while.
  Positive
563e324761a80130652678bf	X	WSDL is XML representation file and a communication standard for any external system to communicate with your webservice regardless its implementation technologies or platforms.
  Negative
FYI, RESTful services can use a definition language file called WADL to describe the service as well.
  Negative
So, it's not about webservices only.
  Negative
563e324761a80130652678c0	X	This should be the accepted answer.
  Neutral
563e324761a80130652678c1	X	Like to know if anyone been able to stream HLS video via AWS Cloudfront with Signed URL.
  Negative
My experience so far is, it is not possible.
  Negative
AWS documentation is not clear.
  Negative
AWS Forum is silent.
  Negative
I seem to be able to get the first file ( .
  Neutral
m3u8 ) then it stops.
  Neutral
Using JW player, which complains cannot get media file.
  Negative
If answer is yes, please point me in the right direction.
  Positive
Thanks.
  Neutral
563e324761a80130652678c2	X	The scenario with unsigned URLs will most definitely work.
  Positive
I've successfully segmented video streams with ffmpeg onto S3 and served from CF. It's all HTTP after all.
  Negative
If you wish to restrict access to your HLS content, your playlist file would need to include signed URLs as well.
  Negative
You would typically compute these as the playlist is requested, based on whatever credentials you wish to authenticate the user with.
  Negative
Thus, you need a server-side implementation that generates session-unique m3u8's for you in order for the signed-URL scheme to make sense.
  Negative
Depending on your needs, another option would be to look into DRM.
  Negative
JW Player supports single/rotating key fragment decryption, which arguably tends to be a more complicated solution.
  Negative
You would then be left with the matter of securely distributing decryption keys to your clients.
  Negative
I hope this somewhat addresses your concerns.
  Neutral
If not, feel free to leave a comment.
  Negative
563e324761a80130652678c3	X	According to this article, cloudfront does support HLS.
  Negative
I am currently attempting to implement this on my site using flowplayer with html5 video.
  Negative
I will update this answer once it is running.
  Neutral
Amazon Web Services site also had this to say about HLS Support although it does not seem as neat and tidy as the rtmp approach.
  Negative
563e324861a80130652678c4	X	According to CloudFront's description of its streaming: Streaming of pre-recorded media: You can deliver your on-demand media using Adobe’s Real Time Messaging Protocol (RTMP) streaming via Amazon CloudFront.
  Negative
You store the original copy of your media files in Amazon S3 and use Amazon CloudFront for low-latency delivery of your media content.
  Negative
Amazon CloudFront integrates with Amazon S3 so you can configure media streaming by making a simple API call or with a few clicks in the AWS Management Console.
  Negative
You also benefit for the high throughput delivery of your media when using Amazon CloudFront, so you can deliver content in full HD quality to your viewers.
  Positive
The short answer is pretty much no.
  Neutral
Streaming from CloudFront is RTMP.
  Negative
Link: http://aws.amazon.com/cloudfront/ That said, AWS's Elastic Transcoder can make HLS filesets and playlists, and those can be served from CloudFront.
  Negative
So then the answer becomes "yes if you can do the work/figure it out."
  Neutral
Here's a link to their FAQ telling you how to do it: http://aws.amazon.com/elastictranscoder/faqs/#Can_I_get_segmented_output_for_HTTP_Live_Streaming_(HLS)
563e324861a80130652678c5	X	Most of these would be considered Integration Tests, right?
  Negative
Also, I'm not sure what you mean by mocking https.
  Negative
563e324861a80130652678c6	X	By mocking https I meant mocking the configuration manager to return both HTTPS and HTTP urls.
  Negative
563e324861a80130652678c7	X	And yes, you are right a lot of these are integration tests, but that is because of how your code is coupled.
  Neutral
Mike Z's suggestions offer ways to help curb that.
  Neutral
563e324861a80130652678c8	X	Both of your answers have been incredibly helpful to get me to start decoupling my code.
  Negative
Thank you!
  Positive
563e324861a80130652678c9	X	You give a lot of good suggestions.
  Positive
I like the ConfigurationManager separation and the IKeySource.
  Neutral
I'm using Ninject instead of a factory method.
  Negative
I see what you mean - adding a new layer for TestClient is just going to add up exponentially.
  Positive
I assume I could use integration tests here, instead.
  Negative
563e324861a80130652678ca	X	I have a fairly simple class that I'm trying to unit test.
  Positive
I'm very new to unit testing in general, and I'm not sure what I should be testing here.
  Negative
The only test case that I can figure out how to code is a null argument of stream.
  Negative
Besides that, I'm not sure how to test the results of a PutObjectRequest or what else.
  Negative
If I should be using mocks here, how?
  Negative
563e324861a80130652678cb	X	Things I would look at: Mock your configuration manager to return invalid data for the bucket and the URL.
  Negative
(null, invalid urls, invalid buckets) Does S3 support https ?
  Negative
If so mock it, if not, mock it and verify you get a valid error.
  Negative
Pass different kinds of streams in (Memory, File, other types).
  Negative
Pass in streams in different states (Empty streams, streams that have been read to the end, ...) I would allow the timeouts to be set as parameters, so you can test with really low timeouts and see what errors you get back.
  Very negative
I would also test with duplicate keys, just to verify the error message.
  Negative
Even though you are using guids, you are storing to an amazon server where someone else could use the S3 API to store documents and could theoretically create a file that appears to be a guid, but could create a conflict down the road (unlikely, but possible)
563e324861a80130652678cc	X	You are having trouble unit testing UploadImage because it is coupled to many other external services and state.
  Negative
Static calls including (new) tightly couple the code to specific implementations.
  Neutral
Your goal should be to refactor those so that you can more easily unit test.
  Neutral
Also, keep in mind that after unit testing this class, you will still need to do the big tests involving actually using the Amazon S3 service and making sure the upload happened correctly without error or fails as expected.
  Negative
By unit testing thoroughly, hopefully you reduce the number of these big and possibly expensive tests.
  Negative
Removing the coupling to the AmazonS3Client implementation is probably going to give you the biggest bang for your testing buck.
  Negative
We need to refactor by pulling out the new AmazonS3Client call.
  Negative
If there is not already an interface for this class, then I would create one to wrap it.
  Negative
Then you need to decide how to inject the implementation.
  Neutral
There are a number of options, including as a method parameter, constructor parameter, property, or a factory.
  Positive
Let's use the factory approach because it is more interesting than the others, which are straight-forward.
  Neutral
I've left out some of the details for clarity and read-ability.
  Negative
A unit test might look like this in MSTest: Now, we have one test verifying that the correct input stream is sent over in the request object.
  Negative
Obviously, a mocking framework would help cut down on a lot of boilerplate code for testing this behavior.
  Negative
You could expand this by starting to write tests for the other properties on the request object.
  Neutral
Error cases are where unit testing can really shine because often they can be difficult or impossible to induce in production implementation classes.
  Negative
To fully unit test other scenarios of this method/class, there are other external dependencies here that would need to be passed in or mocked.
  Negative
The ConfigurationManager directly accesses the config file.
  Negative
Those settings should be passed in.
  Neutral
Guid.NewGuid is basically a source of uncontrolled randomness which is also bad for unit testing.
  Negative
You could define an IKeySource to be a provider of key values to various services and mock it or just have the key passed from the outside.
  Negative
Finally, you should be weighing all the time taken for testing/refactoring against how much value it is giving you.
  Neutral
More layers can always be added to isolate more and more components, but there are diminishing returns for each added layer.
  Negative
563e324861a80130652678cd	X	There is a legacy web site whose tags point to a Microsoft .
  Negative
NET Web API endpoint which currently dumps the bytes of an image to HTTP response output.
  Negative
I want to re-write this Web API and instead of dumping the image on the screen I would like to put the generated image somewhere on Amazon S3 and just return a Cloud Front url which points to that image.
  Very negative
In more crude terms I want the new API to return the path of the image rather than its binary data.
  Negative
My question is that if my API returns an HTTP Redirect , will the image be able to retrieve the image from the new location without redirecting the whole page?
  Positive
What is the best way of replacing the legacy API in such a scenario?
  Neutral
563e324961a80130652678ce	X	I think a better approach could be that the ViewModel hold the final URLs.
  Negative
Then, in your view, you just set the proper URL to the img tag.
  Neutral
So, in your controller, you call the API to do the logic of putting the image in S3 and get the URL.
  Negative
Then you put that URL in the viewmodel with the other data required by your view.
  Negative
This way, you don't need to worry about redirection... and your view is clean of logic.
  Negative
563e324961a80130652678cf	X	A redirect result (301 or 302) would work just fine, the image would still load correctly.
  Negative
I would caution you to use the 301/Permanent redirect in case you have search engine rankings in play.
  Negative
You could always try this yourself.
  Neutral
In a test project, setup a controller action that returns a redirect result to a physical image.
  Positive
Then test!
  Neutral
In your view: This should return whatever image you pointed to in your Redirect()
563e324961a80130652678d0	X	If they are stored in the app (especially in clear text), then they can be retrieved.
  Negative
Can your app retrieve the key on first run (using https) and then store it using a KeychainItemWrapper.
  Negative
If so, you should also make sure you also set the appropriate level of app security for accessing the keychain.
  Positive
563e324961a80130652678d1	X	That is starting to sound like a good idea.
  Positive
I could even work out something to force it to expire after a week, in case I need to change to a different set of credentials.
  Negative
563e324961a80130652678d2	X	For this use case, the Token Vending Machine sounds like the best option.
  Negative
Thanks!
  Positive
563e324961a80130652678d3	X	This doesn't answer the question.
  Negative
How does this hide the key?
  Neutral
This still requires the key to be in the code.
  Negative
563e324961a80130652678d4	X	@rmaddy Is there any way to securely include API keys in an app?
  Negative
.
  Neutral
Probably I misunderstood
563e324961a80130652678d5	X	Read the code in the question.
  Negative
He has a key in his code.
  Positive
He is asking how to hide that key from the code so a hacker can't find it.
  Negative
Using the keychain doesn't hide the key from the app's code.
  Negative
563e324961a80130652678d6	X	This answer is not relevant to the question that was asked.
  Negative
563e324961a80130652678d7	X	You would be either relying on obfuscation which is reversible or encryption.
  Negative
If you encrypted the keys, you would still have to store the key for decryption which is then a recursive problem.
  Negative
How would you store the encryption key?
  Neutral
563e324961a80130652678d8	X	@StephenJohnson Obfuscating the key is still a big improvement over using plain text.
  Negative
563e324a61a80130652678d9	X	@rmaddy Agreed!
  Positive
563e324a61a80130652678da	X	@rmaddy - nice approach to obfuscation: iosdevelopertips.com/cocoa/…
563e324a61a80130652678db	X	Amazon has an AWS SDK for iOS, along with several sample apps.
  Negative
In their samples, they put the API credentials in a Constants.h file: My concern is that these can be extracted by a determined hacker.
  Negative
Is there any way to securely include API keys in an app?
  Negative
The one option I've seen is to include a server of my own as a go-between: the app talks to my server, my server talks to S3.
  Negative
I can see the value in doing this, but one is still presented with the problem: do I allow the app to make API calls on my server without any kind of authentication?
  Negative
Including my own API key in the app has the same problem as including AWS API keys.
  Negative
563e324a61a80130652678dc	X	There are a couple of credential management options to help you avoid embedding credentials in your app.
  Negative
The first is Web Identity Federation, which allows users to log in to your app with Facebook, Google, or Login With Amazon.
  Negative
Another option is to use a Token Vending Machine, which is a server component that distributes temporary credentials to your app.
  Negative
There is a high-level overview with pointers to the relevant documentation and code samples on the AWS Mobile Development Blog: http://mobile.awsblog.com/post/Tx3UKF4SV4V0LV3/Announcing-Web-Identity-Federation
563e324a61a80130652678dd	X	You'll probably want to create temporary write credentials using AWS STS tokens instead of passing keys all the way to the client.
  Negative
You can also create OAIs for CloudFront endpoints so no users directly access S3.
  Negative
http://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html
563e324a61a80130652678de	X	Did you try a KeychainItemWrapper ?
  Negative
to Set to Get Keychain Services Programming Guide Before import Security.framwork I didn't check this code, if something doesn't work, let me know
563e324a61a80130652678df	X	Probably you colud store them in an encoded form and encode them as needed.
  Very negative
563e324a61a80130652678e0	X	Welcome to Stack Overflow.
  Negative
This isn't really a good question for Stack Overflow.
  Negative
Explaining how something works doesn't fit well into the question categories because it can generate a small book.
  Negative
Instead, you need to read their documentation carefully and follow any examples, and, if that doesn't help, consult their tech support who are better equipped to answer specific questions about their tools.
  Negative
563e324a61a80130652678e1	X	I am trying to parse describe_instances api's response from Aws::EC2::Client.
  Negative
But verion 2 of aws ruby sdk comes with response paging feature.
  Negative
I dont understand what exactly is this!
  Positive
563e324b61a80130652678e2	X	Response paging is a feature where you can enumerate calls to an API, yielding one response as a time, until all results have been received.
  Negative
This can be very important for API calls that return a large amount of data, such as enumerating objects in a bucket in Amazon S3.
  Negative
Without response paging you would have to do something like this: Some APIs have more complex paging requirements.
  Negative
Response paging eliminates the need to understand the paging requirements of every API call, and provides an #each method on the response.
  Negative
563e324b61a80130652678e3	X	Hello Dear thanks for giving this knowledge to me.
  Positive
563e324b61a80130652678e4	X	I am using XAMP(PHP 5.3,mySQL) on Windows 7, now in order to upload my App on EC2 is thr any additional requirement .
  Negative
So in this scenario should i need to install their SDK?
  Neutral
563e324b61a80130652678e5	X	Can you please tell me about Amazon EC2.
  Positive
I am having a fully functional small web application -- about 20-25 pages -- completed locally.
  Negative
But now the owners want to get it uploaded on the cloud rather than on a simple server.
  Negative
So please tell me, should I make any changes in my app?
  Positive
Is there any need to use the PHP SDK on Amazon Cloud.
  Neutral
What steps exactly are required to manage the instance on the cloud?
  Neutral
Please provide me some link from where I can get details about this.
  Negative
I am having my application in PHP and MySQL.
  Negative
563e324b61a80130652678e6	X	Amazon EC2 is the Elastic computing cloud by Amazon.
  Negative
EC2 is a platform for hosting dedicated servers in the cloud.
  Negative
This differs from platform as a service models, like Google App Engine, where you definitely need to use their SDK.
  Negative
If your local server is running SUSE, for instance, and the EC2 server is running SUSE, then in theory your app should run the same on both servers.
  Negative
You should be able to access the EC2 server with an SSH connection just like you would a local server.
  Negative
You should be able to copy the app using secure copy (scp).
  Negative
Additionally, assuming they're running SUSE, you'd need to make sure Apache is configured to run PHP scripts, and you would need to install and configure MySQL, just like you would on your local server.
  Negative
Your app should run just fine on Amazon.
  Negative
You're still dealing with a dedicated server.
  Neutral
The main difference is that you can't physically touch it as it's somewhere in Virginia I think.
  Positive
With that said, there is an SDK for PHP for Amazon, but it's not immediately clear what purpose it serves.
  Negative
I've run PHP just fine on EC2 without an SDK.
  Negative
But if you are interested, the link is below: http://aws.amazon.com/sdkforphp/ EDIT: The main advantage of the PHP API is for cases where the application will integrate with Amazon services.
  Negative
As an example, let's say your application will save files to Amazon S3.
  Neutral
S3 uses a REST interface to interact with resources on the S3 Cloud.
  Neutral
Instead of writing a wrapper around the REST interface yourself, the PHP API includes some pre-packaged API's that make development faster.
  Negative
You can learn more here at the Amazon PHP SDK FAQ
563e324c61a80130652678e7	X	All right, I am pretty newbie to network and storage things, but in my research, we need to use AWS S3 to backup data, sounds simple enough!
  Negative
So I follow the "AWS storage gateway user guide (API version 2013-06-30).
  Negative
Below are details I could provide based on my best knowledge: And After all the above completed, I tried to use my Windows 8 iSCSI to connect to VM.
  Negative
It shows as a disk in folder, so I did a initial formatting.
  Negative
But after this, it asks for formatting again.
  Negative
I followed the guide, but unfortunately it didn't work for me this time.
  Negative
Could anyone provide any insight on this issue?
  Negative
Thanks very much in advance.
  Neutral
563e324c61a80130652678e8	X	It turns out that I don't understand how iSCSI works.
  Negative
Quoted from Amazon Storage Gateway User Guide: Each of your storage volumes is exposed as an iSCSI target.
  Negative
Connect only one iSCSI initiator to each iSCSI target Since I thought iSCSI target as a network shared drive, I let multiple machine connect to the iSCSI target, resulting keeping asking for formatting.
  Negative
563e324c61a80130652678e9	X	I figured out why it didn't work.
  Negative
Turns out I was using an old version of the "Amazon S3 PHP Class".
  Negative
I updated and used your suggested code, and now the new files have a Cache-Control set.
  Positive
Great!
  Very positive
I will also look into your second link to set all the Cache-Control headers for the files that are already in the bucket.
  Negative
This should solve all my problems.
  Negative
Thanks for everything!
  Positive
563e324c61a80130652678ea	X	I recently started using Amazon S3 to serve images to my visitors since this will reduce the server load.
  Negative
Now, there is a new problem: Today I looked into my AWS billings.
  Negative
I noticed that I have a huge bill waiting for me - there has been a total of 4TB AWS Data Transfer in 20 days.
  Negative
Obviously this is because the high amount of outgoing Amazon S3 traffic (to Cloudflare which then serves it to the visitors).
  Negative
Now I should to reduce the amount of requested files by setting a Cache header (since Cloudflare's Crawler will respect that).
  Negative
I have modified my code like this: to Still, it does not work.
  Negative
Cloudflare does not respect the Cache because the Cache-Control does not show up as "Cache-Control" in the Header but instead as "x-amz-meta-cachecontrol".
  Negative
Cloudflare ignores this.
  Neutral
Does anyone have an easy solution for this?
  Neutral
TL;DR: I have more or less the same problem as this guy: http://support.bucketexplorer.com/topic734.html (that was in 2008) EDIT: I have stumbled upon this: Amazon S3 not caching images but unfortunately that solution does not work for me.
  Very negative
EDIT 2: Turns out it didn't work because I was using an old version of the "Amazon S3 class".
  Negative
I updated and the code works now.
  Positive
Thank you for your time.
  Positive
563e324c61a80130652678eb	X	If you are getting "x-amz-meta-cachecontrol", it is likely you are not setting the headers correctly.
  Negative
It might just be the exact way you are doing it in your code.
  Neutral
This is supposed to work.
  Neutral
I am deducing this is php using Amazon S3 PHP Class?
  Negative
Try this: In the S3 PHP docs putObjectFile is listed under Legacy Methods: Compare to this: You need to set cache-control as a request header, but appears that there is no way to set request headers with putObjectFile, only meta headers.
  Very negative
You have to use putObject and give it an empty array for meta headers and then another array with the request headers (including cache-control).
  Negative
You can also try some of the other working examples I have listed below.
  Negative
See also: How to set the Expires and Cache-Control headers for all objects in an AWS S3 bucket with a PHP script (php) Updating caching headers for Amazon S3 and CloudFront (python) Set cache-control for entire S3 bucket automatically (using bucket policies?)
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html?r=5225
563e324c61a80130652678ec	X	I can see there are examples with server side only and I am looking for only javascript based upload to s3.
  Negative
I am not sure how to add policy to request as in all examples they are using server side code.
  Negative
please help!!!
  Positive
563e324c61a80130652678ed	X	@KrishnaBhatt I have explained how to do this in my answer.
  Negative
Also, the link provided by Burak explains in detail how to achieve this.
  Negative
563e324c61a80130652678ee	X	I am trying upload files to directly to s3 but as per my research its need server side code or dependency on facebook,google etc. is there any way to upload files directly to amazon using fineuploder only?
  Negative
563e324c61a80130652678ef	X	Yes, with fine uploader you can do.Here is a link that explains very well what you need to do http://blog.fineuploader.com/2013/08/16/fine-uploader-s3-upload-directly-to-amazon-s3-from-your-browser/
563e324c61a80130652678f0	X	There are three ways to upload files directly to S3 using Fine Uploader: Allow Fine Uploader S3 to send a small request to your server before each API call it makes to S3.
  Positive
In this request, your server will respond with a signature that Fine Uploader needs to make the request.
  Negative
This signatures ensures the integrity of the request, and requires you to use your secret key, which should not be exposed client-side.
  Neutral
This is discussed here: http://blog.fineuploader.com/2013/08/16/fine-uploader-s3-upload-directly-to-amazon-s3-from-your-browser/.
  Positive
Ask Fine Uploader to sign all requests client-side.
  Neutral
This is a good option if you don't want Fine Uploader to make any requests to your server at all.
  Positive
However, it is critical that you don't simply hardcode your AWS secret key.
  Neutral
Again, this key should be kept a secret.
  Neutral
By utilizing an identity provider such as Facebook, Google, or Amazon, you can request very limited and temporary credentials which are fed to Fine Uploader.
  Negative
It then uses these credentials to submit requests to S3.
  Negative
You can read more about this here: http://blog.fineuploader.com/2014/01/15/uploads-without-any-server-code/.
  Neutral
The third way to upload files directly to S3 using Fine Uploader is to either generate temporary security credentials yourself when you create a Fine Uploader instance, or simply hard-code them in your client-side code.
  Negative
I would suggest you not hard-code security credentials.
  Negative
563e324d61a80130652678f1	X	Here is what you need.
  Neutral
In this blogpost fineuploader team introduces serverless s3 upload via javascript.
  Positive
http://blog.fineuploader.com/2014/01/15/uploads-without-any-server-code/
563e324d61a80130652678f2	X	I am doing an upload via CORS to Amazon S3 with the Kendo Upload control.
  Negative
I'm having an issue with the fact that I need to grab a signature from my server, then add it to the 'data' for the event object of 'upload' handler I created.
  Negative
The problem is, of course, that in the handler I fire off an async request to get the signature, and the upload handler continues on it's merry way without the signature data i need.
  Negative
The published API has no 'upload()' or something command that I could call when my async request returns.
  Negative
I saw an ASP-Kendo-S3 example somewhere, but it's not exactly clear from that code, how that signature is being obtained, and of course, I'm not using ASP.
  Negative
563e325061a80130652678f3	X	Kendo Upload has an onUpload event.
  Negative
In Kendo's asp.net example there really isn't anything specific to that framework that wouldn't port to anything else.
  Negative
They populate the page initially with the profile (base64 encoded JSON).
  Negative
To get a signature for that base64 encoded json profile they use this method (C#): It looks pretty self explanatory to the point where you could port it to another language.
  Negative
563e325061a80130652678f4	X	I have an account in the Amazon S3 where I upload files - among them video files.
  Negative
What I need to do next, is to show my clients a specific video, and I need something that support all kinds of encoding.
  Positive
Most of the time, the user uses a smartphone to watch the video, and I need some software to stream the video-file right into his/her smartphone - and no matter in which format the video file was uploaded, I need that the user will be able to watch it.
  Negative
I saw some solutions such as Vimeo, Vzaar, etc. but I'm looking for something else - without too many limits on size and bandwidth, with a good API to upload the files into it, and with cross-platform ability to view the file.
  Positive
Is there a solution in Youtube?
  Neutral
in Amazon EC2?
  Neutral
etc.?
  Neutral
Any other solution?
  Neutral
Thanks a lot, Danny
563e325061a80130652678f5	X	One solution for you is to use a conversion utility like ffmpeg for your conversion needs.
  Positive
One format of course cannot support all end users (PC browsers, mobile browsers and so on), so you need to convert the originally uploaded file to multiple formats and render to the user what is best based on the device type which accesses it.
  Negative
You can also look at Darwin streaming server and host it on Amazon or any other platform for that matter to suit your need If you are looking for a solution directly from Amzon by uploading your files on S3 and stream it, then currently Amazon supports only Adobe server as its streaming server in production mode, so you are pretty much restricted to the formats that the streaming server supports.
  Negative
Still the ownus is on you to convert in to different formats.
  Positive
563e325061a80130652678f6	X	thank you, it's cool answer.
  Positive
But what I want is to enter the keyword "iphone", for example, and get some iphone products on amazon with its description and price; you said about paying only for amout of usage, what does it mean: I don't understand, because I'm not going to save information on Amazon, I just wanna get information about amazon products through amazon web servises
563e325161a80130652678f7	X	1) I want to enter the keywork "iphone" and get some iphone products on amazon with its description and price; 2) I'm interesting in price: should I pay for each request or may be I should pay monthly, and where can I find the price list
563e325161a80130652678f8	X	Sometimes your bills might surprise you - we must be very careful on how we use AWS.
  Very negative
563e325161a80130652678f9	X	for example, to use ebay web services I shouldn't pay.
  Negative
How much should I pay?
  Neutral
Should I pay for request number or monthly?
  Negative
Where can I get the price list?
  Negative
563e325161a80130652678fa	X	Amazon charges almost for everything.
  Negative
They will charge you for requests also.
  Positive
There're many things you should read about (how to save costs, for example).
  Negative
And prices info could be found here: aws.amazon.com/pricing
563e325161a80130652678fb	X	I'm new in iOS development, and I faced an issue with amazon.
  Negative
I wanna gain information about amazon products with amazon web servises.
  Negative
I wanna enter the keyword and get information about proper products.
  Negative
I looked at http://aws.amazon.com/mobile/ and saw that I should register.
  Negative
During the registration Amazon asked me about my Visa card information and then tried to withdraw 1 dollar.
  Negative
The questions are:
563e325161a80130652678fc	X	The pricing differs between the various services and is typically listed in http://aws.amazon.com/<service name>/pricing/.
  Negative
Here are couple of examples - EC2, S3.
  Negative
Note that for some of the services there is a free tier for about a year, as long as you stay under certain amount of usage.
  Negative
So, while you WILL get a bill every month, that bill might be for $0.
  Negative
More about the AWS Free Usage Tier.
  Negative
You can download the client SDKs freely and write code against it.
  Negative
However, to actually run it against AWS, you will need AWS Access Key ID and Secret Access Key, so that AWS servers can authenticate the requests from your application (and incidentally also bil you properly for your usage).
  Neutral
You should start with the Getting Started with the AWS SDK for iOS and the AWS SDK for iOS FAQs.
  Negative
The SDK also contains bunch of sample apps into the <SDK install folder>/samples folder.
  Negative
Update: Ah, you want to search the Amazon catalog?
  Neutral
That's different from AWS.
  Neutral
AWS is intended to provide you access to computing resources (storage, CPU, load balancing, and so on) for your own services.
  Negative
For your scenario you need to use the Amazon Affiliate Program Product Advertising API.
  Negative
While that API does share credentials with AWS (it uses the AWS Access Key ID and Secret Key), it most likely is free (but double check to be sure), as amazon will be making money on any product your users buy.
  Negative
Also, the Product Advertising API does not have client SDKs (as far as I know), so you will have to deal with making the HTTP requests yourself.
  Negative
The API supports both REST and SOAP, so you can choose your own poison.
  Negative
There's also bunch of samples for both server and client apps, in PHP, C#, Java, Node.js, Ruby, and so on.
  Negative
563e325161a80130652678fd	X	AWS is great!
  Very positive
Its totally worth the price.
  Positive
So you can download the AWS iOS SDK and integrate it into your project; however, before it will work you need to signup.
  Very negative
I would give you some examples but I don't fully understand what you're asking.
  Negative
The AWS iOS SDK has tons of code samples in it.
  Negative
If you want, you can comment on this post what you want to use AWS for and then I can help you come up with the code to achieve it :) I hope you have fun with iOS Development, its great :) Good Luck!
  Positive
563e325161a80130652678fe	X	Are you maybe confusing Amazon web services with a request API?
  Negative
You said: I want to enter the keywork "iphone" and get some iphone products on amazon with its description and price That is what an amazon web API would do (from this question, I understand there is maybe no such thing for Amazon?)
  Neutral
.
  Neutral
AWS is a cloud service where you can run your programs and pay according to the resources you use.
  Negative
Think of that as a web host.
  Neutral
All in all, AWS is not directly related to Amazon content, if I understood correctly this is not what you want.
  Negative
563e325161a80130652678ff	X	
563e325161a8013065267900	X	I have about 400 GB data on an Amazon EBS volume and I need this data in a S3 bucket for Hadoop EMR usage.
  Very negative
How can I move/copy data from an EBS volume to a S3 bucket (both S3 bucket and EBS volume are in the same AWS region)?
  Neutral
Thanks
563e325161a8013065267901	X	The AWS Command Line Interface is meanwhile the recommended choice for all things AWS: The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services.
  Negative
With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.
  Negative
On top of this unified approach to all AWS APIs, it also adds a new set of simple file commands for efficient file transfers to and from Amazon S3, with characteristics similar to the well known Unix commands, e.g. for the task at hand: So cp would be sufficient for your use case, but be sure to check out sync as well, it is particularly powerful for many frequently encountered scenarios (and sort of implies cp depending on the arguments).
  Positive
563e325161a8013065267902	X	I'm pretty sure they haven't enabled this functionality yet.
  Negative
I've been waiting for them to add the ability for a menu action like share or send or open website to a card insert from GDK.
  Negative
563e325261a8013065267903	X	That was actually the first thought I had.
  Positive
I tried that.
  Neutral
I had a Card with an image and no text and pushed it to the timeline.
  Negative
The issue is that there's nothing you can do with that card as a user once it's in there.
  Negative
Unless I'm doing something wrong here: Card card2 = new Card(this); card2.setImageLayout(Card.ImageLayout.FULL); card2.addImage(R.drawable.glass_share); TimelineManager.from(this).
  Negative
insert(card2);
563e325261a8013065267904	X	Take a look at the grid here (bottom of the page) : developers.google.com/glass/develop/gdk/ui/index.
  Negative
You can't access the user input with Static Cards.
  Negative
You probably should try with a Live Card.
  Negative
There is a good example : developers.google.com/glass/develop/gdk/ui/live-cards
563e325261a8013065267905	X	Ok so let's say I have a live card and I can access the user input.
  Positive
How then would I share the image out other people?
  Neutral
563e325261a8013065267906	X	My GDK app generates an image that I'd like the user to share with other people.
  Negative
I tried creating a Card that contained an image and inserted it into the timeline.
  Negative
There is no Share menu item for that card.
  Negative
I also tried adding the image into the Media Gallery and that does not produce an error but the image does not appear in the timeline: And lastly I've tried the ACTION_SEND intent which doesn't seem to be supported: So what's the right way to do this?
  Very negative
--edit A couple more things I've tried unsuccessfully : -Upload to Amazon S3.
  Negative
It doesn't look like I can use the Amazon .
  Negative
jar file in a GDK app.
  Negative
The app builds but I get a ClassNotFoundException on runtime.
  Negative
-use the JavaMail API to mail that image to a specific email address.
  Neutral
It doesn't look like some of the required classes for JavaMail are included in the GDK environment.
  Negative
563e325261a8013065267907	X	I don't know if a 'Share' system menu exists, but if you want to push a card on the timeline, you should probably use the TimelineManager class, that allow you to interact with the Glass timeline : https://developers.google.com/glass/develop/gdk/reference/com/google/android/glass/timeline/TimelineManager#insert(com.google.android.glass.app.Card) Be careful, you can't use the same insertion strategy if you're pushing static cards or live cards.
  Negative
Julian
563e325261a8013065267908	X	OK I think I've got a solution here.
  Positive
I can post the image to a service and then on the backend do whatever I need to do with it.
  Neutral
I had to Base64 encode the image content b/c as far as I can tell it doesn't look like Multipart is supported.
  Negative
Wrap this bad boy in an AsyncTask and you should be good to go:
563e325261a8013065267909	X	I am trying to create an application where I host a few static html files that a user authors and uploads to GAE.
  Negative
Is it possible to upload these files to WAR folder for hosting without a redeploy.
  Negative
Uploading to WAR may not be a good idea as a redeploy would wipe out the user authored files.
  Negative
Would like these files to be stored across deployments.
  Neutral
May be a GCS bucket ?
  Neutral
But in case of GCS bucket, how would you serve these static html files over a GAE/page.
  Negative
html url ?
  Neutral
I was reading that GCS bucket would serve as a web folder by turning a knob on that bucket.
  Negative
But how scalable would that be ?
  Neutral
Is that even a good approach to use GCS bucket as a web url ?
  Neutral
Would be good to get some ideas on addressing this as I am new to GAE !
  Neutral
Thanks a lot
563e325261a801306526790a	X	You can use Google Cloud Storage to serve static websites and a Google AppEngine application to update the bucket when user edit/upload some content to the website.
  Negative
You would probably need to have different subdomains for this like: This should be pretty scalable & cheap to serve traffic.
  Negative
Apart from GCS you also can considering storing static files on Amazon S3 or any other hosting that has API.
  Negative
Actually you even can develop a solution that upload files to multiple storages like GCS, S3, etc. and serve them using fault-tolerant DNS or reverse-proxy (like CloudFlare) so if Google or Amazon goes down your website is up and running.
  Negative
563e325261a801306526790b	X	What are the pitfalls you are looking to avoid?
  Negative
563e325261a801306526790c	X	I don't know exactly.
  Negative
I just know it's complicated doing video streaming to android and I don't want to spend months going in one direction only to find out it's not compatible with it.
  Negative
563e325261a801306526790d	X	I'm thinking that the first thing I should do is make an android application that uploads a video and a rails app that accepts that video and downloads it to Amazon.
  Negative
Any idea how to do that?
  Neutral
563e325261a801306526790e	X	On SO, you need to try something first.
  Neutral
See: stackoverflow.com/questions/how-to-ask
563e325261a801306526790f	X	I am currently building an app that will allow users to upload videos and view other users videos in a stream.
  Positive
Sort of like Vine.
  Neutral
I have been using rails for over a year now but I am not sure how to go about implementing the backend for the android application.
  Negative
My understanding of the situation is that I must use a json call to my rails api that will upload the video file to Amazon s3 or CloudFront.
  Negative
I then need to make the Amazon video file or url be stored or linked to a URL that the Rails app creates for the user.
  Negative
After that, I would need to play the video (and other people's videos) back to the android application.
  Negative
It looks like there are a lot of pitfalls to this.
  Negative
If anyone knows the correct way to go about doing this, I would be really grateful.
  Negative
Thank you.
  Positive
563e325261a8013065267910	X	Probably want to use a REST API.
  Negative
Should host your videos on a CDN.
  Positive
Can use VideoView to stream some types of videos on Android.
  Neutral
563e325361a8013065267911	X	I'm using s3cmd 1.1.0beta to upload files that are larger than 5 GB to Amazon S3.
  Negative
This is because s3cmd older than 1.1.0 is not able to upload files larger than 5 GB (Amazon single-part upload limit), and the latest beta version is able to upload these files to S3 using multi-part upload.
  Negative
The problem is: I am not able to perform ANY operation on the files larger than 5 GB uploaded through s3cmd 1.1.0.
  Negative
I suspect that this may be happening because Etag set by s3cmd does not match the Etag that Amazon expects: The specific problems are as follows (both through the web console): Is there any way to fix the Etags in the larger-than-5-GB-files so that I am able to perform operations on these files?
  Negative
563e325361a8013065267912	X	OK, after some investigation, I found that the problem has to do with Amazon S3's inability to natively handle files that are larger than 5 GB in size.
  Negative
In order to copy or do any operation on a file larger than 5 GB in size, you have to specifically use Amazon's multi-part upload and related APIs for working on large files.
  Negative
Apparently, even Amazon's AWS web console uses only the simple APIs, which work only on files that are less than 5 GB in size, so if you want to do anything with files larger than 5 GB in size, you need to write your own code with the AWS API to operate on those files!
  Negative
563e325561a8013065267913	X	Great, this worked, thanks a lot!
  Very positive
But only one thing, getObject requires a parameter, I added empty string (""), and worked.
  Negative
563e325561a8013065267914	X	I have this code to get files from rackspace cloud files: In the open php cloud documentation says you can change 'GET' to 'PUT' to what I imagine is being able to put a file, instead of getting it, the problem is that the file doesn't exist yet, and apparently the only way to create a file is uploading it first?
  Very negative
PHP SDK Container In Amazon s3 I can do the following to get what I want: Then I can write to the presignedUrl anyway I prefer, like with Java: So, basically what I want to do, is get the upload URL from RackSpace and write to it like I do with Amazon s3.
  Negative
Is it possible?
  Neutral
And if it is, how can you do it?
  Neutral
I need to do it this way because my API will provide only download and upload links, so no traffic goes directly through it.
  Negative
I can't have it saving the files to my API server then upload it to cloud.
  Negative
563e325661a8013065267915	X	Yes, you can simulate a file upload without actually uploading content to the API - all you need to do is determine what the filename will be.
  Negative
The code you will need is: The getObject method returns an empty object, and all you're doing is setting the remote name on that object.
  Negative
When a temp URL is created, it uses the name you just set and presents a temporary URL for you to use - regardless if the object exists remotely.
  Negative
The temp URL can then be used to create the object.
  Negative
563e325661a8013065267916	X	If the answer below is correct, can you mark it as such ?
  Negative
This would help other customers having the same question and looking for answers.
  Neutral
Thanks
563e325661a8013065267917	X	take a look at Uploadcare.
  Positive
it allows uploading huge files and copying them to your s3 bucket without exposing it.
  Negative
563e325761a8013065267918	X	I have a pretty tough problem here: I need to allow the users of my site to upload very large files to their accounts and I want to store these files on a AWS S3 filesystem.
  Negative
I can't just write a web service to receive these and save them in the S3 fs, because all kinds of things can go wrong during the upload and I need a sophisticated uploader client.
  Negative
The kind of client that Amazon provides to upload files into S3, but of course I can't give my users direct access to that.
  Neutral
I'd seriously appreciate any ideas for this!
  Positive
Thank you
563e325761a8013065267919	X	Best practice would be to let your client application to upload directly to S3, not flowing through your own web infrastructure.
  Negative
This would leverage ether massive parallel nature of S3 and off-load your web infrastructure.
  Negative
Offloading your infrastructure will allow to use less instances or smaller instances to serve the same amount of traffic.
  Negative
Hence a lower infrastructure cost for you.
  Negative
You would need to write an S3 IAM Policy that would limit access for each of your user to their own "directory" (aka key prefix) on S3 Have a look at this blog post : http://blogs.aws.amazon.com/security/post/Tx1P2T3LFXXCNB5/Writing-IAM-policies-Grant-access-to-user-specific-folders-in-an-Amazon-S3-bucke If your application is a web app, you can even let your customers' browsers upload directly to S3.
  Negative
See how to implement this securely at http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html Just a last note about your question : S3 is not a file system, S3 is an object store.
  Negative
Read more about the differences between object storage and file system at http://www.infoworld.com/article/2614094/data-center/what-is-object-storage-.html
563e325761a801306526791a	X	I am uploading objects to amazon s3 using AWS iOS SDK in Iphone, sometime error occurs and some of the objects are uploaded, remaining are not uploaded.
  Very negative
I have created bucket and inside bucket i have created folder in which i have store my objects.
  Positive
I want to delete folder and all its object.
  Negative
Can anyone help me?
  Neutral
563e325761a801306526791b	X	First of all, there is not such thing as "folders" in S3.
  Negative
Most S3 clients (including the AWS web console) show them as folders only for convenience (grouping stuff), but in fact, what you see as a "folder name" is merely a prefix.
  Negative
Being that said, my suggestion to you is using the listObjectsInBucket API call, passing in your "folder name" as prefix in the S3ListObjectsRequest parameter.
  Negative
When you have obtained all the keys (file names including prefix) matching that prefix, use the deleteObjects API call, passing in the keys in S3ListObjectsRequest parameter.
  Negative
For more details on folder/prefix and deleting stuff, please see these related links: Delete files, directories and buckets in amazon s3 java Thread on AWS forum regarding this subject
563e325c61a801306526791c	X	Thanks Wizzard, but I'm looking for a storage service provider so I don't have to spend lots on my own infrastructure.
  Negative
Sorry if that wasn't clear.
  Neutral
563e325c61a801306526791d	X	I have a requirement to store files remotely from a cordova/phonegap application.
  Negative
I have been looking at: However, these seems to want my application to be registered with them, but I can't see whey this is required (see here).
  Negative
I have also looked at Amazon S3.
  Neutral
Howeever, I'm not sure my users will be happy to create an Amazon account just so that their files can be persisted.
  Negative
Question: Are there other service provider API's available that will allow me to create a user account on the service provider and upload/download files?
  Neutral
563e325c61a801306526791e	X	Take a look at various Open Source cloud solutions (example: OpenStack).
  Neutral
Using provided API you can implement user registration in your app.
  Neutral
To have a very basic (OpenStack) installation you really need only 1 server and then you can easily extend your cloud adding additional servers / resources.
  Very negative
563e325d61a801306526791f	X	Thanks, that's useful code to look at.
  Negative
Did you have some client code too?
  Neutral
563e325d61a8013065267920	X	@James: only timestamp seems not sufficient much, during short of time they may simulate the request and sent to server, I have just edited my post, use both would be the best.
  Negative
563e325e61a8013065267921	X	Are you sure this is working as it should?
  Neutral
you are hashing the timestamp with the message and caching that message.
  Positive
This would mean a different signature each request which would render your cached signature useless.
  Negative
563e326061a8013065267922	X	@FilipStas: seems I don't get your point, the reason to use Cache in here is to prevent relay attack, nothing more
563e326061a8013065267923	X	@ChrisO: You can refer [this page] (jokecamp.wordpress.com/2012/10/21/…).
  Very negative
I will update this source soon
563e326061a8013065267924	X	Thanks - I'll take a look at this, though for now I've rolled my own HMAC-based solution.
  Positive
563e326061a8013065267925	X	@CraigShearer - hi, you say you've rolled your own.
  Negative
.
  Neutral
just had a few questions if you don't mind sharing.
  Negative
I'm in a similar position, where i have a relatively small MVC Web API.
  Negative
The API controllers sit alongside other controller/actions which are under forms auth.
  Negative
Implementing OAuth seems an overkill when I already have a membership provider i could use and I only need to secure a handful of operations.
  Negative
I really want an auth action that returns an encrypted token - then used the token in subsequent calls?
  Negative
any info welcome before I commit to implementing an existing auth solution.
  Neutral
thanks!
  Positive
563e326061a8013065267926	X	@Maksymilian Majer - Any chance you can share how you've implemented the provider in more detail?
  Negative
I'm having some problems sending responses back to the client.
  Negative
563e326061a8013065267927	X	Thanks for the example @Dalorzo, but I have some issues.
  Neutral
I looked at the attached link, but following that instructions doesn't quite work.
  Negative
I also found needed info missing.
  Negative
Firstly, when I create the new project, is it right to choose Individual User Accounts for authentication?
  Negative
Or do I leave it at no authentication.
  Neutral
I'm also not getting the mentioned 302 error, but am getting a 401 error.
  Negative
Lastly, how do I pass the needed info from my view to the controller?
  Negative
What must my ajax call look like?
  Neutral
Btw, I'm using forms authentication for my MVC views.
  Negative
Is that a problem?
  Neutral
563e326061a8013065267928	X	Are you sharing/providing source code for this framework as open source?
  Neutral
563e326061a8013065267929	X	I want to build a RESTful web service using ASP.NET Web API that third-party developers will use to access my application's data.
  Negative
I've read quite a lot about OAuth and it seems to be the standard, but finding a good sample with documentation explaining how it works (and that actually does work!)
  Positive
seems to be incredibly difficult (especially for a newbie to OAuth).
  Negative
Is there a sample that actually builds and works and shows how to implement this?
  Positive
I've downloaded numerous samples: I've also looked at blogs suggesting a simple token-based scheme (like this) - this seems like re-inventing the wheel but it does have the advantage of being conceptually fairly simple.
  Negative
It seems there are many questions like this on SO but no good answers.
  Negative
What is everybody doing in this space?
  Neutral
563e326161a801306526792a	X	We have managed to apply HMAC authentication to secure Web Api and it worked okay.
  Negative
Basically, HMAC authentication uses a secret key for each consumer which both consumer and server both know to hmac hash a message, HMAC256 should be used.
  Negative
Most of cases, hashed password of consumer is used as secret key.
  Negative
The message normally is built from data in the HTTP request, or even customized data which is added into HTTP header, message might include: Under the hood, HMAC authentication would be: Consumer sends a HTTP request to web server, after building the signature (output of hmac hash), the template of HTTP request: Example for GET request: The message to hash to get signature: Example for POST request with querystring (signature below is not correct, just an example) The message to hash to get signature Please note that form data and query string should be in order, so the code on server get querystring and form data to build correct message.
  Very negative
When HTTP request comes to server, an authentication action filter is implemented to parse the request to get information: HTTP verb, timestamp, uri, form data and query string, then based on these to build signature (use hmac hash) with secret key (hashed password) on the server.
  Negative
The secret key is got from database with username on the request.
  Positive
Then server code compares the signature on the request with the signature built, if equal, authentication is passed, otherwise, it failed.
  Very negative
The code to build signature: So, how to prevent replay attack?
  Neutral
Add constraint for the timestamp, something like: (servertime: time of request comming to server) And, cache the signature of request in memory (use MemoryCache, should keep in limit of time).
  Negative
If the next request comes with the same signature with previous request, it will be rejected.
  Negative
The demo code is put as here: https://github.com/cuongle/Hmac.WebApi
563e326161a801306526792b	X	Have you tried DevDefined.OAuth?
  Negative
I have used it to secure my WebApi with 2-Legged OAuth.
  Negative
I have also successfully tested it with PHP clients.
  Positive
It's quite easy to add support for OAuth using this library.
  Neutral
Here's how you can implement the provider for ASP.NET MVC Web API: 1) Get the source code of DevDefined.OAuth: https://github.com/bittercoder/DevDefined.OAuth - the newest version allows for OAuthContextBuilder extensibility.
  Negative
2) Build the library and reference it in your Web API project.
  Negative
3) Create a custom context builder to support building a context from HttpRequestMessage: 4) Use this tutorial for creating an OAuth provider: http://code.google.com/p/devdefined-tools/wiki/OAuthProvider.
  Negative
In the last step (Accessing Protected Resource Example) you can use this code in your AuthorizationFilterAttribute attribute: I have implemented my own provider so I haven't tested the above code (except of course the WebApiOAuthContextBuilder which I'm using in my provider) but it should work fine.
  Negative
563e326161a801306526792c	X	I would suggest starting with the simplest solutions first - maybe simple HTTP Basic Authentication + HTTPS is enough in your scenario?
  Negative
If not (for example you cannot use https, or need more complex key management) you may have a look at HMAC-based solutions as suggested by others.
  Negative
A good example of such api would be Amazon S3 (http://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html) I wrote a blog post about HMAC based authentication in ASP.NET Web API.It discusses both Web API service and Web API client and the code is is available on bitbucket.
  Negative
http://www.piotrwalat.net/hmac-authentication-in-asp-net-web-api/ Here is a post about Basic Authentication in Web API: http://www.piotrwalat.net/basic-http-authentication-in-asp-net-web-api-using-message-handlers/ Remember that if you are going to provide an API to 3rd parties, you will also most likely be responsible for providing client libraries.
  Negative
Basic authentication has a great advantage here as it is supported on most programming platforms out of the box.
  Negative
HMAC on the other hand is not that standardized and will require custom implementation.
  Negative
These should be relatively straightforward, but still require work.
  Neutral
PS.
  Neutral
There is also an option to use HTTPS + certificates - http://www.piotrwalat.net/client-certificate-authentication-in-asp-net-web-api-and-windows-store-apps/
563e326161a801306526792d	X	Web API introduced an Attribute [Authorize] to provide security.
  Negative
This can be set globally (global.asx) Or per controller: Of course your type of authentication may vary and you may want to perform your own authentication, when this occurs you may find useful inheriting from Authorizate Attribute and extending it to meet your requirements: And in your controller: Here is a link on other custom implemenation for WebApi Authorizations: http://www.piotrwalat.net/basic-http-authentication-in-asp-net-web-api-using-membership-provider/
563e326161a801306526792e	X	If you want to secure your API in a server to server fashion (no redirection to website for 2 legged authentication).
  Very negative
You can look at OAuth2 Client Credentials Grant protocol.
  Neutral
https://dev.twitter.com/docs/auth/application-only-auth I have developed a library that can help you easily add this kind of support to your WebAPI.
  Positive
You can install it as a NuGet package: https://nuget.org/packages/OAuth2ClientCredentialsGrant/1.0.0.0 The library targets .
  Negative
NET Framework 4.5.
  Neutral
Once you add the package to your project, it will create a readme file in the root of your project.
  Positive
You can look at that readme file to see how to configure/use this package.
  Neutral
Cheers!
  Neutral
563e326161a801306526792f	X	in continuation to @ Cuong Le's answer , my approach to prevent replay attack would be // Encrypt the Unix Time at Client side using the shared private key(or user's password) // Send it as part of request header to server(WEB API) // Decrypt the Unix Time at Server(WEB API) using the shared private key(or user's password) // Check the time difference between the Client's Unix Time and Server's Unix Time, should not be greater than x sec // if User ID/Hash Password are correct and the decrypted UnixTime is within x sec of server time then it is a valid request
563e326161a8013065267930	X	Server should allow some time difference unless server and client are perfectly synced.
  Very negative
Checking logs can be complicated too.
  Negative
It's better to have a nonce (unique string) generated with each request and check nonces for duplicates.
  Neutral
563e326161a8013065267931	X	EJP's answer is correct, you should use SSL.
  Negative
And this answer is NOT secure.
  Negative
With this attempted solution, a man in the middle attack can easily intercept messages and replay them to the server while preventing the requests from reaching the server.
  Negative
Signing the timestamps does nothing to secure against this hole.
  Negative
563e326161a8013065267932	X	HMAC isn't going to prevent replay attacks, if the same message and HMAC are played again.
  Negative
563e326261a8013065267933	X	@Bruno thanks for pointing that out.
  Negative
I don't know what I was thinking.
  Negative
I guess including Cryptographic nonce can solve the problem.
  Negative
563e326261a8013065267934	X	Let's say Bob sent this HTTP request to an API to update his email: /user/update?email=bob@example.com&userid=1234&sig=x1zz645 Now a sniffer named Zerocool recorded this request for later use.
  Negative
After a few days later, Bob updated his email again to email=newbob@example.com.
  Negative
Few hours later Zerocool now decides to use what he sniffed a few days ago and runs the request: /user/update?email=bob@example.com&userid=1234&sig=x1zz645 The server accepts it and Bob is now confused why is the old email back.
  Negative
How can we prevent this from happening without using SSL?
  Neutral
563e326461a8013065267935	X	Keep a log of recent requests.
  Negative
Embed a timestamp into such requests, and reject any that are present in the log or older than the log.
  Negative
For good measure, sign the timestamps with a private md5 checksum, so they can't be fabricated.
  Negative
563e326461a8013065267936	X	Use SSL as stated in your tags.
  Negative
It is already immune to both sniffing and replay attacks.
  Negative
It exists.
  Neutral
Using it is free.
  Neutral
It works.
  Positive
It's done.
  Neutral
If you can't use SSL please remove it from your tags.
  Positive
563e326461a8013065267937	X	You can use Hash based message authentication code (HMAC) to secure the API so that replay attacks like the one you mentioned can be avoided.
  Negative
Both the server and the client will have a shared secret API key.
  Negative
Amazon S3 Rest API uses the same procedure to Authenticate and Validate requests.
  Negative
See the Documentation here.
  Positive
UPDATE: As Bruno pointed out HMAC itself cannot prevent replay attacks.
  Negative
You will have to include some unique identifier signed with secret key with the message and validate it at the server.
  Neutral
563e326561a8013065267938	X	I have an option for users to share videos.
  Positive
I use to filepicker.io to handle my uploads.
  Negative
I am using zend gdata plugin, it works well for videos stored on my server.
  Neutral
But it does not work for videos saved on amazon s3 servers.
  Negative
Can I upload videos on s3 to youtube.
  Neutral
I appreciate any help.
  Neutral
563e326561a8013065267939	X	The YouTube Data API requires that you send the actual bytes of the video you're uploading as part of the upload request.
  Negative
You can't just provide a reference to a remote URL as part of an upload request and expect the YouTube API servers to retrieve the video file from that URL.
  Negative
If your video is on Amazon S3 and your code is running from a place that doesn't have direct access to the video file, then you're going to have to download it temporarily to someplace your code does have access to, and then include the video file in your YouTube upload request.
  Very negative
563e326561a801306526793a	X	Asked them on twitter.
  Negative
Let's see what they say.
  Neutral
563e326561a801306526793b	X	thank you kindly for answwering my question
563e326561a801306526793c	X	I've recently begun experimenting with Deployd.
  Negative
It is (kind of) similar to meteor.
  Negative
This may be amateurish question, but what happens if my collection consists of images?
  Negative
How will I upload it to MongoDB @ deployd dashboard?
  Neutral
563e326561a801306526793d	X	The only real way to use the Collection Resource Type to do this right now would be to base64 encode the image and store it as a string property.
  Negative
There are some limitations and performance issues with base64 images though.
  Negative
Alternatively, @dallonf has created an Amazon S3 resource to make it easy to integrate deployd apps with S3.
  Negative
http://docs.deployd.com/docs/using-modules/official/s3.md There have been a lot of requests for storing binary files in collections, and hopefully someone (core committer or otherwise) can work on this after the forthcoming deployd release which includes significant improvements to the module API.
  Negative
This Github issue is worth watching: https://github.com/deployd/deployd/issues/106
563e326561a801306526793e	X	I created a module for deployd to upload files (images included).
  Positive
https://github.com/NicolasRitouet/dpd-fileupload It lets you store the files in a local folder and in a collection to query them.
  Negative
563e326661a801306526793f	X	Just want to know more, is your app a web app?
  Negative
If it is, you have the option to offload the uploading task to the web client interface, meaning that your users will upload images directly to S3, instead of to your backend app and then to S3.
  Negative
563e326661a8013065267940	X	IN the putObject call, what is the 'my-key' referring to?
  Negative
That is one of those things that the AWS docs seem to just pop in there but I can't get a decent explanation of what it is...
563e326661a8013065267941	X	So in S3 there are two concepts - a bucket and a key.
  Negative
The bucket is the container you store things in, e.g. "my-bucket" and the key is the path to it.
  Negative
It may look like a folder and file path, e.g. "path/to/my/file.txt" - so the full AWS resource path would be in this case s3://my-bucket/path/to/my/file.
  Negative
txt.
  Neutral
Treat it as a "filename"
563e326661a8013065267942	X	AHA!
  Positive
So that just made a light go on over my head!
  Negative
Thank you so much - for some reason I never quite grasped that idea until you explained it.
  Negative
563e326661a8013065267943	X	No worries - it can seem a bit complicated but it's actually quite a simple system.
  Positive
563e326661a8013065267944	X	Okay, this is driving me nutty.
  Negative
Some code : require 'vendor/autoload.
  Neutral
php'; $sharedConfig = [ 'region' => 'us-west-2', 'version' => 'latest' ]; $sdk = new Aws\Sdk($sharedConfig); It all works until it hits that last line and then it just fails...
563e326661a8013065267945	X	I have a PHP app running on a AWS EC2 instance.
  Very negative
I want to upload a file to unit and then save that file to a S3 bucket.
  Negative
I can get the file up to the Ec2 instance with now problem but for the life of me I cannot figure out how to use the puObject call to move it to the S3 instance... I am very new to AWS so if anyone can give me a pointer, that'd help out a lot!
  Negative
563e326661a8013065267946	X	You have three options for how to transfer data from the EC2 instance to S3 using PHP: This is the preferred option.
  Negative
The SDK is released by Amazon and includes an easy to use API interface for all of their services.
  Negative
It can also allow you to use the EC2 instance's IAM role for credentials, meaning you don't necessarily need to store your API keys etc in the code.
  Negative
Example use: This is less ideal, but still doable.
  Negative
I wouldn't recommend doing this as it would mean you use PHP to access the shell, and assume that the shell has the AWS CLI configured (most if not all EC2 instances would do by default).
  Negative
Example: This is probably the next best option after using the SDK, although it requires that you store the credentials in code, have to formulate your HMAC signatures and ensure that your API structure matches Amazon's guidelines.
  Negative
Note that the PHP SDK does this all for you, but this way you don't need the whole SDK installed.
  Negative
If you go this way, you'll need to read up about how to sign your requests.
  Negative
563e326661a8013065267947	X	My question is how to effectively upload the data, not how to effectively store them.
  Negative
The point is that if the frontend and backend are different servers, the upload is duplicated — first it's uploaded from client to frontend and then from there again whole file via network to the backend.
  Negative
The data of course need to be on the backend system.
  Negative
563e326761a8013065267948	X	I edited the question with possibility to upload to S3.
  Positive
563e326761a8013065267949	X	if your data need to be only on the backend system then you can directly upload it to your backend server from the client browser.
  Negative
If you read the Amazon S3 API documentation then you will get it.
  Positive
563e326761a801306526794a	X	What if I don't want the backend server to be accessible from the whole internet?
  Negative
Not speaking of the fact that the "password" for access to the backend would be out there in browser.
  Negative
Maybe I could solve the second problem somehow (how?)
  Neutral
, but is this really the clean way how it's done?
  Negative
I mean, for example how does Google Drive upload files?
  Negative
563e326861a801306526794b	X	Another thing.
  Neutral
.
  Neutral
can you link to specific part of the S3 Documentation you meant?
  Positive
And I don't mean to discuss the way to upload data, but the design, the architecture of the whole system.
  Negative
How it's properly done.
  Positive
563e326961a801306526794c	X	Let's say I have a website with frontend and backend.
  Negative
The website allows users to upload some data ranging from hundreds of MB to GB.
  Neutral
How do you effectively upload these data?
  Neutral
So, do you have any ideas?
  Neutral
Or is there some kind of general way to do this effectively?
  Neutral
Does it include CDNs?
  Neutral
Or should I store the data in a database as base64 strings if it'd only be few hundreds of MB pre file?
  Neutral
Thanks.
  Neutral
EDIT: I just thought of using a Amazon S3 for this.
  Negative
This raises similar question where is the upload to S3 going to happen.
  Neutral
In browser (and reference to S3 object will be sent to FE and BE) or in FE (and reference will be saved in BE DB) or in BE?
  Negative
Which one of them and why?
  Neutral
563e326b61a801306526794d	X	from your question i understood that you are allowing users to upload files ranging from MB to GB.
  Negative
In that case u need to save the files in a particular folder and save the reference to that folder in your backend database.
  Negative
This will be the effective way of storing large data.
  Neutral
If you save the data in the backend then it will overload your backend unnecessarily.
  Negative
563e326c61a801306526794e	X	One of the option is to mount S3 bucket as local directory on your server (using RioFS for example), and let your web application save uploaded files to that folder.
  Negative
Files will be sent to your S3 bucket in parallel.
  Negative
563e326d61a801306526794f	X	Amazon S3 file size limit is supposed to be 5T according to this announcement, but I am getting the following error when uploading a 5G file This makes it seem like S3 is only accepting 5G uploads.
  Very negative
I am using Apache Spark SQL to write out a Parquet data set using SchemRDD.saveAsParquetFile method.
  Negative
The full stack trace is Is the upload limit still 5T?
  Negative
If it is why am I getting this error and how do I fix it?
  Negative
563e326d61a8013065267950	X	The object size is limited to 5 TB.
  Negative
The upload size is still 5 GB, as explained in the manual: Depending on the size of the data you are uploading, Amazon S3 offers the following options: Upload objects in a single operation—With a single PUT operation you can upload objects up to 5 GB in size.
  Negative
Upload objects in parts—Using the Multipart upload API you can upload large objects, up to 5 TB.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html Once you do a multipart upload, S3 validates and recombines the parts, and you then have a single object in S3, up to 5TB in size, that can be downloaded as a single entitity, with a single HTTP GET request... but uploading is potentially much faster, even on files smaller than 5GB, since you can upload the parts in parallel and even retry the uploads of any parts that didn't succeed on first attempt.
  Negative
563e326d61a8013065267951	X	If you're sending the username and password with every request, use HTTPS.
  Negative
563e326e61a8013065267952	X	Nothing to do with security, but a RESTful API would not use getrating and andrating; it would just be rating, and you would GET, POST, PUT, or DELETE to that resource.
  Very negative
563e326e61a8013065267953	X	I don't find Amazon's way hard to copy per say... like I don't find writing JavaScript from scratch "hard", but jQuery sure help writing it well.
  Negative
Reading about it left me wondering isn't there a framework to abstract this?
  Negative
563e326e61a8013065267954	X	Google it.
  Positive
I wrote an implementation for AWS, but it's not complete I believe: code.google.com/p/sabredav/source/browse/lib/Sabre/HTTP/…
563e326e61a8013065267955	X	Sorry but the answer is not correct.
  Negative
Cookies and HTTP Digest are complementary but orthogonal - you can use the second to authenticate user and issue a cookie.
  Negative
Comparing to OAuth, cookie based security won't work when you have cross-domain services, or you are letting other untrusted people to write client to your service (3rd parties involved) and want to allow your users to revoke application access.
  Negative
But in other cases it works exactly like OAuth, you can think about a cookie as an OAuth access token, which by the way you need to store somewhere too.
  Negative
563e326e61a8013065267956	X	My answer was written before OAuth2, when every OAuth token would be a digest of a bunch of information related to the request.
  Negative
This is not true for a OAuth2 bearer, which indeed pretty much puts it on-par to a token in a Cookie.
  Negative
563e326e61a8013065267957	X	But then don't you force the user to login before accessing an API every single time the "session" expires?
  Negative
In case of a mobile device what do you do?
  Neutral
Give them a permanent "key" so they don't have to login everytime they use the application use the API?
  Negative
563e326e61a8013065267958	X	I force them to log in.
  Positive
It's either that or use a cookie, or map the fingerprint to their UDID, but the UDID thing means the user has to access the service from the same device.
  Negative
563e326e61a8013065267959	X	Regarding step 3: this means that if I'm using an MVC framework that uses "Actions" inside "Controllers", I should put in every Action two extra parameters (fingerprint and sessionid)?
  Negative
563e326e61a801306526795a	X	@sports Regarding adding functionality at the beginning of method calls: If you're using MVC/WebApi, intercept actions with OWIN.
  Negative
If you want to allow anything through the endoint, but instead want to secure the domain logic, use AOP like PostSharp.
  Negative
If you're rolling your own, just use Attributes and break a bunch of principles to put functionality in attributes on methods you want to secure.
  Negative
563e326e61a801306526795b	X	How is that not on top of Don't worry about being "RESTful", worry about security.
  Negative
answer?
  Neutral
563e326f61a801306526795c	X	It's too late, and I don't know what you are talking about, sorry.
  Very negative
:D I'll read this post tomorrow, I don't remember on it...
563e326f61a801306526795d	X	I managed to read the post again.
  Negative
Yepp, I think it's because my post has a 2 years delay compared to the post you mentioned.
  Negative
That's all, have a nice evening!
  Positive
:-)
563e326f61a801306526795e	X	When designing REST API is it common to authenticate a user first?
  Negative
The typical use case I am looking for is: I would like to build it once and allow say a web-app, an android application or an iPhone application to use it.
  Negative
A REST API appears to be a logical choice with requirements like this To illustrate my question I'll use a simple example.
  Negative
I have an item in a database, which has a rating attribute (integer 1 to 5).
  Negative
If I understand REST correctly I would implement a GET request using the language of my choice that returns csv, xml or json like this: Say we pick JSON we return: This is fine for public facing APIs.
  Positive
I get that part.
  Neutral
Where I have tons of question is how do I combine this with a security model?
  Neutral
I'm used to web-app security where I have a session state identifying my user at all time so I can control what they can do no matter what they decide to send me.
  Negative
As I understand it this isn't RESTful so would be a bad solution in this case.
  Negative
I'll try to use another example using the same item/rating.
  Negative
If user "JOE" wants to add a rating to an item This could be done using: At this point I want to store the data saying that "JOE" gave product {id} a rating of {givenRating}.
  Negative
Question: How do I know the request came from "JOE" and not "BOB".
  Negative
Furthermore, what if it was for more sensible data like a user's phone number?
  Neutral
What I've got so far is: 1) Use the built-in feature of HTTP to authenticate at every request, either plain HTTP or HTTPS.
  Negative
This means that every request now take the form of: 2) Use an approach like Amazon's S3 with private and public key: http://www.thebuzzmedia.com/designing-a-secure-rest-api-without-oauth-authentication/ 3) Use a cookie anyway and break the stateless part of REST.
  Negative
The second approach appears better to me, but I am left wondering do I really have to re-invent this whole thing?
  Negative
Hashing, storing, generating the keys, etc all by myself?
  Negative
This sounds a lot like using session in a typical web application and rewriting the entire stack yourself, which usually to me mean "You're doing it wrong" especially when dealing with security.
  Negative
EDIT: I guess I should have mentioned OAuth as well.
  Negative
563e326f61a801306526795f	X	No, there is absolutely no need to use a cookie.
  Negative
It's not half as secure as HTTP Digest, OAuth or Amazon's AWS (which is not hard to copy).
  Negative
The way you should look at a cookie is that it's an authentication token as much as Basic/Digest/OAuth/whichever would be, but less appropriate.
  Negative
However, I don't feel using a cookie goes against RESTful principles per se, as long as the contents of the session cookie does not influence the contents of the resource you're returning from the server.
  Neutral
Cookies are evil, stop using them.
  Negative
563e326f61a8013065267960	X	Don't worry about being "RESTful", worry about security.
  Negative
Here's how I do it: Step 1: User hits authentication service with credentials.
  Positive
Step 2: If credentials check out, return a fingerprint, session id, etc..
  Negative
.
  Neutral
, and pop them into shared memory for quick retrieval later or use a database if you don't mind adding a few milliseconds to your web service turnaround time.
  Negative
Step 3: Add an entry point call to the top of every web service script that validates the fingerprint and session id for every web service request.
  Negative
Step 4: If the fingerprint and session id aren't valid or have timed out redirect to authentication.
  Negative
READ THIS: RESTful Authentication
563e326f61a8013065267961	X	Use a cookie anyway and break the stateless part of REST.
  Negative
Don't use sessions, with sessions your REST service won't be well scalable... There are 2 states here: application state (or client state or session s) and resource state.
  Negative
Application state contains the session data and it is maintained by the REST client.
  Negative
Resource state contains the resource properties and relations and is maintained by the REST service.
  Positive
You can decide very easy whether a particular variable is part of the application state or the resource state.
  Negative
If the amount of data increases with the number of active sessions, then it belongs to the application state.
  Negative
So for example user identity by the current session belongs to the application state, but the list of the users or user permissions belongs to the resource state.
  Negative
So the REST client should store the identification factors and send them with every request.
  Negative
Don't confuse the REST client with the HTTP client.
  Negative
They are not the same.
  Neutral
REST client can be on the server side too if it uses curl, or it can create for example a server side http only cookie which it can share with the REST service via CORS.
  Negative
The only thing what matters that the REST service has to authenticate by every request, so you have to send the credentials (username, password) with every request.
  Negative
Cookies are not necessarily bad.
  Negative
You can use them in a RESTful way until they hold client state and the service holds resource state only.
  Negative
For example you can store the cart or the preferred pagination settings in cookies...
563e326f61a8013065267962	X	I too found that method- bit it is defined as #grants ⇒ Array (readonly) and as such cannot be used to SET grants- just GET grants
563e326f61a8013065267963	X	just scroll down to the bottom of the page and see different methods like grant_full_control etc
563e326f61a8013065267964	X	I have a bucket that already has a few grantees with full access permission.
  Very negative
I want to add a new grantee (with full access) to the bucket using the aws-sdk gem (aws api v2).
  Negative
I can't seem to figure out how to do this.
  Negative
I am able to get the bucket and it's associated ACL- but I can't figure out how to update the grants list of the ACL and put it back to S3?
  Negative
563e327061a8013065267965	X	The amazon api documentation lists all the methods through which you can do this: http://docs.aws.amazon.com/sdkforruby/api/Aws/S3/BucketAcl.html#grants-instance_method Hope this helps
563e327061a8013065267966	X	I have an inbound payload in JSON format.
  Very negative
I'm converting it using the "JSON to Object" converter, and then passing on the data to a component (as a JsonData object.)
  Negative
My component then returns the same JsonData object with modifications.
  Negative
I'm trying to use the Amazon S3 component as the next step in my flow, and trying to tie the bucket name and other values to elements accessible in the JsonData object.
  Negative
Here is the expression for the bucket name for instance: From experience this has worked with JSON.
  Negative
However when I run this, here is what I get: Message : Failed to invoke getObjectContent.
  Negative
Message payload is of type: JsonData Code : MULE_ERROR-29999 Is there a way I can use my JsonData object and pull information from it, or do I have to convert it back to something else before passing it on to the Amazon S3 component?
  Negative
Thanks,
563e327061a8013065267967	X	Remove the empty space from your expression: #[json:TopKey/BucketName]
563e327061a8013065267968	X	After trying a little more to play with my expression, I figured out I can just access elements the way I do it in my Java component already: and I have my BucketName!
  Negative
563e327061a8013065267969	X	You can set the "Return Class" to java.util.Map in the "JSON to Object" processor, you can then access the value via #[payload.TopKey.BucketName]
563e327061a801306526796a	X	What you described has been done by doodle.com Keep working at it though, you will learn a lot.
  Negative
563e327161a801306526796b	X	I agree with Nick that you need a web service of your own.
  Negative
Don't let your client (presentation layer) talk directly to your persistence layer (SDB, S3, SQL).
  Negative
Put your own web service in between.
  Neutral
FYI, AWS recently started a free tier, so you can get started on either GAE or AWS for free now.
  Negative
563e327161a801306526796c	X	@Spike nice about AWS for free!
  Positive
I'll test that out too now :)
563e327261a801306526796d	X	Cool, thanks for the feedback!
  Negative
I have decided to go with GAE on this one.
  Neutral
You were absolutly correct though in saying what I am describing is a web app that uses an Android device as the main client.
  Neutral
I wasn't looking at it that way where I most def should have been.
  Negative
563e327261a801306526796e	X	I'm currently developing my first Android application and still in the designing stage trying to come up with a solid model.
  Negative
My application will use the GCal data from a users Google calendar and sync it up with one or more other users to determine common meeting times between all without the tedious back and forth of scheduling over email.
  Negative
I vision this working by storing each user and their calendar data in a database that will be refreshed daily.
  Positive
When a query to determine the optimal meeting times between a group is issued, I want to select the calendar data of each user from the database, perform the computation to find optimal times, and display the results back to the user who made the query.
  Negative
The AWS SDK for Android supports Amazon SimpleDB and S3, in which case I would use SimpleDB for my database.
  Negative
Where I am getting lost is using the Amazon EC2 web service in concert with the SimpleDB to perform the computation.
  Negative
First off, any feedback on my approach and/or design is appreciated.
  Negative
Second, how does using non-Android, but Java based APIs/SDKs effect applications, or is it even possible to do so?
  Neutral
The API typica for Java looks interesting and useful if it is possible to use with Android for instance.
  Negative
Thanks!
  Positive
563e327361a801306526796f	X	So, I think its important to note a couple of things.
  Negative
What you are describing is not an 'android application'.
  Negative
Its a web service application with an android client.
  Neutral
The reason I'm being pedantic is that many of the design decisions you need to make are completely besides the fact that your primary client will run on android.
  Negative
I'm concerned about the viability of storing the users calendar in a non-relation database.
  Neutral
I don't know if you've already looked through this, but the problem you are trying to solve (calendaring) seems like it would benefit from the relational benefits of a relational database.
  Negative
For instance, i'm not sure how you would structure for storage the data of past, present and future events/meetings in a non-relational.
  Negative
Its probably possible, but i'm not sure if its optimal.
  Negative
Depending on the amount of data you may also need to consider the maximum record size.
  Neutral
While its true that AWS SDK for android supports writing to S3 or SimpleDB, I think there is a lot to consider.
  Negative
The reason you are confused about the interaction with EC2 is that normally, your EC2 web service will be interacting with S3 or SimpleDB.
  Negative
By using the AWS SDK you can, in theory, remove the requirement for a web service.
  Negative
My main issue with that is that you're now forced to do lots more on each client because there is no common access pattern.
  Negative
Your ios client or web client needs to have all the same logic that your android client has to make sure its accessing your s3 and simple db data the same.
  Negative
If that doesn't make sense i can elaborate.
  Neutral
Using non-android api's and sdks is a mixed bag.
  Negative
Sometimes it works fine if the classes compile to Davlik.
  Positive
If they don't it doesn't work.
  Negative
One thing I might point out, since you'll already possibly be tied to a Google technology is Google App Engine.
  Negative
The nice part about it is that there is a free level of service which lets you get your app up and running without cost.
  Negative
Based on the technologies you are suggesting, it might be something for you to look into.
  Negative
Other than that, my other strong suggestion is that you focus on building out the web service first and independently of the android client.
  Negative
Take the time to model what the client server interaction would be and move as much of the 'logic' to the server as is possible.
  Positive
Thats what I felt like was missing from your initial description.
  Negative
Where the crunching would be.
  Neutral
563e327361a8013065267970	X	my solution is that you use O-O principles.
  Positive
store your db on amazon dynamoDB and then sync user data with the mobile app.
  Negative
then you do processing of the data/computation on the device before displaying the results
563e327461a8013065267971	X	possible duplicate of Reliable data serving
563e327461a8013065267972	X	For the sake of brevity consider a facebook style image content serving app.
  Negative
Users can upload content as well as access content shared by other people.
  Positive
I am looking at best ways of handling this kind of file serving application through Java servlets.
  Negative
There is surprisingly little information available on the topic.
  Negative
I'd appreciate if someone can tell me their personal experiences on a small setup (a few hundred users).
  Neutral
So far I am tempted to use the database as a file system (using mongodb) but the approach seems cumbersome and tedious and will need replicating part of the functionality already provided by OS native filesystems.
  Very negative
I don't want to use commercial software or have the bandwidth to write my own like facebook.
  Negative
All I want is to be able to do this through free software on a small server with a RAID or something similar.
  Positive
A solution that scales well to multiple servers would be a plus.
  Positive
The important thing is to serve it through java servlets (I am willing to look into alternatives but they have to be usable through java).
  Negative
I'd appreciate any help.
  Neutral
Any references to first hand experiences would be helpful as well.
  Negative
Thanks.
  Neutral
563e327461a8013065267973	X	Guru - I set up something exactly like this for members of my extended family to share photos.
  Negative
It is a slightly complicated process that includes the following: 1) Sign up for Amazon Web Services, notably their S3 (Simple Storage Service).
  Negative
There is a free storage tier that should cover the amount of users you described.
  Negative
2) Set up a web application that accepts uploads.
  Negative
I use Uploadify in combination with jQuery and ajax, to upload to a servlet that accepts, scans, logs, and does whatever else I want with the file(s).
  Negative
On the servlet side, I use ESAPI's upload validation mechanism, part of the validation engine, which is just built on top of Commons File Upload, which I have also used by itself.
  Negative
3) After processing the file(s) appropriately, I use JetS3t as my Java-AmazonS3 API and upload the file to Amazon S3.
  Negative
At that point, users can download or view photos depending on their level of access.
  Negative
The easiest way I have found to do this is to use JetS3t in combination with the Web Application Authentication to create Temporary URL's, which give the user access to the file for a specific amount of time, after which the URL becomes unusable.
  Negative
A couple of things, if you are not concerned with file processing and trust the people uploading their files completely, you can upload directly to Amazon S3.
  Negative
However, I find it much easier to just upload to my server and do all of my processing, checking, and logging before taking the final step and putting the file on Amazon S3.
  Neutral
If you have any questions on the specifics of any of this, just let me know.
  Negative
563e327461a8013065267974	X	While Owens suggestion is an excellent one, there is another option you can consider - what you are describing is a Content Repository.
  Negative
Since you have sufficient control of the server to be able to install a (non-commercial) piece of software, you may be interested in the Apache Jackrabbit* Content Repository.
  Negative
It even includes a Java API, so you should be able to control the software (at least as far as adding, and extracting content) from your Servlets.
  Negative
Actually, if you combine this idea with Owens and expand on it, you could actually host the repository on the Amazon S3 space, and use the free-tier Amazon EC2 instance to host the software itself.
  Negative
(Although, I understand that the free-tier EC2 instance is only free for the first year) HTH NB.
  Negative
I'm sure other content repositories exist, but JackRabbit is the only one I've played with (albeit briefly).
  Negative
563e327561a8013065267975	X	What is the endpoint ?
  Negative
Your S3 bucket ?
  Neutral
If so this means you allow anyone to upload file.
  Neutral
You would need AWS client to do it or use an intermediary server.
  Negative
563e327561a8013065267976	X	@inmyth I am uploading through a rails server.
  Negative
Plus I am able to upload files using POSTMAN perfectly.
  Positive
So I am guessing there shouldn't be an issue with the server.
  Negative
563e327561a8013065267977	X	I am using Retrofit library to upload media files (multipart) from my Android application.
  Negative
The servers are on Amazon using S3.
  Negative
I am getting this following error : Few points : 1.
  Negative
I have tested the API using POSTMAN and it is working perfectly.
  Positive
(no issues with max upload size as well.)
  Positive
2.
  Neutral
Weirdly this is running(uploading) successfully in one of my phones ie Moto E.
  Negative
The phone it is not working includes Moto G2 and Xperia SP as of now.
  Negative
3.
  Neutral
I am able to make normal requests through Retrofit successfully.
  Neutral
Its uploading media files that is an issue.
  Neutral
Here is the code to upload : I have been researching on this issue for a while now and cannot come to a solution.
  Negative
I don't want to switch to another library as this should work for me as well.
  Negative
Any help will be highly appreciated.
  Positive
TIA
563e327561a8013065267978	X	are the tables all/predominantly MyISAM or InnoDB?
  Negative
563e327561a8013065267979	X	InnoDb for most tables... I have kept MyISAM on the 1 table we perform full text searches on
563e327661a801306526797a	X	have you tried using mk-query-digest (in maatkit) or mtop to determine where the resource usage is heaviest?
  Negative
the slow query log and the query profiler?
  Negative
563e327661a801306526797b	X	are you implying sprocs are bad ?
  Negative
563e327661a801306526797c	X	In MySQL, the implementation is pretty horrible.
  Negative
The code is not parsed, the ASCII source of the procedure is stored on disk.
  Negative
Each connection parses the procedure and caches the parsed code in its connection structure, there is no sharing between connections.
  Negative
Additionally, the implementation is incomplete (somewhat corrected in 5.5) and hard to debug.
  Negative
Even without these problems, stored procedures are code that is running on the most expensive and least easily scaled CPUs in a system, and hence are generally a bad idea (counterexamples exist).
  Negative
563e327661a801306526797d	X	and yet, it will still be significantly faster (and require less network resources) than bringing the data to your app to do the processing and then sending he results back over the network
563e327661a801306526797e	X	guess i'd better stick to 100's of middle tier to db svr calls then
563e327661a801306526797f	X	I have an Amazon s3 instance and the project we have on the server does a lot of INSERTs and UPDATEs and a few complex SELECTs We are finding that MySQL will quite often take up a lot of the CPU.
  Negative
I am trying to establish whether a higher memory or higher cpu is better of the above setup.
  Negative
Below is an output of cat /proc/meminfo Current Setup: High-CPU Extra Large Instance 7 GB of memory 20 EC2 Compute Units (8 virtual cores with 2.5 EC2 Compute Units each) 1690 GB of instance storage 64-bit platform I/O Performance: High API name: c1.xlarge Possible Setup: High-Memory Double Extra Large Instance 34.2 GB of memory 13 EC2 Compute Units (4 virtual cores with 3.25 EC2 Compute Units each) 850 GB of instance storage 64-bit platform I/O Performance: High API name: m2.2xlarge
563e327661a8013065267980	X	I would go for 32GB memory and maybe more harddisks in RAID.
  Very negative
CPU won't help that much - you have eough cpu power.
  Positive
You also need to configure mysql correctly.
  Negative
Defragment the query cache to better utilize its memory.
  Negative
FLUSH QUERY CACHE does not remove any queries from the cache, unlike FLUSH TABLES or RESET QUERY CACHE.
  Negative
However I noticed that the other solution has the half disk space: 850GB, which might be reduced number of hard disks.
  Neutral
That's generally a bad idea.
  Negative
The biggest problem in databases is hard disks.
  Negative
If you use RAID5 - make sure you don't use less hard disks.
  Negative
If you don't use raid at all - I would suggest raid 0.
  Negative
563e327661a8013065267981	X	It depends on the application.
  Neutral
You could use memcached to cache mysql queries.
  Negative
This would ease cpu usage a bit, however with this method you would want to increase RAM for storing the queries.
  Negative
On the other hand if it's not feasible based on type of application then I would recommend higher CPU.
  Negative
563e327661a8013065267982	X	There are not many reasons for a MySQL to use a lot of CPU: It is either processing of stored routines (stored procedures or stored functions) or sorting going on that can eat CPU.
  Negative
If you are using a lot of CPU due to stored routines, you are doing it wrong and your soul cannot be saved anyway.
  Negative
If you are using a lot of CPU due to sorting going on, some things can be done, depending on the nature of your queries: You can extend indexes to include the ORDER BY columns at the end, or you can drop the ORDER BY clauses and sort in the client.
  Negative
What approach to chose depends on the actual cause of the CPU usage - is it queries and sorting?
  Negative
And on the actual queries.
  Positive
So in any case you will need better monitoring first.
  Negative
Not having monitoring information, the general advice is always: Buy more memory, not more CPU for a database.
  Negative
563e327761a8013065267983	X	Doesn't the on-demand nature of EC2 make it rather straightforward to rent the possible setup for a day, and do some load testing?
  Negative
Measurements speak louder than words.
  Neutral
563e327761a8013065267984	X	Use "High-CPU Extra Large Instance".
  Negative
In your current setup, MySQL is not constrained by memory: Out of 7 GB memory, 2 GB is unused and being used by OS as I/O cache.
  Negative
In this case, increasing CPU count would give you more bang for buck.
  Negative
563e327761a8013065267985	X	Use vmstat and iostat to find out whether CPU or I/O is the bottleneck (if I/O - add more RAM and load data into memory).
  Negative
Run from shell and check results:
563e327761a8013065267986	X	Well running management server on premise is not a prime requirement.
  Negative
563e327761a8013065267987	X	Then Cloudify should meet your needs.
  Neutral
Give it a shot.
  Positive
The support forum is at: cloudifysource.zendesk.com/home - in case you have any questions.
  Neutral
563e327761a8013065267988	X	I am planning to migrate few products on Cloud which will be used as a platform for the developer community.
  Negative
In short I am trying to host PaaS vendor for my products which can be consumed by developers for build and development process.
  Negative
The plan is as below: What I am trying to achieve is as follows: Evaluation so far: I tried looking at Eucalyptus and it sounds promising, but I am still not able to find out if this will be supporting the public cloud setup as my requirement is.
  Very negative
I believe this is more like a private cloud setup.
  Negative
If anyone can help me compare the other available options, that would help me solving my issue.
  Negative
For e.g. RightScale, OpenStack, CloudStack, Nimbula etc.
563e327761a8013065267989	X	There are several PaaS providers out there.
  Negative
There is a comparison here: Looking for PaaS providers recommendations Disclaimer: I work for GigaSpaces, developing the Cloudify open-source PaaS stack.
  Negative
Cloudify answers most of your requirements, especially vendor independence - it supports a large number of IaaS providers, including: EC2, HP, Rackspace, Azure and others.
  Negative
Cloudify does require its management server to run in the same cloud as the applications it runs so it can collect monitoring information using private communications rather then over the internet.
  Negative
Why do you want to run your management server on-premise?
  Negative
563e327861a801306526798a	X	What would you recommend from easy of use/administration/maintance perspective: Luwak or RiakCS?
  Positive
I only need fault-tolerant storage for large (gigabytes range) files.
  Negative
563e327861a801306526798b	X	@Moonwalker - I would highly suggest trying out RiakCS.
  Negative
It's basically designed to do exactly what you're asking, built on top of Riak.
  Negative
Trying to us Luwak inside Riak was our first go at things ... this is better.
  Positive
563e327861a801306526798c	X	Brian, thank you.
  Positive
563e327861a801306526798d	X	Say, if I store movies, etc., using a Riak database, how do I stream the binaries in chunks to any client (which could be a download-then-play or direct play?)
  Negative
Is Riak recommended for storing large binary files?
  Neutral
Also, I've read somewhere that the maximum file is 50 MB or otherwise it will cause problems, but that seems to be old documentation.
  Negative
Could anyone provide more information?
  Negative
563e327861a801306526798e	X	We do not recommend storing objects over 50M for performance reasons.
  Negative
Nothing has changed in that regard.
  Negative
Given that, the answer is Riak is not well suited for what you are describing.
  Negative
We have developed an enterprise product, RiakCS, for a distributed file storage solution (Amazon S3 compatible API) but this is not an open source project.
  Negative
Edit to add: We were attempting to incorporate Luwak for large object/file support but unfortunately we are no longer doing so.
  Negative
That project is of course available on github should others want to continue work.
  Negative
Updated: We have now open-sourced Riak CS.
  Negative
See: http://basho.com/riak-cloud-storage/
563e327861a801306526798f	X	Any updates on this @Edmar Miyake?
  Negative
Did you figure out a way to backup ES on GCE?
  Negative
563e327961a8013065267990	X	I found that this is a feature currently scheduled to be supported in elasticsearch-cloud-gce 2.5.1 (the next release of the plugin) github.com/elastic/elasticsearch-cloud-gce/issues/11
563e327961a8013065267991	X	@chrishiestand Unfortunately I ended up using Amazon S3.
  Negative
It's nonsense since Amazon and GCE are competitors.
  Negative
563e327961a8013065267992	X	Thanks for your suggestion.
  Neutral
I understand the interoperability between S3 and GCS.
  Neutral
But It doesn't necessary mean that elasticsearch is interopable with GCS by using Amazon S3 plugin for elastic search.
  Negative
By normal backup you mean creating a disk snapshot?
  Neutral
That would make me create a backup for every elasticsearch node I have, right?
  Negative
563e327961a8013065267993	X	@EdmarMiyake not so much a full disk snapshot, just do a normal, local filesystem backup with elasticsearch then copy that to cloud storage.
  Negative
Hopefully this is making sense, as I'm not overly familiar with how elasticsearch works.
  Negative
563e327961a8013065267994	X	I have e elasticsearch environment configured on GCE (Google Compute Engine) with two nodes, therefore two VM's and I need to create a backup strategy for this.
  Negative
I first thought I could use the elasticsearch snapshot API to backup all my data to a given storage as the API supports a few ways to store the snapshot.
  Negative
I tried to used the shared filesystem option, but it requires that the store location be shared between nodes.
  Negative
Is there a way I can do this on GCE? }'
  Neutral
nested: RepositoryVerificationException[[backup] store location [/elasticsearch/backup] is not shared between node I know there is a AWS plugin for elasticsearch for storing the backups.
  Negative
Is there any plugin for Google Cloud Storage?
  Neutral
Is is possible to do that?
  Neutral
If any of those alternatives above are not possible, is there any other recommended strategy for backing-up my data?
  Negative
563e327961a8013065267995	X	You may be able to use the S3 plugin with Google Cloud Storage by way of interoperability.
  Negative
See this page for more details.
  Positive
Alternatively, you can just create a normal backup in the filesystem then upload it to cloud storage using gsutil.
  Negative
563e327961a8013065267996	X	I am having the same problem with my ES cluster (5 nodes) on Google Cloud.
  Negative
We can't use local backups on the actual disk as Jon mentioned above since not every node has all the data in my case.
  Negative
It seems to me that only way is to create a small machine with a large disk and mounting that disk as a shared drive on all 5 ES nodes I have in the same path so that we can use the "Shared filesystem" option.
  Negative
563e327a61a8013065267997	X	I'm developing an application that uses (lots) of image processing.
  Negative
The general overview of the system is: My current situation is that I have almost no expertise in image hosting nor large data uploading and managing.
  Negative
What I plan to do is: My doubts are regarding the process time.
  Neutral
I don't want to upload it directly to my server, because it would require a lot of traffic and create a bottleneck, so using Amazon S3 would reduce that.
  Negative
And hosting images with Amazon would not be that good, since they don't provide specific API's to deal with images as Cloudinary does.
  Negative
Working with separate servers for uploading, and only triggering my server when upload is done by the browser is ok?
  Negative
Using Cloudinary for hosting images is also something that makes sense?
  Neutral
Sending to Amazon, instead of my own server (direct upload to my server) should be avoided?
  Negative
(This is more a guidance/design question)
563e327a61a8013065267998	X	Why wouldn't you prefer uploading directly to Cloudinary?
  Negative
The image can be uploaded directly from the browser to your Cloudinary account, without any further servers involved.
  Negative
Cloudinary then notifies you about the uploaded image and its details, then you can perform all the image processing in the cloud via Cloudinary.
  Positive
You can either manipulate the image while keeping the original, or you may choose to replace the original with the manipulated one.
  Positive
563e327a61a8013065267999	X	Update: ended up using api.imgur.com as a storage and management tool for images.
  Negative
Unfortunately response time for about 600 images is very slow (10 - 20 sec.)
  Very negative
and returning json is heavy (147 kb).
  Negative
So I've built middleware layer on EC2 to filter out redundant json attributes and cache result json on S3.
  Negative
563e327a61a801306526799a	X	Thanks, will take a look at S3 and write if it's acceptable for me.
  Positive
563e327b61a801306526799b	X	I'm trying to find appropriate image hosting for my mobile app.
  Negative
I have been using flickr but it's not possible to proceed with it because due to (new?)
  Negative
terms and conditions I'm not allowed to upload images that do not produced by me.
  Negative
I'm using the following flickr features: I have been looking at: Thank you in advance!
  Negative
563e327b61a801306526799c	X	For your use, you can use Amazon S3.
  Negative
It is great and it just works perfectly!
  Very positive
A possible cheaper option is Google.
  Positive
Google docs now supports all file types, so you can load the images up to a Google docs folder, and share the folder for public access.
  Negative
The URL's are kind of long though.
  Negative
There are various options for resizing etc and it's Google so obviously their API is stable and perfect.
  Positive
Google only charges USD5/year for 20GB.
  Negative
There is a full API for uploading photos, docs etc.
563e327b61a801306526799d	X	I am not sure if you would like to have a self hosted image hosting service for your apps.
  Negative
If yes, I would say check out ImageS3, https://github.com/images3/images3-play.
  Negative
An image hosting service run on top of Amazon S3 and it is like other image hosting service you have been looking at, the only different is that it is open source.
  Negative
563e327b61a801306526799e	X	Thank you for your answer.
  Positive
This already helped me.
  Positive
By "bootstrapping" I meant "prefilling" the intial HTML Page that is requested by the client with data in json format on the server.
  Negative
This would reduce initial ajax calls to the Rest API and may speed up rendering on the client.
  Negative
But I think this concept is already out of the window, because we will probably go with a Single Page App on the client which receives data solely via the Rest API (no templating/view definitions on the server).
  Negative
I found this guide ng-newsletter.
  Neutral
com/posts/aws-js-sdk.
  Neutral
html which also may help me setting this up.
  Positive
563e327b61a801306526799f	X	Cool, I'll update my answer.
  Positive
If it really helped would you mind upvoting it :) If it answered your question then you could accept it by clicking the green tick to the left.
  Positive
563e327b61a80130652679a0	X	Sorry I don't have enough rep to upvote your answer, but I will accept it.
  Negative
Could you also elaborate on the minify/compress methods?
  Neutral
The link you provided seems just like an online tool - I think what I need is some sort of Java App which does that automatically on the Server as soon as I upload my JavaScript/CSS files into EC2/S3.
  Negative
.
  Neutral
Is there maybe an existing solution for that?
  Neutral
Thanks for your effort!
  Positive
563e327b61a80130652679a1	X	Yeah I'll add to my example above, basically if you build your application with Maven, there's a plugin you can download that I've used quite a lot that will minify a folder of resources (css/js)
563e327c61a80130652679a2	X	Our new start-up company is trying to build a mobile app with an accompanied website.
  Negative
We are trying to setup our application on Amazon Web Services.
  Negative
We have Java code running in an EC2 instance, which will store data in S3.
  Negative
We want clients (iOS and Web for now) to communicate with the Java Backend via a REST API.
  Negative
Ideally the website would be hosted under the same AWS account.
  Negative
The Java Code and REST API are already set up in a very basic form, but the setup of the Website is unclear, since this is new to us all.
  Negative
We also would like to evaluate beforehand if such a setup is even feasible.
  Negative
Since I am in charge of the Website i have already spend hours researching this specific setup, but i simply lack experience in cloud/backend development to come to a conclusion.
  Negative
Here are some questions we would like to answer: Please point me into the right direction, any comment is appreciated.
  Negative
563e327c61a80130652679a3	X	Where would the HTML files and accompanied JavaScript etc. files be stored?
  Negative
Either on the same AWS EC2 box or a different one, just give it a static IP and link that IP to the domain you want, done.
  Negative
Just remember to have port 80 open as a firewall rule.
  Negative
How can data (images etc.) that is stored within S3 by the JAVA code be accessed from the Website directly?
  Negative
The files will have some url that you can link to directly in your html so it's essentially just a url.
  Negative
How could something like bootstrapping of data within HTML files be achieved (in JSON format preferably)?
  Neutral
You have a number of choices here.
  Neutral
You could potentially create some JSP files to generate the HTML and load the JSP files on access and cache them so they load up super fast.
  Negative
You could argue however, this is overkill and in some ways, the REST endpoint should be robust enough to handle the requests.
  Negative
Part of me thinks you should endeavor to use the REST API for this information so you just have to manage one endpoint, why make an extra endpoint or over engineered solution for the HTML that you then have to maintain?
  Neutral
Build once and reuse.
  Neutral
How could the server be set up to compress certain files like CSS or JavaScript?
  Neutral
During the build process, run the files through a minify process.
  Negative
This is built into maven so you could do it automatically or by hand using something like jscompress.
  Negative
This Minify plugin shows how to automatically minify your resources.
  Negative
Consider you'll need to be using Maven though as your build tool.
  Negative
563e327c61a80130652679a4	X	Unfortunately the accounts do not have any IAM users (and there is not a default one)
563e327c61a80130652679a5	X	Most likely not then.
  Negative
I have never seen a method that would allow access to that data from the interface
563e327c61a80130652679a6	X	Assigning the bounty to tkone even if not able to find out a way of doing it, as I am not sure by now one exists.
  Negative
Thanks for taking the time to respond
563e327c61a80130652679a7	X	been looking up and down through aws docs -- i see nothing that would let you do this.
  Negative
thanks for the tip.
  Positive
if i see anything in my experiences i'll surely update this.
  Neutral
563e327d61a80130652679a8	X	Given a set of Amazon credentials (not the username and password, just the API credentials), is there a programmatic way of finding out when that account was created?
  Negative
I am guessing if the user has an EBS volume, S3 object, I could ask the date it was created.
  Negative
Is there a better way?
  Neutral
563e327d61a80130652679a9	X	In the IAM service there a GetUser command.
  Negative
That seems to be the most relevant.
  Positive
If that doesn't work, then it might not be possible.
  Negative
563e327e61a80130652679aa	X	Good suggestion, i will check if it's possible to implement this without an additional webserver as we won't be able to get one for this.
  Negative
563e327e61a80130652679ab	X	It's totally possible, I suggested the dedicated server just to share the load.
  Positive
In any case even on a single server, consider using a different virtual host for the last point I mentioned
563e327e61a80130652679ac	X	I'm accepting this solution now since it seems to be the solution that scales best with the least overhead, now i just have to get it through to management.
  Negative
563e327e61a80130652679ad	X	Just wanted to give more feedback: 9 Month and many meetings and discussions later and we switched several of our main pages to this solution, works flawlessly since a week while having minimal load on the server we recycled for this.
  Negative
There are already 12GB Images in the on-demand generated cache.
  Negative
Thanks again for the excellent solution.
  Positive
563e327f61a80130652679ae	X	I recently set up something similar to this (dynamic image resizing, with CloudFront and S3).
  Positive
Images are dynamically resized on the first request, then served directly from S3 on subsequent requests.
  Negative
Python code is available here: github.com/mikery/res3izer
563e327f61a80130652679af	X	Even better than caching them locally... request the images via a CDN and then the CDN will cache all the generated images for you and serve them much faster than you can, with cheaper bandwidth costs.
  Negative
That's how we do it and it's extremely effective.
  Positive
563e327f61a80130652679b0	X	+1 I absolutely second this!
  Positive
Why set up an expensive on-the-fly server when what you need can be achieved with household tools.
  Negative
563e327f61a80130652679b1	X	@Greg Beech: it may be hard to find a CDN which will be happy to cache these kind of files.
  Negative
563e328061a80130652679b2	X	@cherouvim - I would be surprised if you can't find a CDN happy to cache these kind of files.
  Negative
It's what you pay them for after all.
  Neutral
We use Akamai, who do a grand job.
  Positive
563e328061a80130652679b3	X	Thanks for the good suggestion but an external provider is (sadly) not an option (management won't approve it, we already tried to get something similar through)
563e328161a80130652679b4	X	nice suggestion!
  Negative
can this technique be used to support exact height and width ?
  Negative
\
563e328161a80130652679b5	X	Thanks for the suggestions and the code, i'll benchmark it and check if it could be fast enough (and i will create a PHP benchmark, too, to check if my coworkers were right with "PHP is too slow").
  Negative
563e328261a80130652679b6	X	I have to discard this solution as the available budget is exactly 0 Euro (or for you US folks: exactly $0).
  Negative
We couldn't even get management to approve installing a test or development server (but that it now takes us twice the time to develop is no problem because developers cost nothing as they are already there...).
  Negative
Still, thanks for your suggestion.
  Positive
563e328261a80130652679b7	X	A dedicated image server AND a CND network can work well together.
  Positive
You will take load away fro your main site and the images will still be delivered very fast across the globe.
  Positive
563e328261a80130652679b8	X	my company has recently started to get problems with the image handling for our websites.
  Negative
We have several websites (adult entertainment) that display images like dvd covers, snapshots and similar.
  Negative
We have about 100'000 movies and for each movie we have an average of 30 snapshots + covers.
  Negative
Almost every image has an additional version with blurring and overlay for non-members, this results in about 50 images per movie or a total of 5 million base images.
  Negative
Each of the images is available in several versions, depending on where it's placed on the page (thumbnail, original, small preview, not-so-small preview, small image in the top-list, etc.) which results in more images than i cared to count.
  Negative
Now i had the idea to use a server to generate the images on-the-fly since it became quite clumsy to generate all the different images for all the different pages (as different pages sometimes even need different image sizes for basically the same task).
  Neutral
Does anyone know of an image processing server that can scale down images on-the-fly so we only need to provide the original images and the web guys can just request whatever size they need?
  Neutral
Requirements: Security is not that much of a concern as i.e. the unblurred images can already be reached by URL manipulation and more security would be nice but it's not required and frankly i stopped caring (After failing to get into my coworkers heads why (for our small reseller page) it's a bad idea to use http://example.com/view_image.php?filename=/data/images/01020304.jpg to display the images).
  Negative
We tried PHP scripts to do this but the performance was too slow for this many users.
  Negative
Thanks in advance for any suggestions you have.
  Positive
563e328361a80130652679b9	X	I suggest you set up a dedicated web server to handle image resize and serve the final result.
  Negative
I have done something similar, although on a much smaller scale.
  Neutral
It basically eliminates the process of checking for the cache.
  Neutral
It works like this: EDIT: I don't think that PHP itself would slow the process much, as PHP scripting in this case is reduced to a minimum: the image scaling is done by a builtin library written in C. Whatever you do you'll have to use a library like this (GD or libmagick or so) so that's unavoidable.
  Negative
With my system at least you totally skip the overhead of checking the cache, thus further reducing PHP interaction.
  Negative
You can implement this on your existing server, so I guess it's a solution well suited for your budget.
  Negative
563e328361a80130652679ba	X	Based on We tried PHP scripts to do this but the performance was too slow for this many users.
  Negative
I'm going to assume you weren't caching the results.
  Negative
I'd recommend caching the resulting images for a day or two (i.e. have your script check to see if the thumbnail has already been generated, if so use it, if it hasn't generate it on the fly).
  Negative
This would improve performance dramatically as I'd imagine the main/start page probably has a lot more hits than random video X, thus when viewing the main page no images have to be created as they're cached.
  Negative
When User Y views Movie X, they won't notice the delay as much since it just has to generate that one page.
  Negative
For the "On-the-fly resize" aspect - how important is bandwidth to you?
  Neutral
I'd want to assume you're going through so much with movies that a few extra kb in images per request wouldn't do too much harm.
  Negative
If that's the case, you could just use larger images and set the width and height and let the browser do the scaling for you.
  Negative
563e328361a80130652679bb	X	The ImageCache and Image Exact Sizes solutions from the Drupal community might do this, and like most solutions OSS use the libraries from ImageMagik There are some AMI images for Amazons EC2 service to do image scaling.
  Negative
It used Amazon S3 for image storage, original and scales, and could feed them through to Amazons CDN service (Cloud Front).
  Negative
Check on EC2 site for what's available Another option is Google.
  Negative
Google docs now supports all file types, so you can load the images up to a Google docs folder, and share the folder for public access.
  Negative
The URL's are kind of long e.g. http://lh6.ggpht.com/VMLEHAa3kSHEoRr7AchhQ6HEzHVTn1b7Mf-whpxmPlpdrRfPW216UhYdQy3pzIe4f8Q7PKXN79AD4eRqu1obC7I Add the =s paramter to scale the image, cool!
  Positive
e.g. for 200 pixels wide http://lh6.ggpht.com/VMLEHAa3kSHEoRr7AchhQ6HEzHVTn1b7Mf-whpxmPlpdrRfPW216UhYdQy3pzIe4f8Q7PKXN79AD4eRqu1obC7I=s200 Google only charge USD5/year for 20GB.
  Negative
There is a full API for uploading docs etc Other answers on SO http://stackoverflow.com/questions/236139/how-best-to-resize-images-off-server/236264
563e328361a80130652679bc	X	Ok first problem is that resizing an image with any language takes a little processing time.
  Negative
So how do you support thousands of clients?
  Neutral
We'll you cache it so you only have to generate the image once.
  Negative
The next time someone asks for that image, check to see if it has already been generated, if it has just return that.
  Negative
If you have multiple app servers then you'll want to cache to a central file-system to increase your cache-hit ratio and reduce the amount of space you will need.
  Negative
In order to cache properly you need to use a predictable naming convention that takes into account all the different ways that you want your image displayed, i.e. use something like myimage_blurred_320x200.jpg to save a jpeg that has been blurred and resized to 300 width and 200 height, etc.
  Negative
Another approach is to sit your image server behind a proxy server that way all the caching logic is done automatically for you and your images are served by a fast, native web server.
  Negative
Your not going to be able to serve millions of resized images any other way.
  Neutral
That's how Google and Bing maps do it, they pre-generate all the images they need for the world at different pre-set extents so they can provide adequate performance and be able to return pre-generated static images.
  Positive
If php is too slow you should consider using the 2D graphic libraries from Java or .
  Negative
NET as they are very rich and can support all your requirements.
  Positive
To get a flavour of the Graphics API here is a method in .
  Negative
NET that will resize any image to the new width or height specified.
  Negative
If you omit a height or width, it will resize maintaining the right aspect ratio.
  Negative
Note Image can be a created from a JPG, GIF, PNG or BMP:
563e328361a80130652679bd	X	In the time that this question has been asked, a few companies have sprung up to deal with this exact issue.
  Negative
It is not an issue that's isolated to you or your company.
  Negative
Many companies reach the point where they need to look for a more permanent solution for their image processing needs.
  Negative
Services like imgix serve as a proxy and CDN for image operations like resizing and applying overlays.
  Negative
By manipulating the URL, you can apply different transformations to each image.
  Negative
imgix serves billions of requests per day.
  Positive
You can also stand up services on your own and put them behind a CDN.
  Negative
Open source projects like imageproxy are good for this.
  Positive
This puts the burden of maintenance on your operations team.
  Neutral
(Disclaimer: I work for imgix.)
  Neutral
563e328461a80130652679be	X	If each different image is uniquely identifiable by a single URL then I'd simply use a CDN such as AKAMAI.
  Negative
Let your PHP script do the job and let AKAMAI handle the load.
  Negative
Since this kind of business doesn't usually have budget problems, that'd be the only place I'd look at.
  Negative
Edit: that works only if you do find a CDN that will serve this kind of content for you.
  Negative
563e328461a80130652679bf	X	My application needs import a .
  Negative
zip file from a brazilian FTP server.
  Negative
It works nice in my development machine whereas I'm using Net/Ftp Ruby native lib or using linux wget.
  Positive
My Problem is the connection time out in the production environment in heroku: "Connecting to ftp.xxxxxx.com.br (ftp.xxxxxx.com.br)|xxxx.xx.xxx.xxx|:21...".
  Neutral
I have tried use wget in a Digital Ocean's virtual machine and the issue still the same.
  Negative
So, I get the problem is the brazilian host is refusing connection from foreign IP.
  Negative
Have you idea how bypass this problem?
  Neutral
I'm going to try hire a host here in Brazil or even create a Amazon's EC2 instance in Sao Paulo region (will it get a brazilian IP?)
  Negative
just to host a API application who downloads the file from the FTP server and upload it to Amazon S3, where I have fully access to any file.
  Negative
.
  Neutral
Have you a better solution?
  Neutral
563e328461a80130652679c0	X	Is this safe?
  Neutral
Maintaining security using a pre-signed url with AWS S3 Bucket object?
  Negative
Another words - part 1... say I'm storing a bunch of separate individual's files in a bucket.
  Neutral
I want to provide a link to a file for a user.
  Negative
Obviously, each file is uniquely but consecutively named, I don't want people to be able to change the link from 40.pdf to 30.pdf and get a different file.
  Negative
This URL seems to do that.
  Neutral
part 2, and more importantly.... Is this safe or is a it dangerous method of displaying a URL in terms of the security of my bucket?
  Neutral
Clearly, i will be giving away my "access key" here, but of course, not my "secret".
  Negative
Already answered 3 years ago... sorry.
  Negative
How secure are Amazon AWS Access keys?
  Negative
563e328461a80130652679c1	X	AWS Security Credentials are used when making API calls to AWS.
  Negative
They consist of two components: A Signed URL is a method of granting time-limited access to an S3 object.
  Negative
The URL contains the Access Key and a Signature, which is a one-way hash calculated from the object, expiry time and the Secret Key.
  Negative
A Signed URL is safe because: However, anyone can use the URL during the valid time period.
  Negative
So, if somebody Tweets the URL, many people could potentially access the object until the expiry time.
  Negative
This potential security threat should be weighed against the benefit of serving traffic directly from Amazon S3 rather than having to run your own web servers.
  Negative
563e328461a80130652679c2	X	Is there any chance config.eager_load is off?
  Negative
Or you're testing this with config.eager_load off?
  Neutral
cache_classes?
  Neutral
I'd expect to see this behavior if we aren't in production mode.
  Neutral
563e328461a80130652679c3	X	Our codebase doesn't say "eager_load" at all, so I'm guessing it's off, both in dev and in prod.
  Negative
Cache_classes is false in dev, and true in production, as you'd expect.
  Negative
To clarify, this works slowly in production, and fast in dev, but I believe it's the SSD drive.
  Positive
eager_load doesn't seem to have an effect of whether CarrierWave hits the disk for each mounted file, does it?
  Negative
563e328561a80130652679c4	X	For others finding this, we're also discussing this here.
  Negative
563e328561a80130652679c5	X	I have a model where I'm using Carrierwave to let users upload their avatars.
  Negative
I have one version for the avatar, cropped to a square called "cropped".
  Negative
In one of my pages, where I'm listing a few thousand users, each with their avatar, the call to generate the URL for these images is taking a long time.
  Negative
The problem seems to be that just by accessing to the model attribute that has the uploader mounted, Carrierwave seems to be hitting the disk to either read the file, check that it exists, or something.
  Negative
In other words, calling: All of these hit the HDD.
  Positive
In the worst case I found, for one of our clients that have a massive amount of data, the page renders in 10 seconds my dev machine, and 30 seconds on the server.
  Very negative
This page render implies about 1200 calls to "user.avatar.cropped".
  Neutral
The difference seems to be due to SSD vs HDD (or maybe overhead due to VM's, not sure).
  Negative
OS Disk Caching does kick in, though, since rendering the same page for a second time in the server takes 10 seconds, presumably since disk hits are cached.
  Negative
If I generate the URL "manually", instead of using CarrierWave, it's back down to 10 seconds in all machines, so it's definitely Carrierwave that's causing the slowdown.
  Negative
It seems like hitting the HDD shouldn't be necessary to generate the path.
  Negative
Is there any way to avoid this, or to work around this problem?
  Negative
(NOTE: I know 10 seconds is atrocious for a page anyway, we're doing stuff to solve this, but site-wide, we have lots of pages that show tons of avatars, and we're getting a slowdown due to these HDD hits, so we'd like to improve them, without having to manually generate the URLs, since the abstraction Carrierwave provides is awesome) UPDATE (due to comment below): We're not using eager_load.
  Negative
Cache_classes is off in dev, and on in production.
  Negative
(Again, dev is fast, prod is slow, but I don't think it's related to cache_classes)
563e328561a80130652679c6	X	I ran into the same issue with Carrierwave and S3.
  Negative
Here is my scenario, I have an API responsible of uploading avatars into S3, by default a user has an avatar that is stored in the assets folder of my API.
  Negative
If 10 users have a default avatar they all use the same image in order to avoir to uploads a bunch of default avatar to S3.
  Negative
I have created a custom method that essentially checks if an avatar has been uploaded or not.
  Negative
If an avatar has been uploaded we build manually the S3 path to the avatar depending on the size.
  Negative
Otherwise we just return the default avatar path which is stored locally in our assets folder.
  Negative
This works for a public amazon s3 bucket, I'm not sure what would happen with a private bucket or how to generate the expire parameters, but since you're using the file storage system it shouldn't be a problem.
  Negative
This instruction was the key for generating the name of the file and not hitting S3 to get the img path back.
  Negative
I had a time response of 2seconds before, now I'm at 300-500ms Hope this helps!
  Positive
563e328861a80130652679c7	X	What is your definition of 'web service'?
  Negative
563e328861a80130652679c8	X	web service means normal SOAP web services...
563e328861a80130652679c9	X	Hi Maksym, I am working on java...the way you suggest but our client may use .
  Negative
Net to download the file.
  Neutral
as my understanding Amazon S3 will provide a server where they will stage the file and we can request the file by there API.
  Negative
.
  Neutral
is it possible to use webservice communication with Amazon S3 server to download and upload the files.
  Negative
?
  Neutral
563e328961a80130652679ca	X	Yes, it's possible, see my anwser update.
  Negative
563e328961a80130652679cb	X	thanks.
  Positive
.
  Neutral
Maksym .
  Neutral
.
  Neutral
i will try the same
563e328961a80130652679cc	X	I have a requirement where I need to upload a large file (can be 10 GB) to a shared space(windows) ( say APP1) .
  Negative
and we have a separate application( say APP2) different network now I need to download the same file from in second application via internet.
  Negative
My approach is I need to create webservice to upload the document to shared space.
  Positive
and then expose a webservice for outer world to download the document.
  Negative
My point is how I can manage the huge files upload/download through webservice ?
  Positive
Please suggest if some one have any idea.
  Neutral
I have flexibikity to use any third party APIs.but the application can talk only through webservices.
  Negative
563e328961a80130652679cd	X	From your question it's not really clear which development platform you mean, .
  Negative
NET, Java, etc.
  Negative
Also it's important to know how interoperable your services should be, security requirements, etc.
  Negative
Anyway will try to come up with a couple of solutions which you might research in more detail if you found them useful.
  Neutral
.
  Neutral
NET It's relatively easy to built such a web service with WCF.
  Negative
It supports streaming which could be interoperable, reliable and secure to some extend.
  Negative
You can read more on this here.
  Neutral
This approach implies you have a huge disk to store files and you have a good recovery plan for that in case it goes down or just dies.
  Negative
.
  Neutral
NET, Java, etc. - cloud based There are a lot of vendors who provide cloud storage and APIs to work with it.
  Negative
It's an ideal solution for a quick start.
  Positive
They take care of data availability, redundancy, etc.
  Negative
All you have to do is to use their API to upload and download files and to pay them for this :) In many cases it's worth it.
  Negative
Personally I used to work with Amazon S3.
  Negative
Their API is simple to use and there's plenty of docs for that.
  Negative
EDIT: Amazon S3 provides a simple web-services interface that can be used to store and retrieve any amount of data, at any time, from anywhere on the web.
  Negative
I think you should take a look at Amazon S3 overview here.
  Neutral
This also provides API for a number of different platforms - Java, .
  Positive
NET, Node.js, etc.
  Negative
You an find the full list here.
  Negative
Hope it helps!
  Positive
563e328c61a80130652679ce	X	@user2939212 I understand that, but in some cases such as mine, there are so many parameters that you reach a point time when you might find that solution inevitable :)
563e328c61a80130652679cf	X	@user2939212 yeah, and moreover, you cannot name them in argv[], unless you use dash options -param1 val1 -param2 val2 etc.
  Negative
The process becomes cumbersome and error-prone.
  Negative
563e329461a80130652679d0	X	Not done extensive testing, but Northern California seems to do the same thing as Ireland.
  Negative
Not sure which is the desired result.
  Negative
563e329461a80130652679d1	X	Good question and an annoying difference indeed - I'm pretty sure all regions but US Standard will return the same result (i.e. a dedicated object/key for the simulated directory), see my answer for details on this.
  Very negative
563e329461a80130652679d2	X	To ensure the analysis is actually digging into the correct origin: How have these buckets/keys been created in the first place?
  Negative
Or more specifically, have these been created by a 3rd party service/tool or by yourself via the S3 API or the AWS Management Console?
  Neutral
563e329461a80130652679d3	X	Good point, I have keys that have been uploaded using the console, and also some created by boto.
  Negative
I think I will look into if they consistently behave the same way for both.
  Negative
563e329461a80130652679d4	X	+1 for following up with your analysis, thanks!
  Negative
Quite some confusing behind the scene magic indeed ...
563e329461a80130652679d5	X	The AWS console is creating an empty file named 'bucketName/someDir/' which is quite confusing; you can delete that empty "file" from the command line and then that clears up the issue
563e329461a80130652679d6	X	Not very good solution if you have files without extension.
  Very negative
563e329461a80130652679d7	X	I've noticed a difference between the returns from boto's api depending on the bucket location.
  Negative
I have the following code: which im running against two buckets, one in us-west and one in ireland.
  Negative
Path in this bucket is a sub-directory, against Ireland I get the sub directory and any keys underneath, against us-west I only get the keys beneath.
  Negative
So Ireland gives: where as US Standard gives: Obviously, I want to be able to write the same code regardless of bucket location.
  Positive
Anyone know of anything I can do to work around this so I get the same predictable results.
  Negative
Or even if it's boto causing the problem or S3.
  Neutral
I noticed there is a different policy for naming buckets in Ireland, do different locals have their own version of the api's?
  Negative
Thanks, Steve
563e329461a80130652679d8	X	Thanks to Steffen, who suggested looking at how the keys are created.
  Positive
With further investigation I think I've got a handle on whats happening here.
  Negative
My original suposition that it was linked to the bucket region was a red herring.
  Negative
It appears to be due to what the management console does when you manipulate keys.
  Neutral
If you create a directory in the management console it creates a 0 byte key.
  Negative
This will be returned when you perform a list.
  Neutral
If you use boto to create/upload a file then it doesn't create the folder.
  Negative
Interestingly, if you delete the file from within the folder (from the AWS console) then a key is created for the folder that used to contain the key.
  Negative
If you then upload the bey again using boto, then you have exactly the same looking structure from the UI, but infact you have a spurious additional key for the directory.
  Negative
This is what was happening to me, as I was testing our application I was clearing out keys and then finding different results.
  Negative
Worth knowing this happens.
  Positive
There is no indicator in the UI to show if a folder is a created one (one that will be returned as a key) or an interpreted one (based on a keys name).
  Negative
563e329561a80130652679d9	X	I don't have a definite answer for your question, but can throw in some partial ones at least: Amazon S3 doesn't actually have a native concept of folders/directories, rather is a flat storage architecture comprised of buckets and objects/keys only - the directory style presentation seen in most tools for S3 (including the AWS Management Console itself) is based solely on convention, i.e. simulating a hierarchy for objects with identical prefixes - see my answer to How to specify an object expiration prefix that doesn't match the directory?
  Very negative
for more details on this architecture, including quotes/references from the AWS documentation.
  Neutral
I noticed there is a different policy for naming buckets in Ireland, do different locals have their own version of the api's?
  Negative
That's apparently the case indeed for Amazon S3 specifically, which is one of their oldest offerings, see e.g. Bucket Restrictions and Limitations: In all regions except for the US Standard region, You must use the following guidelines when naming a bucket.
  Negative
[...] [emphasis mine] These specifics for the US Standard region are seen in other places of the S3 documentation as well, and US Standard is an unusual construct itself compared to the otherwise clearly geographically constrained Regions: US Standard — Uses Amazon S3 servers in the United States This is the default Region.
  Negative
The US Standard Region automatically routes requests to facilities in Northern Virginia or the Pacific Northwest using network maps.
  Negative
To use this region, select US Standard as the region when creating a bucket in the console.
  Negative
The US Standard Region provides eventual consistency for all requests.
  Positive
[emphasis mine] This implicit CDN behavior is unique for this default Region of S3 (i.e. US Standard) and not seen elsewhere on any other AWS service I think.
  Negative
I have a faint memory of S3 actually placing a zero byte object/key into a bucket for the simulated directory/folder in more recent regions (i.e. all but US Standard), whereas the legacy solution for the US Standard region might be different, for example simply based on the established naming convention for directory separation by / and omitting a dedicated object/key for this altogether.
  Negative
If the analysis is correct, there is nothing you can do but maintain separate code paths for both cases, I'm afraid Good luck!
  Negative
563e329561a80130652679da	X	I've had the same problem.
  Negative
As a work around you can filter out all the keys with a trailing '/' to eliminate the 'directory' entries.
  Negative
563e329561a80130652679db	X	I'm using the fact that a "Folder" has no "."
  Negative
in its path.
  Neutral
A file does.
  Neutral
media/images will not be deleted media/images/sample.
  Negative
jpg will be deleted e.g. clean bucket files
563e329561a80130652679dc	X	Genius as ever - cheers Darin, that's fixed it
563e329661a80130652679dd	X	I'm using fineuploader for uploading images to an ASP.NET 4.5 Web API controller.
  Negative
The controller is hit ok and the image is uploaded.
  Positive
I get a 200 response from the API controller.
  Negative
I'm displaying a thumbnail after successful upload and the filename to the location within Amazon S3 is supposed to come back in the JSON response.
  Negative
When I try this in Chrome or IE10 it works fine.
  Positive
When I try this in Firefox, the responseJson that comes back is an empty object, which is displayed as a failure.
  Negative
I'm using jQuery 1.8.2 and fineuploader 3.2.
  Negative
I actually get undefined displayed on the screen because responseJson.message is undefined.
  Negative
Here's the js code: The response that comes back is application/json.
  Negative
What do I need to do to get this working properly in Firefox?
  Neutral
563e329661a80130652679de	X	The difference between FF and Chrome is the Accept request header being sent.
  Negative
Just use FireBug and Chrome developer toolbar to compare the results between the 2 browsers: FF: Chrome: So as you can see FF is not sending the correct Accept header and the Web API's content negotiation mechanism simply falls back to text/xml (because that's what the client requested).
  Negative
Fortunately the plugin allows you to override the request headers using the customHeader property and force it to the expected type (application/json in your case):
563e329761a80130652679df	X	Which file system is currently in use on the EBS volume?
  Neutral
563e329761a80130652679e0	X	The file system is ext2, which is why I don't understand why it's having a hard time.
  Negative
Isn't that one of the most basic types?
  Negative
I will take your advice and snapshot the volume.
  Positive
Thanks for that tip.
  Neutral
563e329761a80130652679e1	X	I would like to use an EBS volume with data on it that I've been working with in an Ubuntu AMI in a RedHat 6 AMI.
  Negative
The issue I'm having is that RedHat says that the volume does not have a valid partition table.
  Negative
This is the fdisk output for the unmounted volume.
  Negative
Disk /dev/xvdk: 901.9 GB, 901875499008 bytes 255 heads, 63 sectors/track, 109646 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Disk /dev/xvdk doesn't contain a valid partition table Interestingly, the volume isn't actually 901.9 GB but 300 GB.
  Negative
.
  Neutral
I don't know if that means anything.
  Negative
I am very concerned about possibly erasing the data in the volume by accident.
  Negative
Can anyone give me some pointers for formatting the volume for RedHat without deleting its contents?
  Negative
I also just checked that the volume works in my Ubuntu instance and it definitely does.
  Positive
563e329761a80130652679e2	X	I'm not able to advise on the partition issue as such, other than stating that you definitely neither need nor want to format it, because formatting is indeed a (potentially) destructive operation.
  Negative
My best guess would be that RedHat isn't able to identify the file system currently in use on the EBS volume, which must be advertized by some means accordingly.
  Negative
However, to ease with experimenting and gain some peace of mind, you should get acquainted with one of the major Amazon EBS features, namely to create point-in-time snapshots of volumes, which are persisted to Amazon S3: These snapshots can be used as the starting point for new Amazon EBS volumes, and protect data for long-term durability.
  Neutral
The same snapshot can be used to instantiate as many volumes as you wish.
  Negative
This is detailed further down in section Amazon EBS Snapshots: Snapshots can also be used to instantiate multiple new volumes, expand the size of a volume or move volumes across Availability Zones.
  Positive
When a new volume is created, there is the option to create it based on an existing Amazon S3 snapshot.
  Positive
In that scenario, the new volume begins as an exact replica of the original volume.
  Negative
[...] [emphasis mine] Therefore you can (and actually should) always start experiments or configuration changes like the one you are about to perform by at least snapshotting the volume (which will allow you to create a new one from that point in time in case things go bad) or creating a new volume from that snapshot immediately for the specific task at hand.
  Positive
You can create snapshots and new volumes from snapshots via the AWS Management Console, as usual there are respective APIs available as well for automation purposes (see API and Command Overview) - see Creating an Amazon EBS Snapshot for details.
  Positive
Good luck!
  Positive
.
  Neutral
563e329761a80130652679e3	X	Check out firebase.com.
  Negative
It's in beta though.
  Negative
563e329761a80130652679e4	X	Firebase.com looks good.
  Positive
563e329761a80130652679e5	X	I'm really surprised that this is the only one out there.
  Positive
563e329761a80130652679e6	X	update: I ended up using mongolab.com.
  Negative
It has CORS enabled which was exactly what I needed.
  Negative
563e329861a80130652679e7	X	Currently the firebase has the free plan
563e329861a80130652679e8	X	Hey I'm making a small project and I would like to use a JSON style database service where I can set and get parts of the JS object.
  Negative
I would like this to be completely on the front end and not require any server technologies.
  Negative
It dosent need to be secure as its just a hobby project.
  Negative
Security is a bonus.
  Positive
I'm currently using HTML5 local storage.
  Negative
I'm happy to pay for this as a service.
  Positive
563e329861a80130652679e9	X	https://www.firebase.com/ does not have free plan.
  Negative
Direction to go seems to select NoSQL db like MongoDB or CouchDB, and then search/compare for db as service providers.
  Negative
Quick search gives 10 Online Storage APIs That is file oriented APIs Box.net: The “Box Enabled” platform offers a choice of SOAP, REST or XML-POST APIs.
  Negative
Below is Fireloader, a Firefox extension which allows you to upload, and download photos, files and videos using a familiar FTP like interface.
  Positive
In this version Flickr, Picasa, Box.net and Youtube are supported.
  Negative
Cellblock: This multimedia sharing service offers a REST-based API.
  Negative
ref http://webapps.stackexchange.com/questions/8247/seeking-online-hosted-database-web-service-with-rest-api
563e329861a80130652679ea	X	You may also want to check out www.ttmsolutions.com/restjee.
  Negative
Its a lightweight server-side JSON ORM that doesn't require you to develop any server-side code and works with any DB.
  Negative
563e329861a80130652679eb	X	This site is for programming questions.
  Negative
Backup strategies are WAY off topic.
  Negative
563e329861a80130652679ec	X	No problem.
  Negative
Checked the FAQ and thought it were covered by "software tools commonly used by programmers" and "practical, answerable problems that are unique to the programming profession" –
563e329861a80130652679ed	X	+1 Redundant storage and data backups are separate problems.
  Negative
I would add that S3 buckets do support versioning and Multi-factor authentication for deleting objects.
  Negative
99% of data restores I've seen were of the "Oops!
  Negative
I deleted this.
  Neutral
Can you get it back?"
  Neutral
variety.
  Neutral
563e329861a80130652679ee	X	S3 has a way to version files.
  Negative
563e329861a80130652679ef	X	Hi AvkashChauhan, good answer IMHO.
  Negative
Can you just share some best practices to synch data between an S3 bucket and another with RSS?
  Positive
With s3synch?
  Neutral
From an EC2 instance?
  Neutral
563e329961a80130652679f0	X	Say Amazon deletes your account (your credit card expires, someone logs into your account and requests it be deleted, or some other method).
  Negative
You lose your files.
  Negative
563e329961a80130652679f1	X	Thanks.
  Neutral
Is it not an option for me to download it manually - or save it to a harddisk in house.
  Negative
563e329961a80130652679f2	X	I have just finished setting up Amazone S3 as a CDN for our website.
  Negative
From now on we will host tons of picture directly in the cloud.
  Neutral
It is cool!
  Positive
- but leave me with a problem in regards to backup.
  Negative
Earlier we backup up everything by uploading the pictures to amazon once a day, but I really don't want to backup to the same place I host the files.
  Negative
How do I backup from Amazone S3 in the most efficient way?
  Neutral
I have considered: Any solution I do not now of?
  Neutral
A solution like myrepono.com for Amazone s3 would be cool.
  Negative
Cheers, Peter
563e329a61a80130652679f3	X	One of the main things not addressed by Amazon S3 at the moment is any concept of a history or recycle bin for files that are deleted or modified.
  Negative
The reason for this is not to address Amazon's vulnerability to failure of the service, but to address malicious or accidental removal of files by someone with the access credentials, and not having any recovery option.
  Negative
This single point of failure still is a weakspot for AWS services, and even on ones with snapshot capability (RDS, EC2, but not S3), it still is a problem because a malicious user with access could remove snapshots with the same single entry point.
  Very negative
You have to differentiate between the following scenarios: 1) Oops, the infrastructure failed and I lost my file!
  Very negative
(very unlikely with S3) 2) Oops, I (or a client) accidentally deleted that file on purpose but I want it back!
  Negative
3) Oops, That API script on my system was supposed to only remove one file, but due to a programming error it looped through all of them and deleted all of them!
  Negative
4) Oops, someone got access to my AWS account and deleted my files!
  Negative
1 is very low worry based on how S3 works.
  Negative
2 you can program around by making systems utilize a recycling bucket for intentional deletions.
  Neutral
But right now 3 and 4 leave you with real and substantive data loss, and don't have a solution on S3 itself that can solve it.
  Positive
So you either have to hope these last two things don't happen, or you are left keeping periodic backups of S3 elsewhere, which at the moment is really cumbersome.
  Negative
I think the best thing would for Amazon to add some sort of deletion retention automatically as a feature, meaning that any delete of any kind could be reversed for 48 hours or something before it was gone for good.
  Negative
563e329a61a80130652679f4	X	I am very much interested to know the main concern why you wouldn't believe that you still need backup solution after moving to cloud because sooner later everyone will be looking cloud for main storage and backup storage and another backup storage (if needed).
  Negative
I think the time is already now.
  Neutral
My point is that when you have data stored to any Cloud storage, you can depend on storage service provider 99.999999% SLA.
  Negative
These cloud service replicate the data for multiple copies to meet the SLA requirement and sometime copy data to separate location to avoid problems related with a scenario when whole data center at one location completely shutdown.
  Negative
When data is geo-copied it actually satisfy the backup requirement itself.
  Negative
For example with Windows Azure Blob Storage: With Amazon S3 you already have backup storage with ability to get data based on versioning: Finally if i really want to backup my data which is already in Amazon S3, I would like to use "RRS" as backup to my S3 data because:
563e329a61a80130652679f5	X	I'm not sure why you don't want to "backup to the same place as [you] host the files" when Amazon S3 is the hosting solution.
  Very negative
If you are worried about a single point of failure, here's what Amazon says about S3's reliability and durability: Amazon S3 provides a highly durable storage infrastructure designed for mission-critical and primary data storage.
  Negative
Objects are redundantly stored on multiple devices across multiple facilities in an Amazon S3 Region.
  Negative
To help ensure durability, Amazon S3 PUT and COPY operations synchronously store your data across multiple facilities before returning SUCCESS.
  Negative
Once stored, Amazon S3 maintains the durability of your objects by quickly detecting and repairing any lost redundancy.
  Negative
Amazon S3 also regularly verifies the integrity of data stored using checksums.
  Negative
If corruption is detected, it is repaired using redundant data.
  Negative
In addition, Amazon S3 calculates checksums on all network traffic to detect corruption of data packets when storing or retrieving data.
  Negative
Amazon S3’s standard storage is: Amazon S3 provides further protection via Versioning.
  Negative
You can use Versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket.
  Negative
This allows you to easily recover from both unintended user actions and application failures.
  Positive
By default, requests will retrieve the most recently written version.
  Negative
Older versions of an object can be retrieved by specifying a version in the request.
  Negative
Storage rates apply for every version stored.
  Negative
If you want to increase redundancy, you can store your data in multiple S3 regions.
  Negative
Now, if you REALLY want to sync your files between multiple cloud storage providers, you can use CloudBerry to synchronize your data between multiple cloud providers.
  Negative
They support S3, Google Storage, Azure Blog Storage and Rackspace Cloud files.
  Negative
Pro: you pay a one-time fee for the tool.
  Negative
Con: You need a witness server to run CloudBerry.
  Negative
563e329a61a80130652679f6	X	my stupid answer, Use a Service of AWS called "AWS Import/Export" ,now you can backup to your own HDD
563e329b61a80130652679f7	X	I am writing an application where users are required to show their photo, however as my server resources are very limited I can not let them upload it to the server.
  Very negative
So I have three major questions: 1.
  Negative
How to properly validate photo URL?
  Neutral
At least I can validate with regexp, however I need to check for file ending: 2.
  Negative
Security issues?
  Neutral
XSS?
  Neutral
Even I validate the picture at the moment of creation, hacker can replace image with malicious stuff anytime.
  Negative
3.
  Neutral
Maybe there are free asset stores with API?
  Neutral
563e329b61a80130652679f8	X	1.
  Neutral
How to properly validate photo URL?
  Neutral
You can use a plugin that validates the format of an URL or write it your self: 2.
  Neutral
Security issues?
  Neutral
XSS?
  Neutral
There aren't any outstanding security issues as long as you escape the text.
  Negative
<%=h image_tag obj.photo_url %> is safe.
  Negative
Take in mind, that the user can still use a 100MB image that will slow down every visitor.
  Negative
3.
  Neutral
Maybe there are free asset stores with API?
  Neutral
There aren't any that I know of, but rackspace cloud, amazon s3 hosting is pretty cheap.
  Negative
Some image upload plugins have support for these two, so you'll at least save some time.
  Negative
563e329b61a80130652679f9	X	You are right, when you use S3, Content-Dispotion is set on upload.
  Positive
Using other methods, such as the send_file method, will allow you to set the Content-Dispoition on sending.
  Negative
However, send_file only works when you are sending a file from your filesystem, therefore you can not use that in conjunction with S3.
  Neutral
I did figure out that I could set the Content-Dispotion of a file but still send it as an inline element.
  Negative
For example, if an image has the Content-Disposition set to "attachment" it could still be used as the src for an image tag, while download if you put the image URL in the browser.
  Negative
563e329b61a80130652679fa	X	Ah, that's good to know.
  Neutral
Sounds like it would be best to upload them all as "attachment" if they still work as inline "src" for images.
  Negative
563e329c61a80130652679fb	X	I am using Amazon S3 to store and serve user content for user accounts.
  Negative
I need the ability to serve the files either inline (sometimes urls for images will be in blog posts, etc) or as a download.
  Negative
By default when uploading a file to my S3 bucket, the file has no Content-Disposition set (which is fine because it will server inline as long as the browser recognizes the file MIME), however at times I will need to set the Content-Disposition to attachment in order to download the file.
  Negative
Using Rails/S3 gem, is it possible to send a request to Amazon to specify that the file should be sent with the Content-Disposition set to attachment (or vise-versa) for just that request?
  Negative
Possibly it could use some sort of token (in reference to a token for the request, typically used for authenticated reads…just wondering if that can help me in this situation too)?
  Negative
Using the S3 gem, I know how to set and save the Content-Disposition for each file, but that would cause the file to always be downloaded and could not be used as an image inline (not tested).
  Negative
Short of having two files (one with and without the Content-Disposition='download' set), any ideas?
  Negative
Thanks in advance.
  Neutral
PS I using rails Rails 2, attachment_fu and the aws-s3 gem (I can't change these because the above mentioned app is apart of a much larger, already running app and I know conflicts exist between the aws-s3 and aws_right gem)
563e329c61a80130652679fc	X	Just in case anybody stumbles upon this old post, Amazon's API now allows for changing the Content-Disposition for files stored on S3.
  Negative
Read the announcement here.
  Negative
563e329c61a80130652679fd	X	I believe the Content-Disposition is set upon upload, so if you don't want two copies of the file each with their own Content-Disposition, one way would be to stream it from a controller using send_file http://api.rubyonrails.org/classes/ActionController/Streaming.html#method-i-send_file
563e329c61a80130652679fe	X	It doesn't do multi-part upload in parallel, which hurts its speed.
  Negative
It also spews error messages about S3 failing (that's what S3 does, get over it) and tries to slow down even more.
  Very negative
563e329c61a80130652679ff	X	AWS CLI fully saturated my connection at 7.3 MB/s and did not fail.
  Negative
I tried S3 Tools for a 1GB file, and it kept showing errors and uploaded at only 2-3 MB/s.
  Negative
563e329c61a8013065267a00	X	What amazon s3 client do you use in linux with multipart upload feature?
  Negative
I have 6GB of zip files to upload and s3curl is not possible due to maximum limit of 5GB only.
  Negative
Thanks.
  Neutral
James
563e329c61a8013065267a01	X	I use S3 Tools, it will automatically use the multipart upload feature for files larger than 15MB for all PUT commands: Multipart is enabled by default and kicks in for files bigger than 15MB.
  Negative
You can set this treshold as low as 5MB (Amazon’s limit) with —multipart-chunk-size-mb=5 or to any other value between 5 and 5120 MB Once installed and configured, just issue the following command: Alternatively, you could just use split from the command-line on your zip file: and recombine later on your filesystem using: If you choose the second option, you may want to store MD5 hashes of the files prior to upload so you can verify the integrity of the archive when it's recombined later.
  Very negative
563e329c61a8013065267a02	X	The boto library includes an s3 command line tool called s3put that can handle multipart upload of large files.
  Negative
563e329d61a8013065267a03	X	The official AWS Command Line Interface supports multi-part upload.
  Negative
(It uses the boto successor botocore under the hood): The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services.
  Negative
With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.
  Negative
On top of this unified approach to all AWS APIs, it also adds a new set of simple file commands for efficient file transfers to and from Amazon S3, with characteristics similar to the well known Unix commands, e.g.: So cp would be sufficient for the use case at hand, but be sure to check out sync as well, it is particularly powerful for many frequently encountered scenarios (and sort of implies cp depending on the arguments).
  Positive
563e329d61a8013065267a04	X	You could mount the S3 bucket to the filesystem.
  Negative
563e329d61a8013065267a05	X	You can have a look at the FTP/Amazon S3/Glacier client CrossFTP.
  Negative
563e329d61a8013065267a06	X	Personally I created python file s3upload.py with simple function to upload large files using boto and multipart upload.
  Negative
Now every time I need to upload large file, I just run command like this: More details and function code can be found here.
  Negative
563e329d61a8013065267a07	X	Based on this sample http://aws.amazon.com/articles/0006282245644577, it is clear how to use multipart upload using the AWS iOS SDK.
  Negative
However, it seems that my uploads are not stitched together correctly when I try to resume an interrupted upload.
  Neutral
I use the code below to resume an upload.
  Negative
Is this the correct way to set the upload id of a multipart upload?
  Neutral
I'd appreciate any help or pointers.
  Neutral
563e329d61a8013065267a08	X	Your code should be robust enough to handle cases where you may need to track which parts were uploaded.
  Neutral
Part Uploads of the multipart upload can be done in many ways (either in parallel, multithreaded manner or one after the other in sequence).
  Negative
Whatever the above approach may be, you can use the listParts API to determine how many parts were successfully uploaded.
  Negative
Since you would already have the upload ID your design must support the ability to continue from the following part upload.
  Negative
Another useful resource to help optimize multipart uploads: http://aws.typepad.com/aws/2010/11/amazon-s3-multipart-upload.html
563e329d61a8013065267a09	X	I have an article on the website I help maintain, that I want to share on LinkedIn, via the "Share an update" form on the site.
  Negative
The possible thumbnail images are being detected, and LinkedIn is receiving the correct urls for the images in question (they are served from Amazon's S3 service).
  Negative
Inspecting the page, I see that a call is made by the page to https://www.linkedin.com/sharing/api/url-preview and the JSON response includes a "previewImages" field (under "data".
  Negative
"content") which is an array of objects/dicts with the fields "url", "mediaProxyUrl", "width", "height", and "size".
  Positive
The "url" of my preview images is correct.
  Neutral
Copying and pasting into the address bar brings it up.
  Positive
The "mediaProxyUrl" however does not load an image.
  Negative
The "size" field is null.
  Positive
Using a working reference url (an article on another site), I can see that "mediaProxyUrl" is supposed to be linkedin's url for the thumbnail, and that "size" is supposed to be the file size of the original image.
  Negative
So, why are my preview thumbnails blank?
  Negative
Is this a problem on LinkedIn's end?
  Neutral
Is Amazon s3 the problem?
  Negative
I'm at a loss.
  Neutral
P.S. I've checked my og:image and og:image:secure_url headers, they're in order.
  Negative
563e329e61a8013065267a0a	X	I have an application where customers upload files like Powerpoints and Excel spreadsheets to the application through a web UI.
  Negative
The files then have meta data associated with them and they are stored as BLOBs in a MySQL database.
  Negative
The users may download these files occasionally, but not very often.
  Negative
The emphasis here is on archiving.
  Neutral
Security of data is also important.
  Positive
If that is the case, what are the pros and cons of storing the files as BLOBs in MySQL as opposed to putting them on Amazon S3?
  Negative
I've never used S3 before but hear that it's popular for storing files.
  Negative
563e329e61a8013065267a0b	X	The main advantage of relational databases (such as MySQL) is the elegance it permits you to query for data.
  Positive
BLOB columns, however, offer very little in terms of rich query semantics compared to other column types, so If that's your main use case, there's hardly any reason to use a relational database at all, it doesn't offer much above and beyond a regular filesystem or simple key-value datastore (such as s3).
  Very negative
Dollars to bytes, s3 is likely much more cost effective.
  Negative
On the other hand, there are some things that a relational database can bring that would be worhtwhile.
  Negative
The most obvious is transactional semantics (only on the InnoDB engine, not available with MyISAM), so that you can safely know that whole groups of uploads or modifications take place consistencly.
  Negative
Another advantage is that you can still add metadata about your blobs (even if it's only over time, as your application improves) so you can still benefit some from the rich queries MySQL supports.
  Negative
563e329e61a8013065267a0c	X	storing binary data into blob there is no true security If you are archiving the binary data, store into normal disk file If security is important, consider separate between your UI server and storage server, but is hard to archive, you can always consider to embed password / encryption into these binary files security over amazon s3
563e329e61a8013065267a0d	X	Security of data is also important.
  Negative
Do note that files on S3 are not stored on encrypted disks, so you may have to encrypt client-side or on your servers before sending it up to S3.
  Negative
563e329e61a8013065267a0e	X	I've been storing data in S3 for years and completely love it!
  Negative
What I do is upload the file to S3 (where its copied multiple times by the way) and then store a reference to the file path and name into my MySQL files table.
  Negative
If anything else, it takes that much load off of the MySQL DB and S3 now offers AES256 bit encryption with revolving master keys so you know its secure!
  Negative
563e329e61a8013065267a0f	X	On my android application I use the dropbox API.
  Negative
I hardcode the app key and secret.
  Neutral
But to authenticate I need to log in using the dropbox account.
  Negative
But whats the point of using a app key and secret if you have to enter a username and password.
  Negative
Also what if you would like other people to be able to upload to your dropbox without using the accounts username and password.
  Neutral
Can they use the app key and secret to just upload to the account without entering the accounts username/password?
  Negative
563e329e61a8013065267a10	X	You are a bit confused with what are app key/secret used to do.
  Negative
In briefly, a pair of app key/secret is used to identify an app.
  Negative
Is it a valid app?
  Neutral
Is it authorized by user?
  Neutral
And is it out of API call limit/throttling?
  Neutral
Therefore, only with key/secret, app has no right to access an unauthorized user's private data.
  Negative
I'm sorry but I have to say allowing people upload data to your own cloud is not a good idea.
  Negative
At least, dropbox is not for that purpose.
  Negative
Instead, why not try some other cloud storage service, like Amazon S3?
  Negative
563e329f61a8013065267a11	X	use any file hosting (tinypic.com or imagehost.org)
563e329f61a8013065267a12	X	If the admin site manages the other site that shows the content, I'd question splitting them into two seperate sites.
  Negative
Usually I see setups where the main site is www.mysite.com and www.mysite.com/admin or admin.mysite.com for the administrative pages.
  Negative
Really makes everything alot simpler.
  Positive
What is the major motivation to split everything to two sites?
  Neutral
563e329f61a8013065267a13	X	I've got two web sites (written in c#) which are pretty common: Images should be stored outside these two sites but where and how?
  Negative
563e329f61a8013065267a14	X	You could store them on a third site for static content, then both sites would link to that content.
  Negative
That can give you some benefits if you really need scalability as well.
  Positive
However, you may also store it in a shared database, or just a shared file share, that you have encapsulated logic that each sites that needs access to them uses.
  Neutral
If you dont want to both with the storage there are many "cloud" storage servers that will host all your images for you.
  Positive
(like Amazon S3, Smugmug, flickr).
  Negative
But then you will have to build the logic in your app to upload the submitted images to your 3rd party storage provider.
  Negative
563e329f61a8013065267a15	X	You can store images inside a database.
  Neutral
There are plenty of tutorials on how to store images in a database (images in mysql php, images in mysql asp.net, etc).
  Positive
The language isn't important, any of the tutorials can help show how to store images inside the database itself.
  Negative
Then it's just a matter of language specific calling the image out to display it.
  Neutral
563e32a061a8013065267a16	X	Have you considered storing your images on something like Amazon S3?
  Negative
You could then access the images from both sites.
  Neutral
Somehow, though, you'll need to store the URLs to the images and make them available for both sites.
  Neutral
If both sites run off of the same database, this is fairly easy, however, if they are separate databases, then I would suggest creating some sort of API on the creation site so that the content site can easily discover which images are available to be shown.
  Negative
563e32a061a8013065267a17	X	The difference is only in definition.
  Neutral
A file is an entity inside a file system.
  Positive
Since e.g. S3 is not exactly a file system, they call their entities something else.
  Negative
You can think of them as the same.
  Neutral
563e32a161a8013065267a18	X	How do Walrus, S3 or any cloud storage system, take in a file and convert it to object programmatically?
  Negative
563e32a161a8013065267a19	X	If you're asking how does S3 lay its objects out on disk on its internal servers, we don't know (the implementation details are not public.)
  Negative
What they are very likely doing is taking your object's key (bucket + path) and consulting a consistent hash that maps your object to a set of servers.
  Negative
The upload is directed to one of these servers (essentially at random) which stores it and enqueues future work to propagate the new object to the other servers responsible for replicating it.
  Negative
This replication delay is the underlying need for eventual consistency.
  Positive
I also heard once somewhere that Amazon uses an error-correcting encoding at the storage level to further defray bad reads.
  Negative
563e32a161a8013065267a1a	X	To get the general understanding of how Cloud Object Storage systems stores the objects(Binary Files), you can read the documentation of Swift Object Storage-Openstack.
  Negative
Swift is similar to Amazon S3 and hence to Walrus.
  Negative
Swift communicates through the Proxy Server to Clients(Outside of the Cluster) and Client can store,delete the objects through the RESTful HTTP API.
  Very negative
The server maintains the Ring-Configuration file that maintains the mapping between files and their physical location.
  Negative
When there is a request to upload a file, MD5 hash is calculated from the path of the file.
  Negative
Please find the details at: Openstack Swift Architecture Swift Documentation
563e32a161a8013065267a1b	X	I'm not familiar with the Ruby SDK, but S3 only allows you to list 1000 objects at a time, so listing 100,000 objects is going to result in at least 100 HTTP requests.
  Negative
If you want to check for the existence of a particular object then sending a HEAD request for that object is the best way.
  Positive
It sounds like you want to check that one or more files match a given prefix, can you not just adapt your existing prefix search to include the sub dir name?
  Negative
563e32a161a8013065267a1c	X	hi, aws ping take 288 ms - 0.3 sec х 10000= 3000sec = 50min, i`ts very long.
  Negative
563e32a161a8013065267a1d	X	AWS-SDK list ruby code: fail error: "Unable to find marker in S3 list objects response" Structure of directory ... more of 100 000 obj I want to verify that the directory(for example "1474472") was created my plan: aws-s3-list-> ruby-array->find in array (array.include?)
  Negative
!!!
  Neutral
need very fast method - soon the end of the world :)
563e32a261a8013065267a1e	X	There is no such stuff as folders in Amazon S3.
  Negative
It is a "flat" file system.
  Negative
Have a look into this answer.
  Neutral
What you really are looking for is verifying whether a given prefix ("/myshop/products/1474472", for instance) exists in your bucket.
  Negative
Their REST API definitely supports it, have a look into the documentation.
  Positive
You need to list the keys (which would be the "file names") matching a given prefix, that can be passed as parameter.
  Negative
You can also optimize your call by setting the max-keys parameter to 1.
  Negative
That way, if you receive any non-zero amount of items in the response, the bucket already contains files with names starting with the given prefix.
  Negative
563e32a261a8013065267a1f	X	I have a file host website thats burning through 2gbit of bandwidth, so I need to start adding secondary media servers to store the files.
  Negative
What would be the best way to manage a multiple server setup, with a large amount of files?
  Positive
Preferably through php only.
  Neutral
Currently, I only have around 100Gb of files... so I could get a 2nd server, mirror all content between them, and then round robin the traffic 50/50, 33/33/33, etc.
  Very negative
But once the total amount of files grows beyond the capacity of a single server, this wont work.
  Negative
The idea that I had was to have a list of media servers stored in the DB with the amounts of free space left on each server.
  Negative
Once a file is uploaded, php will choose to which server the file is actually uploaded to, and spread out all the files evenly among the servers.
  Negative
Was hoping to get some more input/inspiration.
  Neutral
Cant use any 3rd party services like Amazon.
  Negative
The files range from several bytes to a gigabyte.
  Neutral
Thanks
563e32a261a8013065267a20	X	If you are doing as much data transfer as you say, it would seem whatever it is you are doing is growing quite rapidly.
  Neutral
It might be worth your while to contact your hosting provider and see if they offer any sort of shared storage solutions via iscsi, nas, or other means.
  Negative
Ideally the storage would not only start out large enough to store everything you have on it, but it would also be able to dynamically grow beyond your needs.
  Negative
I know my hosting provider offers a solution like this.
  Positive
If they do not, you might consider colocating your servers somewhere that either does offer a service like that, or would allow you install your own storage server (which could be built cheaply from off the shelf components and software like Freenas or Openfiler).
  Negative
Once you have a centralized storage platform, you could then add web-servers to your hearts content and load balance them based on load, all while accessing the same central storage repository.
  Negative
Not only is this the correct way to do it, it would offer you much more redundancy and expandability in the future if you endeavor continues to grow at the pace it is currently growing.
  Negative
The other solutions offered using a database repository of what is stored where, would work, but it not only adds an extra layer of complexity into the fold, but an extra layer of processing between your visitors and the data they wish to access.
  Neutral
What if you lost a hard disk, do you lose 1/3 or 1/2 of all your data?
  Negative
Should the heavy IO's of static content be on the same spindles as the rest of your operating system and application data?
  Negative
563e32a261a8013065267a21	X	You could try MogileFS.
  Negative
It is a distributed file system.
  Neutral
Has a good API for PHP.
  Positive
You can create categories and upload a file to that category.
  Positive
For each category you can define on how many servers it should be distributed.
  Negative
You can use the API to get a URL to that file on a random node.
  Negative
563e32a261a8013065267a22	X	Your best bet is really to get your files into some sort of storage that scales.
  Negative
Storing files locally should only be done with good reason (they are sensitive, private, etc.) Your best bet is to move your content into the cloud.
  Positive
Mosso's CloudFiles or Amazon's S3 will both allow you to store an almost infinite amount of files.
  Neutral
All your content is then accessible through an API.
  Negative
If you want, you can then use MySQL to track meta-data for easy searching, and let the service handle the actual storage of the files.
  Negative
563e32a261a8013065267a23	X	i think your own idea is not the worst one.
  Negative
get a bunch of servers, and for every file store which server(s) it's on.
  Negative
if new files are uploaded, use most-free-space first*.
  Negative
every server handles it's own delivery (instead of piping through the main server).
  Negative
pros: use multiple servers for a single file.
  Neutral
e.g. for cutekitten.jpg: filepath="server1\cutekitten.jpg;server2\cutekitten.jpg", and then choose the server depending on the server load (or randomly, or alternating, ...) if you're careful you may be able to move around files automatically depending on the current load.
  Negative
so if your cute-kitten image gets reddited/slashdotted hard, move it to the server with the lowest load and update the entry.
  Positive
you could do this with a cron-job.
  Neutral
just log the downloads for the last xx minutes.
  Negative
try some formular like (downloads-per-minute*filesize*(product of serverloads)) for weighting.
  Negative
pick tresholds for increasing/decreasing the number of servers those files are distributed to.
  Negative
if you add a new server, it's relativley painless (just add the address to the server pool) cons: homebrew solutions are always risky your load distribution algorithm must be well tested, otherwise bad things could happen (everything mirrored everywhere) constantly moving files around for balancing adds additional server load * or use a mixed weighting algorithm: free-space, server-load, file-popularity disclaimer: never been in the situation myself, just guessing.
  Negative
563e32a361a8013065267a24	X	Thanks, I know about the existence of OGM, but you are correct, I should be more specific.
  Negative
I will create a new post with more detailed information about my problem.
  Positive
Thank you for your time!
  Positive
karma++!
  Positive
563e32a361a8013065267a25	X	After checking Hibernate and discovering that Hibernate support relational databases (Like MySQL) I would like to know if it support Amazon RDS.
  Negative
I am assuming that it does because of the ORM thing, but since I am very newbie I decided to come here and ask to the people who know.
  Negative
Assuming that Hibernate can read from my MySQL database, I would also like to know if there is a way for it to actually support key-value database, like Amazon S3.
  Negative
I searched but I simply couldn't find anything else.
  Negative
Finally, can someone let me know the level of support that Hibernate has for MySQL joins?
  Neutral
I would like to know if it supports them and how much of it is supported.
  Negative
563e32a361a8013065267a26	X	if there is a way for it to actually support key-value database, like Amazon S3 There's a separate project for this, called Hibernate OGM.
  Negative
It aims to support JPA for NoSQL databases.
  Negative
Check it out.
  Neutral
can someone let me know the level of support that Hibernate has for MySQL joins?
  Negative
I would like to know if it supports them and how much of it is supported Joins are definitely supported through Criteria API, and definitely in HQL.
  Negative
You need to be more specific on what you really want to achieve in order to get a more complete answer.
  Neutral
563e32a361a8013065267a27	X	How can I create that kind of server ?
  Positive
In Visual Studio, choose File => New Project.
  Negative
Then write the code.
  Neutral
563e32a361a8013065267a28	X	what do you mean ?
  Neutral
563e32a361a8013065267a29	X	This question is way too broad and prone for opinion-based answers.
  Negative
563e32a361a8013065267a2a	X	what do you mean ?
  Neutral
Roll up your sleeves and get to it
563e32a361a8013065267a2b	X	How can I "let the cloud client deal with it" ?
  Negative
563e32a361a8013065267a2c	X	@user2992413 did you ever use dropbox or google drive?
  Negative
When you install their client application it creates local folder and automatically synchronizes it with server.
  Positive
You just need to copy the file.
  Negative
In case it's too big and you want to make sure that it has been uploaded before your user shuts down the pc, you might be needing to add some code to determine upload status.
  Negative
563e32a461a8013065267a2d	X	But I want the program to do it by itself
563e32a461a8013065267a2e	X	then use API dropbox.com/developers/core/docs#files_put with 'HttpClient' msdn.microsoft.com/en-us/library/…
563e32a461a8013065267a2f	X	I made a program that saves information on .
  Negative
dat files.
  Neutral
On each time the program is closed the old information is replaced by the new information.
  Negative
When I open the program, it reads the information from the dat files.
  Negative
I want to create a server that I will be able to upload the .
  Positive
dat files to it and on each Form_Close the old information will be deleted from the sever and the new information will be uploaded to the server.
  Negative
When I open the program, I want it to delete the .
  Negative
dat files on the computer and replace them by the information from the server.
  Negative
What is the best choise of service for that kind of problem ?
  Neutral
I thought maybe I should use Google Drive.
  Negative
But I don't know if it's a good choise.
  Negative
Thanks for helping.
  Neutral
563e32a461a8013065267a30	X	You have tons of options here, which one is better depends on your project and goals, but all of them will work.
  Positive
563e32a461a8013065267a31	X	My client has requested for his database to be stored such that he can access via the Android and iOS versions of his mobile software.
  Negative
Is Amazon Web Services a good option for this?
  Neutral
I'm looking into something that is quick, yet scalable.
  Neutral
Thanks for your advice.
  Positive
Pier.
  Neutral
563e32a561a8013065267a32	X	Just to answer this, I implemented it eventually using Amazon SimpleDB and S3, the API is pretty easier to get into.
  Negative
563e32a561a8013065267a33	X	I'm using the aws-sdk gem and I'm trying to basically upload a really large file (it takes 2 days).
  Positive
The file is uploaded in chunks but sometimes the script will crash and I would like to resume uploading (the next chunk).
  Negative
During uploading we want to close close the multipart upload (so we can access the s3 data that has been uploaded so far).
  Negative
Is it possible to add a part after the multipart upload is closed?
  Neutral
(say the next day) basically resuming the upload?
  Neutral
563e32a661a8013065267a34	X	Is it possible to add a part after the multipart upload is closed?
  Neutral
(say the next day) basically resuming the upload?
  Neutral
Not as such, but you can simulate the affect you desire.
  Negative
Uploading Objects Using Multipart Upload API allows you to upload a single object as a set of parts: Each part is a contiguous portion of the object's data.
  Negative
You can upload these object parts independently and in any order.
  Positive
If transmission of any part fails, you can retransmit that part without affecting other parts.
  Negative
After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object.
  Positive
[emphasis mine] This is further detailed in Complete Multipart Upload: You first initiate the multipart upload and then upload all parts using the Upload Parts operation (see Upload Part).
  Negative
[...] Upon receiving this request, Amazon S3 concatenates all the parts in ascending order by part number to create a new object.
  Negative
[...] You must ensure the parts list is complete, this operation concatenates the parts you provide in the list.
  Negative
[...] [emphasis mine] That is, the upload operation is finalized here and cannot be resumed by uploading another part.
  Negative
(Technically speaking the upload ID required for any operation on an initiated multipart upload is not available/valid anymore).
  Negative
You can simple initiate a new multipart upload and upload your previously uploaded S3 object as the first part of this new multipart object by means of the Upload Part - Copy operation, which Uploads a part by copying data from an existing object as data source.
  Negative
563e32a661a8013065267a35	X	Champion!
  Positive
I upvote this,because it is the thing i was searahing for!
  Negative
With the use of httpwebrequest from c# you can EASY upload an image and get the link to it :) thnx!
  Positive
563e32a661a8013065267a36	X	Hi!
  Positive
I'm trying to use deviantsart.com API but I keep getting a XMLHttpRequest cannot load http://deviantsart.com/.
  Negative
No 'Access-Control-Allow-Origin' header is present on the requested resource.
  Negative
Origin 'http://localhost:8080' is therefore not allowed access.
  Negative
I'm doing a POST using jQuery and image data gotten from a canvas.
  Negative
Do you know what may be wrong?
  Negative
Thanks!
  Positive
563e32a661a8013065267a37	X	check out enable-cors.
  Negative
org/index.
  Neutral
html
563e32a661a8013065267a38	X	What is the format you have to send the image?
  Neutral
a byte array?
  Neutral
563e32a661a8013065267a39	X	Any POST with file is acceptable, i.e. via curl cli: ``` curl -X POST deviantsart.com -F my_file=@/path/to/file.jpg ```
563e32a661a8013065267a3a	X	Interesting... just got a drive-by downvote on this 4 year old question with no comment stating just what might be wrong with it.
  Negative
563e32a661a8013065267a3b	X	...You're on 70k(!)
  Negative
and you bother to remark on one downvote?
  Negative
My goodness.
  Neutral
I'm on 14k on my best SE site, and even I don't complain any more.
  Negative
563e32a761a8013065267a3c	X	@NickWiggill: If someone found a problem with this answer, they should share what that problem is.
  Negative
It's not a question of reputation.
  Negative
563e32a761a8013065267a3d	X	...And yet as the rules of site have it, nobody is under any obligation to.
  Negative
Just like numerous other flaws in the system, such as upvote leads on early answers, etc. etc.
  Negative
Call me a realist: It's not going to change.
  Negative
Some people simply troll, that's how they roll.
  Negative
P.S. Your answer looked fine to me, but it's probably the fact that you said "I have not used that particular service" that garnered the unwanted attention.
  Neutral
563e32a761a8013065267a3e	X	@NickWiggill: No obligation certainly, but like the FAQ used to say If you see misinformation, vote it down.
  Negative
Add comments indicating what, specifically, is wrong.
  Negative
Provide better answers of your own.
  Neutral
Best of all edit and improve the existing questions and answers!
  Positive
Words to live by.
  Neutral
PS - Almost all answers got down-voted, without comment.
  Negative
563e32a761a8013065267a3f	X	Flickr is somewhat strict as it comes to commercial usage of their service and they will remove all your images once they discovery they are used to host images from a service that e.g. has ads.
  Negative
563e32a761a8013065267a40	X	I've been considering image hosting services for a project but am starting to wonder if that's just too complicated for my target audience as they'd have to upload all their images to the hosting service and then "attach" the images to the CSS file using the links the hosting service provides them.
  Negative
While that's a fairly simple process for us developers, I'm thinking that might be a large barrier to getting user buy-in for this feature.
  Negative
I could simplify by hosting and serving the images myself but I'm worried about potential scalability issues that could present which I don't have the hardware or bandwidth to handle at the present time.
  Negative
My thought is that I could have users upload their images and CSS to the server in a single zip file to the web server which could then extract the files from the zip, push the images on to an image hosting service, programmatically get the corresponding URL from the service and update the CSS accordingly before attaching it to the user's display profile.
  Negative
This approach could kill both birds with one stone, I wouldn't have to worry about the bandwidth issues caused by serving potentially large images on every profile request and the user doesn't have to go through the headache of needing to set up an account on an image hosting service.
  Very negative
Does anyone know of any image hosting services that I can programmatically integrate with that has good reliability and performance that could assist me with this conundrum?
  Neutral
Thanks in advance
563e32a761a8013065267a41	X	Review the Picasa Web Albums Data API: If you've signed up for Google+ then photos up to 2048x2048 pixels and videos up to 15 minutes won't count towards your free storage.
  Negative
563e32a761a8013065267a42	X	http://deviantsart.com has a public and easy to use API just HTTP POST the image to their domain and you will get a json with the url
563e32a861a8013065267a43	X	I used https://cloudinary.com/ and found it pretty neat!
  Negative
563e32a861a8013065267a44	X	You might consider Amazon CloudFront.
  Negative
I have not used that particular service, but I have used Amazon EC2 and S3 extensively and am quite happy.
  Negative
UPDATE: I recently used CloudFront for a video hosting project and found it quite simple to setup and use.
  Negative
563e32aa61a8013065267a45	X	Check out Flickr's API: http://www.flickr.com/services/api/
563e32ab61a8013065267a46	X	Imgur has an API.
  Negative
From the "Overview": Imgur's API exposes the entire Imgur infrastructure via a standardized programmatic interface.
  Negative
Using Imgur's API, you can do just about anything you can do on imgur.com, while using your programming language of choice.
  Negative
The Imgur API is a RESTful API based on HTTP requests and XML or JSON(P) responses.
  Negative
If you're familiar with the APIs of Twitter, Amazon's S3, del.icio.us, or a host of other web services, you'll feel right at home.
  Positive
This version of the API, version 3, uses OAuth 2.0.
  Negative
This means that all requests will need to be encrypted and sent via SSL/TLS to https://.
  Negative
It also means that you need to register your application, even if you aren't allowing users to login.
  Negative
563e32ac61a8013065267a47	X	What!?
  Neutral
In the name of Trident, I demand the long story!
  Positive
563e32ac61a8013065267a48	X	I am serving files from Amazon S3 now, and I generate a secure URL using the API.
  Negative
Works great everywhere except for people downloading .
  Positive
exe files in IE.
  Negative
I have tested this on IE 8 and 7.
  Neutral
If running a local webserver you can test by putting notepad.exe in your web root.
  Negative
Go to http://localhost/notepad.exe (or equivalent) Now try http://localhost/notepad.exe?
  Negative
It should save the file as notepad, without extension.
  Negative
Is this a 'feature' because googling it is coming up with nothing.
  Negative
Thanks to the whole issue of IE extensions, you can't search for anything on file extensions.
  Negative
Also, if the file has multiple periods in the name, it sometimes gets a [1] or [] added to the end.
  Negative
Any ideas?
  Neutral
Docs on this terrible behavior?
  Negative
It seems like it must be a security feature, but I have yet to find an option to disable it.
  Negative
And as always, thank you.
  Positive
Tim
563e32ac61a8013065267a49	X	There's a long story here, but the simple workaround is to do this: http://www.example.com/dl/test.exe?mysecret=12321412&FixForIE=.exe As for the trailing [1] or whatnot, no, there's not really anything you can do about that if the user happens to have downloaded from that URL before.
  Negative
563e32ac61a8013065267a4a	X	We had the same problem when serving files from S3.
  Negative
Turns out you need to set the content-disposition correctly for IE to handle the files correctly.
  Negative
Namely, the HTTP header Content-Disposition: attachment; filename="text.exe" This article describes in a little more detail: http://www.jtricks.com/bits/content_disposition.html
563e32ad61a8013065267a4b	X	I have a couple of questions, and the first one involves the integration of a Google or Bing map.
  Negative
I am trying to get multiple markers to appear on a map based on the results of a user-submitted property search form.
  Negative
When the user searches for properties from any combination of available criteria (Address, city, Zip code, number of garages, etc.), they are taken to a results page that shows ten matches per page.
  Negative
The map needs to be able to mark the location of those properties as they are being viewed on the results page.
  Neutral
How can this be accomplished in the scripting of the map and/or the search form?
  Neutral
My second question involves storing images on a database.
  Neutral
We need to download a large number of images from a listing server, but unfortunately our server is not large enough to support easily tens of thousands of image files.
  Negative
How can the images be stored on a relational database with PHP and SQL queries?
  Neutral
How can SQL be used to convert the actual images to data for easier storage?
  Neutral
Thank you for any answers you have!
  Positive
563e32ad61a8013065267a4c	X	The basic workflow would be to query your database in a loop and output the address information together with their latitude and longitude to your HTML page.
  Negative
Then you would simply add multiple markers to your map and style the results, markers and infobubbles to your needs.
  Neutral
A simple example on how to add a single marker can be found in the Google Maps API Documentation.
  Positive
Are you sure you need to store the images itself in a database?
  Negative
Perhaps it is also possible to store them on a filesystem or on another location like Amazon S3.
  Neutral
You may want to read this posting for a detailed discussion.
  Neutral
563e32ad61a8013065267a4d	X	Is there a way to bulk upload images to my cloudinary account?
  Negative
I am looking to import 100 images of 3MB each at a time.
  Negative
Thank you.
  Positive
563e32ad61a8013065267a4e	X	You can use Cloudinary's upload API to upload images one by one.
  Negative
Here is a sample upload code in Python.
  Neutral
If your images are already in a public location, you can specify the remote HTTP URL as the file parameter instead of sending the actual image's data.
  Negative
This allows much faster uploading.
  Positive
If your images are in an Amazon S3 bucket, images can be fetched by Cloudinary directly from S3 for reaching even higher upload performance.
  Negative
You can also run your upload code using multiple processes in parallel for quickly uploading multiple files simultaneously.
  Negative
In our Ruby on Rails client library we included a migration tool.
  Negative
Currently there is no dedicated API method of Cloudinary for performing bulk upload of images.
  Negative
563e32ad61a8013065267a4f	X	Easiest way is to use the remote API - and Just pass the url reference to the account and Cloudinary will connect to the image and download it into your account.
  Negative
http://cloudinary.com/documentation/upload_images#remote_upload
563e32ad61a8013065267a50	X	This is for business use, We are testing DRDB on GFS, looking into Gluster,but that requires client pieces,We've played around with ZFS on Solaris (and we are looking at AVS for availability), but Solaris keeps having small problems to be over come.
  Very negative
We have also started looking at windows DFS.
  Negative
563e32ae61a8013065267a51	X	We are planning to scale up over the next few years to 5-10 TB, but nothing huge.
  Negative
It is a production facility, we could spend 15-30 minutes switching over if we had too, but we want to minimize data loss.
  Negative
Our process are almost 24 x 6, so want to minimize the 3 am calls, would prefer HA
563e32ae61a8013065267a52	X	There needs to be a sort of badge out there for this sort of post.
  Negative
563e32ae61a8013065267a53	X	S3 does not have particularly fantastic availability.
  Negative
It is great in many ways, but does not fit the "high availability" requirement the OP is asking for.
  Neutral
563e32ae61a8013065267a54	X	I would like to make 2 TB or so available via NFS and CIFS.
  Negative
I am looking for a 2 (or more) server solution for high availability and the ability to load balance across the servers if possible.
  Negative
Any suggestions for clustering or high availability solutions?
  Negative
This is business use, planning on growing to 5-10 TB over next few years.
  Negative
Our facility is almost 24 hours a day, six days a week.
  Negative
We could have 15-30 minutes of downtime, but we want to minimize data loss.
  Negative
I want to minimize 3 AM calls.
  Neutral
We are currently running one server with ZFS on Solaris and we are looking at AVS for the HA part, but we have had minor issues with Solaris (CIFS implementation doesn't work with Vista, etc) that have held us up.
  Negative
We have started looking at We are looking for a "black box" that serves up data.
  Negative
We currently snapshot the data in ZFS and send the snapshot over the net to a remote datacenter for offsite backup.
  Negative
Our original plan was to have a 2nd machine and rsync every 10 - 15 min.
  Negative
The issue on a failure would be that ongoing production processes would lose 15 minutes of data and be left "in the middle".
  Very negative
They would almost be easier to start from the beginning than to figure out where to pickup in the middle.
  Negative
That is what drove us to look at HA solutions.
  Neutral
563e32ae61a8013065267a55	X	I've recently deployed hanfs using DRBD as the backend, in my situation, I'm running active/standby mode, but I've tested it successfully using OCFS2 in primary/primary mode too.
  Very negative
There unfortunately isn't much documentation out there on how best to achieve this, most that exists is barely useful at best.
  Negative
If you do go along the drbd route, I highly recommend joining the drbd mailing list, and reading all of the documentation.
  Negative
Here's my ha/drbd setup and script I wrote to handle ha's failures:   DRBD8 is required - this is provided by drbd8-utils and drbd8-source.
  Negative
Once these are installed (I believe they're provided by backports), you can use module-assistant to install it - m-a a-i drbd8.
  Negative
Either depmod -a or reboot at this point, if you depmod -a, you'll need to modprobe drbd.
  Negative
You'll require a backend partition to use for drbd, do not make this partition LVM, or you'll hit all sorts of problems.
  Negative
Do not put LVM on the drbd device or you'll hit all sorts of problems.
  Negative
Hanfs1: We must now perform an initial synchronization of data - obviously, if this is a brand new drbd cluster, it doesn't matter which node you choose.
  Negative
Once done, you'll need to mkfs.yourchoiceoffilesystem on your drbd device - the device in our config above is /dev/drbd1.
  Negative
http://www.drbd.org/users-guide/p-work.html is a useful document to read while working with drbd.
  Neutral
Heartbeat Install heartbeat2.
  Neutral
(Pretty simple, apt-get install heartbeat2).
  Negative
/etc/ha.
  Neutral
d/ha.
  Neutral
cf on each machine should consist of: hanfs1: I wrote a wrapper script to deal with the idiosyncracies caused by nfs and drbd in a failover scenario.
  Negative
This script should exist within /etc/ha.
  Negative
d/resources.
  Neutral
d/ on each machine.
  Neutral
Then it's just a case of starting up heartbeat on both machines and issuing hb_takeover on one of them.
  Neutral
You can test that it's working by making sure the one you issued the takeover on is primary - check /proc/drbd, that the device is mounted correctly, and that you can access nfs.
  Negative
-- Best of luck man.
  Positive
Setting it up from the ground up was, for me, an extremely painful experience.
  Negative
563e32b061a8013065267a56	X	These days 2TB fits in one machine, so you've got options, from simple to complex.
  Positive
These all presume linux servers: There are also plenty of commercial solutions, but 2TB is a bit small for most of them these days.
  Negative
You haven't mentioned your application yet, but if hot failover isn't necessary, and all you really want is something that will stand up to losing a disk or two, find a NAS that support RAID-5, at least 4 drives, and hotswap and you should be good to go.
  Negative
563e32b061a8013065267a57	X	I would recommend NAS Storage.
  Negative
(Network Attached Storage).
  Negative
HP has some nice ones you can choose from.
  Positive
http://h18006.www1.hp.com/storage/aiostorage.html as well as Clustered versions: http://h18006.www1.hp.com/storage/software/clusteredfs/index.html?jumpid=reg_R1002_USEN
563e32b161a8013065267a58	X	Are you looking for an "enterprise" solution or a "home" solution?
  Negative
It is hard to tell from your question, because 2TB is very small for an enterprise and a little on the high end for a home user (especially two servers).
  Negative
Could you clarify the need so we can discuss tradeoffs?
  Negative
563e32b161a8013065267a59	X	There's two ways to go at this.
  Positive
The first is to just go buy a SAN or a NAS from Dell or HP and throw money at the problem.
  Negative
Modern storage hardware just makes all of this easy to do, saving your expertise for more core problems.
  Negative
If you want to roll your own, take a look at using Linux with DRBD.
  Neutral
http://www.drbd.org/ DRBD allows you to create networked block devices.
  Positive
Think RAID 1 across two servers instead of just two disks.
  Neutral
DRBD deployments are usually done using Heartbeat for failover in case one system dies.
  Negative
I'm not sure about load balancing, but you might investigate and see if LVS can be used to load balance across your DRBD hosts: http://www.linuxvirtualserver.org/ To conclude, let me just reiterate that you're probably going to save yourself a lot of time in the long run just forking out the money for a NAS.
  Negative
563e32b161a8013065267a5a	X	I assume from the body of your question is you're a business user?
  Negative
I purchased a 6TB RAID 5 unit from Silicon Mechanics and have it NAS attached and my engineer installed NFS on our servers.
  Negative
Backups performed via rsync to another large capacity NAS.
  Positive
563e32b161a8013065267a5b	X	Your best bet maybe to work with experts who do this sort of thing for a living.
  Positive
These guys are actually in our office complex...I've had a chance to work with them on a similar project I was lead on.
  Positive
http://www.deltasquare.com/About
563e32b161a8013065267a5c	X	Have a look at Amazon Simple Storage Service (Amazon S3) http://www.amazon.com/S3-AWS-home-page-Money/b/ref=sc_fe_l_2?ie=UTF8&node=16427261&no=3435361&me=A36L942TSJ2AJA -- This may be of interest re.
  Negative
High Availability Dear AWS Customer: Many of you have asked us to let you know ahead of time about features and services that are currently under development so that you can better plan for how that functionality might integrate with your applications.
  Negative
To that end, we are excited to share some early details with you about a new offering we have under development here at AWS -- a content delivery service.
  Positive
This new service will provide you a high performance method of distributing content to end users, giving your customers low latency and high data transfer rates when they access your objects.
  Positive
The initial release will help developers and businesses who need to deliver popular, publicly readable content over HTTP connections.
  Positive
Our goal is to create a content delivery service that: Lets developers and businesses get started easily - there are no minimum fees and no commitments.
  Negative
You will only pay for what you actually use.
  Neutral
Is simple and easy to use - a single, simple API call is all that is needed to get started delivering your content.
  Neutral
Works seamlessly with Amazon S3 - this gives you durable storage for the original, definitive versions of your files while making the content delivery service easier to use.
  Positive
Has a global presence - we use a global network of edge locations on three continents to deliver your content from the most appropriate location.
  Positive
You'll start by storing the original version of your objects in Amazon S3, making sure they are publicly readable.
  Negative
Then, you'll make a simple API call to register your bucket with the new content delivery service.
  Positive
This API call will return a new domain name for you to include in your web pages or application.
  Positive
When clients request an object using this domain name, they will be automatically routed to the nearest edge location for high performance delivery of your content.
  Negative
It's that simple.
  Positive
We're currently working with a small group of private beta customers, and expect to have this service widely available before the end of the year.
  Negative
If you'd like to be notified when we launch, please let us know by clicking here.
  Positive
Sincerely, The Amazon Web Services Team
563e32b161a8013065267a5d	X	May I suggest you visit the F5 site and check out http://www.f5.com/solutions/virtualization/file/
563e32b261a8013065267a5e	X	According to MSDN, an azure service can conatins any number of worker roles.
  Negative
According to my knowledge a worker role can be recycled at any time by Windows Azure Fabric.
  Negative
If it is the true, then: But i want to make a service which conatains client data and do not want to use Azure storage service.
  Negative
How I can accomplish this?
  Neutral
563e32b261a8013065267a5f	X	The velocity (whatever it is called) component of AppFabric is a distributed cache and can be used in these situations.
  Negative
563e32b261a8013065267a60	X	Azure's web and compute roles are stateless means all its local data is volatile and if you want to maintain the state you need to use some external resource to maintain that state and logic in your app to handle that.
  Negative
For simplicity you can use Azure drive but again internally its a blob storage.
  Negative
563e32b261a8013065267a61	X	You can write to local storage on the worker role by using the standard file IO APIs - but this will be erased upon instance shutdown.
  Negative
You could also use SQL Azure, or post your data off to another storage service by HTTP (e.g. Amazon S3, or your own server).
  Negative
However, this is likely to have performance implications.
  Neutral
Depending on how much data you'll be storing, how frequently, and how big it is, you might be better off with Azure Storage!
  Negative
Why don't you want to use Azure Storage?
  Negative
563e32b261a8013065267a62	X	If the data could be stored in Azure you have a good number of choices: Azure distributed cache, SQL Azure, blob, table, queue, or Azure Drive.
  Positive
It sounds like you need persistence, but can't use any of these Azure storage mechanisms.
  Negative
If data security is the problem, could you encrypt/hashing the data?
  Negative
Understanding why would be useful.
  Negative
One alternative might be not persist at all, by chaining/nesting synchronous web service calls together, thus achieving reliable messaging.
  Negative
Another might be to use Azure Connect to domain join Azure compute resource to your local data centre (if you have one), and use you on-premise storage.
  Negative
563e32b261a8013065267a63	X	Have you tried using the list of CommonPrefixes returned from ListBucket?
  Negative
docs.aws.amazon.com/AmazonS3/latest/dev/…
563e32b361a8013065267a64	X	Originally I couldn't find the common prefixes anywhere, but maybe I was overlooking that during my object dumps in the wrong places.
  Negative
I just found them now.
  Neutral
Thanks for the help :)
563e32b361a8013065267a65	X	Given my S3 bucket that contains images in a structure like so: where root is my bucket, and there are no other files in my root, just those folders (objects), how do I retrieve a list of just those objects?
  Negative
I am familiar with using the Delimiter and Prefix in the ListObjects call.
  Negative
If I do the following, I get no results: If I don't use a Delimiter, I get everything, obviously.
  Negative
I cannot use a prefix because the objects I desire are root-level.
  Negative
Otherwise, I have no problem using the prefix to say, list just the files in 'portraits/' From my searches, I've only managed to find solutions from previous years that only apply to the aws php sdk v1 or v2, and I have had no luck in trying those (v3 is quite different) Any suggestions?
  Negative
I feel like I'm missing something simple, but searching through the documentation, I can't find anything to help me.
  Negative
As a last resort, I'll just have to stick with manually declaring an array But that isn't ideal in the case where I want to add more categories in the future, and not have to worry about adding another category manually.
  Negative
Any help would be greatly appreciated :) Edit - Solution I must have been looking in the wrong places during my object dumps, but eventually saw the Common Prefixes in the returned result from a ListObjects call with a delimiter of '/', like so:
563e32b361a8013065267a66	X	Directories do not actually exist in Amazon S3.
  Negative
However, the Management Console allows the creation of folders, and paths are supported to give the illusion of directories.
  Neutral
For example, object bar.jpg stored in the foo directory has a path of /foo/bar.
  Negative
jpg.
  Neutral
The trick is that the object is actually called foo/bar.
  Neutral
jpg rather than just bar.jpg.
  Negative
Most users wouldn't even notice the difference.
  Negative
From the API, the ability to list directories is provided via the concept of CommonPrefixes, which look the same as directory paths and consist of the portion of object names ('keys') before the final slash.
  Negative
See: Listing Keys Hierarchically Using a Prefix and Delimiter
563e32b361a8013065267a67	X	Wonderful!
  Positive
This is the solution for direct S3 uploads.
  Negative
Thanks Simon!
  Positive
563e32b461a8013065267a68	X	how do i force it to substitute the original file one i try to reupload a new file??
  Negative
right now its creating name_1, name_2 etc
563e32cb61a8013065267a69	X	I am using this file storage engine to store files to Amazon S3 when they are uploaded: http://code.welldev.org/django-storages/wiki/Home It takes quite a long time to upload because the file must first be uploaded from client to web server, and then web server to Amazon S3 before a response is returned to the client.
  Negative
I would like to make the process of sending the file to S3 asynchronous, so the response can be returned to the user much faster.
  Negative
What is the best way to do this with the file storage engine?
  Positive
Thanks for your advice!
  Positive
563e32cb61a8013065267a6a	X	I've taken another approach to this problem.
  Negative
My models have 2 file fields, one uses the standard file storage backend and the other one uses the s3 file storage backend.
  Negative
When the user uploads a file it get's stored localy.
  Negative
I have a management command in my application that uploads all the localy stored files to s3 and updates the models.
  Negative
So when a request comes for the file I check to see if the model object uses the s3 storage field, if so I send a redirect to the correct url on s3, if not I send a redirect so that nginx can serve the file from disk.
  Negative
This management command can ofcourse be triggered by any event a cronjob or whatever.
  Negative
563e32cb61a8013065267a6b	X	It's possible to have your users upload files directly to S3 from their browser using a special form (with an encrypted policy document in a hidden field).
  Negative
They will be redirected back to your application once the upload completes.
  Negative
More information here: http://developer.amazonwebservices.com/connect/entry.jspa?externalID=1434
563e32cc61a8013065267a6c	X	There is an app for that :-) https://github.com/jezdez/django-queued-storage It does exactly what you need - and much more, because you can set any "local" storage and any "remote" storage.
  Negative
This app will store your file in fast "local" storage (for example MogileFS storage) and then using Celery (django-celery), will attempt asynchronous uploading to the "remote" storage.
  Negative
Few remarks: The tricky thing is - you can setup it to copy&upload, or to upload&delete strategy, that will delete local file once it is uploaded.
  Negative
Second tricky thing - it will serve file from "local" storage until it is not uploaded.
  Neutral
It also can be configured to make number of retries on uploads failures.
  Negative
Installation & usage is also very simple and straightforward: append to INSTALLED_APPS: in models.py:
563e32cc61a8013065267a6d	X	You could decouple the process: [*: In case you have only a shared hosting you could possibly build some solution which uses an hidden Iframe in the users browser to start a script which then uploads the file to S3]
563e32cc61a8013065267a6e	X	You can directly upload media to the s3 server without using your web application server.
  Negative
See the following references: Amazon API Reference : http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?UsingHTTPPOST.html A django implementation : https://github.com/sbc/django-uploadify-s3
563e32cc61a8013065267a6f	X	As some of the answers here suggest uploading directly to S3, here's a Django S3 Mixin using plupload: https://github.com/burgalon/plupload-s3mixin
563e32cc61a8013065267a70	X	Is the intention to display them in a browser?
  Negative
563e32cc61a8013065267a71	X	yes, the intention is to display the objects
563e32cc61a8013065267a72	X	For a while I have been building a program that decompiles Adobe flash .
  Negative
swf files.
  Neutral
I have stored them in aws s3, and have keys stored in a local file that helps me locate them.
  Negative
Essentially I am using them as a database of sorts.
  Negative
An example of the keys I have are: In the above case these are jpg files that are stored.
  Positive
I search through my keys and find those that I want.
  Negative
For example I might want an ad hosted on amazon that is a jpg.
  Negative
When I run something like this in my Api Flask file: I would like to have the page .../getToday to display each object.
  Negative
I have the following types of files: .
  Neutral
as (actionscript) .
  Neutral
svg .
  Neutral
swf .
  Neutral
jpg I would like them to each render correctly.
  Negative
I assume I may need to check each key and then display each type differently.
  Negative
So when I go to .../getToday I would like to see all of the objects in a visual form.
  Negative
When I am using jsonify I am getting the following error: ValueError: dictionary update sequence element #0 has length 1; 2 is required But what can I use that isn't jsonify (just text) to see the images render?
  Negative
563e32d161a8013065267a73	X	Sigh...this has been asked and answered about 50 kajillion gazillion times on this site...
563e32d161a8013065267a74	X	I've tried searching for this, Perhaps you could link me too a previous question?
  Very negative
563e32d261a8013065267a75	X	what query you tried to search for?
  Negative
563e32d261a8013065267a76	X	stackoverflow.com/questions/1257488/…
563e32d261a8013065267a77	X	You know, it's even no need to bother with search.
  Negative
I tried to start another question with your title and got a dozen suggestions immediately.
  Negative
Didn't you notice it?
  Negative
563e32d261a8013065267a78	X	Databases work well with data files?
  Negative
563e32d261a8013065267a79	X	@Nicklamort -- yes that is what the BLOB SQL datatype is (Binary Large OBject) here is a link with an interesting description: [link]searchsqlserver.techtarget.com/definition/BLOB
563e32d261a8013065267a7a	X	Ah, I see.
  Negative
Nice man +1 =P
563e32d261a8013065267a7b	X	i always like downvotes without comment.
  Negative
563e32d361a8013065267a7c	X	I am curious also about CSS and js files.
  Negative
pdf, swf and, say, xls are counted too.
  Negative
Is it good to store just whole site contents in a database to make filesystem backups really unnecessary?
  Negative
Are them filesystem backups that evil?
  Negative
563e32d361a8013065267a7d	X	its another thing you need to take care of, monitor, program and maintain.
  Negative
if you just have IMAGES (as stated in the question) and there are not much (as stated as condition in my answer) it MAY be EASIER.
  Negative
i never used the word IS and gave a valid example.
  Neutral
imagine you have a website with big articles that is only text based.
  Negative
and now you want a little image to show up as a preview on facebook.
  Neutral
do you really want to set up a filesytem solution for that with all that comes along with it?
  Positive
563e32d361a8013065267a7e	X	So, these poor pdf's illustrating some articles are doomed to be unsafe.
  Negative
What a pity...
563e32d361a8013065267a7f	X	umm another one file type just came to my mind.
  Negative
php files.
  Neutral
let's say it's just ordinal hosting with no cvs to deal with sources.
  Neutral
is it good idea to store source files in a database too?
  Neutral
just to make sure that no filesystem backup needed ever?
  Negative
ugh, and web-server configs!
  Negative
And php binaries!
  Positive
Whoa, it's really amazing idea to store them all in a database!
  Positive
563e32d361a8013065267a80	X	Possible Duplicate: MySQL - storing images in a database??
  Negative
I'm working on a profile system for my webpage, However I was wondering what the best way to store images are.
  Positive
The first method I have read about is using BLOB in mysql.
  Negative
The second would be how I planned to do it in the first place, First i would get the image from a upload script, then giving the picture a id (md5) then renaming and move the picture to a folder named "md5".
  Negative
jpg I was wondering what the best option is.
  Positive
563e32d361a8013065267a81	X	I have seen this question pop up before, and the generally accepted answer is to store your images in a folder, and then store the URL in the database.
  Negative
I think it is technically possible to store an image in a database using a BLOB, but from the documentation and consensus that I've read online, this is not a ideal scenario.
  Negative
Databases tend to work well with Numbers, Strings and data files.
  Negative
Here are some links on this site for the question you are asking: Storing images on a database Storing Images in DB - Yea or Nay?
  Neutral
563e32d361a8013065267a82	X	From Choosing data type for MySQL?
  Negative
MySQL is incapable of working with any data that is larger than max_allowed_packet (default: 1M) in size, unless you construct complicated and memory intense workarounds at the server side.
  Negative
This further restricts what can be done with TEXT/BLOB-like types, and generally makes the LARGETEXT/LARGEBLOB type useless in a default configuration.
  Negative
So careful with your blobs I like just storing the image on the HDD and a path/id in the DB.
  Negative
The advantages of storing in a db are very small, and it can be slower.
  Positive
563e32d361a8013065267a83	X	both are good.
  Positive
but i have always opted to using the actual file with giving the image an id.
  Positive
This is because your server and php has to do more work for a blob.
  Negative
563e32d361a8013065267a84	X	The best idea is probably to store the actual file on the web/file server and only store the link to that file in the database.
  Negative
This keeps your database server from getting bogged down with unnecessary busy work.
  Negative
563e32d361a8013065267a85	X	Check out http://transloadit.com, it's an service for uploading and storing (Amazon Cloud) images.
  Negative
563e32d361a8013065267a86	X	database storing can have a some advantages that are often forgotten.
  Negative
you can backup your images along with your database, no filesystem backups needed for this.
  Negative
it may be easier to scale when your site gets heavy traffic of course its not suitable if you have a lot of images.
  Negative
like user contributed content etc. but if you have a little newspaper site with not many articles online, you can perfectly store thumbnails in a database and build your system upon replication slaves.
  Negative
however... honestly: store your images on amazon s3.
  Positive
simple api, redundant, highly available and loadbalanced storage for cheap.
  Negative
563e32d361a8013065267a87	X	I'd recommend saving them as regular static files.
  Negative
Then store their file names within your database.
  Neutral
You can pull out the matching file name and either file_get_contents the image data or link directly to the real image file.
  Neutral
Don't store the image data itself within the DB.
  Neutral
563e32d361a8013065267a88	X	I would like to use node.js to download files from s3.
  Negative
The uploaded files were decrypted using java AmazonS3Encryption client.
  Negative
I could not find however equal api for node.js amazon client.
  Negative
The only possibly I see is downloading the files and decrypting them locally.
  Negative
Any ideas?
  Neutral
563e32d461a8013065267a89	X	If you're going to downvote, can you explain why?
  Negative
563e32d461a8013065267a8a	X	I'd like to use Google Cloud Storage to pull down some files for a project I'm building automatically through Docker.
  Negative
To do this, I'd like to do the authentication entirely through commands without opening browser windows (something like authenticating with a client_id and secret as happens with Amazon's S3).
  Negative
Is this possible?
  Neutral
Is there any way to programmatically authenticate for Google Cloud Storage, through gsutil or the API?
  Negative
563e32d461a8013065267a8b	X	You can use Service accounts for your application.
  Negative
For more information check the following link
563e32d461a8013065267a8c	X	You say you also tried md5(key,true) but did you try base64_encode(md5(key,true)) ...?
  Negative
563e32d461a8013065267a8d	X	@Michael-sqlbot Yes sir
563e32d461a8013065267a8e	X	I think my issue is that my key is not actually 256 bit.
  Negative
I will attempt to use openssl_random_pseudo_byte to generate 256 bit, and I suspect that it will output 64 hex string
563e32d461a8013065267a8f	X	Actually, I suspect that your key is 256 bits until you base_64 encode it, at which point it will be more.
  Very negative
I have coded extensively against the REST API but am generally unfamiliar with the PHP SDK... rethinking, now, I suspect your error is base_64 encoding either of those values.
  Negative
The SDK probably handles that for you, in which case, you're double-encoding.
  Negative
Try SSECustomerKey => 'raw key string with 32 chars', SSECustomerKeyMD5 => md5(key,true).
  Negative
The "true" as I understand it returns the binary md5, not hex, which is probably what you need here, since docs mention 128 bits for the md5.
  Negative
563e32d461a8013065267a90	X	See github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/…
563e32d461a8013065267a91	X	Actually, the SDK will also set the SSECustomerKeyMD5 parameter for you if you don't provide it.
  Negative
See github.com/aws/aws-sdk-php/blob/master/src/Aws/S3/…
563e32d461a8013065267a92	X	Nice.
  Positive
Thanks, @JeremyLindblom.
  Positive
563e32d461a8013065267a93	X	the only post which helped me ... though there is no other post regarding to that topic :)
563e32d461a8013065267a94	X	how to create string_of_exactly_32_bytes with php ?
  Negative
563e32d461a8013065267a95	X	@Richerdfuld that string is your encryption key.
  Negative
It has to be exactly 32 bytes.
  Neutral
563e32d461a8013065267a96	X	I am attempting to upload an object to S3 using the customer provided encryption key.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html My code looks like: The error I am getting says AWS Error Message: The calculated MD5 hash of the key did not match the hash that was provided What am I doing wrong?
  Negative
My key 48wk86271sDb23pY23zT5rZJ7q55R7eE is 256 bits.
  Neutral
I've also tried using base64_encode(md5(key, true)).
  Negative
Thanks in advance
563e32d461a8013065267a97	X	The REST API documentation specifies that both the customer key and customer key MD5 be sent base-64 encoded... x-amz-server-side​-encryption​-customer-key Use this header to provide the 256-bit, base64-encoded encryption key for Amazon S3 to use to encrypt or decrypt your data.
  Negative
x-amz-server-side​-encryption​-customer-key-MD5 Use this header to provide the base64-encoded 128-bit MD5 digest of the encryption key according to RFC 1321.
  Negative
Amazon S3 uses this header for a message integrity check to ensure the encryption key was transmitted without error.
  Negative
...however, the PHP SDK handles both encoding steps for you, so the arguments should be passed without any encoding.
  Negative
Of course, you'd probably want that 32 byte key string in a variable rather than copypasting the same literal string in the code twice.
  Negative
The second argument "true" to md5() specifies that the binary md5 hash is to be returned, as expected by the SDK, instead of the hex-encoded variant that would be returned by default.
  Negative
Remember that when using customer-provided encryption keys, if you lose the key, you lose the data.
  Negative
S3 does not store the key, and without the key, fetching the stored object is not possible.
  Negative
563e32d561a8013065267a98	X	See my edit about HTTPS.
  Negative
I am planning to retrieve it as it is required and replacing them dynamically with HTTPS AmazonS3 URLs.
  Negative
563e32d561a8013065267a99	X	See my edit about HTTPS.
  Negative
I wish I could use directly the URL that the API gives me but I can't as it is because it's HTTP.
  Negative
I think you're right, caching is not a good idea for this.
  Negative
563e32d561a8013065267a9a	X	The Twitter API returns this value for the Twitter account 'image_url': http://a1.twimg.com/profile_images/75075164/twitter_bird_profile_bigger.png In my Twitter client webapp, I am considering hotlinking the HTTPS version of avatars which is hosted on Amazon S3 : https://s3.amazonaws.com/twitter_production/profile_images/75075164/twitter_bird_profile_bigger.png Any best practices which would discourage me from doing this ?
  Negative
Do 3rd party Twitter client applications typically host their own copies of avatars ?
  Negative
EDIT: To clarify, I need to use HTTPS for images because my webapp will use a HTTPS connection and I don't want my users to get security warnings from their browser about the page containing some content which is not authenticated.
  Negative
For example, Firefox is known to complain about mixed http/https content.
  Positive
My problem is to figure out whether or not hotlinking the https URLs is forbidden by Twitter, since these URLs are not "public" from their API.
  Negative
I got them by analyzing their web client HTML source when connected to my Twitter account in HTTPS.
  Negative
563e32d561a8013065267a9b	X	Are you thinking of storing the image URL in your application or retrieving it for the user as it is required?
  Negative
If its the latter option then I don't see an issue with hot-linking the images.
  Negative
If you are storing the location of the image url in your own system then I see you having broken links whenever the images change (I'm sure they will change the URLs at some point in the future).
  Negative
Edit Ok, now i see your dilemma.
  Positive
I've looked through the API docs and there doesnt seem to be too much in terms of being able to get images served in HTTPS or getting the URL of the Amazon S3 image.
  Negative
You could possibly write a handler on your own server that would essentially cache & re-serve the HTTP image as HTTPS however thats a bit of un-neccesary load on your servers.
  Negative
Short of that I haven't come across a better solution.
  Neutral
GL
563e32d561a8013065267a9c	X	the things seems updated since that.
  Negative
Please check: https://dev.twitter.com/docs/user-profile-images-and-banners The SSL-enabled path template for a profile image is indicated in the profile_image_url_https.
  Negative
The table above demonstrates how to apply the same variant selection techniques to SSL-based images.
  Neutral
563e32d561a8013065267a9d	X	Why would you want to copy the image to your own webspace?
  Negative
This will increase your bandwidth cost and you get cache consistency issues.
  Positive
Use the URL that the API gives you.
  Positive
I can see that you may want to cache the URL that the API returns for some time in order to reduce the amount of API calls.
  Negative
If you are writing something like an iPhone app, it makes sense to cache the image locally (on the phone), in order to avoid web traffic altogether, but replacing one URL with another URL should not make a difference (assuming that the Twitter image server works reliably).
  Negative
Why do you want HTTPS?
  Negative
563e32d561a8013065267a9e	X	I'm afraid the mail app cannot read your file, have you tried to change the mode in the openFileOutput to MODE_WORLD_READABLE?
  Negative
563e32d561a8013065267a9f	X	Well it says that it was deprecated in API 17, but it seems to work, now Mail can read it and send it.
  Negative
However, by changing it to MODE_WORLD_READABLE, It's lost the APPEND quality, which is pretty important... Is there any way to keep both?
  Negative
563e32d561a8013065267aa0	X	Yeah actually it's not a good move because a security hole, but it lead me to an answer, please wait I will write an answer for you :)
563e32d661a8013065267aa1	X	Append and world readable should not be exclusive, but some mail clients won't take an attachment from another app's internal storage even if it is readable to them (unless you trick them).
  Negative
So it's best to either use external storage, or the modern "android way" with a content provider.
  Positive
563e32d661a8013065267aa2	X	Nice Explanation.
  Positive
Can u provide some Sample code for this??
  Neutral
"Attach Audio file and send to some other user of my App"
563e32d661a8013065267aa3	X	Before anything else, I have actually read through several threads regarding sending attachments on Android.
  Negative
That said, I haven't found a solution to my problem.
  Negative
My app is relatively simple, user types numbers, they get saved to "values.csv" using openFileOutput(filename, Context.MODE_APPEND);.
  Negative
Now, here's the code I'm using to attach the file to an email and send (I got it from one of the other file threads.)
  Negative
This opens my email client, which does everything right except attach the file, showing me a toast notification saying "file doesn't exist."
  Negative
Am I missing something?
  Neutral
I've already added the permissions for reading and writing to external storage, by the way.
  Negative
Any and all help would be much appreciated.
  Negative
EDIT: I can use the File Explorer module in DDMS and navigate to /data/data/com.
  Negative
example.myapp/files/, where my values.csv is located, and copy it to my computer, so the file DOES exist.
  Very negative
Must be a problem with my code.
  Neutral
563e32d661a8013065267aa4	X	So another way to attach the email except using MODE_WORLD_READABLE is using ContentProvider.
  Negative
There is a nice tutorial to do this in here but I will explain it to you shortly too and then you can read that tutorial after that.
  Positive
So first you need to create a ContentProvider that provides access to the files from the application’s internal cache.
  Positive
and then write a temporary file And then pass it to the email Intent and don't forget to add this to the manifest
563e32d661a8013065267aa5	X	I see the answers and of course the question.
  Negative
You asked how you can send a file from android, but in your code example you are talking about sending by email.
  Neutral
Those are two different things at all... For sending by email you already have answers so I will not enter it at all.
  Negative
For sending files from android is something else: you can create a socket connection to your server and send it.
  Positive
you have of course to write both client and server side.
  Neutral
An example can be found here : Sending file over socket... you can use an ftp server and upload it to there.
  Positive
you can use any server.
  Neutral
example : It's using apache ftp client library (open source) you can use a http request and make it a post (this method is preferred only for small files) example : Sending file with POST over HTTP you can also use dropbox api or amazon s3 sdk's so you don't care about any connections issues and retries and so on and at the end you have a link to the file and pass over the link.
  Negative
a. DropBox API : documentation b. Amazon S3 SDK API : documentation c. Google Drive API : documentation The advantages in working with Google Drive.
  Negative
regards
563e32d661a8013065267aa6	X	Great idea, shortens the url considerably too!
  Negative
563e32d661a8013065267aa7	X	Warning!
  Positive
Base64 is a very recognisable encoding.
  Negative
Any tech savvy user will be able to find it out and forge new ones.
  Positive
You must sanitize it (look for .
  Neutral
.
  Neutral
in path for exemple).
  Neutral
You better crypt it with some xor algorithm before encoding it in base64.
  Negative
563e32d661a8013065267aa8	X	@Nicolas Or even easier just put to a uid in the querystring that relates to the database row
563e32d661a8013065267aa9	X	Thanks mark, some interesting thoughts there.
  Negative
You ought to start a blog!
  Neutral
Point #2 and #4 are features of any cdn, and the point behind this querystring -> filename operation is so that images can be stored on the local filesystem rather than retrieved from the database every time (doing it on a first-run-basis is fine), so thats point #3.
  Negative
I think where Amazon excells are in its (international) distribution, pricing model and api (although i havent had personal experience of this.
  Negative
563e32d661a8013065267aaa	X	I have a handler (c#/asp.net) that pulls images from a database.
  Negative
The current format looks like foo.com/_image.ashx?querystring1&querystring2 The querystring keys are things like id, width, zoom level etc.
  Negative
I can either munge this up into a filename (i.e. foo.com/id__999_w__128__h__200.jpg), or a whole url structure (i.e. foo.com/id/999/w/128/h/200/image.jpg).
  Negative
Im interested to see what other people would do given this situation, as there dont seem to be any articles or discussions of this practice on the net.
  Negative
563e32d661a8013065267aab	X	The easiest solution would be to base64 the filename, then you can pretty much guarantee the filename will be correct each time, and not tampered with or broken.
  Negative
563e32d661a8013065267aac	X	Best practices are a tricky thing, a lot of the choices you'll want to make depend on your situation.
  Negative
Today a good way to handle this, is to load the images into a server like Amazon S3 instead of storing them into your database.
  Negative
Amazon S3 has an api that you can program to, so the application that you've already written that is storing the images into the database can be tweaked to upload them into Amazon S3 instead.
  Negative
Then you will store the url to the image in your database.
  Neutral
This will solve a number of problems.
  Negative
1.
  Neutral
Images stored on Amazon S3 can be exposed on Amazon CloudFront which is a CDN and will help you to route the image data to your end users in the fast possible route over the internet.
  Negative
2.
  Neutral
You can apply expires headers to these images so they cache on the client browser and the user will get optimum performance while using your website.
  Negative
In your curren mechanism you would have to add these headers programmatically.
  Negative
3.
  Neutral
Pulling blob data in and out of databases has never been the most performant operation against a database so you would be able to get rid of this code completely.
  Negative
4.
  Neutral
You can apply a cookie-free domain name to these images so that needless cookie passing is prevented and will also have a slight performance increase for your users.
  Negative
For example you might host your website on www.mysite.com, for your images you could assign the name images.mysitestatic.com, and this way you know that you don't have any cookies on the mysitestatic.com domain name that your user would need to upload and download on each request.
  Negative
There are many advantages to hosting you images on a server as opposed to data pass through like your _images.ashx.
  Negative
As far as the urls you presented above, they both are pretty much the same.
  Positive
Unless you are looking to get seo value out of them it doesn't really matter.
  Negative
The url depends on your goals for performance/caching/seo, etc.
  Neutral
Hope this helps.
  Positive
Mark
563e32d661a8013065267aad	X	Sorry, I'm new to this but I wanted to comment on the answer above.
  Negative
I don't understand what you gain by BASE64 encoding the url?
  Negative
It won't make the url shorted, the base64 encoded string will usually be about 33% longer.
  Negative
Here is an example: C:\images\myimage.png Base64 encoded: QzpcaW1hZ2VzXG15aW1hZ2UucG5n If you wanted to keep the url short, and you wanted to make sure the filename is correct per the solution above, you should just use the filename itself.
  Negative
The base64 encoded version isn't offering you any benefit.
  Negative
Although, I'm not sure this helps you because its not related to you database solution at all.
  Positive
unless you were to store "images/myimage.
  Negative
png" in a field in your database and have your url look as follows: foo.com/images/image.jpg?h=5&w=10 With that format, the name of the resource being requested, is separated from the representation of the image at a height of 5 and width of 10.
  Negative
563e32d661a8013065267aae	X	It doesn't really mater even if one is 100x slower than the other it is insignificant compared with the IO speed of the disc, chose the versions where it have less lines of code making it much faster to understand and maintain by other programmers.
  Negative
563e32d761a8013065267aaf	X	When uploading a file to S3 using the TransportUtility class, there is an option to either use FilePath or an input stream.
  Negative
I'm using multi-part uploads.
  Negative
I'm uploading a variety of things, of which some are files on disk and others are raw streams.
  Negative
I'm currently using the InputStream variety for everything, which works OK, but I'm wondering if I should specialize the method further.
  Negative
For the files on disk, I'm basically using File.OpenRead and passing that stream to the InputStream of the transfer request.
  Negative
Are there any performance gains or otherwise to prefer the FilePath method over the InputStream one where the input is known to be a file.
  Neutral
In short: Is this the same thing As: Or are there any significant difference between the two?
  Neutral
I know if files are large or not, and could prefer one method or another based on that.
  Negative
Edit: I've also done some decompiling of the S3Client, and there does indeed seem to be some difference in regards to the concurrency level of the transfer, as found in MultipartUploadCommand.cs
563e32d761a8013065267ab0	X	From the TransferUtility documentation: When uploading large files by specifying file paths instead of a stream, TransferUtility uses multiple threads to upload multiple parts of a single upload at once.
  Negative
When dealing with large content sizes and high bandwidth, this can increase throughput significantly.
  Negative
Which tells that using the file paths will use the MultiPart upload, but using the stream wont.
  Neutral
But when I read through this Upload Method (stream, bucketName, key): Uploads the contents of the specified stream.
  Negative
For large uploads, the file will be divided and uploaded in parts using Amazon S3's multipart API.
  Negative
The parts will be reassembled as one object in Amazon S3.
  Negative
Which means that MultiPart is used on Streams as well.
  Neutral
Amazon recommend to use MultiPart upload if the file size is larger than 100MB http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html Multipart upload allows you to upload a single object as a set of parts.
  Positive
Each part is a contiguous portion of the object's data.
  Negative
You can upload these object parts independently and in any order.
  Positive
If transmission of any part fails, you can retransmit that part without affecting other parts.
  Negative
After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object.
  Positive
In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.
  Negative
Using multipart upload provides the following advantages: Improved throughput—You can upload parts in parallel to improve throughput.
  Positive
Quick recovery from any network issues—Smaller part size minimizes the impact of restarting a failed upload due to a network error.
  Negative
Pause and resume object uploads—You can upload object parts over time.
  Negative
Once you initiate a multipart upload there is no expiry; you must explicitly complete or abort the multipart upload.
  Negative
Begin an upload before you know the final object size—You can upload an object as you are creating it.
  Negative
So based on Amazon S3 there is no different between using Stream or File Path, but It might make a slightly performance difference based on your code and OS.
  Negative
563e32d761a8013065267ab1	X	This question may be a bit subjective but I think will offer some valuable concrete information and solutions to proxying to heroku and debugging latency issues.
  Negative
I have an app built using Sinatra/Mongo that exposes a REST API at api.example.com.
  Negative
It's on Heroku Cedar.
  Neutral
Typically I serve static files through nginx at www and proxy requests to /api through to the api subdomain to avoid cross-domain browser complaints.
  Negative
I have a rackspace cloud instance so I put the front-end there temporarily on nginx and setup the proxy.
  Negative
Now latency is horrible when proxying, every 3 or 4 requests it takes longer than 1 minute, otherwise ~150ms.
  Negative
When going directly to the API (browser to api.example.com) average latency is ~40ms.
  Negative
While I know the setup isn't ideal I didn't expect it to be that bad.
  Negative
I assume this is in part due to proxying from rackspace - server may well be on the west coast - to heroku on amazon ec2 east.
  Negative
My thought at the moment is that getting an amazon ec2 instance and proxying that to my heroku app would alleviate the problem, but I'd like to verify this somehow rather than guessing blindly (it's also more expensive).
  Negative
Is there any reasonable way to determine where the long latency is coming from?
  Neutral
Also, any other suggestions as to how to structure this application?
  Negative
I know I can serve static files on Heroku, but I don't like the idea of my API serving my front-end, would rather these be able to scale independently of one another.
  Negative
563e32d761a8013065267ab2	X	Since you're using Heroku to run your API, what I'd suggest is putting your static files into an Amazon S3 bucket, something named 'myapp-static', and then using Amazon Cloudfront to proxy your static files via a DNS CNAME record (static.myapp.com).
  Negative
What's good about using S3 over Rackspace is that: What's good about using Cloudfront is that it will cache your static files as long as you want (reducing multiple HTTP requests), and serve files from an endpoint closest to the user.
  Positive
EG: If a user in California makes an API request and gets a static file from you, it will be served from them from the AWS California servers as opposed to your East Coast Heroku instances.
  Negative
Lastly, what you'll do on your application end is send the user a LINK to your static asset (eg: http://static.myapp.com/images/background.png) in your REST API, this way the client is responsible for downloading the content directly, and will be able to download the asset as fast as possible.
  Negative
563e32d761a8013065267ab3	X	Thanks!
  Positive
Azure has the best Silverlight integration so I'm going with that.
  Positive
563e32d861a8013065267ab4	X	I'm working on a Silverlight app that would allow a user to upload a few gigs of files to a hypothetical cloud based file store, then allow the user to view some data about those files later (more functionality than a file store).
  Negative
Ideally I'd like to use a free, per-user store such as SkyDrive but I can't seem to find an API for that service (and read elsewhere on stack overflow that programmatic access violates their TOS).
  Negative
Do any services fit this bill?
  Neutral
I've heard of Amazon S3 but I understand that'll cost some money - is anything free?
  Negative
EDIT: Could Mesh be an option?
  Neutral
http://stackoverflow.com/questions/1061926/what-is-livemesh-object-and-its-connection-with-silverlight-3-0
563e32da61a8013065267ab5	X	You could look at using Azure as it offers a blob and table storage cloud infrastrucutre and will happily run silverlight applications in an azure web role.
  Negative
Currently there is no cost but this will change once it RTW's.
  Neutral
More info at http://www.azure.com/
563e32da61a8013065267ab6	X	AFAIK, nothing in this world is free when you're dealing with gigabytes of storage, plus the bandwith to put them in the cloud.
  Negative
Amazon S3 is quite reasonable on its pricing.
  Positive
563e32db61a8013065267ab7	X	Appreciate your comments for pre-signed url we don't need to pass secret password as it will be encrypted in the url.
  Negative
We have to just upload the image.
  Neutral
If I use the same URL in postman it was prefect.
  Negative
563e32db61a8013065267ab8	X	Sounds like a platform bug to me then.
  Negative
One more "stupid" question, you've checked that the image data is actually populated and being passed with the method call and thence with the HTTP request?
  Negative
And that sniffing the request with Fiddler (or equivalent) finds everything sent over the wire matches your test request?
  Negative
563e32dc61a8013065267ab9	X	I am trying to upload an image from my iPhone app to S3 using pre-signed url.
  Negative
AWS ended up with no answer.
  Neutral
Step 1: iPhone send a request to server to GET S3 link to upload an image Step 2: Using "signed_request" value I am trying to upload an image to S3 using method "PUT" //Response from server //DATA i receive
563e32dc61a8013065267aba	X	I am unable to find an error in your code; however, as the response states, your SignatureDoesNotMatch the expected value.
  Negative
No secret password, no access to the secret club.
  Negative
Start with the basics and then narrow in on the details:
563e32dd61a8013065267abb	X	Does Google App Storage provide storage and database access in a centralizes public server environment just as the others do?
  Negative
563e32dd61a8013065267abc	X	code.google.com/appengine/whyappengine.html - "App Engine will always be free to get started, and you can purchase more computing resources, paying only for what you actually use.
  Negative
Detailed pricing for usage that has exceeded the free quota of 500 MB of storage and around 5M pageviews per month is available"
563e32de61a8013065267abd	X	This seems to me like it's a web server to store/serve an application.
  Very negative
I just need to store/serve data, like a cloud database (SC3, Google Storage, etc.) Am I correct?
  Negative
563e32de61a8013065267abe	X	I'm not entirely clear on the use case.
  Negative
It's not impossible to implement an architecture that stores directly to something like S3 from an android device, but typically you'd want to provide other service, like authentication to the storage mechanism, which you couldn't do without a web server.
  Negative
563e32de61a8013065267abf	X	So for my Android app I need a centralized server so multiple phones can use it.
  Negative
This server is only going to hold text.
  Negative
Just coordinates and a string per request.
  Negative
I looked into Amazon S3 and Google Storage.
  Negative
These store "buckets" of info through a RESTful API.
  Negative
So basically, I can't put/get info into these things as I would into a database.
  Negative
I'm going to have to send/receive a text file correct?
  Negative
Would it make more sense (both financially and technically), to buy my own web server so I can just create a MySQL DB or something and do it this way, or would the bucket thing still be the best way to go (cloud storage).
  Positive
Thanks!
  Positive
563e32de61a8013065267ac0	X	The storage services tend to make the most sense when the data is read/served many multiples of times more then it is written/stored.
  Negative
The situation you describes sounds like a good use case for one of the hosted server options (EC2, Rackspace, etc.) but you may also want to look into Google App Engine since it provides you some free service before you incur any cost.
  Negative
That will let you develop an app without requiring any investment, especially since you don't yet know what the future usage rate of your app will be.
  Negative
In any of these environments you could create a web service that allows clients to connect via http / REST to store information.
  Positive
563e32de61a8013065267ac1	X	What specific line is associated with the error?
  Negative
563e32df61a8013065267ac2	X	Thanks for the speedy reply Ray!
  Negative
Have updated with the error above
563e32df61a8013065267ac3	X	You'll need to point at the specific line in your example code above that is failing.
  Negative
You're using a custom build, so I have no idea what line 212 looks like.
  Negative
Or, you can look up that line and post it here (with context).
  Neutral
563e32df61a8013065267ac4	X	When you update your question again, please be sure to include the full stack trace along with any line that references your own code.
  Positive
563e32e061a8013065267ac5	X	It looks like it's this line: ExifLocation.loadFromFile(file, function(err, exifLocation, index) { Line 212 in my build is: window.console[level](message);
563e32e061a8013065267ac6	X	I'm using Fine Uploader to upload direct to Amazon S3.
  Very negative
All is working fine, but I want to use a third party script to access the exif gps data in the image.
  Negative
I've found a script to do it (https://github.com/mattcg/exiflocation.git) which requires a file field in the example script.
  Negative
Is there a way I can pass the local file on to this script using the Fine Uploader API?
  Negative
Here's my callback script: Console logging the file outputs a file object.
  Negative
When I run this I get: Caught exception in 'onSubmit' callback - undefined is not a function Any ideas?
  Negative
Here's the complete error from the console: [Fine Uploader 5.0.8] Caught exception in 'onSubmit' callback - undefined is not a function custom.fineuploader-5.0.8.
  Negative
js:212
563e32e061a8013065267ac7	X	You're quite right Ray - my mistake.
  Negative
It's been a long day!
  Neutral
Thanks for your help.
  Positive
The function is called using ExifLocation.prototype.loadFromFile.
  Negative
Thanks for your help!
  Positive
563e32e161a8013065267ac8	X	did you figure this out?
  Neutral
I'm having the same issue
563e32e261a8013065267ac9	X	I'm building a tool that uses javascript & canvas to take screenshots of a video and it all works fine, as long as the video is on the same domain as my code.
  Negative
Now my client is asking if it is possible to create some kind of API so that users could include an iframe from domain X, but insert their own video's from domain Y. I've managed to do this, It works on Chrome, Firefox, BUT on IE10 & safari I keep getting the common "Uncaught SecurityError: Failed to execute 'getImageData' on 'CanvasRenderingContext2D': The canvas has been tainted by cross-origin data."
  Very negative
-error.
  Negative
I've been using a video that was hosted on Amazon S3, with the following CORS-configuration: And I've added the crossOrigin="anonymous" attribute to the video-element.
  Negative
According to Caniuse.com CORS should be working fine on those browsers, so what is causing these issues on IE10 & Safari?
  Neutral
563e32e261a8013065267aca	X	Why have the upload pass-through the play server at all and not do a direct upload to S3 using a CORS configuration?
  Negative
563e32e261a8013065267acb	X	@LimbSoup I am creating a service that sort of manages the uploads.
  Negative
In order to keep track of all of that I need to do post-processing once the file is uploaded.
  Negative
Wouldn't it make more sense to use my server as a passthrough as opposed to making separate calls to upload the file from the client and then inform my server about it afterwards?
  Negative
563e32e261a8013065267acc	X	It seems like a waste of resources to have two IO streams that can be avoided by having users upload directly to S3 with a signed form.
  Negative
To do this I have the client upload the file, then confirm to the server when the upload is finished.
  Negative
The server can then find the file on S3 and change it's properties from there if necessary.
  Negative
Downloads work the same way using signed URLs.
  Neutral
563e32e261a8013065267acd	X	I am using Play Framework version 2.2 and I am trying to get the mime type of a partially uploaded file so I can do a direct upload to an Amazon S3 instance.
  Negative
What is the best practice for doing this?
  Neutral
I am currently using FlowJS but it doesn't look like they have anything in particular for dealing with mime types.
  Negative
Additionally, I plan on making mobile apps that will use the same API so it would be best if it was on the server side and not the client side.
  Negative
The only solution I can think of is parsing the extension and mapping that to a mime type, but that sounds like a hacky way to do it.
  Negative
563e32e361a8013065267ace	X	perhaps do a partial get, and write each block to GCS, then stitch them all together?
  Negative
563e32e361a8013065267acf	X	@PaulCollingwood: thank you for the suggestion, it worked!
  Positive
:) See my answer below.
  Positive
563e32e361a8013065267ad0	X	I'm curious if you can avoid needing the larger instance if you write each chunk immediately to GCS (in the while loop) instead of writing them all at the end.
  Negative
563e32e361a8013065267ad1	X	This is pretty cool.
  Positive
I wonder if you could do this in parallel?
  Neutral
Once you have the length from the HEAD request you could start tasks in a queue and allow a couple of them in parallel.
  Neutral
You might even combine with resumable uploads (cloud.google.com/storage/docs/concepts-techniques#resumable) so you wouldn't need to stitch the files at the end.
  Negative
563e32e361a8013065267ad2	X	@Kekito: your suggestion worked, thank you!
  Positive
563e32e461a8013065267ad3	X	I'm writing an API that needs to ingest HD videos (at least 100MB).
  Negative
I only have access to the videos through an HTTP XML feed, so I can only pull the videos (with a GET) once I have the video's URL.
  Negative
The plan is to store the videos in GCS.
  Neutral
But I'm running into the 32MB-per-request limit in AppEngine before I can upload/write to GCS.
  Negative
Is there a GAE-way around these two limitations: I know of Amazon S3, if I must go outside of Google Cloud products, but I don't know if that can be configured to pull in large data.
  Negative
Thank you.
  Positive
563e32e461a8013065267ad4	X	Following Paul Collingwood's advice, I came up with the following.
  Positive
I decided not to write chunks to GCS and then stitch them back together.
  Neutral
Instead I chose to do it all in-memory, but I might change that depending on resource costs (had to run an F4@512MB to avoid exceeding an F2's 256MB soft limit).
  Negative
Which looks something like this in the logs: Update, 6/May/15 Along the lines of Kekito's suggestion, I moved the GCS write into the loop, keeping the file handle open for the entire duration.
  Negative
Following the advice here, I used top to monitor the Python processes running the GAE local dev server, started an upload, and recorded the memory footprints between download-and-upload cycles.
  Negative
I also experimented with changing how big a chunk is processed at a time: dropping the chunk size from 30 MB to 20 MB reduced the max memory usage by ~50 MB.
  Negative
In the following chart a 560 MB file is being ingested, and I'm trying to track:  The 20-MB-Chunk-Test maxes out at 230 MB while the 30-MB-Chunk-Test maxes out at 281 MB.
  Negative
So, I could run an instance at only 256 MB, but will probably feel better running at 512 MB.
  Negative
I might also try a smaller chunk size.
  Neutral
563e32e461a8013065267ad5	X	Try to specify full path to 'assets/cacert.
  Negative
pem'
563e32e461a8013065267ad6	X	yes.
  Neutral
in the $this->curl->ssl function it will turn it into realpath "C:/wamp/www/assets/cacert.pem"
563e32e461a8013065267ad7	X	sorry because is self signed certification the name should be AIMS-BSN-WEB01.
  Negative
crt instead of cacert.pem
563e32e561a8013065267ad8	X	Im using codeigniter-curl extension (https://github.com/philsturgeon/codeigniter-curl) to call API that return Json format data.
  Negative
a simple code it will return the result etc everything seem ok until the URL changed to HTTPS.
  Negative
In order to call SSL url (self signed certificate), i added these few line above my $this->curl->simple_get.... code.
  Negative
the cacert.pem i saved from the firefox certificate viewer and place it in my web directory.
  Negative
reference: http://unitstep.net/blog/2009/05/05/using-curl-in-php-to-access-https-ssltls-protected-sites/ and i get this error.
  Negative
i've searched the answer for a day long, changed the php.ini setting Amazon S3 on wamp localhost SSL error put false for CURLOPT_SSL_VERIFYPEER.
  Negative
it return SSL: certificate subject name 'AIMS-BSN-WEB01' does not match target host name 'api.example.com' all these are not working.
  Negative
563e32e561a8013065267ad9	X	ok.
  Neutral
i manage to solved it by skipping to verify the peer or host of the certification - PHP CURL CURLOPT_SSL_VERIFYPEER ignored and instead of putting FALSE in CURLOPT_SSL_VERIFYPEER i use 0.
  Negative
now it works.
  Positive
563e32e561a8013065267ada	X	Why not email them and find out?
  Negative
563e32e561a8013065267adb	X	Right, like they'd tell me.
  Negative
:)
563e32e661a8013065267adc	X	Awesome, just as I thought.
  Negative
Thanks heaps for the detailed answer, wish I could +100 you!
  Negative
:)
563e32e661a8013065267add	X	I wonder how apps like these generate screenshots for different browsers.
  Negative
Are they using EC2 instances to run various browsers and generate and store (Amazon S3?)
  Negative
screenshots?
  Neutral
563e32e661a8013065267ade	X	Are they using EC2 instances to run various browsers and generate and store (Amazon S3?)
  Negative
screenshots?
  Neutral
That's apparently exactly what they are doing - as far as I know this is not officially documented in detail indeed, but one can deduce it to some extent from the following information.
  Negative
1) Their Feature Tour -> Email Tests answers How does it work?
  Negative
as follows: You send us a copy of your email design, either by uploading the HTML or sending us a test email.
  Negative
Within a couple of minutes you'll see screenshots of your email as it's rendered by all the different email clients.
  Positive
Made a change?
  Neutral
One click starts a re-test.
  Negative
This is exactly what one would expect, i.e. they are apparently running a test harness which exercises all supported email clients (and dito for browsers) after a new test is scheduled via a queue.
  Negative
This requires a decent amount of automation around all these clients; while some may nowadays offer a dedicated automation API/component to allow rendering without running the full application, I expect this to be a fairly complicated process all in all still, likely requiring external UI automation, which used to be brittle and slow (OS support for this improved in recent years though).
  Very negative
2) Litmus has fortunately participated in an AWS Case Study in February 2010 (updated in April 2011), which confirms their infrastructure to be (meanwhile) running on AWS (specifically Amazon EC2 and Amazon S3) and provides additional insight: Initially, Litmus was hosted on a combination of in-house hardware and dedicated servers.
  Negative
The company grew quickly, and soon they outgrew their hardware.
  Neutral
[...] Paul Farnell tells us about the process, “We looked for solutions that would meet our needs of scalability and cost.
  Negative
We chose Amazon S3 because there was nothing else like it when we first started.
  Negative
For Amazon EC2 we initially trialed a competitor to Amazon, but found it to be tremendously unreliable.
  Negative
[emphasis mine] Furthermore (as of April 2011) Litmus uses Amazon S3 to store over 6TB of customers’ images and Amazon EC2 for running customers’ tests: When we first started out we stored the images on our own hardware, but as we grew we realized this was quickly going to become a headache.
  Negative
By using S3 we were able to focus on improving our product, not worrying about scaling up our storage.
  Neutral
We also use Amazon EC2 to run the automated email tests for our customers; we currently have 400 EC2 servers.
  Very negative
By using EC2 we’re able to add more servers to our grid during the busy periods of the day, and remove them during quieter periods.
  Neutral
Finally, they are using Spot Instances [to] gain significant EC2 cost savings: Specifically, we have a queue-based architecture where a worker node will pull a job from the queue and then process it.
  Negative
As worker nodes appear after a Spot bid is accepted, they can just take jobs off of the queue.
  Negative
[emphasis mine]
563e32e661a8013065267adf	X	It seems unlikely that there is a hard limit, or it would be documented.
  Negative
You must be running up against some capacity constraint.
  Neutral
Amazon monitors their services, so it wouldn't surprise me if they were working to handle the higher load, now that they see users hitting it.
  Negative
563e32e661a8013065267ae0	X	I'm using AWS to run some data processing.
  Negative
I have 400 spot instances in EC2 with 4 processes each, all of them writing to a single bucket in S3.
  Negative
I've started to get a (apparently uncommon) error saying: 503: Slow Down Does anyone know what the actual request limit is for an S3 bucket?
  Negative
I cannot find any AWS documentation on it.
  Negative
Thank you!
  Positive
563e32e661a8013065267ae1	X	From what I've read, Slow Down is a very infrequent error.
  Negative
However, after posting this question I received an email from AWS that said the had capped my LIST requests to 10 requests per second because I had too many going to a specific bucket.
  Neutral
I had been using a custom queuing script for the project I am working on, which relied on LIST requests to determine the next item to process.
  Negative
After running into this problem I switched to AWS SQS, which was a lot simpler to implement than I'd thought it would be.
  Negative
No more custom queue, no more massive amount of LIST requests.
  Negative
Thanks for the answers!
  Positive
563e32e661a8013065267ae2	X	AWS documents 503 as a result of temporary error.
  Negative
It does not reflect a specific limit.
  Negative
According to "Best Practices for Using Amazon S3" section on handling errors (http://aws.amazon.com/articles/1904/): 500-series errors indicate that a request didn't succeed, but may be retried.
  Negative
Though infrequent, these errors are to be expected as part of normal interaction with the service and should be explicitly handled with an exponential backoff algorithm (ideally one that utilizes jitter).
  Negative
One such algorithm can be found at http://en.wikipedia.org/wiki/Truncated_binary_exponential_backoff.
  Positive
Particularly if you suddenly begin executing hundreds of PUTs per second into a single bucket, you may find that some requests return a 503 "Slow Down" error while the service works to repartition the load.
  Negative
As with all 500 series errors, these should be handled with exponential backoff.
  Negative
While less detailed, the S3 Error responses documentation does include 503 Slow Down (http://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html).
  Negative
563e32e761a8013065267ae3	X	To add to what James said, there are some internals about S3 partitioning that have been discussed and can be used to mitigate this in the future because exponential backoff is required.
  Negative
See here: http://aws.typepad.com/aws/2012/03/amazon-s3-performance-tips-tricks-seattle-hiring-event.html Briefly, don't store everything with the same prefix or there is a higher likelihood you will have these errors.Find some way to make the very first character in the prefix be as random as possible to avoid hotspots in S3's internal partitioning.
  Negative
563e32eb61a8013065267ae4	X	Yes, your script will be similar to that.
  Neutral
You've asked several fairly broad questions.
  Neutral
What have you tried so far?
  Neutral
What stumbling blocks have you reached?
  Neutral
563e32eb61a8013065267ae5	X	To be honest - right now I am still in the planning phase.
  Negative
I wrote a script that overlays text on top of images (similar to the one in the link).
  Negative
Do you have any experience implementing something like this?
  Neutral
563e32eb61a8013065267ae6	X	Currently our application is restricted to text, images and audio mime types.
  Negative
I have been given the responsibility of adding a new feature - overlaying text on images (similar to snapchat).
  Neutral
The image editing will be done on the server side (my task) and the text would be provided by the client using a jquery.
  Negative
I think that the ImageFont module of Python's Imaging Library (PIL) can be used to do this.
  Negative
So my question - how would I go about implementing this?
  Neutral
Will the script be similar to this - http://python-catalin.blogspot.com/2010/06/add-text-on-image-with-pil-module.html ?
  Neutral
Currently we are using amazon s3 for our object store.
  Negative
If I write the script in our DB API, Will it be major performance issue (the ultimate goal is to have a separate service for image processing)?
  Negative
I am quite new to Python and PIL so any help would be much appreciated.
  Negative
563e32eb61a8013065267ae7	X	did you implement proper protocol method that RestKit calls when response is available?
  Negative
does it get called?
  Neutral
can you sniff your traffic with wireshark to verify that request gets to S3 and that proper 200 is returned?
  Negative
563e32eb61a8013065267ae8	X	What is your question?
  Neutral
The code you have displayed looks fine
563e32ec61a8013065267ae9	X	@IvorPrebeg the delegate doesnt get called, My implementation has the protocol working for the normal RKCLient.
  Negative
this code is actually withing that part, It will send a request to a very specific url which is totally different from the rkclient base url which is why i need to send it this way.
  Negative
It never gets called and if i print the URL property i receive Null.
  Negative
563e32ec61a8013065267aea	X	@PauldeLange please see my edit
563e32ec61a8013065267aeb	X	@PauldeLange if you put this as an answer ill accept it, there was an extra \n at the end of my url.
  Negative
563e32ec61a8013065267aec	X	I am having trouble using the RestKit api to create a simple RKRequest.
  Negative
All i need is to make a PUT request with a provided url (which is a presigned amazon s3 url) and then attach some NSData to this request.
  Negative
I also want to set the delegate of this request.
  Negative
I have tried many ways but none seem to work.
  Negative
Any help will be greatly appreciated.
  Positive
Edit: The problem is that the delegate never gets called, it does get called for the RKCLient sent requests but not for this specific one.
  Negative
When I NSLOG the url from the uploadpicture request i get NULL and the message i recieve is: E restkit.network:RKRequest.m:623 Failed to send request to (null) due to connection timeout.
  Negative
Timeout interval = 0.000000
563e32ec61a8013065267aed	X	I'm having this issue at the moment - were you able to resolve yours?
  Negative
563e32ec61a8013065267aee	X	I posted what I ended up doing as an answer below.
  Negative
Let me know if you have any questions!
  Negative
563e32ec61a8013065267aef	X	I am still learning a lot about Rails and Android development, so forgive me if my question is a bit unclear.
  Negative
Fundamentally, what I'd like to do is uploading photos to my rails app using an android app.
  Negative
I have a Rails app that uses Carrierwave and Amazon S3 for image uploading.
  Negative
I'm writing a companion Android application app that can be used for updating entries on the site and for uploading photos.
  Negative
I created a REST API for the rails app so that I can perform http post / get / and delete requests using the Android app, which is working for updating text entries.
  Negative
But I'm unsure how to approach doing image uploading since when I look at the POST parameters in my Rails logs, it includes a lot of CarrierWave specific actions (such as @headers, @content_type, file, etc.).
  Negative
Can anyone recommend a way for me to get started?
  Negative
Many thanks!
  Positive
563e32ec61a8013065267af0	X	I ended up piecing together code snippets and getting something that worked.
  Negative
I had to send the image in a multipart entity: The asynctask requires a filepath and filename.
  Negative
In my app, I allowed users to pick images from the gallery.
  Negative
I then retrieve the filepath and filename like this: Hope that helps!
  Negative
563e32ed61a8013065267af1	X	I had a problem with the args[] being generated in Java.
  Negative
Where my command had been this: /home/hadoop/.
  Negative
versions/hive-0.7.1/bin/hive '-f' 's3://anet-emr/scripts/admin.
  Negative
q' '-d rawDataLocation=s3://anet-emr/raw -d year=2010 -d cycle=1' it should have been this /home/hadoop/.
  Negative
versions/hive-0.7.1/bin/hive '-f' 's3://anet-emr/scripts/admin.
  Negative
q' '-d' 'rawDataLocation=s3://anet-emr/raw' '-d' 'year=2010' '-d' 'cycle=1'
563e32ed61a8013065267af2	X	I have been developing a data processing application using Amazon Elastic MapReduce and Hive.
  Negative
Now that my Hive scripts work when I SSH and run them using the Interactive Mode Job Flow, I'm trying to create a Job Flow using the AWS Java API.
  Negative
Using http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/calling-emr-with-java-sdk.html as my starting point, I create a step config like this I assumed/hope that scriptPath can be an s3 url to my Hive script, like: s3://bucketName/hive-script.
  Negative
The only documentation I've found talks about using a script from the master node's file system.
  Negative
But if the master node is an instance started for the sake of this Job Flow, I don't understand how I can get any script (Hive or otherwise) onto the file system.
  Negative
When I try my idea (passing an s3 location to the stepFactory method), the runScript step fails.
  Negative
I've checked the logs via the AWS console.
  Negative
The stdout logs end with 2012-11-19 19:28:33 GMT - ERROR Error executing cmd: /home/hadoop/.
  Negative
versions/hive-0.7.1/bin/hive '-f' 's3://anet-emr/scripts/admin.
  Negative
q' '-d rawDataLocation=s3://anet-emr/raw -d year=2010 -d cycle=1' The stderr logs end with java.lang.NoSuchMethodError: org.apache.commons.cli.CommandLine.getOptionProperties(Ljava/lang/String;)Ljava/util/Properties; at org.apache.hadoop.hive.cli.OptionsProcessor.process_stage1(OptionsProcessor.java:115) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:399) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:155) at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79) at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68) and the Controller log has 2012-11-19T19:28:27.406Z INFO Executing /usr/lib/jvm/java-6-sun/bin/java -cp /home/hadoop/conf:/usr/lib/jvm/java-6-sun/lib/tools.
  Negative
jar:/home/hadoop:/home/hadoop/hadoop-0.18-core.jar:/home/hadoop/hadoop-0.18-tools.jar:/home/hadoop/lib/:/home/hadoop/lib/jetty-ext/ -Xmx1000m -Dhadoop.log.dir=/mnt/var/log/hadoop/steps/3 -Dhadoop.log.file=syslog -Dhadoop.home.dir=/home/hadoop -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,DRFA -Djava.io.tmpdir=/mnt/var/lib/hadoop/steps/3/tmp -Djava.library.path=/home/hadoop/lib/native/Linux-i386-32 org.apache.hadoop.mapred.JobShell /mnt/var/lib/hadoop/steps/3/script-runner.jar s3://us-east-1.
  Negative
elasticmapreduce/libs/hive/hive-script --base-path s3://us-east-1.
  Negative
elasticmapreduce/libs/hive/ --hive-versions latest --run-hive-script --args -f s3://anet-emr/scripts/admin.
  Negative
q -d rawDataLocation=s3://anet-emr/raw -d year=2010 -d cycle=1 2012-11-19T19:28:34.143Z INFO Execution ended with ret val 255 2012-11-19T19:28:34.143Z WARN Step failed with bad retval The problem seems to lie with the arguments I'm passing via Amazon's API to Hive's call to the Apache CLI library... I've tried passing a single string with "-d arg1=val1 -d arg2=val2", I've tried "-d,arg1=val1 etc.."
  Negative
and I've tried various ways of chopping up into String arrays - ie { "-d", "arg1=val1" ...}.
  Negative
Can't find any documentation of the proper way to do this!
  Neutral
Any help appreciated, thank you Coleman
563e32ed61a8013065267af3	X	Hi this code works for me: Hope this helps :)
563e32ed61a8013065267af4	X	I answered this before, the key part was source.sourceBuffers[0].
  Negative
timestampOffset = duration.
  Neutral
However, that no longer works in newer versions of Chrome/Firefox.
  Neutral
Even Eric Bidelman's Demo, which everyone seems to use at first, isn't fully working now.
  Negative
Do you have any code that works with a single video?
  Neutral
Creating a playlist is tough when I can't even find a basic demo that works any longer.
  Negative
563e32ed61a8013065267af5	X	I don't have any working code beyond that demo, however the demo that you pointed out does seem to work for me.
  Negative
563e32ed61a8013065267af6	X	Does the video in that demo actually play the full 6 seconds of the clip?
  Negative
For me, the video stops at 04.19 in Chrome and 05.08 in Firefox with the mediasource.enabled flag, but it used to go the full 6 seconds.
  Negative
563e32ed61a8013065267af7	X	Ah, no, you're right.
  Negative
Maybe we should abandon this idea until browsers/stdanards are updated.
  Negative
What are your thoughts?
  Neutral
563e32ed61a8013065267af8	X	Yeah, unfortunately I had to table my mediasource plans.
  Negative
I have a hacky fallback where I load the second video in a separate <video> element set to display: none, then toggle display on both and start playing the second when the first finishes.
  Negative
Good enough for our needs right now, but not ideal because of lack of controls for seeking through the "full video" and possible stuttering between clips on slow devices.
  Negative
563e32ed61a8013065267af9	X	I am trying to achieve a reliable, gapless video playlist in HTML5/JS.
  Negative
To do this, I want to buffer a dynamic playlist in memory and then send the buffer to an HTML5 video element.
  Negative
I have a choice in how the videos are encoded, and this only has to work on the Chrome browser, so I'm thinking of using webm videos and MediaSource extensions.
  Negative
The video files will be stored on Amazon S3 and delivered with CloudFront.
  Negative
I've seen the following example of the MediaSource API.
  Neutral
The key difference is that instead of reading chunks of a file, I'm reading in lots of files.
  Negative
http://bluishcoder.co.nz/2013/08/20/progress-towards-media-source-extensions-in-firefox.html How can this be adapted to work with multiple files rather than chunks of a file?
  Neutral
563e32ed61a8013065267afa	X	We ended up writing this Javascript library to handle video playback: https://github.com/jameshadley/LifemirrorPlayer/blob/master/LifemirrorPlayer.js It doesn't use the MediaSourceAPI but it works surprisingly well.
  Negative
563e32ee61a8013065267afb	X	The answer you provided cannot explain why there is no native public cloud storage service in market.
  Negative
563e32ee61a8013065267afc	X	I think object storage has a lot to do with scale, that is why is is becoming so popular(Google File System, Amazon S3).
  Negative
563e32ee61a8013065267afd	X	The nature of object storage allows it to be implemented at HyperScale using distributed architectures.
  Negative
563e32ee61a8013065267afe	X	Big thanks for your describe.
  Positive
But I know and I read amazons docs.
  Neutral
I know about openstack swift, also I know about RADOS object store.
  Negative
But I still dont understand difference between object storage and file storage.
  Negative
Because in all case we save files to file system, and both can be retrieved via url.
  Positive
Both have metadata (for 'object storage' we can add more attributes, thats all difference???)
  Negative
.
  Neutral
And no one cant give me simple example of difference.
  Negative
How for instance to store image like object using some programming language (for example python, java, php)?
  Neutral
563e32ee61a8013065267aff	X	You say: "I use both Rackspace Cloud files and Amazon S3 (in addition to EBS and Glacier) for backing up, storing, and archiving data."
  Negative
You can do all that (backing up, storing, and archiving data) even if amazon just store your files.
  Negative
I think "object storage" just new marketing word.
  Neutral
Because, nobody can givee really usefull example.
  Negative
563e32ee61a8013065267b00	X	You are correct in that "object storage" is a popular industry and marketing term, some will say its new, yet its been around for at least a decade (e.g. EMC Centera among others).
  Positive
When I backup up files to S3 or Rackspace, the software tool maps the files into objects that are then saved within those repositories.
  Negative
If I go to S3 or Rackspace and look at my accounts, I dont see files/folders per say, rather I see buckets with objects, in those objects are the streams for the backup sets.
  Negative
Did you look at the preso I mentioned?
  Neutral
563e32ee61a8013065267b01	X	You say you know about RADOS, S3, OpenStack Swift, etc having read the docs, on one hand having looked at all of those and more, I can see where it is easy to come to the conclusion of what is the difference.
  Negative
On the other hand, I would also think that you would be able to start to see the difference?
  Negative
Having said that, and industry and marketing hype aside, as well as API access vs. file name access, the lines between the two can be blurred as many scaleout filesystems are themselves object based designs (e.g. Lustre and others).
  Negative
Hence there are object access, and object architectures.
  Negative
563e32ef61a8013065267b02	X	Thank you again for your kindly reply.
  Positive
How can I understand from here: managedview.emc.com/2012/09/… we can download file (as they say 'object') knowing only IDs (which of course, already in metadata)?
  Neutral
Then how system find file by ID (I just want to know thats programming language work or OS)?
  Neutral
563e32ef61a8013065267b03	X	Thank you.
  Positive
But i dont exactly agree with your explanation.
  Negative
In both case we need database to save file location.
  Negative
Then why I need metadata?
  Neutral
if I can just save it also in database?
  Neutral
And why just PUT API, I also can use POST.
  Negative
563e32ef61a8013065267b04	X	File systems only have a limited set of metadata (access time, modification time, etc.).
  Negative
If you want to add additional metadata, object storage provides the ability to add additional metadata.
  Positive
With a file system, there is no database whereas in case of object storage there is.
  Negative
Finally, reg the API, you are correct that it could be a PUT or POST.
  Negative
I was just providing an example.
  Neutral
All great questions and hope this clarifies everything.
  Positive
Ask more if you need to.
  Neutral
563e32ef61a8013065267b05	X	Thank you!
  Positive
But I already know about that article.
  Neutral
I need real example if that possible.
  Positive
563e32ef61a8013065267b06	X	Could someone explain what difference between Object Storage and File Storage is please?
  Negative
I read about Object Storage on wiki, also I read http://www.dell.com/downloads/global/products/pvaul/en/object-storage-overview.pdf, also I read amazons docs(S3), openstack swift and etc.
  Negative
But could someone give me an example to understand better?
  Negative
All the difference is only that for 'object storage' objects we add more metadata?
  Negative
For example how to store image like object using some programming language (for example python)?
  Negative
Thanks.
  Neutral
563e32ef61a8013065267b07	X	IMO, Object storage has nothing to do with scale because someone could build a FS which is capable of storing a huge number of files, even in a single directory.
  Negative
It is also not about the access methods.
  Negative
HTTP access to data in filesystems has been available in many well known NAS systems.
  Negative
Storage/Access by OID is a way to handle data without bothering about naming it.
  Negative
It could be done on files too.
  Negative
I believe there is an NFS protocol extension that allows this.
  Negative
I would muster this: Object storage is a (new/different) ''object centric'' way of thinking of data, its access and management.
  Negative
Think about these points: What are snapshots today?
  Neutral
They are point in time copies of a volume.
  Negative
When a snapshot is taken, all files in the volume are snapped too.
  Negative
Whether all of them like it or not, whether all of them need it or not.
  Negative
A lot of space can get used(wasted?)
  Negative
for a complete volume snapshot while only a few files needed to be snapped.
  Negative
In an object storage system, you will rarely see snapshots of volumes, objects will be snapshot-ed, perhaps automatically.
  Negative
This is object versioning.
  Neutral
All objects need not be versioned, each individual object can tell if it is versioned.
  Negative
How are files/volumes protected from a disaster?
  Neutral
Typically, in a Disaster Recovery(DR) setup, entire volumes/volume-sets are setup for replication to a DR site.
  Negative
Again, this does not bother whether individual files want to be replicated or not.
  Negative
The unit of disaster protection is the volume.
  Negative
Files are small fry.
  Neutral
In an object storage system, DR is not volume centric.
  Negative
Object metadata can decide how many copies should exist and where(geo locations/fault domains).
  Negative
Similarly for other features: Tiering - Objects placed in storage tiers/classes based on its metadata independent of other unrelated objects.
  Negative
Life - Objects move between tiers, change the number of copies, etc, individually, instead of as a group.
  Negative
Authentication - Individual objects can get authenticated from different authentication domains if required.
  Negative
As you can see, the change in thinking is that in an object store, everything is about an object.
  Negative
Contrast this with the traditional way of thinking about and management and access larger containers like volumes(containing files) is not object storage.
  Negative
The features above and their object-centric-ness fits well with the requirements of unstructured data and hence the interest.
  Positive
If a storage system is object(or file) centric instead of volume centric in its thinking, (irrespective of the access protocol or the scale,) it is an object storage system.
  Negative
563e32ef61a8013065267b08	X	The simple answer is that object accessed storage systems or services utilize APIs and other object access methods for storing, retrieving and looking up data as opposed to traditional file or NAS.
  Negative
For example with file or NAS, you access storage using NFS (Network File System) or CIFS (e.g. windows file share) aka SMB aka SAMBA where the file has a name/handle with associated meta data determined by the file system.
  Negative
The meta data includes info about create, access, modified and other dates, permissions, security, application or file type, or other attributes.
  Positive
Files are limited by the file system in terms of their size, as well as the number of files per file system.
  Negative
Likewise, file systems are limited by their total or aggregate size in terms of space capacity and the number of files in the filesystem.
  Negative
Object access is different in that while file or NAS front-end or gateways or plugins are available for many solutions or services, primary access is via an API where an object can be of arbitrary size (up to the maximum of the object system) along with variable sized meta data (depends on the object system/service implementation).
  Negative
With most object storage systems/services you can specify anywhere from a few Kbytes of user defined meta data or GBytes.
  Negative
What would you use GBytes of meta data for?
  Neutral
How about in addition to normal info, adding more data for policies, managements, where other copies are located, thumbnails or small previews of videos, audio, etc.
  Negative
Some examples of object access APIs or interfaces include Amazon Web Services (AWS) simple storage services (S3) or other HTTP and REST based ones, SNIA CDMI.
  Negative
Different solutions will also support IOS (e.g. iphone/ipad) access, SOAP, Torrent, WebDav, JSON, XAM among others plus NFS/CIFS.
  Negative
In addition many of the object storage systems or services support programmatic bindings for python among others.
  Negative
The APIs allow you to essentially open a stream and then get or put, list and other functions supported by the API/system to determine how you will use it.
  Negative
For example, I use both Rackspace Cloud files and Amazon S3 (in addition to EBS and Glacier) for backing up, storing, and archiving data.
  Negative
I can access the objects stored via a web browser or tools including Jungle disk (JD) which is what I backup and synchronize files with.
  Negative
JD handles the object management and moves data to both Rackspace as well as Amazon for me.
  Negative
If I were inclined, I could also do some programming using the APIs and then directly access either of those sites supplying my security credentials to do things with my stored objects.
  Negative
Here is a link to object and cloud storage primer from a session I did in Holland last year that has some simple examples of objects and access.
  Negative
http://storageio.com/DownloadItems/Nijkerk_Nov2012/SIO_IndustryTrends_CloudObjectStorage.pdf Using the programmatic binding, you would define your data structures or objects in your program and then use the APIs or calls for storing, retrieving, listing of data, meta data access etc.
  Negative
If there is a particular object storage system, software or service that you are looking to work with or need to know how to program to, go to their site and you should find their SDK or API info with examples.
  Negative
With objects, once you create your initial bucket or container on a service or with a product/system, you then simply create and store additional objects as you go.
  Negative
Here is a link as an example to AWS S3 API/programming: http://docs.aws.amazon.com/AmazonS3/latest/API/IntroductionAPI.html In theory object storage systems are talked about has having unlimited numbers of objects, or object size, in reality, most systems, solutions, software or services are limited by what they have either tested or currently support, which can be billions of objects, with objects sizes of 5GByte or larger.
  Very negative
Pay attention to the limits on specific services or products as to what is actually tested, supported vs. what is architecturally possible or what is implemented on webex or powerpoint.
  Negative
Again its very service and product/service/software dependent as to the number of objects, size of the objects, size of meta data, and amount of data that can be moved in/out via their APIs.
  Negative
However, it is generally safe to assume that object storage can be much more scalable (depending on implementation) than file systems (without using global name space, federation, file virtualization or other techniques).
  Neutral
Also in my book Cloud and Virtual Data Storage Networking (CRC Press) that is Intel Recommended Reading, you will find more information about cloud and object storage.
  Negative
I will be adding more related material to www.objectstorage.us soon.
  Negative
Cheers gs
563e32ef61a8013065267b09	X	There are some very fundamental differences between File Storage and Object Storage.
  Negative
File storage presents itself as a file system hierarchy with directories, sub-directories and files.
  Negative
It is great and works beautifully when the number of files is not very large.
  Very positive
It also works well when you know exactly where your files are stored.
  Positive
Object storage, on the other hand, typically presents itself via.
  Positive
a RESTful API.
  Neutral
There is no concept of a file system.
  Negative
Instead, an application would save a object (files + additional metadata) to the object store via.
  Negative
the PUT API and the object storage would save the object somewhere in the system.
  Negative
The object storage platform would give the application a unique key (analogous to a valet ticket) for that object which the application would store in the application database.
  Negative
If an application wanted to fetch that object, all they would need to do is give the key as part of the GET API and the object would be fetched by the object storage.
  Negative
Hope this is now clear.
  Neutral
563e32ef61a8013065267b0a	X	This link explains the differences between the two: http://www.dell.com/downloads/global/products/pvaul/en/object-storage-overview.pdf
563e32ef61a8013065267b0b	X	I think the white paper explains the idea of object storage quite well.
  Negative
I am not aware of any standard way to use object storage devices (in the sense of a SCSI OSD) from a user application.
  Negative
Object storage is in use in some large scale storage products like the storage appliances of Panasas.
  Negative
However, these appliances then export a file system to the end user.
  Neutral
It is IMHO fair to say that the T10 OSD idea never really caught momentum.
  Negative
Related ideas to the OSD standard can be found in cloud storage systems like S3 and RADOS.
  Negative
563e32ef61a8013065267b0c	X	OpenStack Swift is made for Object Storage & is production at HPcloud, Rackspace, Internap, Korea Telecom, Disney and many others across globe.
  Negative
You can find the release note with the havana release for Swift here You can contribute as developer to the project following the developer doc You can also read detailed explanation from here.
  Negative
Hope it gives a good idea about what is Object Storage & how OpenStack Swift makes most of it & is widely adopted in production for more than 2 years.
  Positive
563e32ef61a8013065267b0d	X	Oh I wish I could down vote some answers and up vote others with an account.
  Negative
The one with the most votes, as of this writing, doesn't even explain anything about the differences.
  Positive
There are some very fundamental differences between File Storage and Object Storage.
  Negative
File storage presents itself as a file system hierarchy with directories, sub-directories and files.
  Negative
It is great and works beautifully when the number of files is not very large.
  Very positive
It also works well when you know exactly where your files are stored.
  Positive
Object storage, on the other hand, typically presents itself via.
  Positive
a RESTful API.
  Neutral
There is no concept of a file system.
  Negative
Instead, an application would save a object (files + additional metadata) to the object store via.
  Negative
the PUT API and the object storage would save the object somewhere in the system.
  Negative
The object storage platform would give the application a unique key (analogous to a valet ticket) for that object which the application would store in the application database.
  Negative
If an application wanted to fetch that object, all they would need to do is give the key as part of the GET API and the object would be fetched by the object storage.
  Negative
Hope this is now clear.
  Neutral
This explained a large portion of it; but you argued about the meta data.
  Neutral
The following is from what I have been reading these last two days, and since this hasn't been resolved, I will post.
  Negative
Object storage has no sense of folders, or any kind of organization structure which makes it easy for a human to organize.
  Negative
File Storage, of course, does have all those folders that make it so easy for a human to organize and shuffle through...In a server environment with the number of files in a scale that is astronomical, folders are just a waste of space and time.
  Very negative
Databases you say?
  Neutral
Well he's not talking about the Object storage itself, he is saying your http service (php, webmail, etc) has the unique ID in its database to reference a file that may have a human recognizable name.
  Negative
Metadata, well where is this file stored you say?
  Negative
That's what the metadata is for.
  Neutral
You single file is split up into a bunch of small pieces and spread out of geographic location, servers, and hard drives.
  Negative
These small pieces also contain more data, they contain parity information for the other pieces of data, or maybe even outright duplication.
  Negative
The metadata is used to locate every piece of data for that file over different geographic locations, data centres, servers and hard drives as well as being used to restore any destroyed pieces from hardware failure.
  Negative
It does this automatically.
  Neutral
It will even fluidly move these pieces around to have a better spread.
  Negative
It will even recreate a piece that is gone and store it on a new good hard drive.
  Positive
This maybe a simple explanation; but I think it might help you better understand.
  Positive
I believe file storage can do the same thing with the metadata; but file storage is storage that you can organize as a human (folders, hierarchy and such) whereas object storage has no hierarchy, no folders, just a flat storage container.
  Negative
563e32f061a8013065267b0e	X	Most companies with object based solutions have a mix of block/file/object storage chosen based on performance/cost reqs.
  Negative
From a use case perspective: Ultimately object storage was created to address unstructured data which is growing explosively, far quicker than structured data.
  Negative
For example, if a database is structured data, unstructured would be a word doc or PDF.
  Negative
How do you search 1 billion PDFs in a file system?
  Neutral
(if it could even store that many in the first place).
  Negative
How quickly could you search just the metadata of 1 billion files?
  Negative
Object storage is currently used more for long term or archival, cheap and deep storage, that keeps track of more detail of what that data is.
  Negative
This metadata becomes very powerful when searching or mining very large data sets.
  Neutral
Sometimes you can get what you need from the metadata without even accessing the data itself.
  Neutral
Object storage solutions can typically replicate automatically with geographic failover built-in.
  Negative
The problem is that application would have to be re-written to use object access methods rather than file hierarchy (which is simpler from a app dev perspective).
  Negative
It's really a change in the philosophy of data storage, and storing more actionable information about that data from a management standpoint as well as usage.
  Negative
Quick example might be an MRI scan image.
  Negative
On Filesystem you have owner/creation date, but not much else.
  Neutral
If it were an object, all of the information surrounding the MRI could be stored along with it in metadata, like patient name, MRI center location, the requesting Dr., insurance carrier, etc.
  Negative
Block/file are more well suited for local access or OTLP where performance is more important than retention and cost.
  Negative
For example, you would not want to wait minutes for a Word doc to open, but you could wait a few minutes for a data mining/business intelligence process to complete.
  Negative
Another example would be a legal search where you have to search everything from 5 years ago to present.
  Negative
With retention policies in place to decrease the active data set and cost, how would you even do that without restoring from tape?
  Negative
Object storage is a great solution for replacing long term archival methods like tape.
  Negative
Setting up replication and failover for block and file can get very expensive in the enterprise and usually requires very expensive software and services.
  Very negative
Note: At the lower level, object storage access happens via the RESTful API which is more like a web request than accessing a file at the end of a path.
  Negative
563e32f061a8013065267b0f	X	Actually you can mount an bucket/container and access the objects or subfolders (and their objects) from Linux.
  Negative
For example, I have s3fs installed on Ubuntu that I have setup a mount point to one of my S3 buckets and able to do regular cp, ls and other functions just as though it were another filesystem.
  Negative
The key is getting the software tool of which there are plenty that allows you to map a bucket/container and present it as mount point.
  Negative
There are also software tools that allow you to access S3 and other buckets/containers via iSCSI in addition to as NAS.
  Negative
563e32f061a8013065267b10	X	Thanks!
  Positive
A rough estimate is that with the current application I use for transfer I would end up with around 1-1.5 milj transactions per month if I transfer 1 16GB file per day.
  Negative
Wich is not that much cost wise even if it was 10 or 50 times that.
  Negative
I think I understand it better now.
  Negative
Thanks!
  Positive
563e32f061a8013065267b11	X	Storage is so cheap these days that often theactual discussion about cost optimisations (using everybody's hourly rate) is more expensive than a years' worth of optimisation!
  Very negative
Not off-topic though - developers need to know how much things will cost, so it is a good and valid question.
  Negative
563e32f061a8013065267b12	X	we are considering using Azure blob storage as storage for our backups.
  Negative
But we are not sure of what the transaction price would mean for us in reality.
  Negative
(they charge cost per storage volume and cost per transactions) For example if I transfer one 16 GB file to the storage every day (and deleting so I in the end always keep 10 versions).
  Negative
Does that only mean 1 transaction per day (+ maybe a few for listing and such) or is a transaction like per packet of some size so that it will cost me loads each day.
  Negative
or what does the transaction mean?
  Neutral
563e32f061a8013065267b13	X	Be careful, it may not be as simple as you think.
  Negative
Firstly, it depends on if you are using page or block blobs.
  Neutral
It also depends on what library you are using to upload the blob.
  Neutral
For block blobs, the storage client has a default value of the maximum size of the block being uploaded (32MB) and will split the file into n blocks - each block will be a transaction (see Understanding Block Blobs and Page Blobs.
  Negative
You will also need to consider retries, and as you point out, listing, deleting etc.
  Negative
I suggest you look closely at how you are backing up and find the size of the blocks - then do the calculations.
  Neutral
Then do some controlled trails in an isolated account and see if you can reconcile the billing transactions against your estimate.
  Negative
563e32f061a8013065267b14	X	Do take a look at this blog post from Storage team about billing: http://blogs.msdn.com/b/windowsazurestorage/archive/2010/07/09/understanding-windows-azure-storage-billing-bandwidth-transactions-and-capacity.aspx To summarize, you're charged for 3 things in Windows Azure Storage: I also built a simple calculator which would give you a rough idea about your Windows Azure Blob Storage bill.
  Very negative
You can use this calculator here: http://gauravmantri.com/2012/09/03/simple-calculator-for-comparing-windows-azure-blob-storage-and-amazon-s3-pricing/.
  Negative
It was basically built to compare Amazon S3 costs and Windows Azure Blob Storage costs but can be used for just Windows Azure Blob Storage as well.
  Negative
563e32f061a8013065267b15	X	I'd say your first guess : 1 transaction per day, based on their explanation : Transactions – Each individual Blob, Table and Queue REST request to the storage service is considered as a potential transaction for billing.
  Negative
Applications can then control their transaction costs by controlling how often and how many requests they send to the storage service.
  Negative
We analyze each request received and then classify it as billable or not billable based upon our ability to process the request and the request’s outcome.
  Negative
Quoted from here.
  Neutral
But the best thing to do would be to go for the trial.
  Positive
I think Azure is free for a certain time, that would allow you to see how many requests are really going.
  Neutral
563e32f161a8013065267b16	X	As I understand, Store Kit API will handle storage and retrieval of history related to "Non-Consumables" products, while for "Consumables" and "Subscriptions" you have to have your own server/backend.
  Negative
What are the best hosted existing solutions for that kind of server (backend)?
  Neutral
Inexpensive (or free?
  Neutral
:-) and reliable?
  Neutral
Thanks!
  Positive
563e32f161a8013065267b17	X	There is not any service (that I know of) that includes the needed functionality by default.
  Negative
There is more here than just hosting the files.
  Positive
You need to use the included functionality to verify the receipt that is sent from the StoreKit API to your server (to ensure that it is a valid receipt): http://developer.apple.com/library/ios/#documentation/NetworkingInternet/Conceptual/StoreKitGuide/VerifyingStoreReceipts/VerifyingStoreReceipts.html So you would need to have a web application that managed this process as well as hosting the actual consumable content (in a way that made it inaccessible from the outside).
  Negative
You might want to look at a Java application hosted on a solution like Elastic Beanstalk with a connection to a protected Amazon S3 account.
  Negative
If this is too complex, you could also create a simple PHP application that could be run from most any web server that could also do this functionality and manage access to the S3 account's files.
  Negative
563e32f161a8013065267b18	X	Usually best practice questions are not well received in SO, so it'd be better you you shared at least a bit of code to show what you've tried so far.
  Positive
Bu anyway, take a look at the picasso library, it's an elegant way of solving your problem.
  Positive
563e32f161a8013065267b19	X	you can upload and save the image in either web or file system and save the corresponding path in sqllite db.
  Negative
to change the picture by you, you can save it in filesystem of your server where the app can connect and you can change them there.
  Negative
download would be just reading the images/buffer from the path stored in sqlite db during upload.
  Negative
563e32f161a8013065267b1a	X	I will write to you later.
  Positive
I just finished a view that load image and save it to cache memory.
  Negative
563e32f161a8013065267b1b	X	@EdsonMenegatti thank you, the picasso library looks definitly like a solution to a part of my problem!
  Negative
563e32f161a8013065267b1c	X	@ssh thank you, i would really appreciate that!
  Negative
563e32f161a8013065267b1d	X	yeah, seems like Picasso library is an absolutely must have for this problem!
  Negative
563e32f261a8013065267b1e	X	thank you for your input, the links seems to contain a lot of useful information for my problem!
  Negative
563e32f261a8013065267b1f	X	thank you for your help.
  Positive
Do you also know an option for storing images online which is for free?
  Negative
(if there is one...)
563e32f261a8013065267b20	X	thank you for the answer!
  Negative
could you be more specific on how it works to make such kind of API and where to host this flat file?
  Negative
GCM sounds interesting, that is definitely an option
563e32f261a8013065267b21	X	Well you would need to get a server to host it on, depending on the amount of traffic you generate you could perhaps find a free one.
  Negative
Putting a lets say json file on a server shouldn't be too expensive.
  Negative
Alternatively you could write a regular API in a language you are proficient with, but it's probably overkill for such a simple case.
  Negative
563e32f261a8013065267b22	X	How would you approach this problem: My app should download different packages of pictures (containing up to 300 pngs, each about 20 kb) and store the pictures on the phone, so i can display them.
  Negative
I want to upload the pictures somewhere online, so I can change them every time and the user can enjoy the newest pictures.
  Positive
(I upload the pictures not with the app) I read that storing them in a sqlite db isn't the best option.
  Negative
At the moment I am storing the pictures in the app, but then I don't know how I can upload and replace pictures on all apps immediately without the need of updating the whole app.
  Negative
I don't need code or stuff, so don't waste your precious time on that, just some general hints where and how you would store the pictures online, and how android can download the pictures easily.
  Negative
563e32f261a8013065267b23	X	Take a look at the Glide or Picasso libraries.
  Neutral
Those are super easy to use for thread-safe downloading of images.
  Neutral
Personally, I just fetch/store the images on imgur.
  Negative
If you want to upload a dedicated databse, you'll have to set one up.
  Positive
Some common ones are Amazon, Google, etc.
  Negative
There are tons.
  Neutral
563e32f261a8013065267b24	X	Have a look at this answer.
  Neutral
In this answer Picasso library is used to handle image download.
  Negative
Picasso gets rid of a lot of coding and testing to handle image download.
  Positive
In a project that I am working on, we use Amazon S3 to store our pictures, it's very reliable and is one of the goto solutions right now.
  Positive
From what I heard Snapchat and some other big firms use S3 to store their picture!
  Negative
It's also very cheap, plus I believe they have free hosting to a certain degree.
  Negative
This is their API guide for android.
  Neutral
We use a service called File Picker to handle upload and download from amazonS3, it reduces a lot of work, but I don't think it's a free service.
  Negative
563e32f261a8013065267b25	X	You can use Picasso for downloading images from network in Android.
  Negative
For storing images Amazon S3 or Google cloud storage can be your options.
  Negative
563e32f261a8013065267b26	X	Not sure if downloading packages is better than downloading individual pictures (archiving won't save you much space).
  Negative
As for your question, you can make some kind of API you will query from your app, even a flat file hosted somewhere with changing content would work.
  Negative
Your app could check it periodically for the new address to download pictures from (assuming it will change).
  Negative
another way is using push messages - sending out a push through GCM that your apps will receive that will notify them about new content available.
  Negative
It would even work when the app is closed.
  Neutral
563e32f361a8013065267b27	X	Can you catch the exception and retry?
  Neutral
The 2nd request should b OK
563e32f361a8013065267b28	X	When you call connect, do you receive onConnected() callback?
  Negative
There shouldn't be any need to catch an exception in the app and try again.
  Negative
563e32f361a8013065267b29	X	I need to track down the device that is presenting with the error and debug whether I'm getting the onConnected() callback.
  Negative
I will update after I test that.
  Neutral
563e32f361a8013065267b2a	X	Just remember that until you receive onConnected, you should not try to launch your app
563e32f361a8013065267b2b	X	We don't try to launch the app until we receive the onConnected callback.
  Negative
563e32f361a8013065267b2c	X	Interestingly enough, the Chromecasts could not be seen when the time was 2 hours off.
  Negative
It did not reproduce the issue that I described.
  Negative
563e32f361a8013065267b2d	X	I work on a Chromecast app that has been out on the market for a little while.
  Negative
We started to receive reports that people cannot connect to the Chromecast on the first try after it has been booted up.
  Negative
We could not reproduce this until recently (and only one device happens to exhibit this behavior) The Chromecast icon appears that it is connected, but the app never launches.
  Negative
Eventually the Chromecast icon shows that it is disconnected.
  Negative
I grabbed a logcat from this device.
  Negative
This appears to be all in the Chromecast API in Google Play Services.
  Negative
We use Amazon S3 to host and use their SSL certificate.
  Negative
It seems odd that after this initial error, the devices do connect.
  Negative
I haven't been able to wrap my brain around this one.
  Negative
Our app is only available on Android devices, and not all devices exhibit this behavior.
  Negative
563e32f361a8013065267b2e	X	The "CertificateNotYetValidException" exception usually happens when the client's clock is off.
  Neutral
Perhaps the users that encountered this issue have their current date set in the past.
  Neutral
You can reproduce this by changing your device's current date to be in the past.
  Neutral
563e32f461a8013065267b2f	X	And that traffic between Windows Azure and Amazon datacenters is charged for (bandwidth in/out).
  Neutral
563e32f461a8013065267b30	X	I'm currently looking at Windows Azure to host an ElasticSearch implementation.
  Negative
Loading the application and running it under Java is not that difficult.
  Negative
Currently, ElasticSearch only supports Amazon's S3 when it comes to cloud storage.
  Negative
As a result, I've made a request to add support for Azure Blob Storage in ElasticSearch.
  Negative
Right after I made the request, it occured to me that while I can host ElasticSearch in Azure, I can create an Amazon S3 account and then have the instance running in Azure connect to the S3 account for storage.
  Negative
However, I do have concerns about the speed between the two.
  Neutral
While I am sure both Azure Storage and Amazon's S3 are both optimized for really fast speeds, I have a nagging feeling that storage systems are really optimized when accessed from their respective computing clusters.
  Negative
That said, is there any definitive information on this?
  Neutral
It makes sense, but I'm looking for specific confirmation or denial.
  Neutral
563e32f461a8013065267b31	X	It's not so much a matter of optimization of the Azure storage API for Azure Roles, but simply a matter of physical co-location and network distance / number of hops.
  Negative
You can (and should) specify that your Azure storage service resides in the same data center as the Azure roles that will be using that storage service.
  Negative
You can expect network bandwidth to be greatest and latency to be lowest between an Azure role and an Azure storage residing in the same data center.
  Neutral
Bandwidth will be lower and latency higher when your Azure role connects to anything outside of its own data center - be that Azure storage in another data center, or Amazon S3 storage in another data center.
  Negative
Besides performance, also keep in mind that you pay for all data traffic in and out of the Azure data center for your services.
  Negative
Having your Azure role accessing data on Amazon S3 or in another Azure data center will take a bite out of your bandwidth quota, whereas accessing Azure storage within the same data center costs you nothing, no matter how much traffic you use between your role and your Azure storage.
  Very negative
563e32f461a8013065267b32	X	Is it possible to delete a folder with the SDK that still contains content?
  Negative
It is possible to do in the amazon console but doesn't appear to work with the php sdk.
  Negative
563e32f561a8013065267b33	X	Not Sure What you are asking for , if you want to delete an object completely here is the code for that using Amazon SDK for PHP, If you want to delete a specified version of object of S3 then the code here does that You can find still more info on API calls here AMAZON SDK FOR PHP , API's FOR S3
563e32f561a8013065267b34	X	You can use delete_bucket().
  Very negative
There's a parameter that allows you to "force" delete the bucket and all of its contents.
  Positive
There are also delete_all_objects() and delete_all_object_versions().
  Negative
563e32f561a8013065267b35	X	Are skip/take properly pagination?
  Negative
How this implemented?
  Neutral
Fullscan?
  Neutral
563e32f561a8013065267b36	X	1) Client Access: Is there anyway to perform CRUD operations on DynamoDB using client side JavaScript (REST/Ajax/jQuery)?
  Negative
I know Amazon has support for .
  Negative
NET and Java.
  Neutral
2) Server Access: Is there any way we can access DynamoDB using server side JavaScript (Node.js) without having to install Java/.
  Negative
NET on the server?
  Neutral
563e32f561a8013065267b37	X	Update 2012-12-05 There is now an official AWS SDK for Node.js, see the introductory post AWS SDK for Node.js - Now Available in Preview Form for details, here are the initially supported services: The SDK supports Amazon S3, Amazon EC2, Amazon DynamoDB, and the Amazon Simple Workflow Service, with support for additional services on the drawing board.
  Very negative
[emphasis mine] Update 2012-02-27 Wantworthy has implemented a Node.js module for accessing Amazon DynamoDB a week after its launch date, thus covering 2) as well, see dynode: Dynode is designed to be a simple and easy way to work with Amazon's DynamoDB service.
  Negative
Amazon's http api is complicated and non obvious how to interact with it.
  Negative
This client aims to offer a simplified more obvious way of working with DynamoDB, but without getting in your way or limiting what you can do with DynamoDB.
  Negative
Update 2012-02-11 Peng Xie has implemented a Node.js module for accessing Amazon DynamoDB at its launch date basically, thus covering 2) already, see dynamoDB: DynamoDB uses JSON for communication.
  Negative
[...] This module wraps up the request and takes care of authentication.
  Negative
The user will be responsible for crafting the request and consuming the result.
  Neutral
Unfortunately there is no official/complete JavaScript SDK for AWS as of today (see AWS Software Development Kits and boto [Python] for the available offerings).
  Very negative
Fortunately decent coverage for several AWS services in JavaScript is provided by the Node.js library aws-lib already though, which would be a good starting point for adding DynamoDB accordingly.
  Neutral
An as of today unresolved feature request to Add support for DynamoDB has been filed already as well.
  Negative
Further, AWS forum user gmlvsk3 has recently implemented dedicated JavaScript interface for DynamoDB, but supposedly you need [a] Java runtime to run it, because it is based on the Mozilla Rhino JavaScript engine - I haven't reviewed the code in detail yet (at first sight it looks a bit immature though in comparison to e.g. aws-lib, but may cover your needs regardless of course), so you should check it out yourself.
  Negative
Finally, you can implement JavaScript HTTP Requests to Amazon DynamoDB yourself of course (see the API Reference for Amazon DynamoDB for details): If you don't use one of the AWS SDKs, you can perform Amazon DynamoDB operations over HTTP using the POST request method.
  Negative
The POST method requires you to specify the operation in the header of the request and provide the data for the operation in JSON format in the body of the request.
  Negative
563e32f561a8013065267b38	X	I created a module called Dino to make it easier to work with the AWS SDK in web applications.
  Negative
You can use something like Restify to expose your data to jQuery via a REST interface.
  Negative
Suppose you wanted to display pages of blog posts for a user.
  Negative
Using Dino and Restify, you would do the following:
563e32f561a8013065267b39	X	Regarding 1), there is now the AWS SDK for JavaScript in the Browser that allows you to access services including DynamoDB.
  Negative
563e32f561a8013065267b3a	X	as for 2) we've been working as well since DDB launch date.
  Negative
One of its key features are simplicity/performance and how close it is (retry behavior, etc) to Amazon official Java/PHP libraries: https://github.com/teleportd/node-dynamodb It's successfully used in production at various places with 100+ write/s (at teleportd).
  Negative
Additionally we're working on a a mocked version to enable efficient testing of the library's client code.
  Negative
563e32f661a8013065267b3b	X	Can you elaborate more on what you're planning to do?
  Neutral
This is an interesting question but a bit broad.
  Neutral
I wrote a (non distributed) NoSQL database in Go so I can share some insight.
  Positive
563e32f661a8013065267b3c	X	Basically, my current plan is building a distributed NoSQL, might be a Key-value store, that is designed for high reliability, also designed for high write performance.
  Negative
It should be able to be the backend storage for hadoop MapReduce tasks, so we can run "hadoop jar foo.jar gonosql://some/key/prefix" to use it.
  Negative
563e32f661a8013065267b3d	X	I would like to build a distributed NoSQL database or key-value store using golang, to learn golang and practice distribute system knowledge I've learnt from school.
  Negative
The target use case I can think of is running MapReduce on top of it, and implement a HDFS-compatible "filesystem" to expose the data to Hadoop, similar to running Hadoop on Ceph and Amazon S3.
  Negative
My question is, what difficulties should I expect to integrate such an NoSQl database with Hadoop?
  Neutral
Or integrate with other languages (e.g., providing Ruby/Python/Node.
  Neutral
js/C++ APIs?)
  Negative
if I use golang to build the system.
  Neutral
563e32f661a8013065267b3e	X	Ok, I'm not much of a Hadoop user so I'll give you some more general lessons learned about the issues you'll face: Protocol.
  Negative
If you're going with REST Go will be fine, but expect to find some gotchas in the default HTTP library's defaults (not expiring idle keepalive connections, not necessarily knowing when a reader has closed a stream).
  Negative
But if you want something more compact, know that: a. the Thrift implementation for Go, last I checked, was lacking and relatively slow.
  Negative
b. Go has great support for RPC but it might not play well with other languages.
  Neutral
So you might want to check out protobuf, or work on top the redis protocol or something like that.
  Negative
GC.
  Neutral
Go's GC is very simplistic (STW, not generational, etc).
  Negative
If you plan on heavy memory caching in the orders of multiple Gs, expect GC pauses all over the place.
  Negative
There are techniques to reduce GC pressure but the straight forward Go idioms aren't usually optimized for that.
  Negative
mmap'ing in Go is not straightforward, so it will be a bit of a struggle if you want to leverage that.
  Negative
Besides slices, lists and maps, you won't have a lot of built in data structures to work with, like a Set type.
  Negative
There are tons of good implementations of them out there, but you'll have to do some digging up.
  Neutral
Take the time to learn concurrency patterns and interface patterns in Go.
  Neutral
It's a bit different than other languages, and as a rule of thumb, if you find yourself struggling with a pattern from other languages, you're probably doing it wrong.
  Negative
A good talk about Go concurrency is this one IMHO http://www.youtube.com/watch?v=QDDwwePbDtw A few projects you might want to have a look at: Groupcache - a distributed key/value cache written in Go by Brad Fitzpatrick for Google's own use.
  Positive
It's a great implementation of a simple yet super robust distributed system in Go.
  Positive
https://github.com/golang/groupcache and check out Brad's presentation about it: http://talks.golang.org/2013/oscon-dl.slide InfluxDB which includes a Go based version of the great Raft algorithm: https://github.com/influxdb/influxdb My own humble (pretty dead) project, a redis compliant database that's based on a plugin architecture.
  Negative
My Go has improved since, but it has some nice parts, and it includes a pretty fast server for the redis protocol.
  Positive
https://bitbucket.org/dvirsky/boilerdb
563e32f661a8013065267b3f	X	I want to enable SSE-S3 on Amazon S3.
  Negative
I click properties and check the encryption box for AES-256.
  Negative
It says encrypting, then done.
  Negative
But I can still read the files without providing a key, and when I check properties again, it shows the radio buttons unchecked.
  Negative
Did I do this correctly?
  Negative
Is it encrypted?
  Neutral
So confusing.
  Negative
563e32f661a8013065267b40	X	You're looking at a view of a bucket in the S3 console that shows more than one file, or shows only one file but that file isn't selected.
  Negative
The radio buttons allow you to set all items you select to the values you select in the radio buttons, but the radio buttons remain blank whenever multiple files are shown, because they're only there to let you make a change -- not to show you the values of existing object.
  Negative
Click on an individual file and view its properties and you'll see that the file is stored with server-side-encryption = AES256.
  Negative
Yes, you can download the file without needing to decrypt it, because this feature is server-side encryption of data at rest -- the files are encrypted by S3 prior to storage on the physical media that S3 runs on.
  Very negative
This is often done for compliance purposes, where regulatory restrictions or other contractual obligations require that data to be encrypted at rest.
  Negative
The encryption keys are stored, separately from the object by S3, and are managed by S3.
  Negative
In fact, the encryption keys are actually stored, encrypted, by S3.
  Negative
(They generate a key for each object, and store that key in an encrypted form, using a master key).
  Positive
Decryption of the encrypted data requires no effort on your part.
  Negative
When you GET an encrypted object, we fetch and decrypt the key, and then use it to decrypt your data.
  Negative
https://aws.amazon.com/blogs/aws/new-amazon-s3-server-side-encryption/ For data in transit, S3 encrypts that whenever you use HTTPS.
  Negative
Different than the feature that's available in the console, S3 also supports server-side AES-256 encryption with keys you manage.
  Negative
In this scenario, called SSE-C, you still aren't responsible for the actual encryption/decryption, because S3 still does that for you.
  Negative
The difference is that S3 doesn't store the key, and you have to present the key to S3 with a GET request in order for S3 to fetch the object, decrypt it, and return it to you.
  Negative
If you don't provide the correct key, S3 won't bother to return the object -- not even in encrypted form.
  Negative
S3 knows whether you've sent the right key with a GET request, because S3 stores a salted HMAC of the key along with the object, for validation of the key you send when you try to fetch the object, later.
  Negative
This capability -- where you manage your own keys -- requires HTTPS (otherwise you'd be sending your encryption key accross the Internet unencrypted) and is only accessible through the API, not the console.
  Negative
You cannot use the Amazon S3 console to upload an object and request SSE-C.
  Negative
You also cannot use the console to update (for example, change the storage class or add metadata) an existing object stored using SSE-C.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html And, of course, this method -- with customer-managed keys -- is particularly dangerous if you don't have a solid key-management infrastructure, because if you lose the key you used to upload a file, that file is, for all practical purposes, lost.
  Negative
563e32f761a8013065267b41	X	Saying "by use of parameters" is vague.
  Negative
Try doing this in your PHP script: var_dump($_POST); var_dump($_GET); var_dump($_FILES); and see where the image comes in.
  Negative
563e32f761a8013065267b42	X	Or does it pass the image back via JavaScript?
  Negative
If so, does it pass the binary string of the image back, or does it pass a URL to the image?
  Negative
If it is passed back to JavaScript (which is executing on the client-side), then you will need an AJAX call to pass the image to your PHP script, which can then save it to a file.
  Negative
563e32f761a8013065267b43	X	The image is coming back in a binary string via POST method
563e32f761a8013065267b44	X	Thanks much Travesty3.
  Negative
All images will be in png format.
  Neutral
How would I add this extension to the file name after it is on my server?
  Neutral
563e32f761a8013065267b45	X	I'm getting these errors: Warning: fopen() [function.fopen]: Filename cannot be empty in .
  Very negative
.
  Neutral
/test/target.
  Neutral
php on line 4 and this one: Warning: fclose(): supplied argument is not a valid stream resource in .
  Negative
.
  Neutral
/test/test.
  Neutral
php on line 6
563e32f761a8013065267b46	X	@user1322707: You need to set the value of $imageName before doing this.
  Negative
You must specify the absolute path to the folder and give the file a name, ending with .
  Negative
png.
  Neutral
It completely depends on where on your server that you want to save the file.
  Negative
If you are using a Linux server, you might be able to use something like $imageName = "/var/www/uploadedPictures/".
  Negative
time() ."
  Neutral
.
  Neutral
png";.
  Neutral
You can name the file whatever you want, but you want to give it some sort of unique name to make sure that subsequent uploads don't overwrite it.
  Negative
563e32f761a8013065267b47	X	Thanks again Travesty3.
  Negative
We're close!
  Neutral
An image file is being saved, but when attempting to open it, it's empty.
  Negative
Also, I get the following error, pointing to fclose($outputFile);.
  Negative
Here is the error: Warning: fclose(): supplied argument is not a valid stream resource in /home/path/domain.
  Negative
com/test.
  Neutral
php on line 8 where test.php is the file with the code in it.
  Negative
563e32f761a8013065267b48	X	@user1322707: My fault.
  Negative
Copy and paste error in my code.
  Negative
Use fclose($handle);.
  Negative
563e32f761a8013065267b49	X	I am using the API of an image editing website (pixlr.com) for use by members of my site.
  Negative
I open Pixlr.com in an iframe where they can create an image and upon SAVE, pixlr sends the image file by use of parameters.
  Negative
I want to save these image files (unique for each member) in a folder on my server (or on Amazon's S3 image server), using PHP.
  Negative
How do I receive their parameters ("image") of the image file and store them on my/Amazon's image server?
  Neutral
563e32f761a8013065267b4a	X	If the image is sent to your PHP script via POST, then you should be able to do something like this: Where $imageName is the absolute path and filename of the image where you want to save it (make sure you Apache user has write permissions to that directory).
  Negative
Depending on the picture's encoding you may need to figure out which extension to save it with (ie .
  Neutral
jpg, .
  Neutral
bmp, .
  Neutral
png, etc).
  Positive
Looks like they are sending the image via $_FILES.
  Negative
Try this:
563e32f861a8013065267b4b	X	What database are you using?
  Neutral
563e32f861a8013065267b4c	X	I was going to up vote this comment until you stated that you think it is better to put the file in the database :-\ if you have lots of images stored in the database, it can cause serious performance issues.
  Negative
563e32f861a8013065267b4d	X	I understand there is a limitation.
  Negative
but by my own experience, the sql server handles it pretty well... :-)
563e32f861a8013065267b4e	X	Andrew, see my comment re: FILESTREAM storage - that addresses the performance concern
563e32f861a8013065267b4f	X	If you are in a load balanced environment though, you will want to either have a server designated as a resource server, or create some sort of duplication mechinism between the servers.
  Very negative
563e32f861a8013065267b50	X	What would be the best method to implement the following scenario: The web site calls for a image gallery that has both private and public images to be stored.
  Negative
I've heard that you can either store them in a file hierarchy or a database.
  Neutral
In a file hierarchy setup how would prevent direct access to the image.
  Negative
In a database setup access to the images would only be possible via the web page view.
  Neutral
What would be a effective solution to pursue?
  Neutral
[Edit] Thanks all for the responses.
  Positive
I decided that the database route is the best option for this application since I do not have direct access to the server.
  Neutral
Confined to a webroot folder.
  Neutral
All the responses were most appreciated.
  Negative
563e32f861a8013065267b51	X	Having used both methods I'd say go with the database.
  Negative
If you store them on the filestore and they need protecting then you'd have to store them outside the web-root and then use a handler (like John mentions) to retrieve them, anyway.
  Negative
It's as easy to write a handler to stream them direct from database and you get a few advantages: The disadvantage is that of performance, but you can use caching etc. to help with that.
  Positive
You can also use FILESTREAM storeage in SQL Server 2008 (and 05?)
  Negative
which means you get filesystem performance but via the DB: "FILESTREAM integrates the SQL Server Database Engine with an NTFS file system by storing varbinary(max) binary large object (BLOB) data as files on the file system.
  Negative
Transact-SQL statements can insert, update, query, search, and back up FILESTREAM data.
  Negative
Win32 file system interfaces provide streaming access to the data.
  Negative
FILESTREAM uses the NT system cache for caching file data.
  Neutral
This helps reduce any effect that FILESTREAM data might have on Database Engine performance.
  Negative
The SQL Server buffer pool is not used; therefore, this memory is available for query processing."
  Negative
563e32f861a8013065267b52	X	Using file hierarchy, you can put the files out of the website file folder, for example, suppose the web folder is c:/inetpub/wwwroot/somesite, put the file under c:/images/, so that the web users won't be able to access the image files.
  Negative
but you cannot use the direct link in your website neither, you need to create some procedure to read the file, return the stream.
  Negative
personally I think it's better to put the file in the database, still create some procedure to retrieve the binary image data and return to wherever it needed.
  Negative
563e32f961a8013065267b53	X	In reality both scenarios are very similar, so it's up to you... Databases weren't designed to serve files, but if the size isn't really a concern for you, I don't see a problem with doing it.
  Negative
To answer your question about direct access, you'd setup the file images the same way you would for the database: You'd use some sort of page (probably a .
  Negative
ashx handler) that serves the images, allowing you a layer of logic between the user and image to determine whether or not they should have access to it.
  Positive
The actual directory the images are located in would then need to either a) not be part of the directory structure in IIS or b) if it is part of IIS, only allow windows authenticated access, and only allow the account the application process is running under access to the directory.
  Negative
563e32f961a8013065267b54	X	If you're using IIS7, since .
  Negative
net jumps in the pipeline early I believe you can protect jpg files as well, just by using a role manager and applying roles to file system folders.
  Positive
If you're using IIS6, I've done something similar to the answer by John, where I store the actual file outside of the wwwroot, and use a handler to decide if the user has the correct credentials to view the image.
  Negative
I would avoid the database unless you have a strong reason to do this - and I don't think a photo gallery is one of them.
  Negative
563e32f961a8013065267b55	X	Neither.
  Neutral
Amazon S3 offers a very simple API for accepting uploads.
  Positive
You can use SimpleDB or your SQL database to track the URLs and permissions.
  Negative
Set the entire S3 bucket to private, and authenticate to it using your AWS key on the ASP.NET server.
  Negative
Very little code is required to upload to S3, and very little more would be required to perform bookeeping in SQL.
  Negative
Once they're in S3, grab the image resizer library and the S3 Reader plugin and you can have your entire system running in under an hour.
  Negative
And - it will scale properly.
  Positive
No disk or database space limits.
  Neutral
Ever.
  Neutral
You can implement authorization using the AuthorizeImage event of the Image Resizer library.
  Negative
Just throw an AccessDeniedException if access isn't allowed for the current user.
  Negative
If you want to tune performance a bit mare, add both the DiskCache and CloudFront plugins.
  Negative
CloudFront can edge-cache the public images (inexpensively), and DiskCache will handle the private images, serving them at static-file speeds.
  Positive
563e32fa61a8013065267b56	X	
563e32fa61a8013065267b57	X	Requests: There's a Drive API daily quota of 500,000 requests/day ("courtesy limit") which you can find from your API dashboard (https://code.google.com/apis/console/) Bandwidth: Google Apps for Business and Education have published bandwidth limits although I don't know if these are the same as for the Drive API: http://support.google.com/a/bin/answer.py?hl=en&answer=1071518 Compared to Amazon's S3 / CloudFront?
  Negative
I'm not convinced that it's a like for like comparison.
  Negative
Drive would have to be compared to DropBox, Box.com, SkyDrive, iCloud and the rest.
  Negative
Google Cloud Storage enables application developers to store their data on Google’s infrastructure with very high reliability, performance and availability.
  Negative
If you’re looking to store personal data in the cloud, consider using Google Drive instead.
  Negative
Or, if you are are an app developer who wants to integrate with Google Drive, you can do so using the Google Drive SDK.
  Neutral
https://developers.google.com/storage/docs/getting-started
563e32fa61a8013065267b58	X	This post on the AWS blog suggests otherwise: java.awsblog.com/post/Tx2Q9SGR6OKSVYX/Amazon-S3-TransferManager
563e32fa61a8013065267b59	X	@DGolberg no, it's not, that blog post doesn't claim otherwise and besides I've got proof after several hours of checking out the source code and doing profiling.
  Negative
The API of TransferManager is non-blocking, in the sense that it's offloading work to a configured thread-pool, but the threads in that thread-pool are blocking and hence unavailable for doing anything else.
  Negative
I'll probably write an article about it.
  Positive
Thanks for the down-vote, wasn't necessary.
  Negative
563e32fa61a8013065267b5a	X	@AlexandruNedelcu Indeed I believe you are correct, it's asynchronous in the sense that if you are using it then you can continue doing other things in whatever thread you made the API call, but the underlying implementation just makes another thread to do the work.
  Negative
Have an upvote =D
563e32fb61a8013065267b5b	X	@AlexandruNedelcu I guess I interpreted your question incorrectly; I apologize for that.
  Negative
Use of the transfer manager on the current thread is asynchronous, but no, the underlying threads are not, as you've stated.
  Negative
I need to remember to stop trying to answer questions when hindered by a lack of sleep...
563e32fb61a8013065267b5c	X	@DGolberg that's OK, I asked myself this question because I was wondering in an application whether I should use the global and limited thread-pool that the app has configured for CPU-bound tasks or not.
  Very negative
It would have been cool for efficiency reasons, but we've got a thread-pool meant for blocking I/O stuff anyway and the TransferManager is still pretty cool because it can batch multiple uploads together.
  Positive
563e32fb61a8013065267b5d	X	I've been reading about TransferManager in the Amazon's AWS SDK for doing S3 uploads, the provided API allows for non-blocking usage, however it's unclear to me if the underlying implementation actually does asynchronous I/O.
  Negative
I did some reading on the source-code of TransferManager and I cannot understand if the threads in the provided ExecutorService are being blocked or not.
  Negative
My problem is that if this manager actually does asynchronous I/O without blocking that executor, then I could use the application's global thread-pool that is meant for CPU-bound stuff.
  Negative
So is this actually doing asynchronous I/O or not?
  Negative
563e32fb61a8013065267b5e	X	After profiling and trying to understand the SDK's source-code I have come to the conclusion that yes, TransferManager does not work asynchronously, because it piggybacks on AmazonS3Client.putObject and such calls, while not blocking the threads per se, go in a loop until the http requests are finished, thus preventing progress in processing the thread-pool's queue.
  Very negative
563e32fb61a8013065267b5f	X	This sounds to me the best usecase describe so far and moreover your comment on the other answer confirms what I kind of think/feel : multi module provides value for Multipackaging or huge project.
  Positive
563e32fb61a8013065267b60	X	I'm by no means a fanboi of maven but there are some cases where it has "just worked" and this seems to be one of its strengths to me.
  Negative
Glad the answer helped.
  Positive
563e32fb61a8013065267b61	X	I am actually plotting to use this answer to demonstrate in which cases the multi module should not be used.
  Negative
563e32fc61a8013065267b62	X	I know this is old, but I was wondering what the benefit of including the api / impl with the webapp was?
  Negative
I guess this just furthers the original question for me - I see that maybe the impl should be modularized with the API, but why things like the webapp and the other consumers of the API?
  Negative
563e32fc61a8013065267b63	X	@BrandonV So you are thinking that rather than having the webapp be a module within the overall build it would be a project of its own with a dependency on the project that contains the api/impl modules?
  Negative
I can see that as a totally valid approach.
  Negative
In my case I didn't see or experience any pain with it being a single project with multiple modules.
  Negative
Since it was all so closely related I didn't want to take the time to separate it all out into a separate project and found it easier to just make modules within my own project.
  Negative
563e32fc61a8013065267b64	X	The build order seems a valid argument to me but can you maybe describe why you would need 30 (!)
  Negative
submodules ?
  Neutral
563e32fc61a8013065267b65	X	I have worked in a situation where we had 12-15 teams working on a single project.
  Negative
We had a main project that had around 30 modules.
  Negative
It allowed each team to have ownership of their area (2-4 modules) and provided clear separation of responsibilities among teams.
  Negative
It wasn't perfect, but it wasn't as bad as having 50 developers all mucking around in the same packages all the time (which I've also experienced at another time)
563e32fc61a8013065267b66	X	I have some years of experience with maven projects, even with multi modules ones (which has made me hate the multi modules feature of maven (so the disclaimer is now done)) and even if I really like maven there is something I cannot get a clear answer about : What is a typical usecase of a multi module maven project ?
  Very negative
What is the added value of such a structure compared to simple dependencies and parent pom ?
  Positive
I have seen a lot of configuration of multi module projects but all of them could have clearly been addressed by creating a simple structure of dependency library living their own life as deliverables (even with a parent pom, as a separate deliverable : factorising depedencies and configuration) and I haven't found any usecase where I can clearly see an added value of the multi module structure.
  Negative
I have always found that this kind of structure brings an overkilling complexity with no real benefit : where am I missing something ?
  Negative
(to be truly honest, I can get that some ear can benefit from this kind of structure but except from that particular usecase, any other real use and benefit ?)
  Neutral
563e32fc61a8013065267b67	X	Here's a real life case.
  Neutral
I have a multi-module project (and to your rant... I haven't seen any complications with it.)
  Negative
The end result is a webapp but I have different modules for api, impl, and webapp.
  Negative
12 months after creating the project I find that I have to integrate with Amazon S3 using a stand-alone process run from a jar.
  Negative
I add a new module which depends on api/impl and write my code for the integration in the new module.
  Neutral
I use the assembly plugin (or something like it) to create a runnable jar and now I have a war I can deploy in tomcat and a process I can deploy on another server.
  Negative
I have no web classes in my S3 integration process and I have no Amazon dependencies in my webapp but I can share all the stuff in api and impl.
  Negative
3 months after that we decide to create a REST webapp.
  Negative
We want to do it as a separate app instead of just new URL mappings in the existing webapp.
  Negative
Simple.
  Neutral
One more module, another webapp created as the result of the maven build with no special tinkering.
  Negative
Business logic is shared easily between webapp and rest-webapp and I can deploy them as needed.
  Negative
563e32fc61a8013065267b68	X	The major benefit of multi modules are I already worked in a project with about 30 submodules.
  Negative
Sometimes, you need to change something in more than module, and running one single command and being sure that everything that need to be compiled is compiled in the correct order is a must.
  Negative
EDIT Why 30 submodules ?
  Negative
Huge framework with lot's a features, lot's of developers, separation of features on a module base.
  Positive
It's a real life use case and the separation of the code into module was really meaningful.
  Positive
563e32fc61a8013065267b69	X	I think you are correct in that most project that use multi modules, actually don't need them.
  Negative
At where I work we use multimodule projects (and I think that for a good reason).
  Negative
We have something similar to a service oriented architecture, so each application I agree that putting that implementation and war module in the same actual module would be ok, but the (arguably) benefit of this is that is very clear division between the classes that solve the problem and how the application communicates with the external world.
  Very negative
In previous projects that involved just a web application, I've tried to put everything in the same module, as it made testing easier, given the modules I was using.
  Negative
563e32fd61a8013065267b6a	X	I am trying to upload multiple files asynchronously on Amazon S3 using the .
  Negative
NET SDK.
  Neutral
Any examples to get me started will be greatly appreciated.
  Neutral
Thanks in advance.
  Neutral
563e32fd61a8013065267b6b	X	The Amazon S3 and AWS SDK for .
  Negative
NET functionality you are looking for is Using the High-Level .
  Negative
NET API for Multipart Upload: The AWS SDK for .
  Negative
NET exposes a high-level API that simplifies multipart upload (see Uploading Objects Using Multipart Upload API).
  Negative
You can upload data from a file, directory, or a stream.
  Neutral
[...] You can optionally set advanced options such as the part size you want to use for the multipart upload, number of threads you want to use when uploading the parts concurrently, optional file metadata, the storage class (STANDARD or REDUCED_REDUNDANCY), or ACL.
  Negative
The high-level API provides the TransferUtilityUploadRequest class to set these advanced options.
  Positive
[emphasis mine] An example snippet is provided in Upload a Directory:
563e32fd61a8013065267b6c	X	I have many data files (let's call them input_files) that are stored in Amazon S3.
  Negative
I would like to start about 15 independent Amazon EC2 linux instances.
  Negative
These instances should load the input_files (that are stored in S3) and process them independently.
  Negative
I'd like all the 15 independent Amazon EC2 linux instances to write to the same output file.
  Negative
Upon completion, this output file will be saved in S3.
  Negative
Two questions: (1) Is it possible for Amazon EC2 linux instances to connect to S3 and read data from it?
  Negative
(2) How can I arrange that all the 15 independent Amazon EC2 linux instances would write to the same output file?
  Negative
Can I have this file in S3, and all instances will write to it?
  Negative
563e32fd61a8013065267b6d	X	(1) Yes.
  Neutral
You can access S3 from anywhere on the internet using the S3 public API (2) You are describing a database it seems.
  Negative
S3 is simply a file store, you don't write to files on S3 - you save files to S3.
  Negative
Maybe you should look into some type of database instead.
  Neutral
563e32fd61a8013065267b6e	X	I suggest you to take a look at this : http://docs.aws.amazon.com/IAM/latest/UserGuide/role-usecase-ec2app.html Imagine that you are an administrator who manages your organization's AWS resources.
  Negative
Developers in your organization have applications that run on Amazon EC2 instances.
  Negative
These applications require access to other AWS resources—for example, making updates to Amazon S3 buckets.
  Negative
Applications that run on an Amazon EC2 instance must sign their AWS API requests with AWS credentials.
  Negative
One way to do this is for developers to pass their AWS credentials to the Amazon EC2 instance, allowing applications to use the credentials to sign requests.
  Negative
However, when AWS credentials are rotated, developers have to update each Amazon EC2 instance that uses their credentials.
  Neutral
and to see how to do this with python: https://groups.google.com/forum/?fromgroups=#!topic/boto-users/RPoFskVw1gc The basic procedure is as follows: First, you have to create a JSON policy document that represents what services and resources the IAM role should have access to.
  Negative
for example, this policy grants all S3 actions for the bucket "my_bucket".
  Negative
You can use whatever policy is appropriate for your application.
  Neutral
BUCKET_POLICY = """{ "Statement":[{ Next, you need to create an Instance Profile in IAM.
  Negative
import boto c = boto.connect_iam() instance_profile = c.create_instance_profile('myinstanceprofile') Once you have the instance profile, you need to create the role, add the role to the instance profile and associate the policy with the role.
  Negative
role = c.create_role('myrole') c.add_role_to_instance_profile('myinstanceprofile', 'myrole') c.put_role_policy('myrole', 'mypolicy', BUCKET_POLICY) Now, you can use that instance profile when you launch an instance: ec2 = boto.connect_ec2() ec2.run_instances('ami-xxxxxxx', ..., instance_profile_name='myinstanceprofile') And the new instance should have the appropriate role and credentials associated with it once it is launched.
  Negative
there are same tutorials for Java, Ruby, ... Amazon website.
  Negative
you can refer to first url to see other tutorials.
  Positive
563e32fd61a8013065267b6f	X	Thanks for your detailed reply Steffen :-) Yes, you are right - Properties window in VS does show the right region.
  Positive
563e32fd61a8013065267b70	X	If I create an S3 bucket as follows: As you see I have clearly specified to create my bucket in EU region, but when I go to AWS explorer, I can see my bucket available in all the regions.
  Positive
What is the point of specifying bucket region if my bucket is always replicated in all the regions?
  Neutral
Can anyone please clarify?
  Neutral
Thank you!
  Positive
563e32fd61a8013065267b71	X	Presumably you are referring to the Amazon S3 node within the AWS Explorer view of the AWS Toolkit for Eclipse or the AWS Toolkit for Microsoft Visual Studio?
  Negative
Amazon S3 is unique amongst the AWS services concerning its region handling in various ways (likely just a legacy issue due to it being one of the early offerings), which is also reflected in the AWS Management Console 'til this day: The major design aspect relevant here is that an S3 bucket name must be globally unique, no matter in which region you create it.
  Positive
That's probably why AWS has decided to show all buckets in a a single view rather than separated by region like all other services, which can admittedly be very confusing (and gets unwieldy as well with a growing number of buckets).
  Negative
However, the bucket still gets created in the region you specified, it just isn't obvious due to the unified presentation within the AWS Explorers and the AWS Management Console.
  Negative
You can see a bucket's region by opening its properties view (via the Properties context menu in the toolkits resp.
  Positive
the Properties button in the console).
  Negative
I just realized that the AWS Toolkit for Eclipse surprisingly lacks such a properties window for S3 buckets actually, which isn't only severely limiting its functionality (because you can't change advanced buckets options for example), but must be considered a notable usability bug in the light of your question.
  Negative
It's pretty puzzling actually, given the available view estate and the utterly simple API operation needed for this - I'm usually using both Visual Studio and Eclipse on a daily basis and have long switched to the AWS Toolkit for Microsoft Visual Studio due to its perceived performance benefits and greater and deeper service coverage to begin with, but wasn't aware of this really surprising omission yet.
  Negative
Accordingly, you'll need to resort to the AWS Management Console or the AWS Toolkit for Microsoft Visual Studio to visually inspect/verify your region for the time being.
  Negative
563e32fe61a8013065267b72	X	Would you like to share us your final decision on it?
  Neutral
563e32fe61a8013065267b73	X	I was disappointed by glusterfs performance / reliability under heavy IO loads.
  Negative
563e32fe61a8013065267b74	X	Can you please share what "heavy IO loads" mean?
  Positive
how many IOPS?
  Neutral
563e32fe61a8013065267b75	X	What happens if a node falls out?
  Negative
I'm curious about a "gluster" like setup, where the cluster can contribute data (for redundancy, or for additional storage, at the server's choice), and disconnect whenever it wants without destroying the "raid array".
  Neutral
563e32fe61a8013065267b76	X	Having used it extensively, I would describe the POSIX filesystem layer of ceph as experimental and horribly buggy, FYI.
  Negative
563e32fe61a8013065267b77	X	@PaulWheeler: I concur.
  Negative
what i wanted to note is that other non-fs-like layers (RADOS, rdb) are getting quite reliable.
  Negative
For POSIX compatibility, it seems MooseFS is much better.
  Negative
I'd love to see ceph-fs mature, since rdb is quite desirable to have in the same cluster...
563e32fe61a8013065267b78	X	Isn't this a duplicate?
  Positive
563e32ff61a8013065267b79	X	@dpavlin - does it matter if it's a duplicate?
  Negative
Yes, the answerer shouldn't have added it since it was already there, but downvoting just because it's a duplicate seems wrong
563e32ff61a8013065267b7a	X	Glusterfs is fat, eats lots of memory during high IO load, and very slow.
  Very negative
563e32ff61a8013065267b7b	X	Agreed with correction: MooseFS is now proprietary so its successor LizardFS is the best IMHO.
  Negative
563e32ff61a8013065267b7c	X	@Onlyjob - MooseFS is no longer proprietary
563e32ff61a8013065267b7d	X	Technically speaking.
  Negative
But it does not have public VCS nor bug tracker.
  Negative
What if author take down source archive and provide it by request again?
  Negative
LizardFS already has community behind it and (unlike MooseFS) LizardFS will be in Debian soon.
  Negative
LizardFS is unrestricted (i.e. no "community edition" etc.).
  Negative
563e32ff61a8013065267b7e	X	fhghfs, from the people who gave us the mp3 patent?
  Negative
563e32ff61a8013065267b7f	X	Experience confirms such claim.
  Negative
563e32ff61a8013065267b80	X	FhGFS is a proprietary software without sources.
  Negative
Don't waste everyone's time please.
  Positive
-1.
  Neutral
563e32ff61a8013065267b81	X	I have a lot of spare intel linux servers laying around (hundreds) and want to use them for a distributed file system in a web hosting and file sharing environment.
  Negative
This isn't for a HPC application, so high performance isn't critical.
  Negative
The main requirement is high availability, if one server goes offline, the data stored on it's hard drives is still available from other nodes.
  Negative
It must run over TCP/IP and provide standard POSIX file permissions.
  Negative
I've looked at the following: Lustre (http://wiki.lustre.org/index.php?title=Main_Page): Comes really close, but it doesn't provide redundancy for data on a node.
  Negative
You must make the data HA using RAID or DRBD.
  Neutral
Supported by Sun and Open Source, so it should be around for a while gfarm (http://datafarm.apgrid.org/): Looks like it provides the redundancy but at the cost of complexity and maintainability.
  Negative
Not as well supported as Lustre.
  Negative
Does anyone have any experience with these or any other systems that might work?
  Neutral
563e32ff61a8013065267b82	X	check also GlusterFS Edit (Aug-2012): Ceph is finally getting ready.
  Negative
Recently the authors formed Inktank, an independent company to sell commercial support for it.
  Negative
According to some presentaions, the mountable POSIX-compliant filesystem is the uppermost layer and not really tested yet, but the lower layers are being used in production for some time now.
  Negative
The interesting part is the RADOS layer, which presents an object-based storage with both a 'native' access via the librados library (available for several languages) and an Amazon S3-compatible RESP API.
  Negative
Either one makes it more than adequate for adding massive storage to a web service.
  Negative
This video is a good description of the philosophy, architecture, capabilities and current status.
  Positive
563e330061a8013065267b83	X	Gluster is getting quite a lot of press at the moment: http://www.gluster.org/
563e330061a8013065267b84	X	In my opinion, the best file system for Linux is MooseFS , it's quite new, but I had an opportunity to compare it with Ceph and Lustre and I say for sure that MooseFS is the best one.
  Very positive
563e330061a8013065267b85	X	If not someone forces you to use it, I would also highly recommend using anything else than Lustre.
  Negative
From what I hear from others and what also gave myself nightmares for quite some time is the fact that Lustre quite easily breaks down in all kinds of situations.
  Negative
And if only a single client in the system breaks down, it puts itself into an endless do_nothing_loop mode typically while holding some important global lock - so the next time another client tries to access the same information, it will also hang.
  Negative
Thus, you often end up rebooting the whole cluster, which I guess is something you would try to avoid normally ;) Modern parallel file systems like FhGFS (http://www.fhgfs.com) are way more robust here and also allow you to do nice things like running server and client components on the same machines (though built-in HA features are still under development, as someone from their team told me, but their implementation is going to be pretty awesome from what I've heard).
  Negative
563e330061a8013065267b86	X	Lustre has been working for us.
  Negative
It's not perfect but it's the only thing we have tried that has not broken down over load.
  Negative
We still get LBUGS from time to time and dealing with 100TB + file systems is never easy but the Lustre system has worked and increased both performance and availability.
  Negative
563e330061a8013065267b87	X	Ceph looks to be a promising new-ish entry into the arena.
  Negative
The site claims it's not ready for production use yet though.
  Negative
563e330061a8013065267b88	X	I read a lot about distributed filesystems and I think FhGFS is the best.
  Positive
http://www.fhgfs.com/ It worth a try.
  Positive
See more about it at: http://www.fhgfs.com/wiki/
563e330161a8013065267b89	X	Thanks for the answer, but the problem with Zencoder thumbnails is that you can only generate one along with transcoding a video--you can't just generate a thumbnail alone later on.
  Negative
I'll look at Transloadit but it's not really reasonable for us to switch services right now.
  Negative
I'd really appreciate an answer that uses only scripts on my own server to handle this.
  Neutral
563e330161a8013065267b8a	X	I have videos hosted on Amazon S3.
  Negative
I encode them with Zencoder and store a thumbnail for the video then using Zencoder.
  Negative
However, I need a way to generate thumbnails at certain points in the video (i.e. 00:00:03, 00:10:32, 01:40:18) and store them either on S3 or my server.
  Neutral
ffmpeg allows remote thumbnailing, however it takes a very long time (sometimes several minutes) to get a thumbnail from the middle of a file--I believe this is because it downloads the entire file up to that point to get the thumbnail.
  Very negative
My plan is to somehow download the header of the video file via HTTP byte-range request, guesstimate the byte range where I should be looking for the thumbnail, download about a second of video from that part of the file via HTTP byte-range request, then save the header and tiny video locally.
  Negative
I pull the thumbnail from that using ffmpeg and delete the temporary video.
  Negative
I have no idea on how exactly this would work (I believe the H.264 MP4 files I'm working with have a dynamic length header, for another issue).
  Negative
Any suggestions or better ideas?
  Negative
Edit: To clarify, Zencoder thumbnailing is great, but they only allow thumbnail creation in combination with transcoding.
  Negative
I don't want to transcode my video every time I create a new thumbnail, so I need to do this on my own without Zencoder.
  Negative
563e330161a8013065267b8b	X	As expected, a quick search through the Zencoder documentation reveals similar functionality to be available there as well, please check their API reference for Thumbnails: And (similar to Transloadit), Zencoder seems to support to upload and download files from your Amazon S3 bucket as well, see Using Zencoder with S3 for details.
  Negative
Good luck!
  Positive
Since you are using a cloud encoding service anyway, I'm going to take "Any suggestions or better ideas?"
  Negative
literally here and recommend to check out Transloadit eventually, insofar their offering includes your desired functionality (I'd actually expect this to be available from Zencoder as well Zencoder offers similar functionality indeed, see update above) - there are several demos for Thumbnail extraction from videos, e.g. Extract 8 thumbnails from an encoded video: This is the simplest demo to extract thumbnails from a video encoding.
  Very negative
By default it extracts 8 thumbnails at equal time intervals each having the same dimensions as the video. »
  Negative
See full documentation The offset parameter of the /video/thumbs robot allows you to specify the thumbnail position more fine grained in either seconds of the file duration or respective percentage values instead.
  Negative
Transloadit supports Storing files in Amazon S3 as well, see e.g. the demo Encode a video, extract 8 thumbnails and store everything in your S3 bucket for a combined solution addressing your use case.
  Negative
563e330261a8013065267b8c	X	And what are the "clients" here?
  Neutral
563e330261a8013065267b8d	X	And yes, I have an idea -- in fact it is a project I will be working on in a few days' time: a FileSystem implementation over S3; I already have a working implementation over DropBox.
  Negative
563e330261a8013065267b8e	X	Why do you want to 'shield' S3?
  Negative
Are you concerned about introducing a security hole?
  Negative
With a proper policy, you allow users to upload, without exposing any other files.
  Negative
Solution may depend on what you are really trying to accomplish.
  Negative
563e330261a8013065267b8f	X	The clients are a javascript app and mobile apps.
  Negative
I think I didn't express myself correctly.
  Negative
The "shielding" part is not for security, I simply want to be able to reliably record data about a file, such as knowing who uploaded it, its size and so on.
  Negative
By giving clients access directly to S3, I can't know who uploaded file X or Y, right?
  Negative
( users may sign up for free with email, using IAM is not feasible )
563e330261a8013065267b90	X	Awesome, exactly the functionality I was looking for.
  Negative
Only have to figure out a way for native mobile apps to do this, but thanks a lot!
  Neutral
563e330261a8013065267b91	X	If you're using a mobile app, the better method would be to generate temporary credentials via Security Token Service (STS) and use the native mobile SDK to upload files to S3.
  Negative
See: docs.aws.amazon.com/STS/latest/UsingSTS/STSUseCases.html
563e330261a8013065267b92	X	That looks like a better solution, reading its docs right now.
  Negative
Thanks again !
  Positive
563e330261a8013065267b93	X	I am trying to figure out the simplest method for allowing clients to upload media (photos and video) to my S3 bucket, without giving them direct access, or using pre-signed URLs.
  Negative
The idea is that I don't want any kind of media processing to occur, the only thing that I am interested in is to shield the S3 bucket from direct contact with the clients, and record information about the files being uploaded (such as size, type etc.).
  Negative
Do you have any ideas on how this architecture might be implemeted in a simple way?
  Negative
563e330261a8013065267b94	X	To upload a file from a mobile application to Amazon S3: The temporary credentials can be granted a limited set of permissions (eg upload to a specific bucket and path) and are valid only for a limited duration, up to one hour.
  Negative
This is good security practice because no permanent credentials are kept on the mobile device.
  Neutral
Use a browser-based upload via an HTML form.
  Positive
This allows a form in an HTML page to securely upload directly the Amazon S3 -- even to private folders.
  Positive
It uses a signed policy to define the permitted action (eg upload to a specific location, up to a certain file size, using a particular permission set).
  Positive
The form can be static -- no need to recalculate signatures for every individual file to be uploaded.
  Negative
See: Authenticating Requests in Browser-Based Uploads Using POST
563e330361a8013065267b95	X	I am struggling with the same question.
  Negative
Authentication seems to be the elephant in the REST-room...
563e330361a8013065267b96	X	jbandi, this question is more about securing the traffic rather than authentication (if I am reading this right).
  Negative
If you have a specific question about how to handle authentication in RESTful/Web APIs, I wouldn't mind answering it.
  Negative
Once its laid out, the authentication/session management options are identical to traditional web applications.
  Negative
563e330361a8013065267b97	X	Great answer for a typical web app.
  Negative
But I believe OP is asking about the options for authentication for a REST API server.
  Negative
Session management is not required for example if your REST API uses HTTP Basic Auth/Digest Auth.
  Negative
563e330361a8013065267b98	X	@jemeshsu they are one and the same.
  Neutral
There is little difference between creating a REST API and a traditional web application in terms of session management, authentication/authorization, and transmission encryption.
  Negative
563e330361a8013065267b99	X	I have to lay out a plan to develop a RESTful API (Python/Flask) that could be used by our future web app (Angularjs) and mobile apps (iOS/Android).
  Negative
I have been researching for three days and have come across several scenarios: Using HTTPS is one way on top of the methods below to keep it safer.
  Negative
But https is slower, which could mean we need faster and more expensive servers.
  Negative
How to keep the private key “secure” in a pure HTML5 app ?
  Neutral
You are exactly right; in a pure HTML5 (JS/CSS/HTML) app, there is no protecting the key.
  Negative
You would do all communication over HTTPS in which case you wouldn’t need a key since you could safely identify a client using a standard API_KEY or some other friendly identifier without the need or complexity of an HMAC.
  Negative
So in other words there is even no point of using the method for an web app in first place.
  Negative
And honestly I don't understand how this should work on the mobile device either.
  Positive
A user downloads our app and how do I send the private key from the iphone to the server?
  Negative
The moment I transferred it, it will be compromised.
  Positive
The more I am researching the more indecisive I am getting.
  Negative
I was hoping to ask some pros who have done this previously and could share their experience.
  Neutral
Many Thanks
563e330361a8013065267b9a	X	You seem to be confusing/merging two different concepts together.
  Negative
We start of talking about encrypting traffic (HTTPS) and then we start talking about different ways to manage authenticated sessions.
  Neutral
In a secure application these are not mutually exclusive tasks.
  Negative
There also seem to potentially be a misunderstanding how session management can impact authentication.
  Negative
Based on that I will provide a primer on web application/web api session management, authentication, and encryption.
  Negative
Session Management HTTP transactions are stateless by default.
  Negative
HTTP does not specify any method to let your application know that a HTTP request has been sent from a specific user (authenticated or not).
  Negative
For robust web applications, this is not acceptable.
  Negative
We need a way to associate requests and data made across multiple requests.
  Negative
To do this, on initial request to the server a user needs to be assigned a "session".
  Negative
Generally sessions have some kind of unique id that is sent to the client.
  Neutral
The client sends that session id with every request and the server uses the session id sent in every request to properly prepare a response for the user.
  Negative
It is important to remember that a 'session id' can be called many other things.
  Negative
Some examples of those are: session token, token, etc.
  Negative
For consistency I will use 'session id' for the rest of this response.
  Negative
Each HTTP request from the client needs to include the session id; this can be done in many ways.
  Negative
Popular examples are: Most web application frameworks use cookies.
  Negative
However application that rely on JavaScript and single page designs may opt to use a HTTP header/store it in some other location that is observable by the server.
  Neutral
It is very important to remember that the HTTP response that notifies the client of their session id and the client's requests that contain the session id are completely plain text and 100% unsafe.
  Negative
To battle that, all HTTP traffic needs to be encrypted; that is where HTTPS comes in.
  Negative
It is also important to point out we have not talked about linking a session to a specific user in our system.
  Negative
Session management is just associating data to a specific client accessing our system.
  Negative
The client can be in both authenticated and unauthenticated states, but in both states they generally have a session.
  Neutral
Authentication Authentication is where we link a session to a specific user in our system.
  Negative
This is generally handled by a login process where a user supplies credentials, those credentials are verified, and then we link a session to a specific user record in our system.
  Negative
The user is in turn associated with privileges for fine grained access control via access control lists and access control entries (ACL and ACE).
  Negative
This is generally referred to as "Authorization".
  Neutral
Most system always have both Authentication and Authorization.
  Positive
In some simple systems all authenticated users are equals in which case you won't have authorization past simple authentication.
  Negative
Further information on this is out of scope for this question, but consider reading about ACE/ACL.
  Negative
A specific session can be flagged as representing an authenticated user in different ways.
  Negative
Either option is fine.
  Positive
It generally comes down to the technology you are working in and what they offer by default.
  Neutral
A client generally initiates the authentication process.
  Negative
This can be done by sending credentials to a specific url (e.g. yoursite.com/api/login).
  Negative
However if we want to be 'RESTful' we generally would referencing a resource by some noun and doing the action of 'create'.
  Negative
This could be done by requiring a POST of the credentials to yoursite.com/api/authenticatedSession/.
  Negative
Where the idea would be to create an authenticated session.
  Negative
Most sites just POST the credentials to /api/login or the like.
  Negative
This is a departure from "true" or "pure" RESTful ideals, but most people find this a simpler concept rather than thinking of it as "creating an authenticated session".
  Neutral
Encryption HTTPS is used to encrypt HTTP traffic between a client and server.
  Negative
On a system that relies on authenticated and unauthenticated users, all traffic that relies on a user being authenticated needs to be encrypted via HTTPS; there is no way around this.
  Negative
The reason for this is that if you authenticate a user, share a secret with them (their session id, etc) and then begin to parade that secret in plain HTTP their session can be hijacked by man-in-the-middle attacks.
  Negative
A hacker will wait for for the traffic to go through an observed network and steal the secret (since its plain text over HTTP) and then initiate a connection to your server pretending to be the original client.
  Negative
One way people combat this is by associating the requests remote IP address to an authenticated session.
  Negative
This is ineffective alone as any hacker will be able to spoof their requests remote IP address in their fake requests and then observe the responses your sever is sending back.
  Negative
Most would argue that this is not even worth implementing unless you are tracking historical data and using it to identify a specific user's login patterns (like Google does).
  Negative
If you need to split up your site between HTTP and HTTPS sections, it is imperative that the HTTP traffic does not send or receive the session id or any token used to manage the authentication status of a user.
  Negative
It is also important that you do not send sensitive application data within non-HTTPs requests/responses.
  Neutral
The only way to secure data within web applications/APIs is to encrypt your traffic.
  Negative
Basic-Http-Auth This is a method for authenticating by web resource only.
  Neutral
Basic authentication authenticates uses by resource identified by URL.
  Negative
This was most popularly implemented by Apache HTTP Web Server with the use of .
  Negative
htaccess based directory/location authentication.
  Neutral
Credentials have to be sent with each request; clients generally handled this transparently for users.
  Negative
Basic authentication can be used by other systems as a mode of authentication.
  Negative
However, the systems that utilize Basic-Http-Auth are providing authentication and session management, not the Basic-Http-Auth itself.
  Neutral
Digest-Auth This is exactly the same as Basic-Http-Auth with the addition of some simple MD5 digesting.
  Negative
This digesting should not be relied upon instead of using encryption.
  Negative
OAuth OAuth just lets you have an external service validate credentials.
  Negative
After that it is up to you to manage/work with the result of authentication request to your OAuth provider.
  Neutral
Gangster Handshake / Custom HTTP header "Custom HTTP header" is a type of "Gangster Handshakes"; as such I will use the same section to discuss them.
  Negative
The only difference is that a "Custom HTTP header" is specifying where the hanshake (session id, token, user authentication toke, etc) will be stored (i.e. in a HTTP header).
  Negative
It is important to note that these do not specify how authentication will be handled, nor do they specify how session management will be handled.
  Negative
They essentially describe how and where session ids/authentication tokens will be stored.
  Negative
Authentication would need to be handled by your application or via a third party (e.g. OAuth).
  Negative
Session management will still need to be implemented as well.
  Negative
The interesting thing is you can choose the merge the two if you wish.
  Positive
...I highly suggest you make sure that you understand that a robust web application that is secure needs the following: Authorization relies upon Authentication.
  Positive
Authentication relies upon Session Management and Encryption makes sure the session isn't hijacked and that the credentials are not intercepted.
  Negative
Flask-Login I think you should look into flask-login as a way to avoid re-implementing the wheel.
  Negative
I have personally never used it (I use pyramid for web applications in python).
  Negative
However, I have seen it mentioned before in web application/python boards.
  Neutral
It handles both authentication and session management.
  Neutral
Throw your web api/application through HTTPS and you have all three (Encryption, Session Management, and User Authentication).
  Negative
If you do not / can not use flask-login, be prepared to write your own, but do research first on how to create secure authentication mechanisms.
  Negative
If at all possible, if you do not understand how to write an authentication procedure please do not attempt it without first learning how hackers use pattern based attacks, timing attacks, etc.
  Negative
Please Encrypt Your Traffic ...move past the idea that you can avoid using HTTPS with some "clever" token use.
  Neutral
Move past the idea that you should avoid using HTTPS/encryption because "its slow", process intensive, etc.
  Negative
It is process intensive because it is an encryption algorithm.
  Negative
The need to ensure the safety of your user's data and your applications data should always be your highest priority.
  Negative
You do not want to go through the horror of notifying your users that their data was compromised.
  Negative
563e330361a8013065267b9b	X	The https it is slower, but not a not.
  Negative
Only the handshaking is slower.
  Neutral
For us the biggest problem it is to upkeep the key pair on server-mobiles side and the rights.
  Negative
We have implemented a message digest too.
  Negative
The problem it is: is hard to set up the php-android-ios version properly.
  Negative
After this is done ( a parameter need to changes what is suggesting Google at first results only at android side) the problem will be with low-end devices: to much CPU usage, slow on decrypt-encrypt process, a lot slower than https, especially when you need to transform 10kb String(can take several minutes).
  Negative
If I don't transfer Nasa data to Hamas, than I would go with a very simple encryption over simple HTTP: like invert the bits or so...
563e330461a8013065267b9c	X	Is that your real private key you are showing us here?
  Negative
563e330461a8013065267b9d	X	NO, that is not my real private key... do you have any sample for me?
  Negative
563e330461a8013065267b9e	X	trying Fiddler first, i'll be back and report my result.
  Negative
.
  Neutral
563e330461a8013065267b9f	X	Alternatively, TIdHTTP can give you access to the XML in your code.
  Negative
A 403 reply will cause an EIdHTTPProtocolException exception to be raised.
  Negative
The XML will be in the EIdHTTPProtocolException.ErrorMessage property.
  Neutral
563e330461a8013065267ba0	X	Also, you can use Indy's own TIdLog... components, such as TIdLogFile or TIdLogEvent, to see exactly what TIdHTTP is sending and receiving at the socket layer.
  Negative
563e330461a8013065267ba1	X	Awesome to learn about this - so you would hardly need Fiddler!
  Positive
563e330461a8013065267ba2	X	I do not know my code is right or wrong.
  Negative
when i try to run a program error occurs 403.
  Negative
.
  Neutral
can any body trace my error??
  Negative
563e330561a8013065267ba3	X	Amazon actually sends back a XML document that precisely describes why you got the 403 error.
  Negative
The easiest way to see the message would be to use Fiddler and set up your Indy HTTP to use 127.0.0.1 as a proxy.
  Negative
That way all your traffic goes through Fiddler and you'll see both what you sent and what Amazon returned.
  Negative
When I implemented my REST API to work with the Amazon S3 service I had some problems figuring out the "Canonical Headers" that need to be signed.
  Negative
Happily the Amazon API sends you back the text they're signing to test your signature, so you can compare that byte-by-byte and figure out if you're doing it wrong.
  Positive
Failure to prepare those "canonical headers" exactly as they're preparing those headers will obviously result in an 403.
  Negative
For example the line separator Amazon is using is LINEFEED (#10).
  Negative
Since you're putting your headers in a TMemo, you're going to get the Windows-style CRLF separator.
  Negative
That alone is enough for your code to fail.
  Negative
An other thing I had problems with was sending the extra headers with my Indy requests.
  Negative
I was following the on-line API samples, looking at what I'm supposed to send and what Amazon is supposed to answer.
  Negative
Fiddler was the only way to actually test and see what I'm sending, as opposed to what I thought I was sending.
  Negative
For example I mistakenly used TIdHttp.Request.RawHeaders to write my custom headers, but those headers get flushed while the Request is prepared.
  Negative
I was supposed to write my headers to TIdHttp.Request.CustomHeaders - but without Fiddler's help I wouldn't know I'm not actually sending my headers.
  Negative
My code looked just fine.
  Neutral
563e330561a8013065267ba4	X	Did you fix the FileNotFoundException?
  Neutral
563e330561a8013065267ba5	X	Yes, my answer helped me as a workaround.
  Positive
563e330561a8013065267ba6	X	Hi, I have managed to introduce the URI of the file into the distributed cache.
  Negative
However, when I try to read it from the mapper, a file not found exception occurs.
  Neutral
I am working on Amazon EMR and S3, and, at the moment, I am using the new Hadoop API (2.4.0).
  Negative
I have checked the file location and everything seems to be in place (other s3 files have been used without problems).
  Negative
563e330561a8013065267ba7	X	I am actually working with the SDK provided by amazon for Elastic MapReduce, so I am not using the command line at all.
  Negative
However, I appreciate your answer, I will look forward to it.
  Neutral
563e330561a8013065267ba8	X	I am trying to use Hadoop in java with multiple input files.
  Negative
At the moment I have two files, a big one to process and a smaller one that serves as a sort of index.
  Positive
My problem is that I need to maintain the whole index file unsplitted while the big file is distributed to each mapper.
  Negative
Is there any way provided by the Hadoop API to make such thing?
  Neutral
In case if have not expressed myself correctly, here is a link to a picture that represents what I am trying to achieve: picture Update: Following the instructions provided by Santiago, I am now able to insert a file (or the URI, at least) from Amazon's S3 into the distributed cache like this: However, when the mapper tries to read it a 'file not found' exception occurs, which seems odd to me.
  Very negative
I have checked the S3 location and everything seems to be fine.
  Positive
I have used other S3 locations to introduce the input and output file.
  Negative
Error (note the single slash after the s3:) FileNotFoundException: s3:/myBucket/input/index.
  Negative
txt (No such file or directory) The following is the code I use to read the file from the distributed cache: I am using Amazon's EMR, S3 and the version 2.4.0 of Hadoop.
  Negative
563e330561a8013065267ba9	X	You could push the index file to the distributed cache, and it will be copied to the nodes before the mapper is executed.
  Negative
See this SO thread.
  Positive
563e330561a8013065267baa	X	As mentioned above, add your index file to the Distributed Cache and then access the same in your mapper.
  Negative
Behind the scenes.
  Neutral
Hadoop framework will ensure that the index file will be sent to all the task trackers before any task is executed and will be available for your processing.
  Negative
In this case, data is transferred only once and will be available for all the tasks related your job.
  Negative
However, instead of add the index file to the Distributed Cache in your mapper code, make your driver code to implement ToolRunner interface and override the run method.
  Neutral
This provides the flexibility of passing the index file to Distributed Cache through the command prompt while submitting the job If you are using ToolRunner, you can add files to the Distributed Cache directly from the command line when you run the job.
  Neutral
No need to copy the file to HDFS first.
  Neutral
Use the -files option to add files You can access the files in your Mapper or Reducer code as below:
563e330661a8013065267bab	X	Here's what helped me to solve the problem.
  Negative
Since I am using Amazon's EMR with S3, I have needed to change the syntax a bit, as stated on the following site.
  Negative
It was necessary to add the name the system was going to use to read the file from the cache, as follows: job.addCacheFile(new URI("s3://myBucket/input/index.
  Negative
txt" + "#index.
  Negative
txt")); This way, the program understands that the file introduced into the cache is named just index.txt.
  Negative
I also have needed to change the syntax to read the file from the cache.
  Negative
Instead of reading the entire path stored on the distributed cache, only the filename has to be used, as follows:
563e330661a8013065267bac	X	Why aren't you using the official PHP SDK for all of this work?
  Negative
You're re-inventing the wheel here.
  Neutral
563e330661a8013065267bad	X	I'm using below library for codeigniter github.com/psugand/CodeIgniter-S3.
  Negative
Amazon PHP SDK consist of core PHP.
  Negative
Is there any library which supports codeigniter and givesme session token?
  Negative
please help me
563e330661a8013065267bae	X	I'm new to AWS can anyone please help me how to generate session token using STS API to upload files to S3 Brief: I went through AWS documentation and researched on Google I have found below library for codeigniter to upload files to S3 https://github.com/psugand/CodeIgniter-S3 It is working fine and I'm able to upload files using my Access ID and secret key.
  Negative
But our requirement is to generate get temporary credentials from Amazon and send to iOS developers so that they can upload files directly to S3.
  Negative
I found below link on Amazon documentation where I need to follow 4 Tasks to get the temporary credentials.
  Negative
http://docs.aws.amazon.com/general/latest/gr/sigv4-create-canonical-request.html But some how response always says signature that I'm creating is not matching.
  Negative
Below is my code and response from Amazon.
  Neutral
If I'm doing anything wrong please help me.
  Positive
Task 1 Canonical Request Task 2 creating String-to-sign Task 3 Calculating Signature Task 4 Add the Signing Information to the Request Executing Curl Request Response from Amazon I'm using codeiginter for this.
  Negative
Please help me .
  Neutral
Thanks in advance
563e330661a8013065267baf	X	That works like a charm, thank you.
  Very positive
563e330661a8013065267bb0	X	I am attempting to create a REST API in PHP and I'd like to implement an authentication scheme similar to Amazon's S3 approach.
  Negative
This involves setting a custom 'Authorization' header in the request.
  Neutral
I had thought I would be able to access the header with $_SERVER['HTTP_AUTHORIZATION'], but it's nowhere to be found in var_dump($_SERVER).
  Negative
The apache_request_headers() function would solve my problem, but my host implements PHP as CGI, so it's unavailable.
  Negative
Is there another way I can access the complete request headers in PHP?
  Negative
563e330661a8013065267bb1	X	You'll need to do some mod_rewrite wizardry to get your headers past the CGI barrier, like so: Note that if you're using mod_rewrite for other purposes, it could end up being $_SERVER['REDIRECT_HTTP_AUTHORIZATION'].
  Negative
563e330661a8013065267bb2	X	Try When using CGI interface instead of Apache Module interface, HTTP headers should be available as environment variables.
  Negative
563e330761a8013065267bb3	X	There is a fantastic PECL extension that allows for all sorts of HTTP related access.
  Positive
PECL_HTTP and more specifically http://php.net/http and http://php.net/manual/en/function.http-get-request-headers.php.
  Positive
563e330761a8013065267bb4	X	I have a node app and using the aws-sdk I'm able to successfully call the getSignedUrl() method and get a url to a specific file.
  Negative
However I'd like to be able to grant * access recursively inside a specific directory rather than just a single file.
  Neutral
Is this even possible?
  Neutral
563e330761a8013065267bb5	X	A Pre-Signed URL permits access to private objects stored on Amazon S3.
  Negative
It is a means of keeping objects secure, yet grant temporary access to a specific object.
  Positive
It is created via a hash calculation based on the object path, expiry time and a shared Secret Access Key belonging to an account that has permission to access the Amazon S3 object.
  Positive
As such, each pre-signed URL is unique to each object and cannot act as a wildcard for an entire directory.
  Negative
Some alternatives: See also: AWS CLI copy command
563e330761a8013065267bb6	X	Without commenting on the actual question - you don't actually need to have three copies of it around at once - as you describe it, you can discard the BufferedImage before you call toByteArray() to build the third copy.
  Negative
563e330861a8013065267bb7	X	this is not a discussion forum, and you are trying to turn this Question into a debate.
  Negative
Alternatively, if you are trying to get something done about this, you are talking to the wrong people.
  Negative
You might have more luck if you put together a concrete proposal >>WITH WORKING CODE<< and lots of motivating examples, and submitted it to the Apache Commons IO folks.
  Negative
563e330861a8013065267bb8	X	That is another bad thing, as the class is locked now to that implementation/internals as it exposed it.
  Negative
563e330861a8013065267bb9	X	This will fail if the internal array needs to be reallocated due to continued output into the ByteArrayOutoutStream after the internal array has been wrapped in a ByteArrayInputStream.
  Negative
The reallocation cannot be monitored because it takes place in a private method.
  Negative
563e330861a8013065267bba	X	>> The current implementation uses a single byte array .... That doesn't prevent you to have InputStream that knows all the internals and does exactly what is needed.
  Negative
BTW, commons-io has different implementation and private method that actually provides InputStream
563e330861a8013065267bbb	X	>> is thread safe: i am sorry i was not clear on this.
  Negative
I didn't want to say the is not thread safe.
  Negative
It is, you can not corrupt the internal state.
  Negative
But how useful is to write to it from multiple threads.
  Neutral
It is similar with StringBuffer that being thread safe i almost useless.
  Negative
563e330861a8013065267bbc	X	>> The API would need to be more complicated... I guess you want to say the implementation would be more complicated, but that is not the concern.
  Negative
563e330961a8013065267bbd	X	>> When you expose the byte array... I did not ask to expose the byte array.
  Negative
But my idea was to have something like getInputStream() that will actually have read-only access.
  Negative
563e330961a8013065267bbe	X	@Op De Cirkel - I'm not going to debate this.
  Negative
SO is not a discussion forum.
  Negative
563e330961a8013065267bbf	X	I know that pipes are way to approach the problem, and it has it's own pros and cons.
  Negative
But my questions is about ByteArrayOutputStream.
  Neutral
>> It wasn't designed for what you have in mind.
  Negative
Whatever is the purpose, you always have to duplicate the array?
  Negative
It simply doesn't feel right.
  Negative
563e330961a8013065267bc0	X	It's an easy way to capture byte-oriented output that would otherwise have to go to a file, socket, or some other hard-to-recover destination.
  Negative
The purpose of forcing a copy of the contents is to insulate the result from the effects of further writes (kind of the opposite of what you want, I gather).
  Negative
563e330961a8013065267bc1	X	There are many java standard and 3rd party libraries that in their public API, there are methods for writing to or reading from Stream.
  Negative
One example is javax.imageio.ImageIO.write() that takes OutputStream to write the content of a processed image to it.
  Positive
Another example is iText pdf processing library that takes OutputStream to write the resulting pdf to it.
  Negative
Third example is AmazonS3 Java API, which takes InputStream so that will read it and create file in thir S3 storage.
  Negative
The problem araises when you want to to combine two of these.
  Negative
For example, I have an image as BufferedImage for which i have to use ImageIO.write to push the result in OutputStream.
  Negative
But there is no direct way to push it to Amazon S3, as S3 requires InputStream.
  Negative
There are few ways to work this out, but subject of this question is usage of ByteArrayOutputStream.
  Negative
The idea behind ByteArrayOutputStream is to use an intermidiate byte array wrapped in Input/Output Stream so that the guy that wants to write to output stream will write to the array and the guy that wants to read, will read the array.
  Negative
My wondering is why ByteArrayOutputStream does not allow any access to the byte array without copying it, for example, to provide an InputStream that has direct access to it.
  Negative
The only way to access it is to call toByteArray(), that will make a copy of the internal array (the standard one).
  Negative
Which means, in my image example, i will have three copies of the image in the memory: How this design is justified?
  Neutral
Moreover, there is second flavor of ByteArrayOutputStream, provided by Apache's commons-io library (which has a different internal implementation).
  Negative
But both have exactly the same public interface that does not provide way to access the byte array without copying it.
  Negative
563e330961a8013065267bc2	X	Luckily, the internal array is protected, so you can subclass it, and wrap a ByteArrayInputStream around it, without any copying.
  Negative
563e330961a8013065267bc3	X	My wondering is why ByteArrayOutputStream does not allow any access to the byte array without coping it, for example, to provide an InputStream that has direct access to it.
  Negative
I can think of four reasons: The current implementation uses a single byte array, but it could also be implemented as a linked list of byte arrays, deferring the creation of the final array until the application asks for it.
  Negative
If the application could see the actual byte buffer, it would have to be a single array.
  Negative
Contrary to your understanding ByteArrayOutputStream is thread safe, and is suitable for use in multi-threaded applications.
  Positive
But if direct access was provided to the byte array, it is difficult to see how that could be synchronized without creating other problems.
  Negative
The API would need to be more complicated because the application also needs to know where the current buffer high water mark is, and whether the byte array is (still) the live byte array.
  Negative
(The ByteArrayOutputStream implementation occasionally needs to reallocate the byte array ... and that will leave the application holding a reference to an array that is no longer the array.)
  Negative
When you expose the byte array, you allow an application to modify the contents of the array, which could be problematic.
  Negative
How this design is justified?
  Neutral
The design is tailored for simpler use-cases than yours.
  Neutral
The Java SE class libraries don't aim to support all possible use-cases.
  Negative
But they don't prevent you (or a 3rd party library) from providing other stream classes for other use-cases.
  Negative
The bottom line is that the Sun designers decided NOT to expose the byte array for ByteArrayOutputStream, and (IMO) you are unlikely to change their minds.
  Negative
(And if you want to try, this is not the right place to do it.
  Negative
You might have more success convincing the Apache Commons IO developers of the rightness of your arguments, provided that you can come up with an API design that isn't too dangerous.
  Neutral
Alternatively, there's nothing stopping you from just implementing your own special purpose version that exposes its internal data structures.
  Negative
The code is GPL'ed so you can copy it ... subject to the normal GPL rules about code distribution.
  Negative
563e330961a8013065267bc4	X	I think that the behavior you are looking for is a Pipe.
  Negative
A ByteArrayOutputStream is just an OutputStream, not an input/output stream.
  Negative
It wasn't designed for what you have in mind.
  Negative
563e330961a8013065267bc5	X	Brilliant!
  Positive
You've helped me understand things immensely.
  Positive
First of all, I now see "how" the files are actually getting saved to disk (this was previously unknown to me).
  Negative
It is happening in GetStream of MultipartFileStreamProvider (which is base of MultipartFormDataStreamProvider, from which my custom provider is deriving).
  Negative
Now, what I can do is derive directly from MultipartStreamProvider and override GetStream so that it does not save to disk but rather commits to S3, as you have suggested.
  Negative
Many thanks!
  Positive
563e330a61a8013065267bc6	X	I would really like to see the part clarified where the stream gets written to the myAWSStream.
  Negative
I would like to write something to an Azure Blob storage instead of AWS.
  Negative
But I don't know how (in method GetStream) to get access to the stream.
  Negative
In this method I would like to do: BlobService.StoreImageToBlobFromStream(stream)
563e330a61a8013065267bc7	X	That the solution appears to be "let's create our own class by copying a load of code from this class and the rest from this class" almost makes me cry.
  Negative
563e330a61a8013065267bc8	X	Yeah it is unexplainable that MS would ship an API that requires access to the filesystem to do processing of such a basic scenario.
  Negative
It'd like they couldn't think that people might not to want to have access to file writing and perform disk IO to handle a form POST.
  Negative
Grr.
  Neutral
563e330a61a8013065267bc9	X	@Kiran can you please elaborate on the full solution this is unusable to me as I don't understand where you got myAWSStream var.
  Negative
563e330a61a8013065267bca	X	I have an ASP.Net Web API application that allows clients (html pages and iPhone apps) to upload images to.
  Negative
I am using an async upload task as described in this article.
  Negative
Everything works great when I want to save to the file system because that's what this code does automatically, behind the scenes it seems.
  Positive
But, I don't want to save the uploaded files to the file system.
  Negative
Instead, I want to take the uploaded stream and pass it through to an Amazon S3 bucket using the AWS SDK for .
  Negative
Net.
  Neutral
I have the code set up to send the stream up to AWS.
  Neutral
The problem I can't figure out is how to get the uploaded content stream from the Web API method instead of having it automatically save to disk.
  Negative
I was hoping there would be a virtual method I could override in MultipartFormDataStreamProvider which would allow me to do something else with the uploaded content other than save to disk, but there doesn't seem to be.
  Negative
Any suggestions?
  Neutral
563e330a61a8013065267bcb	X	You could override MultipartFormDataStreamProvider's GetStream method to return a stream which is not a file stream but your AWS stream, but there are some issues doing so(which I will not elaborate here).
  Negative
Instead you could create a provider deriving from the abstract base class MultipartStreamProvider.
  Neutral
Following sample is heavily based on the actual source code of MultipartFormDataStreamProvider and MultipartFileStreamProvider.
  Neutral
You can check here and here for more details.
  Neutral
Sample below:
563e330a61a8013065267bcc	X	Up until recently, I've been hosting my dev app on a single heroku instance.
  Negative
The API and the angular app are hosted from the same Express.js server.
  Negative
Along with a front-facing sales page, separate from the angular app, sitting at the base domain.
  Negative
It's time for production, so I need to split the two into the api server and any other service to host the static pages.
  Negative
My Current Setup all on an express.js server I would love to use a service like Amazon S3, Cloudfront, or Divshot to host #1, #3, and #4 on a CDN service.
  Negative
And keep the API (#2) on Heroku or AWS ec2.
  Positive
Right now, my only thought is that I'd have to run a separate server myself and host the static files with Nginx, apache, or express.js to allow for the routing based on url because as far as I can find, the CDN services don't allow for .
  Negative
htaccess redirecting and such.
  Neutral
Thank you for the time.
  Positive
563e330a61a8013065267bcd	X	To simplify, you only have 2 categories here: You should consider exposing all your static assets under mydomain.com/* hosted from whatever CDN, and have the dynamic stuff being served from your (probably AWS) instance @ api.mydomain.com It is not the exact layout that you had in mind, but that one is trivial to setup.
  Negative
The only minor addition here, is that you might need to setup CORS due to the double domains.
  Negative
563e330b61a8013065267bce	X	Can you rephrase the first sentence "I have the name of files in the list with folders."
  Neutral
to something for understandable?
  Neutral
Which list?
  Neutral
563e330b61a8013065267bcf	X	@J.
  Neutral
C.Leitão , I am getting that list from amazon S3 and i want o index the list in my database
563e330b61a8013065267bd0	X	thanks for the info.
  Negative
I am using amazon S3 as my storage.
  Negative
Also i am thinking of building the index every week or when i add new files.
  Neutral
I am thinking of first removing the old notes and repopulating every time.
  Negative
i run the cron job of updating the file index.
  Positive
Is that ok
563e330b61a8013065267bd1	X	i didn't undertsand , why i don't need to update the index.
  Negative
because suppose i delete 3 files from S3 but my database will still have those files
563e330b61a8013065267bd2	X	I mean, if you use django-admin to manage your files you don't need to update index.
  Negative
For example if you are going to delete or create a file you should delete or create it through django-admin.
  Negative
(for delete you have to create signal in which you delete a file physically stackoverflow.com/questions/5372934/…).
  Negative
Of course if you use external tools to manage your files that don't work through django you should maintain your index (tables in this case).
  Negative
563e330b61a8013065267bd3	X	I have the name of files in the list with folders.
  Negative
The list contains 2000 file names like this and so on.
  Negative
I want to index those files in database so that i can have hierarchical directory structure.
  Negative
IN my Django app i want to display first the root level menus like countries --- US , Australia, canada Then i someone click on country then it get the second level of folders and so on and in end i want to see files if there are no more folders.
  Negative
rather than querying my storage evry time , i want to store all that info in my database so that my web pages are displayed from DB and when user click download then i get the file from my Storage i am not able to find how should i make the Model or database table for that
563e330b61a8013065267bd4	X	I suggest following way: Create models to store your tree structure and files for example: After that move your files in one or few directories (read this How many files in a directory is too many?)
  Very negative
also you can rename them (for example by using hash from files).
  Negative
Update the model File to put there new paths to your files.
  Negative
Having done this you are able to easy show files and build path to files etc.
  Positive
For the model Node use [django-mptt][1] (there are other solutions for django, google it) to get an efficient API to manage a Tree-like model.
  Negative
You can also create your own Django Storage Backend (or find there are many solutions on the Internet).
  Neutral
Updated You can add new files by using django admin.
  Negative
You should use amazon s3 django storage backend http://django-storages.readthedocs.org/en/latest/backends/amazon-S3.html.
  Negative
Change: In this case you have not to update index.
  Neutral
563e330b61a8013065267bd5	X	Thx for your answer Stefan.
  Negative
I checked out the tutorial you linked and indeed this fits what I plan to implement.
  Positive
Main difference is, that all of this steps should be done automatically, in my system.
  Negative
The user is just dropping the file into the browser.
  Negative
That's the point where data and communication flow between server and client starts being not that obvious to me.
  Negative
How can the client get informed about the transcoding has finished if server starts this process?
  Neutral
563e330b61a8013065267bd6	X	Im writing a single-page-web-app (angularJs) and a server back-end (node.js).
  Negative
The communication between them is done via REST.
  Positive
Currently im trying to implement the following scenario: Upload big files from browser to S3 public bucket.
  Negative
Copy uploaded file to private bucket on S3 Transcode uploaded file to HTML 5 compatible format (AWS Elastic Transcoder) Store Meta-Object about the file in DB to access later I'm racking my brains to get a well working design of the communication/ data-workflow between server and client, but always got stuck at the following questions?
  Negative
Store file meta-object at the end or at the beginning of the process.
  Negative
If it is at the beginning, i have to store and handle some state information?
  Negative
Who should start copying uploaded files to private bucket.
  Neutral
Server or client?
  Neutral
If it is the server, how can the client get informed about the job succeeded?
  Negative
Who starts the transcoding process?
  Neutral
If it is the server, how can the client get informed about the job succeeded?
  Negative
How would you do this?
  Neutral
563e330c61a8013065267bd7	X	there is a pretty good tutorial which describes the use case you are planning to implement: http://www.bitcodin.com/blog/2015/02/create-mpeg-dash-hls-content-for-amazon-s3-and-cloudfront/ If your transcoding system has a RESTfull API (like bitcodin which is used in this tutorial, or any other service) you can do your application also client-side and use the API calls to get the state of your transcodings, etc.
  Negative
However, using the API you can do the same also server-side, whatever fits better for you.
  Neutral
I personally would store the metadata infos at the beginning of the process, as this is the point of time where you generate the "asset" in your database/CMS/etc.
  Negative
563e330c61a8013065267bd8	X	I I have been searching a lot for answer to this question.
  Negative
However after reading a lot of resources including Spring Security documentation as well I am still away from completely understanding as to how to authenticate a user for REST API using Spring Security .
  Neutral
Here is what I want to do: From above I am not sure how to do this in Spring Security.
  Negative
What I have understood is as follows: 1) Use a <http> element along with custom filter.I have done this as follows <http use-expressions="true" create-session="stateless" authentication-manager-ref="restAuthenticationManager" entry-point-ref="jkwebRestAuthenticationEntryPoint"> <intercept-url pattern="/api/**" access="isAuthenticated()"> </intercept-url> <custom-filter ref="jkWebSecurityHmacAuthenticationFilter" position="FORM_LOGIN_FILTER"/> </http> I can post more code for clarity if needed.
  Negative
563e330c61a8013065267bd9	X	Here is the best example I have found.
  Positive
https://code.google.com/p/spring-rest-hmac/ which is mirrored on GitHub here No warranty implied, proceed at your own risk, etc.
  Negative
I started copying over code but you should really review the whole package.
  Positive
It's probably a good starting point, but you'll need to work hard to make this secure I would think.
  Positive
563e330c61a8013065267bda	X	for documents like submitted forms you may want to use a nosql server like couchdb or mongodb, for relational data use a rdbms like e.g. mysql or postgresql.
  Negative
sqlite is a bad idea for scaling, more then 10 users at a time make it unusable due to its performance.
  Negative
563e330c61a8013065267bdb	X	What web-host are you using?
  Negative
Every web host offers some sort of database system hosted on their servers.
  Negative
563e330c61a8013065267bdc	X	FYI: StackMob is ceasing operations.
  Negative
This is why I never trust tiny startups especially when there is a danger of lock-in.
  Negative
563e330c61a8013065267bdd	X	I have a website that I've built (hosted on Amazon S3) and it works great.
  Positive
The only problem is that all of the data is static.
  Negative
I'd like to create a SQL database in the cloud that would allow me to store basic text data from users after they submit forms.
  Negative
I'm still a novice web-developer but I've used sqlite3 for several of my Java desktop apps and I'd like to use that SQL knowledge to create this online database.
  Negative
I guess what i'm asking (in my ignorance) is: how can I create a sqlite-type database that is stored in the cloud and that I can query against using javascript?
  Negative
Where do I get started?
  Neutral
Is there a service like Amazon AWS or Azure or something where I can create this database and then use some sort of jQuery/Javascript API to query data from it?
  Negative
I don't need a ton of storage space and my queries would be very basic SQL type stuff.
  Negative
Any help would be greatly appreciated.
  Positive
Thanks in advance.
  Neutral
563e330d61a8013065267bde	X	StackMob or Parse if you want a (client-side) JavaScript API with user management, facebook/twitter integration, data store (with geospatial), and push notifications.
  Negative
StackMob also lets you host your website.
  Positive
For more flexibility, less service lock-in, and cheaper scalability: I would suggest CouchDB (though you would likely still use a hosting service like Cloudant).
  Negative
CouchDB can host your website, and provides a HTTP API for storing data, to which your client-side JavaScript can make REST calls.
  Positive
563e330d61a8013065267bdf	X	StackMob has a free package that you can use.
  Positive
You can use the JS SDK to write your HTML5 app and save stuff to the StackMob DB.
  Negative
You can host your HTML5 on StackMob for free and point your own domain to it as well.
  Negative
There is also S3 integration.
  Positive
Some references: JS SDK JS SDK Tutorial Hosting your HTML5 Custom Domains
563e330d61a8013065267be0	X	SQLite isn't really a good choice for web facing applications due to its scaling issues.
  Negative
Both AWS and Azure support SQL databases.
  Negative
They also each support alternatives like MongoDB and Redis.
  Negative
For something as basic as you describe the only real difference is cost.
  Negative
563e330d61a8013065267be1	X	As you mentioned your website is hosted on Amazon S3 I am sure it is a static website with lots of JavaScript embedded HTML files.
  Negative
Due to having a static website, I can understand your requirement to use a database which can be connected from your static site and to be very honest there are not a lot options you have.
  Negative
Static website are considered to have no dependency on database so honestly you have very limited choice because what you are looking for is "A Database which is accessible over HTTP which you can call from scripting language withing HTML" If you have ability to write back to S3 directly from your JavaScript code you can create a JavaScript based database within your static site which is complex but consider a choice.
  Negative
In my thinking you are better off to have an extra-small instance in Windows Azure (or your choice of cloud service) and connect with a cloud based database which will be comparative cheaper and fit for your requirement.
  Negative
Or unless Amazon can come up with a DB accessible from status content as S3, you really have no great choices here.
  Neutral
563e330d61a8013065267be2	X	Since you are already familiar some of AWS's offerings, you should check out: But to do what you are asking (access data via JavaScript), check out www.stackmob.com.
  Negative
You can host an HTML5 application with data access via backbone (javascript based framework) on StackMob.
  Negative
563e330d61a8013065267be3	X	And this is my web config setting <httpRuntime useFullyQualifiedRedirectUrl="true" maxRequestLength="2147482624" executionTimeout="9999" requestLengthDiskThreshold="95360" />
563e330d61a8013065267be4	X	check with amazon for max file size that they allow
563e330d61a8013065267be5	X	ok .
  Negative
But When i try to upload file of size 6 gb it shows me error http 400 and it does not executes the controller code also
563e330d61a8013065267be6	X	Thanks for replying but do i need to break file in chunks and the upload.If yes how would i get file in stream if i upload file whose size is more than gb my controller does not get executed
563e330d61a8013065267be7	X	I am Doing this in browser
563e330d61a8013065267be8	X	Thank u for replying maxRequestLength="2147482624" that is the max can i do using jquery or html file uploader like BlueImp can you suggest any thing like that i am searching not getting any solution
563e330d61a8013065267be9	X	Unfortunately, "Questions asking us to recommend or find a book, tool, software library, tutorial or other off-site resource are off-topic for Stack Overflow" -- stackoverflow.com/help/on-topic
563e330d61a8013065267bea	X	I am trying to upload files of size more than 3 gb to amazon when i upload file whose size is small they get uploaded happily but When i try to upload big file it does it show me error http 400 Can Any one tell me where i am going wrong or do i need upload file in chunks.
  Negative
thanks in advance
563e330e61a8013065267beb	X	First, you're not allowed to upload files of more than 5gb (6gb MUST FAIL, but not 2gb file): But uploading large files can occur various issues, so, to avoid problems with Single Upload, is recommended Multipart Upload Upload objects in parts—Using the Multipart upload API you can upload large objects, up to 5 TB.
  Very negative
The Multipart Upload API is designed to improve the upload experience for larger objects.
  Negative
You can upload objects in parts.
  Neutral
These object parts can be uploaded independently, in any order, and in parallel.
  Negative
You can use a Multipart Upload for objects from 5 MB to 5 TB in size.
  Negative
For more information, see Uploading Objects Using Multipart Upload.
  Negative
For more information, see Uploading Objects Using Multipart Upload API.
  Negative
Check here The following Java code example uploads a file IN PARTSto an Amazon S3 bucket:
563e330e61a8013065267bec	X	@JordiCastilla is correct about S3 multipart and the 5GB threshold... but your first problem is a local one: Hmmm.
  Negative
That's about 2 GiB.
  Neutral
So, when you say your controller isn't firing, that suggests your 400 error isn't even from S3 as the question implies.
  Negative
Your local configuration is causing the browser's request to be canceled in flight because it's larger than your local configuration allows.
  Neutral
Your first step seems like it would be to increase this configuration value, bearing in mind that you will subsequently also need to transition to multipart uploads to S3 when you cross the 5GB threshold.
  Negative
Remember, also, that a request in progress is also eating up temporary disk space on your server, so having this value even as large as it already is (not to mention setting it even larger) could put you at risk for a denial of service attack, where spurious requests could disable your site by exhausting your temp space.
  Very negative
563e330e61a8013065267bed	X	Check out this lib .
  Negative
It uploads the file in chunks of configurable sizes.
  Negative
As your requirement is to get it done in a browser this should work.
  Negative
563e330e61a8013065267bee	X	@Justice - Thank you for a complete and very well-reasoned answer.
  Negative
It is exactly the reasoning that I needed for using S3.
  Negative
The assets stored on S3 are not super-critical as far as privacy goes, and the URLs are only available to logged-in users.
  Negative
I suppose I have to do some kind of salted hash to generate the random number.
  Negative
563e330e61a8013065267bef	X	+1 one for logical points.
  Neutral
Generating a new randomized url for each update to the resource is also important.
  Negative
563e330e61a8013065267bf0	X	If you really need the ACLs, this is definitely how to do it.
  Positive
However, on Heroku, and depending on the access patterns for these assets, this strategy will force you to "crank your dynos" much faster than otherwise.
  Neutral
563e330f61a8013065267bf1	X	Justice: I'm not sure that it's any worse than it would be to store the file locally and stream it out through your application, though.
  Very negative
If you wanted to lock the files down in any non-trivial way, streaming through the application is basically the only solution.
  Negative
Of course, few applications have that kind of requirement.
  Neutral
I'm also used to working in a dedicated server environment, so maybe my advice is not as applicable to heroku.
  Negative
563e330f61a8013065267bf2	X	Thank you for a very complete answer.
  Positive
563e330f61a8013065267bf3	X	I want to ask more about "That controller downloads the file using the API, then streams it out to the user with correct mime-type, cache headers, file size, etc." So your app will download the file into your app server and then serve that file to the app user from your app server?
  Very negative
Or does your download/{s3-path} show a webpage that contains a time-expiry inclusive link?
  Neutral
like this one docs.aws.amazon.com/AmazonS3/latest/dev/S3_QSAuth.html
563e330f61a8013065267bf4	X	They have a similar feature for signed URLs in the PHP SDK.
  Negative
docs.aws.amazon.com/aws-sdk-php/guide/latest/… I think this is the best current solution for this poster's kind of problem.
  Negative
563e330f61a8013065267bf5	X	I fired up a sample application that uses Amazon S3 for image hosting.
  Negative
I managed to coax it into working.
  Positive
The application is hosted at github.com.
  Negative
The application lets you create users with a profile photo.
  Positive
When you upload the photo, the web application stores it on Amazon S3 instead of your local file system.
  Negative
(Very important if you host at heroku.com) However, when I did a "view source" in the browser of the page I noticed that the URL of the picture was an Amazon S3 URL in the S3 bucket that I assigned to the app.
  Negative
I cut & pasted the URL and was able to view the picture in the same browser, and in in another browser in which I had no open sessions to my web app or to Amazon S3.
  Negative
Is there any way that I could restrict access to that URL (and image) so that it is accessible only to browsers that are logged into my applications?
  Negative
Most of the information I found about Amazon ACLs only talk about access for only the owner or to groups of users authenticated with Amazon or AmazonS3, or to everybody anonymously.
  Negative
EDIT----UPDATE July 7, 2010 Amazon has just announced more ways to restrict access to S3 objects and buckets.
  Negative
Among other ways, you can now restrict access to an S3 object by qualifying the HTTP referrer.
  Negative
This looks interesting...I can't wait until they update their developer documents.
  Positive
563e331061a8013065267bf6	X	S3 is a separate service and does not know about your sessions.
  Negative
The generic solution is to recognize the benefits and security properties that assigning each asset a separate, unique, and very long and random key, which forms part of the URL to that asset.
  Positive
If you so choose, you can even assign a key with 512 effective bits of randomness, and that URL will remain unguessable for a very long time.
  Negative
You have to determine if this is sufficient security.
  Neutral
If it isn't, then maybe S3 isn't for you, and maybe you need to store your images as binary columns in your database and cache them in memcached, which you can do on Heroku.
  Negative
563e331061a8013065267bf7	X	For files where privacy actually matters, we handle this as follows: Using this method, you end up using a lot more bandwidth than you need, but you still save on storage.
  Negative
For us this works out, because we tend to run out of storage much more quickly than bandwidth.
  Negative
For files where privacy only sort of matters, we generate a random hash that we use for the URL.
  Negative
This is basically security through obscurity, and you have to be careful that your hash is sufficiently difficult to guess.
  Negative
However, when I did a "view source" in the browser of the page I noticed that the URL of the picture was an Amazon S3 URL in the S3 bucket that I assigned to the app.
  Neutral
I cut & pasted the URL and was able to view the picture in the same browser, and in in another browser in which I had no open sessions to my web app or to Amazon S3.
  Negative
Keep in mind that this is no different than any image stored elsewhere in your document root.
  Negative
You may or may not need the kind of security you're looking for.
  Negative
563e331061a8013065267bf8	X	Amazon's Ruby SDK (https://github.com/aws/aws-sdk-ruby) has useful methods that make it a snap to get this done.
  Negative
"url_for" can generate a temporary readable URL for an otherwise private S3 object.
  Negative
Here's how to create a readable URL that expires after 5 minutes: object = AWS::S3.new.buckets['BUCKET'].
  Negative
objects['KEY'] object.url_for(:read, :expires => 300).
  Negative
to_s AWS documentation: http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html#url_for-instance_method
563e331061a8013065267bf9	X	I think the best you can do is what drop.io does.
  Positive
While the data is in principle accessible to anyone, you give it a large and random URL.
  Negative
Anyone who knows the URL can access it, but your application controls who gets to see the URL.
  Positive
Kind of security through obscurity.
  Neutral
You can think of it as the password included in the URL.
  Neutral
This means that if you are serious about security, you have to treat the URL as confidential information.
  Positive
You have to make sure that these links do not leak to search engines, too.
  Negative
It is also tricky to revoke access rights.
  Negative
The only thing you can do is invalidate a URL and assign a new one.
  Negative
563e331061a8013065267bfa	X	http://twitpic.com/show/full/<imageid> this is what I was looking for.
  Negative
thanks.
  Positive
563e331161a8013065267bfb	X	So in this URL: http://twitpic.com/2paihn The Twitpic ID is: 2paihn And the actual image URL is: http://s3.amazonaws.com/twitpic/photos/large/163413275.jpg?AWSAccessKeyId=0ZRYP5X5F6FSMBCCSE82&Expires=1284740401&Signature=6lgT6ruyyUDDjLOB7d42XABoCLU%3D I've tried getting the integer id through the api (i.e. 163413275) and replacing it in the s3.amazon.com url, but this seems to only work some of the time.
  Negative
Most of the time I get an 'access denied' message when I request the amazon-hosted image.
  Negative
Do you know of another hack to do this?
  Negative
563e331361a8013065267bfc	X	Use http://twitpic.com/show/thumb/<imageid> to get a thumbnail version.
  Negative
563e331361a8013065267bfd	X	This is the direct link to the image: http://twitpic.com/show/full/2paihn You can use it in an img tag or link it.
  Negative
It will forward to the amazon S3 url with a new expiration date and signature so whoever you're showing it to won't get the access denied message.
  Negative
Source: http://dev.twitpic.com/docs/thumbnails/
563e331361a8013065267bfe	X	It certainly does.
  Negative
Thanks so much.
  Positive
The only part I'm still not clear on is uploading whole folders - does software like Cyberduck enable you to upload multi-layered directories and it "converts" the files to flat files with slashes in their names for you?
  Negative
563e331361a8013065267bff	X	Yep.
  Neutral
Cyberduck and other tools handle all of that stuff for you.
  Negative
:)
563e331361a8013065267c00	X	Currently, I have a website which serves dynamic (PHP-MySQL) content from an Apache server, and serves static content (JavaScript, images) from a separate Lighthttpd server.
  Negative
For reasons of scale I would like to use Amazon Cloudfront and possibly S3.
  Negative
To be honest I'm not entirely sure how S3 or CloudFront work.
  Negative
I'm used to the normal server behaviour of "upload a file... it becomes available" and S3 "buckets" and CloudFront edge-mirroring are daunting.
  Negative
I need a better understanding of how this works and have some questions: 1) I don't want to store any images on my own servers.
  Negative
I want them to be entirely in the cloud.
  Neutral
Am I correct that this means I will need to use S3 for storage as an "origin server"?
  Negative
Will CloudFront on its own not be enough?
  Negative
Is CloudFront just the edge-CDN service?
  Neutral
2) We currently upload images via a PHP script which FTPs them to our image server, or via manual FTP upload.
  Negative
How will that change if I use S3?
  Neutral
I heard you can't FTP to it?
  Neutral
:( 3) If using S3, can I still create hierarchical directories and store images inside these?
  Negative
The images are stored various folders deep and I can't afford to change the code, but I heard S3 was a flat "bucket"?
  Negative
4) Finally, I heard that with CloudFront, if a file changes, you have to issue an invalidation request, which costs money.
  Negative
Is this because CloudFront is caching the image from the origin?
  Neutral
I'm not used to this as in my current setup I just replace an image via FTP and it updates!
  Negative
Is there no way to imitate this classic behaviour?
  Negative
Sincerest thanks for help.
  Positive
563e331361a8013065267c01	X	1) I don't want to store any images on my own servers.
  Negative
I want them to be entirely in the cloud.
  Neutral
Am I correct that this means I will need to use S3 for storage as an "origin server"?
  Negative
Will CloudFront on its own not be enough?
  Negative
Is CloudFront just the edge-CDN service?
  Neutral
S3 is designed for the long-term, reliable storage of data with eleven 9s of durability.
  Negative
Buckets (as they're called) are region-specific and live in one of Amazon's regional data centers.
  Negative
Conversely, CloudFront is designed as a series of edge servers.
  Negative
By default, when you request an object (i.e., file) from a CloudFront hostname, that object is pulled from the origin location and cached in the nearest CloudFront edge location for 24 hours (this can be adjusted programmatically).
  Negative
At the end of 24 hours the cache expires, and CloudFront will pull a fresh copy the next time that object is requested.
  Positive
A common setup is to configure CloudFront to use S3 as its origin location.
  Negative
CloudFront also has the ability to use any server, if that's what you prefer (it sounds like you don't).
  Positive
2) We currently upload images via a PHP script which FTPs them to our image server, or via manual FTP upload.
  Negative
How will that change if I use S3?
  Neutral
I heard you can't FTP to it?
  Neutral
:( S3 isn't an FTP server, so it doesn't speak the (S)FTP protocol.
  Negative
However, nearly all FTP clients for Mac OS X include support for Amazon S3.
  Neutral
Amazon S3 has a web service API, so you can automate the push using one of the AWS SDK's if you'd like.
  Negative
One tool, Cyberduck, does SSH, SFTP, FTP, Amazon S3, and a few other things.
  Negative
It's available for both Mac and Windows.
  Neutral
There are also other tools out there that provide a GUI for uploading to S3 as simply as though you were uploading via FTP.
  Negative
3) If using S3, can I still create hierarchical directories and store images inside these?
  Negative
The images are stored various folders deep and I can't afford to change the code, but I heard S3 was a flat "bucket"?
  Negative
Yes and no.
  Neutral
Yes, S3 is a flat file system, but files can have slashes in their names.
  Negative
For example, "abc/def/ghi/jkl.txt" is not actually 3 folders and a file, but rather one file with slashes in its filename.
  Negative
Most GUI tools choose to visualize this as folders and subdirectories, and the S3 URL looks just like any other URL.
  Negative
Speaking personally, I've never needed to do anything different for S3 than I used to do for SFTP.
  Negative
4) Finally, I heard that with CloudFront, if a file changes, you have to issue an invalidation request, which costs money.
  Negative
Is this because CloudFront is caching the image from the origin?
  Neutral
I'm not used to this as in my current setup I just replace an image via FTP and it updates!
  Negative
Is there no way to imitate this classic behaviour?
  Negative
Right.
  Neutral
Because CloudFront caches the source file to the nearest edge server.
  Negative
By default, the expiration is 24 hours, but you can set it as low as 1 hour, or even expire it sooner with an "invalidation request".
  Negative
I've seen this take anywhere from 3-15 minutes to complete, because CloudFront has to check all of the edge servers to make sure that they're all cleared.
  Negative
If you don't want the caching, you can just use S3 straight-up.
  Negative
This is the closest equivalent to replacing an image via FTP, but then you lose all of the benefits of using a CDN in the first place.
  Negative
According to the Amazon CloudFront pricing page: "No additional charge for the first 1,000 files that you request for invalidation each month.
  Negative
$0.005 per file listed in your invalidation requests thereafter."
  Negative
That's half-of-a-penny for each file you invalidate over 1,000 in a month.
  Negative
I use CloudFront regularly and have never crossed that limit, but if you're running a larger site with lots and lots of changes, then it's certainly a possibility.
  Negative
I hope this helps!
  Positive
:)
563e331361a8013065267c02	X	I tested a whole range of client applications and found CloudBerry S3 Explorer for windows and CrossFTP for Mac more powerful then Cyberduck, but I work a lot with private streaming video and audio, so my requirements are a bit higher then displaying images on a site.
  Negative
But to answer your last question, you can drag a folder with subfolders into a bucket, the hierarchy is respected.
  Negative
You can work folders in a bucket, just as you work with FTP.
  Positive
But you have to make sure to set your images to public or your images will not show.
  Neutral
Standard, any file is uploaded as a private file, requiring a signed URL to access it.
  Negative
But with a client application, you can set the inheritance of a bucket, so that files become automatically public.
  Negative
You can find a lot of info about this here: http://www.miracletutorials.com/category/s3-amazon-cloudfront/ Cheers, Rudolf+++
563e331461a8013065267c03	X	I have to research a bit more, but wouldn't client_body_in_file_only cause more disk access and thus decreased performance?
  Very negative
The Nginx docs say it should be used for debugging primarily.
  Neutral
563e331461a8013065267c04	X	@aergistal no, it works in production many years for us, all is perfect.
  Positive
I talked to Nginx core team developers, they confirmed it is pretty stable for production work load.
  Negative
563e331461a8013065267c05	X	It does solve but only one particular problem with client side upload, what about everything else?
  Negative
563e331461a8013065267c06	X	just added some info about image cropping and PDF preview
563e331461a8013065267c07	X	Upload files with multipart/form-data is straight forward and works well most of time until you started to be focused on big files upload.
  Positive
If we look closely what happens during a file upload: client sends POST request with the file content in BODY webserver accepts the request and initiates data transfer (or returns error 413 if the file size is exceed the limit) webserver starts to populate buffers (depends on file and buffers size), store it on disk and send it via socket/network to back-end back-end verifies the authentication (take a look, once file is uploaded) back-end reads the file and cuts few headers Content-Disposition, Content-Type, stores it on disk again back-end performs all you need to do with the file To avoid such overhead we dump file on disk (Nginx client_body_in_file_only) and manage the callback to be send further down the line.
  Negative
Then queue worker picks the file up and do what required.
  Neutral
It works for inter-server communication pretty slick but we have to solve similar problem with client side upload.
  Neutral
We also have client-side S3 upload solution.
  Negative
No back-end interaction happens.
  Neutral
For video upload we manage the video to convert to the format h.264 Baseline / AAC with Zencoder.
  Neutral
Currently we use modified Flash uploader based on s3-swf-upload-plugin with combination of Zencoder JS SDK which is really efficient but uses Flash.
  Negative
Question.
  Neutral
How to reach the same goal with HTML5 file uploader?
  Neutral
Does Filepicker.io and Zencoder solve the problem?
  Neutral
What is the recommended way to manage HTML5 file upload with no back-end interaction?
  Neutral
The requirements are the following: Does https://www.filepicker.com make a good job?
  Positive
563e331461a8013065267c08	X	The requirements are the following: HTML5, not flash Filepicker now supports a full responsive widget that is pure html and css.
  Negative
to upload video with post-processing to make it compatible with HTML5 players and mobile Filepicker now offers the ability to transcode most video formats to h264 & webm for mobile playback.
  Negative
https://www.filepicker.com/documentation/file_processing/video_conversion/video to upload images wtih post-processing (resize, crop, rotate) Filepicker does offer Crop & rotate in the new widget as well as resize, sharpening and watermarking via API.
  Negative
to upload documents like PDF with a preview functionality We offer the ability to convert from 19 different file formats to numerous output formats.
  Negative
https://www.filepicker.com/documentation/file_processing/document_conversion/document
563e331561a8013065267c09	X	I'm using filepicker for 2 years now, and without doubt it's worth the price.
  Negative
don't try to manage file upload (from google drive, from ios, from my camera, from dropbox...) Filepicker handles that very well and provide you a ready to use url.
  Negative
Spend more time working on your core business, file upload is really easy to delegate
563e331561a8013065267c0a	X	To upload a big files to S3 there is a REST API for Multipart Upload, which works the following way the API is also available for calling from javascript and the uploaded file can be split to multiple requests using File/Blob slice API The only problem is that to be able to authenticate to S3 from javascript you need to pass your authentication details.
  Negative
This is usually solved by some interlayer like PHP so the authentication details are not stored in javascript files.
  Negative
Similar question on SO: HTML5 and Amazon S3 Multi-Part uploads EDIT
563e331561a8013065267c0b	X	Rafał Łużyński do you know if Sphinx works with nosql databases
563e331561a8013065267c0c	X	sphinxsearch.com/about as you can see, they have support for nosql in xml.
  Negative
Non-SQL storage indexing.
  Neutral
Data can also be streamed to batch indexer in a simple XML format called XMLpipe, or inserted directly into an incremental RT index
563e331561a8013065267c0d	X	We are working for a client to redesign an existing system which basicaly deals with a lot of files.
  Negative
The files(more than 5 million) are currently stored on the servers filesystem.The client wants the new system to store the file in S3.
  Negative
The files also have metadata associated(name,authors name,price ,description etc.).
  Negative
The search functionality is also to be redesigned.The following are the basic requirements Also , based on the file description, the system should also be able to give recommendation for similar files.
  Negative
I do not have experience with creating such solution before,so asking for help and suggestion.
  Negative
I was thinking on the lines of following solutions: There was this project that I found,that is very similar to what I require http://www.thriftdb.com - On the home page it says its a datastore with search builtin.
  Negative
Please let me know if this question should be a community wiki.
  Neutral
Thanks in advance.
  Neutral
563e331561a8013065267c0e	X	Amazon has a custom AMI for Lucene/Solr and we have been happily using it in our projects.
  Negative
Lucene has a powerful indexing capability and executes at exceptional speeds.
  Positive
I would strongly recommend using Apache Lucene/Solr for all your search needs.
  Negative
563e331561a8013065267c0f	X	You're in luck, announced today: http://aws.amazon.com/about-aws/whats-new/2012/04/11/aws-announces-cloudsearch/
563e331561a8013065267c10	X	About searching files and filtering by attributes, the best would be Sphinx Search Engine which is used in filestube (google was using it also years ago).
  Negative
I dont know if it will work on amazon servers.
  Negative
563e331561a8013065267c11	X	At the web app I'd developing there is an option for users to download large files via browser.
  Negative
The files could be stored at different remote storages, for example Amazon S3.
  Negative
If the file download took more than a few minutes it would be handy to track it's progress at the server side for various reasons.
  Negative
So the web app and node.js API are located at one place and the file could be located anywhere.
  Negative
What I was thinking is to proxy the file download through node.js.
  Negative
So it looks like --> user clicks the file download button at web app --> node asks S3 for a file and streams it to the user and updates the DB to track the progress as the file size and data sent are known.
  Negative
What I'd like to know: Thanks a lot for your answers !
  Positive
563e331661a8013065267c12	X	I am working on a project in python that is starting to overwhelm my low-end windows lap-top and I wanted to ask for advice about how to find the additional computing power I think I need.
  Negative
Here are some details about my project: I am processing and analyzing a fairly large database of text from the web.
  Negative
Approximately 10,000 files each equivalent to on average approximately 500 words or so (though with a lot of variance around this mean).
  Negative
The first step is pulling certain key phrases and using GenSim to do a fairly simple similarity analysis.
  Positive
This takes my computer a while but it can handle it if I'm gentle.
  Neutral
Second, once I have identified a short list of candidates I fingerprint each candidate document to more closely assess similarity.
  Negative
Each file requires fingerprinting and comparison over 2-10 other files - so its not really an n-to-n comparison of the sort that would require months of computer time I don't think.
  Negative
It is this second step where my computer starts to struggle.
  Negative
I was considering looking into running the script in an EC2 environment but when I started reading about that on here, I saw a a comment to the effect that effectively doing so requires a linux sys admin level of sophistication - I am about as far from that level of sophistication as any member of this site can be.
  Negative
So is there another option?
  Neutral
Or is getting a fairly simply python script running on ES2 not so hard.
  Neutral
The part of the script that seems the most resource-intensive is below.
  Negative
For each text file, it creates a list of fingerprints by selecting certain text files from amdt_word_bags trim according to criteria in PossDupes_1 (both of which are lists).
  Negative
It uses the fingerprintgenerator module which I found here: https://github.com/kailashbuki/fingerprint.
  Positive
563e331661a8013065267c13	X	How about using Amazon's Elastic Map Reduce (EMR).
  Neutral
This is Amazon's hadoop service which basically runs on top of EC2.
  Negative
You can copy you your data files to AmazonS3 and have your EMR cluster pick up the data from there.
  Negative
You can also send your results to files in Amazon S3.
  Negative
When you launch your cluster you can customize how many EC2 instances you want to use and what size for each instance.
  Negative
That way you can tailor how much CPU power you need.
  Positive
After you are done with your job you can tear down your cluster when you are not using it.
  Positive
(Avoiding paying for it) You can also do all of the above programmatically too.
  Negative
For example python I use the boto Amazon API which is quite popular.
  Positive
For getting started on how to write python map reduce jobs you can find several posts on the web explaining how to do it.
  Negative
Here's an example: http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/ Hope this helps.
  Neutral
563e331661a8013065267c14	X	Could you give more details on these methods, and what they require/ask?
  Negative
Bit hard to tell what you need to do otherwise.
  Negative
563e331661a8013065267c15	X	To be honest the main question here is where do those images originate from?
  Positive
If they are from form uploads then the image is already stores in a temporary location so you would have no need to store the image yourself.
  Negative
563e331661a8013065267c16	X	@Peter, they are not from form uploads.
  Negative
I'm pulling one image off of a CDN, cropping it, and sending the cropped version back to the CDN (Amazon S3 CDN + their classes & methods).
  Negative
563e331661a8013065267c17	X	You can do it like now or You can send the resource data in base64_encodeed form - that will be string.
  Negative
Don't worry about the data amount, in PHP 5 the parameters are transferred just as a pointers to a memory...
563e331661a8013065267c18	X	Thanks!
  Negative
Very helpful answer.
  Positive
563e331661a8013065267c19	X	I have an image resource that is manipulated with imagecopyresampled.
  Negative
I need to pass that image to a set of methods that expect a string input, not a resource.
  Negative
But I don't need to store the file locally.
  Negative
Is this the proper way: Is that right?
  Neutral
Seems sloppy.
  Negative
Note: the image is not coming from a file upload and hence can't be accessed with $_FILES["Filedata"]["tmp_name"]
563e331661a8013065267c1a	X	I took a look at the Amazon S3 PHP API: http://docs.amazonwebservices.com/AWSSDKforPHP/latest/index.html#m=AmazonS3/upload_part I assume you are using something like the upload_part method that takes a string filename.
  Very negative
In that case, unless you plan to modify their library, you will need to store the file to disk and pass them the filename so they can read the file and perform the upload.
  Negative
Besides the steps mentioned in your question you can take a look at imagedestroy to make sure you are freeing up the memory for your image resource after it is written to disk with imagepng.
  Negative
And then, as you stated, you can delete your temp file with unlink after your upload is complete.
  Negative
I agree, it does seem a bit wasteful, but in this case necessary since the API doesn't seem to provide an alternative.
  Negative
563e331761a8013065267c1b	X	Hi codenoob.
  Negative
Any news on this?
  Neutral
Did you finish your solution?
  Neutral
Did you find the time try app engine?
  Negative
563e331761a8013065267c1c	X	Hey Johe, I've implemented it and am working on the iOS client now.
  Negative
I used Ruby with Sinatra running on Heroku with static files on Amazon S3 and a MongoDB database running at MongoHQ.
  Negative
It was very simple to implement a RESTful protocol using Sinatra and everything is extremely scalable, and I only pay for the resources I use.
  Very negative
I haven't tried the app engine because this worked perfectly for me.
  Negative
563e331761a8013065267c1d	X	Link to Heroku: heroku.com
563e331761a8013065267c1e	X	This looks very interesting, I will give it a go.
  Very positive
I think I can get away with the free Blossom service for now and in case the app's demands grow, the prices seem very reasonable going up.
  Negative
I might try the google app engine at some in the future though.
  Negative
Thanks for the info!
  Positive
563e331761a8013065267c1f	X	I have decided to go with Sinatra and Heroku for now because it seems very simple to do what I want it to do.
  Negative
I have looked at GAE and already have some experience in Java so I will definitely try this in the future too.
  Negative
Thanks for the tip!
  Positive
563e331761a8013065267c20	X	I am developing an iPhone app and would like to create some sort of RESTful API so different users of the app can share information/data.
  Negative
To create a community of sorts.
  Neutral
Say my app is some sort of game, and I want the user to be able to post their highscore on a global leaderboard as well as maintain a list of friends and see their scores.
  Negative
My app is nothing like this but it shows the kind of collective information access I need to implement.
  Negative
The way I could implement this is to set up a PHP and MySQL server and have a php script that interacts with the database and mediates the requests between the DB and each user on the iPhone, by taking a GET request and returning a JSON string.
  Negative
Is this a good way to do it?
  Positive
Seems to me like using PHP is a slow way to implement this as opposed to say a compiled language.
  Negative
I could be very wrong though.
  Negative
I am trying to keep my hosting bills down because I plan to release the app for free.
  Negative
I do recognise that an implementation that performs better in terms of CPU cycles and RAM usage (e.g. something compiled written in say C#?)
  Neutral
might require more expensive hosting solutions than say a LAMP server so might actually end up being more expensive in terms of $/request.
  Negative
I also want my implementation to be scalable in the rare case that a lot of people start using the app.
  Negative
Does the usage volume shift the performance/$ ratio towards a different implementation?
  Neutral
I.e. if I have 1k request/day it might be cheaper to use PHP+MySQL, but 1M requests/day might make using something else cheaper?
  Negative
To summarise, how would you implement a (fairly simple) remote database that would be accessed remotely using HTTP(S) in order to minimise hosting bills?
  Negative
What kind of hosting solution and what kind of platform/language?
  Neutral
UPDATE: per Karl's suggestion I tried: Ruby (language) + Sinatra (framework) + Heroku (app hosting) + Amazon S3 (static file hosting).
  Negative
To anyone reading this who might have the same dilemma I had, this setup is amazing: effortlessly scalable (to "infinity"), affordable, easy to use.
  Positive
Thanks Karl!
  Positive
Can't comment on DB specifics yet because I haven't implemented that yet although for my simple query requirements, CouchDB and MongoDB seem like good choices and they are integrated with Heroku.
  Negative
563e331761a8013065267c21	X	Have you considered using Sinatra and hosting it on [Heroku]?
  Negative
This is exactly what Sinatra excels at (REST services).
  Positive
And hosting with Heroku may be free, depending on the amount of data you need to store.
  Negative
Just keep all your supporting files (images, javascript, css) on S3.
  Negative
You'll be in the cloud and flying in no time.
  Negative
This may not fit with your PHP desires, but honestly, it doesn't get any easier than Sinatra.
  Negative
563e331761a8013065267c22	X	It comes down to a tradeoff between cost vs experience.
  Negative
if you have the expertise, I would definitely look into some form of cloud based infrastructure, something like Google App Engine.
  Negative
Which cloud platform you go with depends on what experience you have with different languages (AppEngine only works with Python/Java for e.g).
  Negative
Generally though, scalable cloud based platforms have more "gotchas" and need more know-how, because they are specifically tuned for high-end scalability (and thus require knowledge of enterprise level concepts in some cases).
  Neutral
If you want to be up and running as quickly and simply as possible I would personally go for a CakePHP install.
  Negative
Setup the model data to represent the basic entities you are managing, then use CakePHP's wonderful convention-loving magic to expose CRUD updates on these models with ease!
  Negative
563e331861a8013065267c23	X	The technology you use to implement the REST services will have a far less significant impact on performance and hosting costs than the way you use HTTP.
  Negative
Learning to take advantage of HTTP is far more than simply learning how to use GET, PUT, POST and DELETE.
  Negative
Use whatever server side technology you already know and spend some quality time reading RFC2616.
  Negative
You'll save yourself a ton of time and money.
  Neutral
563e331861a8013065267c24	X	In your case its database server that's accessed on each request.
  Negative
so even if you have compiled language (say C# or java) it wont matter much (unless you are doing some data transformation or processing).
  Negative
So DB server have to scale well.
  Positive
here your choice of language and DB should be well configured with host OS.
  Positive
In short PHP+MySQL is good if you are sending/receiving JSON strings and storing/retrieving in DB with minimum data processing.
  Negative
next app gets popular and if your app don't require frequent updates to existing data then you can move such data to very high scalable databases like MongoDB (JSON friendly).
  Negative
563e331861a8013065267c25	X	Can you post your messages or logs?
  Neutral
563e331861a8013065267c26	X	that's a great question, how should I proceed?
  Positive
in my NuGet, the only thing left to install are the AWSSDK 2 and the ImageResizer packages for 3.4.2 --> which ones should I install first?
  Negative
or should I remove the old AWSSDK 1 & IR first?
  Neutral
563e331861a8013065267c27	X	When in doubt, remove AWSSDK and IR completely, then re-install IR and let it install AWSSDK and any required binding redirects.
  Negative
563e331861a8013065267c28	X	I'm currently using 3.4.1 with and the original Image Resizer S3 I'm trying to do a couple of upgrades here like net451, mvc 5, web api 2 etc. and also move to the latest Amazon AWS SDK 2.0.8.2 & 3.4.2 of IR but the process is filled with errors as the new AWSSDK 2 says that it can't find the proper S3 package or a compatible one.
  Very negative
STEPS?
  Neutral
1.
  Neutral
Should I remove AWS SDK 1 & IR 3.4.1 completely 1a.
  Negative
Then re-install AWS SDK 2 & IR 3.4.2 afterwards Is there a proper way that has worked for people to get to the latest version of IR 3.4.2?
  Negative
563e331861a8013065267c29	X	Did you try setting debug="true" on the root configuration element?
  Neutral
563e331861a8013065267c2a	X	re c3p0, you might try overriding korma's version choice and using c3p0-0.9.5-pre4.
  Negative
c3p0 0.9.5.x supports logging directly to the slf4j api without use of the log4j bridge, and configuration might be more straightforward that way.
  Negative
It'll probably happen automatically if you remove the log4j api, but to be sure you log to slf4j, add to c3p0.properties or as a System property 'com.mchange.v2.log.MLog= slf4j' See mchange.com/projects/c3p0-0.9.5-pre4/#configuring_logging
563e331961a8013065267c2b	X	@Alex: yep, I have tried that, and I don't recall it telling me anything useful.
  Very negative
That said, I'll certainly try it again.
  Positive
563e331961a8013065267c2c	X	@SteveWaldman Thank you for that excellent tip!
  Positive
I'll give a different version of c3p0 a shot.
  Positive
563e331961a8013065267c2d	X	How can I have my application code log at the DEBUG level, but restrain certain library dependencies to the WARN level?
  Negative
Specifically, I'm having a hard time controlling c3p0 and the Amazon SDK.
  Negative
Can anyone tell me why my setup isn't working?
  Negative
Because I don't understand why I'm still seeing AWS's incredibly verbose DEBUG logging still in the log files.
  Negative
Below I've included my logback.xml and relevant excerpts of my project.clj.
  Negative
563e331a61a8013065267c2e	X	It never fails that I spend 30 minutes looking for something, and then 30 seconds after I post I realize the combination of keywords that get me some results.
  Very negative
Posting some potential answers now, but will be interested in comments from anyone who has used these tools.
  Negative
Literally it occurred to me to search for odbc for the cloud and found a blog on the exact same subject: janakiramm.net/blog/do-we-need-odbc-for-the-cloud
563e331a61a8013065267c2f	X	Oxtio bascially have developed code to talk to virtually ALL the cloud providers (they support LOADS) and built a web app on top of it to let users administrate credentials.
  Very negative
They briefly mentioned an API some years ago but doesn't look like it's been launched.
  Negative
blog.otixo.com/tag/api-2
563e331a61a8013065267c30	X	out of curiosity, did you write all the authorization and background syncing for every cloud service, or were you able to find a library to do it for you?
  Negative
563e331a61a8013065267c31	X	Do any APIs/Libraries/tools exist that act as adapters/provider interfaces for accessing different cloud storage services through a common interface?
  Negative
Something similar to ODBC or OLE-DB, except for cloud storage instead of databases.
  Negative
Such that, if I wrote a front end for taking notes, and I utilized such an API, and let the user provide configuration for which cloud storage provider they have an account with, the API library would handle translating my cloud.Save() call into the commands specific to whiever provider was being utilized.
  Negative
This would allow my front-end app to be cloud storage provider agnostic.
  Negative
So maybe I wrote some chrome extension or portable thumb drive app for storing notes, or encrypting and storing passwords, or some such, and you tell it which cloud storage provider you have an account with, and it uses it for syncing.
  Negative
This way your use of that tool doesn't tie you to a specific cloud provider.
  Negative
As long as you backup your data, you could migrate to another provider and just reconfigure the app should you become unhappy with that provider or they go bankrupt.
  Negative
WebDAV for example is one potential candidate since it seems some storage services offer it, but that is not quite what I have in mind, since it depends on the storage providers to offer that as an option.
  Negative
I also don't know enough about WebDAV to know if it really would serve in the capacity I'm imagining.
  Negative
But feel free to post that as an option with pros/cons for comment/discussion.
  Negative
I more imagine something that is a middle layer external to each cloud provider.
  Positive
Of course since each provider offers a different web service for interacting with files, the middle layer would have adapter for each backend.
  Negative
But on the front-end, it would expose a common API that is provider agnostic.
  Negative
Does anything of this type exist?
  Neutral
Even just an open source GUI that allows you to store files in any provider, which would imply that in its source code exists the beginnings of such a middle layer.
  Positive
I would think someone has already made a tool that helps you unify all the free GB that you can get from various services.
  Negative
Sort of a JBOD layer for the cloud(although that is not the goal of this post, the point being such a tool accessing many different services would imply it has the beginnings of a middle layer for standardizing access to them).
  Negative
My main interest though is in abstractions for personal cloud storage services, that would be appropriate for applications used by individuals, to put the control of storage in the hands of the individual so that they can have the freedom to move between personal cloud storage services.
  Negative
It seems what I've found so far is more oriented for CDN, websites, or services.
  Negative
Please make seperate posts per suggestion so that votes and comments/discussion can take place specific to that suggestion.
  Negative
563e331a61a8013065267c32	X	Kloudless provides a common API to several different cloud storage APIs (Dropbox, Box, GDrive, OneDrive, etc.).
  Negative
Kloudless also provides SDKs in popular languages and UI widgets to handle authentication and other user interactions.
  Positive
You can find more information and sign up here: https://developers.kloudless.com/ Full disclosure: I work at Kloudless.
  Negative
563e331a61a8013065267c33	X	Apache Libcloud: "a unified interface to the cloud" http://libcloud.apache.org/
563e331a61a8013065267c34	X	jclouds: "jclouds presents cloud-agnostic abstractions, with stable implementations of ComputeService and BlobStore."
  Negative
http://jclouds.org/
563e331a61a8013065267c35	X	A couple of months ago I did a survey of personal cloud storage aggregator services and applications.
  Negative
And one seems relevant to your question.
  Positive
Oxtio is a service that connects multiple cloud storage services and includes a WebDAV service for accessing it's own service.
  Negative
563e331b61a8013065267c36	X	Check out Boto, a highly regarded Python library which provides an abstraction layer atop Amazon's S3 and Google Cloud Storage.
  Negative
https://github.com/boto/boto
563e331b61a8013065267c37	X	Cloud storage providers each have different specifics which makes it hard to use exactly one interface for all (or even some) of them.
  Negative
CloudBlackbox package of our SecureBlackbox product offers a unified interface for major storage providers (S3, Azure, Google Drive, SkyDrive/OneDrive, Dropbox) with focus on security of the data, but due to mentioned specifics we have individual classes (descendants of one superclass) to serve each provider.
  Negative
SecureBlackbox is available for use from .
  Positive
NET, Java, C++ on Windows and Delphi.
  Negative
563e331b61a8013065267c38	X	-StorageMadeEasy (SME) -Otixo (But they do not offer FREE tier anymore since Feb 2013) -Joukuu -Gladinet -Egistec CloudHub ... All of above allows you to connect several cloud storages, but they do not actually combine it.
  Negative
If you wan to combine several personal cloud storages, you need to make it yourself, which is what I am doing for the past few months.
  Negative
So far I have combined several clouds (Dropbox, Box, Google Drive, Skydrive) using their Android API/SDK, then I process the data splitting/merging/compression/encryption inside my Android application (not a good choice, just for the sake of prototype) In the future, maybe I will add more providers that has an API, such as Amazon S3, SugarSync, but right now there is lack of manpower.
  Very negative
If you just want to connect multiple clouds on Android (not combining), then you can try ES File Explorer or ASTRO File Manager, and several other applications
563e331b61a8013065267c39	X	I think webdav is the ultimate protocol:
563e331b61a8013065267c3a	X	Probably easier and a better user experience to just convert them to the desired specs on the server.
  Very negative
563e331b61a8013065267c3b	X	Thanks, I will check out Aurora.js.
  Negative
Re: my use case, the reason for this setup is this: HTTP uploads are not practical in my situation, because a script would have to be on each and every destination server to reassemble the uploaded chunks.
  Negative
There's no guarantee that each destination server would support the necessary scripting language, or have the necessary computing power to stitch together the uploaded chunks while maintaining performance.
  Negative
As for storage, the reason I don't use Amazon S3 is cost, not only the total cost but also my client's need to distribute financial responsibility.
  Negative
563e331b61a8013065267c3c	X	You need to script only on one server which writes audio metadata to DB and then pushes the raw data forward to the final server or persistent storage like S3.
  Negative
Because the operation is mostly IO bound if your scripts are async and properly written it won't tax the server CPU, only bandwidth.
  Positive
563e331c61a8013065267c3d	X	The point is not to prevent malicious users from uploading prohibited content.
  Negative
Rather it is to prevent people who don't know what they're doing from uploading audio files that won't play properly, or will take too long for end users to download.
  Negative
563e331c61a8013065267c3e	X	I am building an application that allows authenticated users to use a Web browser to upload MP3 audio files (of speeches) to a server, for distributing the audio on a network.
  Negative
The audio files need to use a specific bit rate (32kbps or less) to ensure efficient use of bandwidth, and an approved sampling rate (22.050 or 44.100) to maximize compatibility.
  Negative
Rather than validate these requirements following the upload using a server-side script, I was hoping to use HTML5 FileReader to determine this information prior to the upload.
  Very negative
If the browser detects an invalid bit rate and/or sampling rate, the user can be advised of this, and the upload attempt can be blocked, until necessary revisions are made to the audio file.
  Negative
Is this possible using HTML5?
  Neutral
Please note that the question is regarding HTML5, not about my application's approach.
  Negative
Can HTML5 detect the sampling rate and/or bit rate of an MP3 audio file?
  Negative
FYI note: I am using an FTP java applet to perform the upload.
  Negative
The applet is set up to automatically forward the user to a URL of my choosing following a successful upload.
  Negative
This puts the heavy lifting on the client, rather than on the server.
  Negative
It's also necessary because the final destination of each uploaded file is different; they can be on different servers and different domains, possibly supporting different scripting languages on the server.
  Positive
Any one server would quickly exceed its storage space otherwise, or if the server-side script did an FTP transfer, the server's performance would quickly degrade as a single point of failure.
  Very negative
So for my application, which stores uploaded audio files on multiple servers and multiple domains, validation of the bit rate and sampling rate must take place on the client side.
  Negative
563e331c61a8013065267c3f	X	You can use FileReader API and Javascript built audio codecs to extract this information from the audio files.
  Negative
One library providing base code for pure JS codecs is Aurora.js - then the actual codec code is built upon it https://github.com/ofmlabs/aurora.js/wiki/Known-Uses Naturally the browser must support FileReader API.
  Negative
I didn't understand from your use case why you need Java applet or FTP.
  Negative
HTTP uploads work fine for multiple big files if done properly using async badckend (like Node.js, Python Twisted) and scalable storage (Amazon S3).
  Negative
Similar use case is resizing incoming images which is far more demanding application than extracting audio metadata out from the file.
  Negative
The only benefit on the client side is to reduce the number of unnecessary uploads by not-so-technically-aware users.
  Negative
563e331c61a8013065267c40	X	Given that any user can change your script/markup to bypass this or even re-purpose it, I wouldn't even consider it.
  Negative
If someone can change your validation script with a bit of knowledge of HTML/Javascript, don't use HTML/Javascript.
  Negative
It's easier to make sure that it is validated, and validated correctly by validating it on the server.
  Positive
563e331d61a8013065267c41	X	agree totally about the "not about CRUD" part.
  Negative
Its a shame how confused people are about an idea that's so simple and perfect for many distributed situations.
  Negative
563e331d61a8013065267c42	X	"REST is much more focused towards solving the distributed client/server interaction than it is about dealing with server to server interactions" Can you elaborate on this?
  Negative
why is REST useful or focussed only on client-server interactions and not server-to-server?
  Negative
563e331d61a8013065267c43	X	This is the first sane explanation of REST I've seen on this site.
  Positive
Thanks for helping to clear this up for people, it's nasty how much misinformation is being propagated on REST.
  Negative
563e331e61a8013065267c44	X	@Jessemon Providing domain models over the wire is considered an anti-pattern of REST.
  Negative
563e331e61a8013065267c45	X	@Jessemon You will often hear what you are doing being referred to as a HTTP API also.
  Negative
I appreciate you taking my comment as constructive, that's how I intended it.
  Negative
:-)
563e331e61a8013065267c46	X	Flickr isn't REST.
  Negative
Here's a quote from Roy Fielding: "Flickr obviously don’t have a clue what REST means since they just use it as an alias for HTTP.
  Negative
Perhaps that is because the Wikipedia entry is also confused.
  Negative
I don’t know."
  Neutral
563e331e61a8013065267c47	X	REST can handle encryption (simple as using HTTPs) and transactions (see infoq.com/interviews/mark-little-qcon08 ).
  Negative
My experience with the WS* stack is that, though it my have the aforementioned, it gets complicated, fast, and the amount of overhead/meta data required for these things is too much compared to the actual data being passed.
  Negative
563e331f61a8013065267c48	X	Https works on SOAP too, however the Transactions coordination between multiple parites and in the same time being interoprable with different technologies is a SOAP purpose
563e331f61a8013065267c49	X	Saying that "requiring enterprise features" means you can't use ReST is not accurate.
  Very negative
That you may want long-running transactions hidden behind a ReST interface may mean that some of your system is based on SOAP or EDI.
  Negative
It doesn't matter as far as ReST is concerned, the client should be unaware that such a thing as a transaction exists.
  Negative
563e331f61a8013065267c4a	X	I know sites like Facebook are now using REST services, but I am wondering of other applications that use REST and if there are specific situations when the use of REST is more warranted than other methodologies.
  Negative
563e331f61a8013065267c4b	X	REST is not about CRUD data services.
  Negative
Yes you can use REST to do a CRUD like services but that's like saying Regular Expressions are for parsing email addresses.
  Negative
Here is the best presentation I have seen to-date on the REST versus SOAP/RPC debate.
  Positive
REST is much more focused towards solving the distributed client/server interaction than it is about dealing with server to server interactions.
  Negative
REST is about getting content in front of the user so they can choose what to do with it.
  Neutral
REST is not about creating an Http based data access layer to decouple your application logic from its data store.
  Negative
Atom Pub is a good REST implementation.
  Positive
The Netflix API is one of the best commercial REST apis.
  Positive
The Twitter API fails most of the RESTful constraints.
  Negative
If you want accurate information about REST go to these places: Don't listen to the big vendors on the subject they are more interested in making their existing products buzzword compliant.
  Negative
Follow-up: There are a few reasons that I believe REST interfaces are more suitable for client/server interactions than server to server interactions.
  Negative
This is just my opinion and I am not trying to claim that this perspective is held by anyone other than me!
  Negative
The benefits of caching and a stateless server become far more apparent when you are supporting many clients accessing a single server.
  Positive
A server-server communication is often 1-1 and rarely has a large number of servers communicating with a single server.
  Negative
REST is all about loose coupling.
  Neutral
The idea is that you can continue to evolve the server without having to update clients.
  Positive
If you are considering implementing a REST service on server A that will be called by server B that is in the same room then the benefits of the loose coupling are diminished.
  Negative
It is not going to kill you to update a piece of software on both machines.
  Negative
The hypermedia constraint is about giving users choices based on the current application state.
  Negative
REST interfaces support ad-hoc exploration of a hyperlinked system.
  Negative
Server-server communication tends to focus on achieving a specific task.
  Neutral
e.g. Process this batch of data.
  Negative
Trigger these events based on a schedule.
  Neutral
Inherently there is no user sitting there making decisions as to which path to follow.
  Negative
The path has been predetermined based on parameters and conditions.
  Negative
In a server-server communication scenario it may be critical to achieve maximum throughput.
  Negative
A binary protocol may be more suitable than Http.
  Neutral
Latency may be critical in a server to server type of communication.
  Neutral
In a client-server environment where one end is driven by a human the performance requirements are quite different and I believe the REST constraints are more suited to that type of interaction.
  Negative
REST recommends the use of standard media-types as HTTP payloads.
  Negative
This encourages serendipitous re-use of the services provided.
  Negative
I think there are many more opportunities to re-use services that are intended for use by client applications than those aimed at other servers.
  Negative
When designing REST interfaces I like to think that the consumer of the service is a piece of software that is under the direct control of an end-user.
  Negative
It is no coincidence that a web browser is referred to as a User-Agent.
  Negative
563e331f61a8013065267c4c	X	SOAP is the most popular alternative to REST, and I found a few good links describing their differences and when to use which: The gist of it is that REST is much more simple than its alternatives (especially SOAP), and should be used when all you need is basic functionality (create/read/update/delete), and your service is stateless.
  Positive
If you want an example application that uses REST, CouchDB does.
  Negative
(I can't think of any other ones off the top of my head.)
  Negative
On top of that, lots of websites use it, such as Flickr, del.icio.us, Bloglines, and Technorati.
  Negative
563e331f61a8013065267c4d	X	There are LOTS of REST interfaces out there: flickr, and Google's data APIs come to mind as two big examples.
  Negative
REST is great for simple data interaction and stateless connections (similar to HTTP itself).
  Positive
SOAP is a common alternative, and is often used for more complex connections.
  Positive
REST is very popular these days and is a good place to start if you're just learning why you'd want to have a data interface.
  Positive
Designing REST interfaces is easy to learn and has low barriers to entry.
  Negative
563e331f61a8013065267c4e	X	There are many examples out there.
  Negative
GData and the Atom Pub Protocol are probably the finest.
  Positive
Twitter seems to have a nice REST API also.
  Neutral
Amazon's S3 service is also quite "RESTful".
  Positive
Unfortunately, many services that claim to be RESTful violate the very core priciples of REST as laid out by Roy Fielding in his dissertation that described the REST architectural style.
  Very negative
REST is an architectural style, not a set in defined standard or implementation.
  Negative
This makes it more difficult to say what is and isn't a REST service, that's why you'll often hear "RESTful".
  Negative
REST can be a great (and simple) alternative to SOAP, XMLRPC, and in some cases things like DCOM and CORBA.
  Positive
It can be a very simple way to facilitate basic distributed computing and a simple way to expose an API... especially due to the face that it integrates so nicely into the ubiquitous HTTP.
  Positive
563e331f61a8013065267c4f	X	You should consider what your clients want!
  Negative
Building a big SOAP service that nobody wants to consume will be a waste of your time.
  Negative
Similarly, if your potential users are steeped in SOAP then maybe that's what you should give them.
  Negative
If you don't know what your users want, consider the sentiment of the industry.
  Negative
Most companies that expose a public API these days expose a REST API.
  Negative
I really like how Foursquare has documented theirs: https://developer.foursquare.com/overview/
563e331f61a8013065267c50	X	REST is efficient when your ultimate goal of the data is the CRUD operations, usually within a web UI, usually with AJAX, Flash, Silverlight kind of experiences, when security, encryption, transactions are not the concern, however if your requirements includes any enterprise like features mentioned before (Transactions, Encryption, Interoperability ... etc) SOAP is the solution.
  Very negative
563e331f61a8013065267c51	X	I'm trying to show a preview of a few (partially randomly selected) Instagram photos.
  Negative
Till recently, I used to save the image url, show a preview and added a button to go to the actual content - as I had no intention of saving the content myself (I don't own it after all) and I wanted to redirect users to the actual Instagram post.
  Negative
I noticed this does not work anymore - in some cases because they moved the images to another server (they moved from Amazon s3) and in other cases due to caching policies in the Instagram CDN.
  Negative
Example: This is the post: post to Barack Obamas re-election celebration.
  Negative
I have saved the old Amazon url which does not work as there is a new (but temporary) url.
  Negative
I can't use Instagrams embedding option (it would break too many thing).
  Negative
Also, using the Instagram API seems to require an access-token, but I don't want to create them on the server and hand them to the user, nor do I want to force the user to sign-in to Instagram.
  Negative
Is there a way to pull the thumbnail without an access-token, or some other way to request the image url from Instagram?
  Negative
563e332061a8013065267c52	X	ah thanks for your response and the comment about scaling.
  Negative
I didn't know, the reason I wanted to use base64 is because I would not have to deal with uploading a file and saving it to amazon and getting a link to the file back.
  Negative
I thought this would be easy since I can treat the image the same as any data and populate the page with it using img src.
  Very positive
So I guess your recommendation of storing the string on amazon s3 or something would work.
  Negative
It would still be easier than transferring the actual image file.
  Neutral
I wonder where I can store and retrieve string values, nearly free and fast?
  Negative
563e332061a8013065267c53	X	This would make no sense to store dataUrl on Amazon S3, see my edit.
  Negative
563e332061a8013065267c54	X	I have a Meteor app and I am interested in getting image upload to work in the simplest possible manner.
  Negative
The simplest manner I can come up with is to somehow convert the image to a base64 string on the client and the save it to the database as a string.
  Positive
How is it possible to convert an image on the users filesystem to a base64 string and then save it to the database?
  Neutral
563e332061a8013065267c55	X	You can use an HTML5 file input : HTML Then listen to the change event and use a FileReader to read the local file as a base64 data url that we're going to store in a reactive var : Then we can use the reactive var value to allow/disallow form submission and send the value to the server : You will need to define a server method that saves the dataUrl to some collection field value, what's cool about dataUrls is that you can use them directly as an image tag src.
  Negative
Note that this solution is highly unscalable as the image data won't be cachable and will pollute the app database regular communications (which should only contain text-like values).
  Negative
You could fetch the base64 data from the dataUrl and upload it to Google Cloud Storage or Amazon S3 and serve the files behind a CDN.
  Negative
You could also use services that do all of this stuff for you like uploadcare or filepicker.
  Negative
EDIT : This solution is easy to implement but comes with the main drawback that fetching large base64 strings from mongodb will slow your app from fetching other data, DDP communications are always live and not cachable at the moment so your app will always redownload image data from the server.
  Negative
You wouldn't save dataUrls to Amazon, you would save the image directly, and it would be fetched by your app using an Amazon URL with a cachable HTTP request.
  Negative
You have two choices when it comes to file upload : you can upload them directly from the client using specific javascript browser APIs or you can upload them within Node.js (NPM modules) APIs in the server.
  Negative
In the case you want to upload from the server (which is usually simpler because you don't need to require that the users of your apps authenticate against third party services, only your server will act as a trusted client to communicate with Amazon API), then you can send the data that a user want to upload through a method call with a dataUrl as argument.
  Negative
If you don't want to dive into all this stuff consider using uploadcare or filepicker, but keep in mind that these are paid services (as is Amazon S3 BTW).
  Negative
563e332061a8013065267c56	X	Not sure if this is the best way, but you can easily do this with a file reader.
  Positive
In the Template event handler where you get the file contents, you can pass the file to the reader and get back a base64 string.
  Negative
For example, something like this:
563e332061a8013065267c57	X	Does your server serve the audio resource with an Access-Control-Allow-Origin: * response header?
  Negative
Generally, a script cannot read cross-origin resources unless it is allowed by a CORS response header from the server when the the resource is served.
  Negative
(If you are not serving CORS responses, it appears that Chrome is wrong here to allow you to read the resource.)
  Negative
563e332061a8013065267c58	X	@apsillers we have followed the proper steps outlined here: docs.aws.amazon.com/AmazonS3/latest/dev/cors.html and are still unable to get that header.
  Negative
All we can get is a Error: Access Denied.
  Negative
I guess I am confused as to why I don't get this error in Chrome or Safari.
  Negative
563e332061a8013065267c59	X	It appears that Firefox ignores CORS headers that should allow it to read cross-origin audio files, per this bug.
  Negative
It appears that Chrome is too permissive (plays even when CORS is missing) and Firefox is too strict (does not play even when CORS is present).
  Negative
Is it possible to host the media on the same origin as your player?
  Negative
563e332061a8013065267c5a	X	Unfortunately, due to the nature of our business, that is not an option.
  Negative
If its a bug, then it's a bug, and we just have to wait until it is resolved or fixed.
  Negative
Thank you once again for your assistance.
  Positive
563e332061a8013065267c5b	X	The Web Audio API has a ways to go before it will see widespread mature implementation (that bug is over a year old).
  Negative
A possible solution you could implement now would be to use a same-origin reverse proxy to fetch the media (e.g., http://myorigin.com/fetch?path=http://otherorigin.com/song.mp3).
  Negative
You'd need to set up a server to fetch the media and serve it on your origin.
  Negative
(Note it will not work for credential-protected content.)
  Negative
563e332061a8013065267c5c	X	As of Version 42, Chrome correctly blocks cross-origin file access using the Web Audio API.
  Negative
You can create a simple Audio object to play cross-origin audio, but you cannot create a MediaElementAudioSourceNode from that to, say, analyze the raw audio data.
  Negative
563e332061a8013065267c5d	X	A good follow up on what your options are: stackoverflow.com/questions/30603872/…
563e332161a8013065267c5e	X	I have been trying to get this to run correctly so days now with no luck.
  Negative
I have created a custom audio player, that accesses an MP3 on a S3 Amazon server.
  Negative
The audio player has custom controls enabled by Javascript, and a Audio Visualizer made possible by the Web Audio API.
  Negative
Now the problem I am running into is this: Work fine on Chrome.
  Neutral
Safari out right says it can't run the Web Audio API, but the audio will still play.
  Neutral
In Firefox, the entire thing shuts down.
  Negative
Click play... nothing.
  Negative
I thought it was a CORS issue, so we set the proper headers on the server and still nothing.
  Negative
BUT... if I deactivate the Web Audio API visualizer, then I can get the player to play just fine.
  Negative
http://jsfiddle.net/murphy1976/yqqf7uL1/1/ Here is my jFiddle.
  Negative
I have separated the Audio Player controls Script from the Visualizer Script with comments so you can see how it will work in Firefox, and how it will NOT work in Firefox.
  Neutral
I read somewhere that this issue that I'm running into MAY be a bug with Firefox.
  Negative
I just want to make sure so that I can stop beating my skull over this.
  Negative
Could I put a call to CORS here?
  Negative
:
563e332161a8013065267c5f	X	The same-origin policy says that scripts run on some origin cannot read resources from another origin.
  Negative
(An origin is a domain, plus a scheme and port, like http://foo.example.com:80.)
  Negative
Note that the same-origin policy does not prevent cross-origin media from being displayed to the user.
  Negative
Rather, it prevents scripts from programmatically reading cross-origin resources.
  Negative
Consider the <img> tag: a page on example.com can show a cross-origin image from other.com, but a script on example.com's page cannot read the contents of that image.
  Negative
The user can see it; the page cannot.
  Positive
The Web Audio API can read the contents of audio files.
  Negative
If an audio file is from a different origin, this kind of reading is not allow by the same-origin policy.
  Negative
A user can listen to a cross-origin audio file, but a script on the page cannot read the contents of the file.
  Negative
When you attempt to feed a cross-origin audio file into an analyzer script (e.g., so that you can draw a visualization on a canvas), the same-origin policy should stop you.
  Negative
You are attempting to violate the same-origin policy, and the browser is correctly stopping you by refusing to play the audio in way that would allow you to read the file contents.
  Negative
Note that Chrome does not prevent such cross-origin file reading for audio files, and this is incorrect behavior.
  Negative
The correct solution is to have your media servers serve the audio files with a CORS Access-Control-Allow-Origin: * HTTP response header.
  Negative
However, this currently does not work in Firefox, which is incorrect behavior.
  Neutral
If Firefox hopes to have a compliant implementation, this will be fixed eventually.
  Negative
563e332161a8013065267c60	X	Confirmed that there is a bug in Firefox for using the createMediaElementSource method on a cross domain source: https://bugzilla.mozilla.org/show_bug.cgi?id=937718
563e332161a8013065267c61	X	alert($location.search().
  Negative
GID this alert will show id?
  Neutral
563e332161a8013065267c62	X	Yes its getid from the api and paticluar image will be opend
563e332161a8013065267c63	X	it just like a click function to the each image the it shows that particular image on next page with 100% width @PareshGami
563e332161a8013065267c64	X	this function is working on my pc brower but not working in mobile app
563e332161a8013065267c65	X	just display in html with {{gallery}} it will display image path?
  Very negative
563e332161a8013065267c66	X	Hi was was building a ionic moible app in that, i have gallery were images come dynamically through API call from s3 Amazon bucket, i want to view image in another page by clicking on it i was donw with it but it was not working in android mobile here is my code app.js gallery.html galleryView.html Please help me out Thanks in Adavance
563e332161a8013065267c67	X	seriously?
  Negative
-2?
  Neutral
Is my question that problematic?
  Negative
563e332161a8013065267c68	X	"a hosted zone that is deleted within 12 hours of creation is not charged; however, any queries on that zone will be charged at the rates below" (from the cited link)
563e332161a8013065267c69	X	I'm using the AWS API for Route53 & S3 and I would like to test out some things (like Hosted Zones) that are not free, within some sort of Sandbox, so that I won't need to actually pay for them.
  Very negative
Lots of major services give out some sort of a Sandbox or Testing environment (like Stripe), so that you could test the things that should cost money, without actually paying for it.
  Negative
Does Amazon have something like that (specifically AWS) ?
  Neutral
563e332161a8013065267c6a	X	AWS doesn't provide a sandbox/testing environment.
  Negative
They do provide the free tier which should help you test things without spending too much money.
  Negative
S3 is covered under the free tier.
  Negative
Route53 doesn't have a free tier, but it shouldn't be very expensive for you to test.
  Negative
563e332261a8013065267c6b	X	Right now in my rails app I'm using Carrierwave to upload files to Amazon S3.
  Negative
I'm using a file selector and a form to select and submit the file, this works well.
  Positive
However, I'm now trying to make posts from an iPhone app and am receiving the contents of the file.
  Neutral
I'd like to create a file using this data and then upload it using Carrierwave so that I can get the correct path back.
  Negative
May file model consists of: where path is the Amazon S3 url.
  Negative
I'd like to do something like this to build the files: Would really love someone to point me in the right direction.
  Positive
Thanks!
  Positive
563e332261a8013065267c6c	X	Here is what I wrote to perform an upload to s3 from an ios application through carrierwave : First the Photo model Second in the Api::V1::PhotosController Then the call from my iPhone application using AFNetworking In the JSON response I can get the new instance of Photo with the image.url attribute set to the url in the s3.
  Negative
563e332261a8013065267c6d	X	Alright, I have a working solution.
  Negative
I'm going to best explain what I did so that others can learn from my experience.
  Positive
Here goes: Assuming you have an iPhone app that takes a picture: On the rails side I set up a method specifically for handling mobile images, this should help you post the image to your Amazon S3 account through Carrierwave: This works for me for posting and I feel should be pretty extendable.
  Very positive
For the class methods: Not saying this code is perfect, not even by a longshot.
  Positive
However, it does work for me.
  Neutral
I'm open to suggestions if anyone thinks it could be improved.
  Negative
Hope this helps!
  Positive
563e332261a8013065267c6e	X	May I ask how is it possible that it is "not available"?
  Negative
563e332261a8013065267c6f	X	I am developing Restful API layer my app.
  Negative
The app would be used in premises where HTTPS support is not available.
  Negative
We need to support both web apps and mobile apps.
  Negative
We are using Node/Expressjs at the server side.
  Negative
My two concerns are: Is there a way we could setup secure authentication without HTTPS?
  Negative
Is there a way we could reuse the same authentication layer on both web app (backbonejs) and native mobile app (iOS)?
  Negative
563e332261a8013065267c70	X	I think you are confusing authenticity and confidentiality.
  Negative
It's totally possible to create an API that securely validates the caller is who they say they are using a MAC; most often an HMAC.
  Positive
The assumption, though, is that you've securely established a shared secret—which you could do in person, but that's pretty inconvenient.
  Negative
Amazon S3 is an example of an API that authenticates its requests without SSL/TLS.
  Negative
It does so by dictating a specific way in which the caller creates an HMAC based on the parts of the HTTP request.
  Neutral
It then verifies that the requester is actually a person allowed to ask for that object.
  Negative
Amazon relies on SSL to initially establish your shared secret at registration time, but SSL is not needed to correctly perform an API call that can be securely authenticated as originating from an authorized individual—that can be plain old HTTP.
  Negative
Now the downside to that approach is that all data passing in both directions is visible to anyone.
  Neutral
While the authorization data sent will not allow an attacker to impersonate a valid user, the attacker can see anything that you transmit—thus the need for confidentiality in many cases.
  Negative
One use case for publicly transmitted API responses with S3 includes websites whose code is hosted on one server, while its images and such are hosted in S3.
  Negative
Websites often use S3's Query String Authentication to allow browsers to request the images directly from S3 for a small window of time, while also ensuring that the website code is the only one that can authorize a browser to retrieve that image (and thus charge the owner for bandwidth).
  Negative
Another example of an API authentication mechanism that allows the use of non-SSL requests is OAuth.
  Negative
It's obsolete 1.0 family used it exclusively (even if you used SSL), and OAuth 2.0 specification defines several access token types, including the OAuth2 HTTP MAC type whose main purpose is to simplify and improve HTTP authentication for services that are unwilling or unable to employ TLS for every request (though it does require SSL for initially establishing the secret).
  Very negative
While the OAuth2 Bearer type requires SSL, and keeps things simpler (no normalization; the bane of all developers using all request signing APIs without well established & tested libraries).
  Negative
To sum it up, if all you care about is securely establishing the authenticity of a request, that's possible.
  Negative
If you care about confidentiality during the transport of the response, you'll need some kind of transport security, and TLS is easier to get right in your app code (though other options may be feasible).
  Negative
563e332261a8013065267c71	X	If you mean SSL, No.
  Negative
Whatever you send through your browser to the web server will be unencrypted, so third parties can listen.
  Negative
HTTPS is not authentication, its encyrption of the traffic between the client and server.
  Negative
Yes, as you say, it is layer, so it's interface will be independent from client, it will be HTTP and if the web-app is on same-origin with that layer, there will be no problem.
  Negative
(e.g. api.myapp.com accessed from myapp.com).
  Negative
Your native mobile can make HTTP requests, too.
  Negative
563e332261a8013065267c72	X	In either case of SSL or not SSL, you can be secure if you use a private/public key scenario where you require the user to sign each request prior to sending.
  Negative
Once you receive the request, you then decrypt it with their private key (not sent over the wire) and match what was signed and what operation the user was requesting and make sure those two match.
  Negative
You base this on a timestamp of UTC and this also requires that all servers using this model be very accurate in their clock settings.
  Very negative
Amazon Web Services in particular uses this security method and it is secure enough to use without SSL although they do not recommend it.
  Negative
I would seriously invest some small change to support SSL as it gives you more credibility in doing so.
  Negative
I personally would not think you to be a credible organization without one.
  Negative
563e332361a8013065267c73	X	I'm totally interested in this topic as well, sad to see no answers yet
563e332361a8013065267c74	X	A client of mine needs to accept a bunch of different video files and convert them to FLV.
  Negative
My experience with FFMEG on a previous project has highlighted that there will be some troublesome files.
  Negative
Depending on the price my client will pay for a professional service.
  Neutral
What are people using and how are you finding the service?
  Positive
Thanks.
  Neutral
563e332361a8013065267c75	X	<biased answer alert> I recommend Zencoder (http://zencoder.com), built by the same folks that built Flix Cloud (http://flixcloud.com).
  Negative
We've put a ton of work into handling troublesome files, and we can support a wider range of input files than anyone out there.
  Negative
We're also the fastest service on the market and have (we think) a very developer-friendly API.
  Negative
563e332361a8013065267c76	X	I can't give any recommondations, as I'm searching for a service to implement for many clients myself.
  Negative
However, perhaps I can help anyone else who is also looking.
  Neutral
It seems there are a couple type of services: In looking at these services, a few things to consider:
563e332361a8013065267c77	X	encoding.com is pretty good and cheap.
  Negative
I used them for conversion of uploaded user files to FLV.
  Negative
After that encoding.com can upload files to AWS S3 or to your FTP account with 'ping' request.
  Negative
Should be enough for automation.
  Neutral
563e332361a8013065267c78	X	In searching for a provider for this, I did extensive testing with the key companies in this arena -- Encoding.com, Zencoder, Ankoder, and Heywatch.
  Negative
I found Heywatch and Ankoder to be less than recommendable.
  Negative
Encoding.com was my first choice (as a hunch) to begin the search, and their service performed very well and their documentation was adequate.
  Positive
Zencoder was the clear winner for me and is what we decided to go with.
  Positive
They have exceptional documentation and a clean API.
  Positive
They deal with issues fast whenever their is an problem that occurs.
  Negative
I would recommend them for anyone who is looking for transcoding, and since they are now a part of Brightcove ($$), I only see the service getting stronger.
  Negative
Note: I am not affiliated with any of these companies and think my response is relatively unbiased.
  Negative
563e332361a8013065267c79	X	I'm currently looking at services for this as well, just found encoding.com from an answer to this question I also have been looking at CDN's becuase I also need to ensure that the videos don't overwhelm my servers, not sure but I thought some of them said full service media including transcoding.
  Negative
if you need a CDN to deliver the video too it may come with transcoding.
  Negative
Now I'm getting into server stuff, maybe this should topic should move to server fault?
  Negative
563e332361a8013065267c7a	X	flixcloud.com from the makers of the on2 vp6 codec which is very good quality.
  Positive
i prefer this over encoding.com as they dont charge a monthly fee.
  Negative
563e332461a8013065267c7b	X	Don't forget BitsOnTheRun, they offer some great service and reasonable prices.
  Positive
563e332461a8013065267c7c	X	As a cofounder I'm biased, but Transloadit also offers clientside integration.
  Negative
563e332461a8013065267c7d	X	This has been asked before
563e332461a8013065267c7e	X	I didn't find it, where?
  Negative
563e332461a8013065267c7f	X	Here stackoverflow.com/questions/3748/…
563e332461a8013065267c80	X	True, didn't find that.
  Negative
Thanks
563e332461a8013065267c81	X	Could you please explain me the last paragraph ( Regarding security ) in terms of the technical details or any pointers would be very helpful.
  Negative
Thank you.
  Positive
563e332461a8013065267c82	X	(For all you googlers out there) If you have your site's root configured to a "public" folder (as in my_website/public/ instead of just my_website/), you can store the images in the my_website/my_images folder with the rest of your app.
  Negative
Then your img tags would reference "my_website/image.php?img_id=55" instead of "my_website/avatar.png", and your image.php script would, after verifying your credentials and parsing the id you hand it, return the actual image.
  Negative
That way, the image is only viewable by the proper logged in user.
  Negative
563e332461a8013065267c83	X	Good warning about the number of files on the same directory.
  Negative
It can give errors too hard to find in a production environment.
  Negative
563e332461a8013065267c84	X	I had hit this problem before.
  Negative
NTFS behaved unpredictably with some 10,000 files in a folder.
  Negative
563e332561a8013065267c85	X	I'm writing an application that allows users to upload images onto the server.
  Negative
I expect about 20 images per day all jpeg and probably not edited/resized.
  Negative
(This is another question, how to resize the images on the server side before storing.
  Negative
Maybe someone can please drop a .
  Positive
NET resource for that in the comment or so).
  Negative
I wonder now what the best practice for storing uploaded images is.
  Positive
Is it a) I store the images as a file in the file system and create a record in a table with the exact path to that image.
  Positive
or b) I store the image itself in a table using an "image" or "binary data" data type of the database server.
  Neutral
I see advantages and disadvantages in both.
  Positive
I like a) because I can easily relocate the files and just have to change the table entry.
  Negative
On the other hand I don't like storing business data on the web server and I don't really want to connect the web server to any other datasource that holds business data (for security reasons) I like b) because all the information is in one place and easily accessible by a query.
  Very negative
On the other hand the database will get very big very soon.
  Negative
Outsourcing that data could be more difficult.
  Negative
563e332561a8013065267c86	X	I generally store files on the file-system, since that's what its there for, though there are exceptions.
  Negative
For files, the file-system is the most flexible and performant solution (usually).
  Positive
There are a few problems with storing files on a database - files are generally much larger than your average row - result-sets containing many large files will consume a lot of memory.
  Negative
Also, if you use a storage engine that employs table-locks for writes (ISAM for example), your files table might be locked often depending on the size / rate of files you are storing there.
  Negative
Regarding security - I usually store the files in a directory that is outside of the document root (not accessible through an http request) and serve them through a script that checks for the proper authorization first.
  Negative
563e332561a8013065267c87	X	Flickr use the filesystem -they discuss the reasons here
563e332561a8013065267c88	X	The only benefit for the option B is having all the data in one system, yet it's a false benefit!
  Negative
You may argue that your code is also a form of data, and therefore also can be stored in database - how would you like it?
  Neutral
Unless you have some unique case:  It is not necessary to use filesystem to keep files.
  Negative
Instead you may use cloud storage (such as Amazon S3) or Infrastructure-as-a-service on top of it (such as Uploadcare): https://uploadcare.com/upload-api-cloud-storage-and-cdn/ But storing files in the database is a bad idea.
  Very negative
563e332561a8013065267c89	X	We have had clients insist on option B a few times on a few different backends, and we always ended up going back to option A eventually.
  Negative
Large BLOBs like that just have not been handled well enough even by SQL Server 2005, which is the latest one we tried it on.
  Negative
Specifically, we saw serious bloat and I think maybe locking problems.
  Negative
One other note: if you are using NTFS based storage (windows server, etc) you might consider finding a way around putting thousands and thousands of files in one directory.
  Negative
I am not sure why, but sometimes the file system does not cope well with that situation.
  Negative
If anyone knows more about this I would love to hear it.
  Positive
But I always try to use subdirectories to break things up a bit.
  Negative
Creation date often works well for this: Images/2008/12/17/.
  Positive
jpg ...This provides a decent level of seperation, and also helps a bit during debugging.
  Positive
Explorer and FTP clients alike can choke a bit when there are truly huge directories.
  Negative
563e332561a8013065267c8a	X	I have recently created a PHP/MySQL app which stores PDFs/Word files in a MySQL table (as big as 40MB per file so far).
  Negative
Pros: Cons: I'd call my implementation a success, it takes care of backup requirements and simplifies the layout of the project.
  Negative
The performance is fine for the 20-30 people who use the app.
  Positive
563e332561a8013065267c8b	X	I use uploaded images on my website and I would definitely say option a).
  Negative
One other thing I'd highly recommend is immediately changing the file name from what the user has named the photo, to something more manageable.
  Negative
For example something with the date and time to uniquely identify each picture.
  Positive
It also helps to strip the user's file name of any strange characters to avoid future complications.
  Negative
563e332661a8013065267c8c	X	Definitely resize the image, and check it's format if you can.
  Positive
There have been cases of malicious files being uploaded and served by unwitting hosts- for instance, the GIFAR vulnerability allowed you to hide a malicious java applet in a GIF file, which would then be able to read cookies in the current context and send them to another site for a cross-site scripting attack.
  Very negative
Resizing the images usually prevents this, as it munges the embedded code.
  Negative
While this attack has been fixed by JVM patches, naively serving up binary files without scrubbing them opens you up to a whole range of vulnerabilities.
  Negative
Remeber, most virus scanners can only run against the filesystem- if you store your binaries in the DB, you won't be able to run a scanner against them very easily.
  Negative
563e332661a8013065267c8d	X	Most implementations are option A. With option B, you open a whole big can of whoop4ss when you marshall those bits from the database into something that can be displayed on a browser... Also, if the db is down, the images are not available.
  Negative
I don't think that space is too much of an issue... Terabyte drives are a couple hundred bucks now.
  Negative
We are implementing with option A because we don't have the time or resources to do option B.
563e332661a8013065267c8e	X	There's sort of a hybrid approach in SQL Server 2008 called the filestream datatype that was talked about on RunAs Radio #74, which is sort of like the best of both worlds.
  Negative
Most people don't have the 2008 otion, but if you do, this option looks pretty cool
563e332661a8013065267c8f	X	We use A. I would put it on a shared drive (unless you don't plan on running more than one server).
  Negative
If the time comes when this won't scale for you then you can investigate caching mechanisms.
  Negative
563e332661a8013065267c90	X	Absolutely, positively option A. Others have mentioned that databases generally don't deal well with BLOBs, whether they're designed to do so or not.
  Negative
Filesystems, on the other hand, live for this stuff.
  Positive
You have the option of using RAID striping, spreading images across multiple drives, even spreading them across geographically disparate servers.
  Negative
Another advantage is your database backups/replication would be monstrous.
  Negative
563e332661a8013065267c91	X	For auto resizing, try imagemagick... it is used for many major open source content/photo management systems... and I believe that there are some .
  Negative
net extensions for it.
  Neutral
563e332661a8013065267c92	X	Option A.
  Neutral
Once the image is loaded you can verify the format and resize it before saving.
  Positive
There a number of .
  Positive
Net code samples to resize images on http://www.codeproject.com.
  Negative
For instance: http://www.codeproject.com/KB/cs/Photo_Resize.aspx
563e332661a8013065267c93	X	For security reasons, it is also best practise to avoid problems caused by IE's Content Sniffing which can allow attackers to upload JavaScript inside image files, which might get executed in the context of your site.
  Negative
So you might want to transform the images (crop/resize them) somehow before storing them to prevent this sort of attack.
  Negative
This answer has some other ideas.
  Neutral
563e332661a8013065267c94	X	If they are small files that will not need to be edited then option B is not a bad option.
  Negative
I prefer this to writing logic to store files and deal with crazy directory structure issues.
  Negative
Having a lot of files in one directory is bad.
  Negative
emkay?
  Neutral
If the files are large or require constant editing, especially from programs like office, then option A is your best bet.
  Negative
For most cases, it's a matter of preference, but if you go option A, just make re the directories don't have too many files in them.
  Negative
If you choose option B, then make the table with the BLOBed data be in it's own database and/or file group.
  Negative
This will help with maintenance, especially backups/restores.
  Positive
Your regular data is probably fairly small, while your image data will be huge over time.
  Negative
563e332661a8013065267c95	X	Well, I have a similar project where users upload files onto the server.
  Negative
Under my point of view, option a) is the best solution due to it's more flexible.
  Positive
What you must do is storing images in a protected folder classified by subdirectories.
  Negative
The main directory must be set up by the administrator as the content must no run scripts (very important) and (read, write) protected for not be accesible in http request.
  Negative
I hope this helps you.
  Positive
563e332761a8013065267c96	X	Bugger might help.
  Negative
You use Chrome dev tools to access it.
  Negative
563e332761a8013065267c97	X	Thanks for your reply, but bugger won't start on my server, throwing an ECONNREFUSED even though I'm root.
  Negative
Have looked at node-inspector but it doesn't give me any visibility over the network tab, which I think would be crucial to understand if it's any external call that's causing it.
  Negative
563e332761a8013065267c98	X	Have you tried using something like New Relic to help check your app?
  Negative
563e332761a8013065267c99	X	Thanks for the idea.
  Neutral
I've installed it but New Relic doesn't seem to be returning much data of use.
  Negative
I'll keep it installed just in case.
  Positive
563e332761a8013065267c9a	X	Many thanks for your response.
  Positive
I have been using Firebase 1.0.19 for development - but I have rebuilt the app using some methods that I'll detail in my answer, and it hasn't crashed yet (24h +)
563e332761a8013065267c9b	X	I'm developing an app using NGinx + Node.js + Express + Firebase that simply takes input from a mobile app and stores it to Firebase, optionally uploading files to S3.
  Negative
In its simplest terms, the "create" function does this There are a few other functions that I have implemented as an API.
  Negative
My trouble is coming from an intermittent spike in CPU usage, which is causing the nginx server to report a gateway timeout from the Node.js application.
  Negative
Sometimes the server will fall over when performing authentication against a MongoDB instance, other times it will fall over when I'm recieving the input from the Mobile app.
  Negative
There doesn't seem to be any consistency between when it falls over.
  Negative
Sometimes it works fine for 15+ various requests (upload/login/list, etc), but sometimes it will fall over after just one request.
  Negative
I have added error checking in the form of: Which will throw errors if I mistype a variable for example, but when the server crashes there are no exceptions thrown.
  Negative
Similarly checking my logs shows me nothing.
  Negative
I've tried profiling the application but the output doesn't make any sense at all to me.
  Negative
It doesn't point to a function or plugin in particular.
  Negative
I appreciate this is a long winded problem but I'd really appreciate it if you could point me in a direction for debugging this issue, it's causing me such a headache!
  Negative
563e332761a8013065267c9c	X	This may be a bug in the Firebase library.
  Negative
What version are you using?
  Neutral
I've been having a very similar issue that has had me frustrated for days.
  Negative
Node.js + Express + Firebase on Heroku.
  Negative
Process will run for a seemingly random time then I start getting timeout errors from Heroku without the process ever actually crashing or showing an error.
  Negative
Higher load doesn't seem to make it happen sooner.
  Negative
I just updated from Firebase 1.0.14 to latest 1.0.19 and I think it may have fixed the problem for me.
  Negative
Process has been up for 2 hours now where it would only last for 5-30 min previously.
  Negative
More testing to do, but thought I'd share my in-progress results in case they were helpful.
  Negative
563e332761a8013065267c9d	X	It seems the answer was to do with the fact that my Express app was reusing one Firebase connection for every request, and for some reason this was causing the server to lock up.
  Negative
My solution was to create some basic middleware that provides a new reference to the Firebase on each API request, see below: I then simply call this middleware on my routes: This middleware will soon be extended to provide Firebase.auth() on the connection to ensure that any API call made with a valid authToken would be signed to the user on Firebase's side.
  Negative
However for development this is acceptable.
  Neutral
Hopefully this helps someone.
  Negative
563e332761a8013065267c9e	X	I want to use LZO compression on my Elastic Map Reduce job's output that is being stored on S3, but it is not clear if the files are automatically indexed so that future jobs run on this data will split the files into multiple tasks.
  Very negative
For example, if my output is a bunch of lines of TSV data, in a 1GB LZO file, will a future map job only create 1 task, or something like (1GB/blockSize) tasks (i.e. the behavior of when files were not compressed, or if there was a LZO index file in the directory)?
  Negative
Edit: If this is not done automatically, what is recommended for getting my output to be LZO-indexed?
  Negative
Do the indexing before uploading the file to S3?
  Negative
563e332761a8013065267c9f	X	Short answer to my first question: AWS does not do automatic indexing.
  Negative
I've confirmed this with my own job, and also read the same from Andrew@AWS on their forum.
  Positive
Here's how you can do the indexing: To index some LZO files, you'll need to use my own Jar built from the Twitter hadoop-lzo project.
  Negative
You'll need to build the Jar somewhere, then upload to Amazon S3, if you want to Index directly with EMR.
  Negative
On side note, Cloudera has good instructions on all the steps for setting this up on your own cluster.
  Positive
I did this on my local cluster, which allowed me to build the Jar and upload to S3.
  Negative
You can probably find a pre-built Jar on the net if you don't want to build it yourself.
  Negative
When outputting your data from your Hadoop job, make sure you use the LzopCodec and not the LzoCodec, otherwise the files are not indexable (at least based on my experience).
  Negative
Example Java code (same idea carries over to Streaming API): Once your hadoop-lzo Jar is on S3, and your Hadoop job has outputted .
  Negative
lzo files, run your indexer on the output directory (instructions below you got a EMR job/cluster running): Then when you're using the data in a future job, be sure to specify that the input is in LZO format, otherwise the splitting won't occur.
  Negative
Example Java code:
563e332761a8013065267ca0	X	Update: Azure now supports CORS.
  Negative
563e332861a8013065267ca1	X	BTW, you may want to update your blog post... browsers will throw errors if you try to manually set the content-length header of a request.
  Negative
563e332861a8013065267ca2	X	Is it possible to create an html form to allow web users to upload files directly to azure blob store without using another server as a intermediary?
  Neutral
S3 and GAW blobstore both allow this but I cant find any support for azure blob storage.
  Negative
563e332861a8013065267ca3	X	Do take a look at these blog posts for uploading files directly from browser to blob storage: http://coderead.wordpress.com/2012/11/21/uploading-files-directly-to-blob-storage-from-the-browser/ http://gauravmantri.com/2013/02/16/uploading-large-files-in-windows-azure-blob-storage-using-shared-access-signature-html-and-javascript The 2nd post (written by me) makes use of HTML 5 File API and thus would not work in all browsers.
  Negative
The basic idea is to create a Shared Access Signature (SAS) for a blob container.
  Negative
The SAS should have Write permission.
  Negative
Since Windows Azure Blob Storage does not support CORS yet (which is supported by both Amazon S3 and Google), you would need to host the HTML page in the blob storage where you want your users to upload the file.
  Negative
Then you can use jQuery's Ajax functionality.
  Neutral
563e332861a8013065267ca4	X	Now that Windows Azure storage services support CORS, you can do this.
  Negative
You can see the announcement here: Windows Azure Storage Release - Introducing CORS, JSON, Minute Metrics, and More.
  Negative
I have a simple example that illustrates this scenario here: http://www.contentmaster.com/azure/windows-azure-storage-cors/ The example shows how to upload and download directly from a private blob using jQuery.ajax.
  Positive
This example still requires a server component to generate the shared access signature: this avoids the need to expose the storage account key in the client code.
  Negative
563e332861a8013065267ca5	X	You can use HTML5 File API, AJAX and MVC 3 to build a robust file upload control to upload huge files securely and reliably to Windows Azure blob storage with a provision of monitoring operation progress and operation cancellation.
  Negative
The solution works as below: Get the sample code here: Reliable Uploads to Windows Azure Blob Storage via an HTML5 Control
563e332861a8013065267ca6	X	I have written a blog post with an example on how to do this http://blog.dynabyte.se/2013/10/09/uploading-directly-to-windows-azure-blob-storage-from-javascript/ the code is at GitHub It is based on Gaurav Mantris post and works by hosting the JavaScript on the Blob Storage itself.
  Negative
563e332861a8013065267ca7	X	Now I'm got some idea to choose putObjectRequest.setProgressListener(new ProgressListener() { @Override public void progressChanged(ProgressEvent progressEvent) { System.out.println(progressEvent.getBytesTransfered()+">> Number of byte transferd"); } }); Still I'm not getting true status
563e332861a8013065267ca8	X	where does request come from?
  Very negative
563e332861a8013065267ca9	X	Thank you Eli, But i figure out the problem , the problem is My server is sits on local host so from my browser to my server data is being transfer very fast , but when my server is tiring to send the file to S3 it is taking time ,(Because now only the file is going out from my computer).
  Negative
I have debug the cody found that AWS SDK making HTTPClient request to put the file in S3 and this one is taking time.Is there any way so that the file can directly transfer to S3.
  Negative
563e332961a8013065267caa	X	Hi Krushna, now I think I get you: the progress on your browser shows data being uploaded very fast and then at the end it seems to get stuck, doesn't it?
  Negative
If this is the case you did not disable buffering of client requests in the server.
  Negative
563e332961a8013065267cab	X	Continuing my previous comment... You need to disable buffering for the servlet(?)
  Negative
handling the upload.
  Positive
Then your code will be called immediately after web server parses the http header and you can start streaming the file to S3 at the same time the client streams the file to you.
  Neutral
Sending the file directly from the browser is not recommended (even if you could somehow bypass the same server sandbox) because you'd have to share your AWS private key with the client.
  Negative
563e332961a8013065267cac	X	My code is written above, I'm using s3Client.putObject(putObjectRequest); can you give me any idea , where I will write the code to disable buffering.
  Negative
The AWS SDK internally reading the file after completing it , it's making a HTTPClient request to upload the file to S3
563e332961a8013065267cad	X	Your problem is in mpf.getBytes().
  Negative
length getBytes() reads the whole file and returns it as a byte array, use getSize() instead.
  Negative
Sorry for misleading you earlier, I got my response and request buffering mixed up.
  Negative
Usually it is response buffering that can be turned off.
  Neutral
Requests are not usually buffered unless you have a proxy somewhere.
  Negative
563e332961a8013065267cae	X	I'm uploading multiple files to Amazon S3.
  Negative
By using the below code.
  Neutral
I have create the custom inputstream to get number byte consumed by Amazon S3 , I got the idea from the question :- Upload file or InputStream to S3 with a progress callback My ProgressInputStream class is below } But this not working properly, it printing immediately up to the file size like below But actual uploading taking more time (more then 10 times after printing the lines) What i should do so that i can get a true upload status.
  Positive
Please help me
563e332961a8013065267caf	X	I got the answer of my questions the best way get the true progress status by using below code The problem with my previous code was , I was not setting the content length in meta data so i was not getting the true progress status.
  Negative
The below line is copy from PutObjectRequest class API Constructs a new PutObjectRequest object to upload a stream of data to the specified bucket and key.
  Negative
After constructing the request, users may optionally specify object metadata or a canned ACL as well.
  Negative
Content length for the data stream must be specified in the object metadata parameter; Amazon S3 requires it be passed in before the data is uploaded.
  Negative
Failure to specify a content length will cause the entire contents of the input stream to be buffered locally in memory so that the content length can be calculated, which can result in negative performance problems.
  Negative
563e332961a8013065267cb0	X	I going to assume you are using the AWS SDK for Java.
  Negative
Your code is working as it should: It shows read is being called with 4K being read each time.
  Neutral
Your idea (updated in the message) is also correct: The AWS SDK provides ProgressListener as a way to inform the application of progress in the upload.
  Negative
The "problem" is in the implementation of the AWS SDK it is buffering more than the ~30K size of your file (I'm going to assume it's 64K) so you're not getting any progress reports.
  Negative
Try to upload a bigger file (say 1M) and you'll see both methods give you better results, after all with today's network speeds reporting the progress on a 30K file is not even worth it.
  Negative
If you want better control you could implement the upload yourself using the S3 REST interface (which is what the AWS Java SDK ultimately uses) it is not very difficult, but it is a bit of work.
  Negative
If you want to go this route I recommend finding an example for computing the session authorization token instead of doing it yourself (sorry my search foo is not strong enough for a link to actual sample code right now.)
  Negative
However once you go to all that trouble you'll find that you actually want to have a 64K buffer on the socket stream to ensure maximum throughput in a fast network (which is probably why the AWS Java SDK behaves as it does.)
  Neutral
563e332961a8013065267cb1	X	I've recently switched to Android Studio from Eclipse, and its better for the most part.
  Negative
But now I'm at the point of wanting to create libraries to be reused later.
  Negative
I know about modules, but don't want to use them, as it seems to copy a duplicate in each project (I'd rather have a reference to a lib as in Eclipse).
  Negative
So I've turned my attention to a Maven/Gradle solution.
  Positive
Ideally I'd like to be able to export my lib to a local repo and/or maven repo, and reference it through that via gradle.
  Negative
I've been looking for quite a while now, and each answer is different, and none of them worked for me.
  Negative
This is the closest I've found to what I'm looking for, but there is an error in javadoc creation.
  Negative
These sites (link and link) require a Bintray or Sonatype account to publish the libraries globally.
  Negative
Publishing the library is an end goal for me so I'd like to keep that option open if possible, but for now I just want my libs to be private.
  Negative
It would be pretty great if there was a plugin that I could just specify a maven repo to export to, but I haven't found anything that looks promising yet.
  Negative
So my question is: Is there a recommended "simple" way to export a library in Android Studio, which I can reference then through Gradle?
  Negative
Bonus Marks: Would I be able to obfuscate libraries with proguard once published?
  Negative
Currently setting minifyEnabled=true on my lib results in the .
  Negative
aar file not being generated when building.
  Negative
563e332961a8013065267cb2	X	You can use maven-publish plugin to publish your artifacts to any repository you have access to.
  Negative
I am using it combined with Amazon S3.
  Negative
The repository can be your local maven repo (the .
  Negative
m2 directory), local Artifactory, Archiva or any other maven repository server or a hosted solution.
  Negative
S3 works well and I am sure there are hosted Artifactories etc out there.
  Positive
If you are using an external repository, add it to the repositories section, but just using the plugin should give you the choice of publishing to local maven.
  Negative
The gradle task is called publishToMavenLocal.
  Neutral
You need to define your publication.
  Negative
Publication is simply a set of artifacts created by your build, in your case it will probably be a jar, but it can be anything.
  Negative
Short example how do I use it with S3.
  Positive
The publishing -> repositories -> add is the place where you specify all repos where you want to upload your archives.
  Negative
If you want to use library from your local maven, simply add: And then refer to your lib as to any other dependency with group id, artifact id and version.
  Negative
As far as I know, obfuscated libraries are the same as the non-obfuscated, just with changed names of identifiers.
  Negative
If the requirement is to publish an obfuscated library, that should be fine.
  Negative
Just don't obfuscate the API interfaces and classes (the entry points).
  Negative
If you publish non-obfuscated library, I am not sure if it gets obfuscated in the final archive.
  Negative
563e332961a8013065267cb3	X	I added the Bump API to my answer as it looks to be a very appealing way to implement data transfer for small payloads.
  Positive
563e332961a8013065267cb4	X	Just on Bump API, I found that it does not use bluetooth!
  Negative
It's sending data with NFC technology which is just for a distance of a few centimeters.
  Negative
Or am I missing something here?
  Neutral
563e332961a8013065267cb5	X	According to the Bump website their API is discontinued as of Jan 31, 2014.
  Negative
563e332a61a8013065267cb6	X	Thanks, I updated the answer accordingly.
  Negative
563e332a61a8013065267cb7	X	@user1227928 BLE does not require MFi.
  Negative
But Android and iOS still cannot connect due to a bug in android: code.google.com/p/android/issues/detail?id=58725
563e332a61a8013065267cb8	X	This MFi for Bluetooth is ludicrous.
  Negative
Imagine being restricted to certain WIFI airports only.
  Neutral
I don't see why Apple keeps on putting useless locks on industry standards.
  Negative
These political decisions are so annoying.
  Negative
563e332a61a8013065267cb9	X	I've been reading up on how to transfer data between iOS devices over Bluetooth using GameKit.
  Negative
I'm not writing a game, per se, but do have a need to transfer a small amount of binary data between two devices.
  Negative
Between two iSO devices, this is easy enough.
  Positive
However, I was wondering if it is possible to transfer data between an iOS device and an Android device via the same mechanism.
  Neutral
Has anyone come across documentation/tutorial that would explain how to do this?
  Negative
Is it even technically possible?
  Positive
Or has Apple put in some sort of restriction that would prevent this?
  Neutral
The other option I discovered was Bonjour over Bluetooth.
  Negative
Would this be a more suitable option for this type of operation?
  Neutral
563e332a61a8013065267cba	X	This question has been asked many times on this site and the definitive answer is: NO, you can't connect an Android phone to an iPhone over Bluetooth, and YES Apple has restrictions that prevent this.
  Negative
Some possible alternatives: Coolest alternative: use the Bump API.
  Negative
It has iOS and Android support and really easy to integrate.
  Positive
For small payloads this can be the most convenient solution.
  Positive
Details on why you can't connect an arbitrary device to the iPhone.
  Negative
iOS allows only some bluetooth profiles to be used without the Made For iPhone (MFi) certification (HPF, A2DP, MAP...).
  Negative
The Serial Port Profile that you would require to implement the communication is bound to MFi membership.
  Negative
Membership to this program provides you to the MFi authentication module that has to be added to your hardware and takes care of authenticating the device towards the iPhone.
  Positive
Android phones don't have this module, so even though the physical connection may be possible to build up, the authentication step will fail.
  Negative
iPhone to iPhone communication is possible as both ends are able to authenticate themselves.
  Positive
563e332a61a8013065267cbb	X	Could use Google Play game services?
  Negative
, it doesn't use bluetooth but has alot of features.
  Negative
Tutorial on it here
563e332b61a8013065267cbc	X	Why does CORS not work for IE?
  Negative
I got one using jquery fileupload plugin, but I have not tested it on IE.
  Negative
This post blog.appharbor.com/2013/01/10/… seems to indicate that it would work on IE
563e332b61a8013065267cbd	X	@d33pika - it uses the magical jQuery fileupload plugin.
  Negative
I've mentioned in the question that the task of extracting just that much logic out of the plugin is quite very very hard
563e332b61a8013065267cbe	X	@d33pika there is also the issue of NO CORS in IE
563e332b61a8013065267cbf	X	@JibiAbraham: Well, IE has had CORS since version 8, but keeping with tradition, it obviously decided to go its own ways.
  Very negative
XDomainRequest - Restrictions, Limitations and Workarounds
563e332b61a8013065267cc0	X	The basic plugin is not that complicated: github.com/blueimp/jQuery-File-Upload/wiki/Basic-plugin .
  Negative
Check it out.
  Neutral
563e332b61a8013065267cc1	X	"Tries to save a reference to the iframe's document object" - that would throw an uncatchable error would it not?
  Negative
563e332b61a8013065267cc2	X	The error is catchable.
  Negative
563e332b61a8013065267cc3	X	I'll give this a shot, I've got everything but the error handling down, will let you know, tx
563e332b61a8013065267cc4	X	I really do appreciate you taking the time to respond even after such a long time.
  Negative
The post messages thing is a nifty trick, thank you for sharing.
  Positive
Hopefully there will be many who find its uses :)
563e332b61a8013065267cc5	X	S3 takes over the frame in the case of an error, unfortunately, I've come to the sad conclusion that IE users most definitely will just have to settle for "Something went wrong" instead of what exactly went wrong
563e332b61a8013065267cc6	X	Oh I see... that's a shame but that means you know if it has been taken over or not, just by sending a postMessage to said iFrame, if you get a reply all is good, if not it has been taken over because of an error...
563e332c61a8013065267cc7	X	@JibiAbraham I updated the script to trace if the iframe has been taken over by S3
563e332c61a8013065267cc8	X	Sigh, we're back to this.
  Negative
I can easily enough use CORS on any decent enough browser to directly upload files to my AWS S3 bucket.
  Negative
But (it was coming), with IE I have to fall back to Iframes.
  Negative
Easy, set up a hidden Iframe, create a form, set its target to Iframe name/id, submit form.
  Positive
If the upload is successful, the Iframe is redirected to a url I specify and I can access the whatever I need to.
  Negative
But if an error occurs, since the Iframe is now on an AWS domain, I won't have access to the XML content of the error.
  Negative
Infact, I won't even know that an error has occurred.
  Negative
I've seen brave people on the internet talking about hosting an html file, on the same bucket to which files are to be uploaded, and then using postMessages to route the Iframe content, or something of that sort.
  Positive
Could someone please explain to me how to achieve this mythical solution?
  Negative
The jQuery file uploader by Blueimp seems to solve this, but by God the code is so jQueryified that I haven't been able to get the gist of it.
  Negative
563e332c61a8013065267cc9	X	Almost everything you need to know about how the jQuery File Upload plugin does iframe uploads is in its Iframe Transport plugin (along with supporting result.html page).
  Negative
As an introduction, you may want to read their user instructions on their Cross domain uploads wiki page, specifically the Cross-site iframe transport uploads section.
  Negative
(Note that according to their Browser support page, niceties like upload progress are not supported for IE <10, so I wouldn't consider these possible using the iframe transport, at least without significant effort.)
  Negative
(Also, I don't believe any S3 upload implementation using the File Upload plugin has access to the XML content of a file upload error) The Iframe Transport plugin adds a new Ajax "transport" method for jQuery and is not specific to the File Upload plugin.
  Negative
You may want to read the documentation for jQuery.ajaxTransport() to understand the API that jQuery provides for adding a new transport.
  Negative
I'll try to summarize what the Iframe Transport plugin is doing, and how it relates to uploading files to Amazon S3: When a file upload is triggered, the send() function is called.
  Negative
This function: Creates a hidden form element Creates an iframe element with src="javascript:false;", and binds a load event handler to the iframe Appends the iframe to the hidden form, and appends the hidden form to the document.
  Negative
When the iframe is created and its "page" loaded, its load event handler is called.
  Negative
The handler: Clears itself from the iframe, and binds another load event handler Configures the hidden form: The form's action will be the URL for the S3 bucket The form's target is set to the iframe, so that the server response is loaded in the iframe Other fields, e.g. AWSAccessKeyId, are added.
  Very negative
Specifically, success_action_redirect is set to the URL of result.html on your server, e.g. http://example.org/result.html?%s.
  Negative
Normally, the %s token should be replaced with the upload results by server-side code, but with S3 this can be hard-coded with a success value by your code, since Amazon will redirect to this URL only if the upload succeeded.
  Positive
File input fields from the original form are moved into the hidden form, with cloned fields left in the original fields' place Submits the hidden form Moves the file input fields back into the original form, replacing the cloned fields The file(s) are uploaded to S3.
  Negative
If successful, Amazon redirects the iframe to the success_action_redirect URL.
  Negative
If not successful, Amazon returns an error, which is also loaded in the iframe.
  Negative
The iframe's load event handler is called.
  Negative
The handler: Tries to save a reference to the iframe's document object.
  Negative
If the file upload failed, the handler saves an undefined instead.
  Negative
Calls the complete callback with a success code and a reference to the iframe's document object (or undefined) Removes the hidden form (and iframe) Before control is returned to your code, the iframe's document object is passed to a converter (at the bottom of the Iframe Transport plugin), depending on what type of data you were expecting.
  Negative
The converter extracts that data from the document object and returns it (or undefined if the file upload failed) to your callback(s).
  Negative
Your callback(s) (success and/or complete as passed to jQuery.ajax()) is called.
  Negative
A success code is always returned by the plugin, and so any error callback will not be triggered.
  Positive
If the data passed to your callback(s) is the value you included in the success_action_redirect, then the file upload succeeded.
  Positive
If the data is undefined, then the file upload failed.
  Negative
Update: If the error XML page stays on the same origin as the S3 bucket, then another page from the S3 bucket, loaded into another iframe, can access the original iframe's content (because they are from the same origin).
  Negative
Your main page can communicate with this second iframe using postMessage() (or easyXDM's FlashTransport, if you need to support IE6/7).
  Negative
563e332c61a8013065267cca	X	This problem, of providing accurate feedback to users using browsers with no FileReader or FormData support has troubled me a lot as wel.
  Negative
I spent a whole 3 days trying to come up with a solution and finally came up with something close to nothing.
  Negative
Lets get down to the facts: Ok, then there is no other way of uploading the file than using an iframe.
  Negative
Right?
  Neutral
So, jQuery File Upload using jQuery Iframe Transport as @jeferry_to describes so well is the tool for the job.
  Negative
*Actually the tool/plugin doesn't change a thing.
  Negative
.
  Neutral
What now?
  Neutral
Well... we need to access the S3 response inside the transport iframe.
  Negative
But we can't because its on a different domain.
  Negative
So we decide to deal with it by using this trick involving a second iframe.
  Negative
The setup: The scenario: First of all we need to modify jQuery Iframe Transport so that it does not auto remove the auto-generated form and transport frame.
  Neutral
We need to do this cause #postMessage which will use later is asynchronous by nature and we don't want the iframe gone by the time we try to access it.
  Negative
Ok, everything should work now cause everything is done by the book.
  Negative
Nahh, you should not even bother.
  Negative
You see... if you force a modern browser to use the iframe transport instead of the XHR2 the above solution will indeed work like a charm.
  Positive
However that's pointless.
  Neutral
We want it to work in IE8 + 9.
  Negative
Well... in IE8/9 it sometimes work, it sometimes doesn't.
  Negative
Usually it doesn't.
  Neutral
Why?
  Negative
Because of the IE's friendly HTTP error messages.
  Negative
Oh yes you read just fine.
  Positive
In case of an error, S3 responds with an HTTP error status depending on the error (400, 403 etc).
  Very negative
Now, depending on the status and the length of the response as shown here, IE discards the S3 response and replaces it with a friendly error message.
  Negative
In order to overcome this, you must make sure the response is always > 512 bytes.
  Negative
In this case you cannot guarrantee anything like that cause you don't control the response.
  Negative
S3 does and the typical errors are less than 512 bytes.
  Neutral
In short: The iframe trick works on those browsers that do not need it, and doesn't on those who do.
  Negative
Unfortunately, I can't think of anything else so that case is closed for me now.
  Negative
563e332c61a8013065267ccb	X	Summarizing my answer in the comments: IE has CORS support with some restrictions: http://www.html5rocks.com/en/tutorials/cors/ and this implementation of direct upload to S3 looks much simpler than jquery fileupload and its not in jquery: http://codeartists.com/post/36892733572/how-to-directly-upload-files-to-amazon-s3-from-your Hope this helps!
  Negative
563e332c61a8013065267ccc	X	AS for the "postMessage" scenario, maybe the iframe should contain a simple javascript [edit] for iframes taken over by an errormessage IFRAME script Now parent knows the iFrame perfectly well and can track it's status (depending on if it's answering a simple postMessage) PARENT script IF iframe is not responding with it's "code" iFrameTakenOver will be permanently set to false checking that will verify if an error has occured or not.
  Negative
563e332c61a8013065267ccd	X	Thanks a lot for your thoughts.
  Positive
Even since it's possible to perform anonymous requests to S3 if the bucket is publicly available, your assumption seems true, that multipart uploads are just not made for my case.
  Negative
And that pre-signed URLs are not available for multipart-uploads is too bad.
  Negative
However, I decided to use the AWS IAM mechanism to set a writeonly-policy for the bucket, and to store the credentials for a new, accordingly configurated user in the applet.
  Neutral
From security perspective, it should be fine.
  Positive
563e332c61a8013065267cce	X	@schneck: Facilitating an IAM write-only policy is an excellent alternative indeed, I've focused too much on how you are trying to achieve your goal rather than the actual use case - you could take that even further by Making Requests Using IAM User Temporary Credentials to entirely avoid storing permanent credentials within your applet.
  Negative
563e332c61a8013065267ccf	X	Isn't the danger with a write-only, public bucket is that someone could spam your bucket using nothing more than curl?
  Negative
Or am I missing something?
  Neutral
563e332c61a8013065267cd0	X	I'm trying to upload a file with the Amazon Java SDK, via multipart upload.
  Negative
The idea is to pass an upload-id to an applet, which puts the file parts into a readonly-bucket.
  Negative
Going this way, I avoid to store AWS credentials in the applet.
  Negative
In my tests, I generate an upload-id with boto (python) and store a file into the bucket.
  Negative
That works well.
  Positive
My Applet gets a "403 Access denied" from the S3, and I have no idea why.
  Negative
Here's my code (which is partially taken from http://docs.amazonwebservices.com/AmazonS3/latest/dev/llJavaUploadFile.html): In the applet debug log, I find this, then: Do you find any obvious failures in the code?
  Negative
Thanks, Stefan
563e332c61a8013065267cd1	X	While your use case is sound and this is an obvious attempt indeed, I don't think the Multipart Upload API has been designed to allow this and you are actually violating a security barrier: The upload ID is merely an identifier to assist the Multipart Upload API in assembling the parts together (i.e. more like a temporary object key) not a dedicated security mechanism (see below).
  Negative
Consequently you still require proper access credentials in place, but since you are calling AmazonS3Client(), which Constructs a new Amazon S3 client that will make anonymous requests to Amazon S3, your request yields a 403 Access denied accordingly.
  Negative
What you are trying to achieve is possible via Uploading Objects Using Pre-Signed URLs, albeit only without the multipart functionality, unfortunately: A pre-signed URL gives you access to the object identified in the URL, provided that the creator of the pre-signed URL has permissions to access that object.
  Negative
That is, if you receive a pre-signed URL to upload an object, you can upload the object only if the creator of the pre-signed URL has the necessary permissions to upload that object.
  Negative
[...] The pre-signed URLs are useful if you want your user/customer to be able upload a specific object [...], but you don't require them to have AWS security credentials or permissions.
  Negative
When you create a pre-signed URL, you must provide your security credentials, specify a bucket name an object key, an HTTP method (PUT of uploading objects) and an expiration date and time.
  Negative
[...] The lenghty quote illustrates, why a system like this likely needs a more complex security design than 'just' handing out an upload ID (as similar as both might appear at first sight).
  Negative
Obviously one would like to be able to use both features together, but this doesn't appear to be available yet.
  Negative
563e332d61a8013065267cd2	X	Interesting info, but not sure about this, as the access to the files are more file based than credential based.
  Negative
Every download/upload has to be approved by our server permissions policies.
  Negative
I guess I could configure permissions on every resource too, is that the way to go?
  Negative
I will have a look at this approach.
  Positive
Also, the python clients can be several thousands, can this be an issue?
  Negative
563e332d61a8013065267cd3	X	The IAM Limits documentation don't seem to have any limits to these types of authentication.
  Negative
563e332d61a8013065267cd4	X	I am exploring this approach, so far so good, I think it might be a better approach than pre-signed URLs, but I still have to figure out a valid permissions approach per-resource and user policies.
  Negative
563e332d61a8013065267cd5	X	I was aware about this pre-signed URLs, what I dont really fully understand is why I cannot use boto in the client side.
  Very negative
With that URL I can use python-requests to handle it, but I have to implement file management/streaming, error checks, and some other things that are already implemented in boto.
  Very negative
563e332d61a8013065267cd6	X	I want to directly upload/download files to Amazon S3 from python clients, running in some users machines.
  Negative
I have a server, that hosts the access Id and Secret keys, as they cannot be in the users side, that can be used to generate a pre-signed url, and that the clients can connect via API to request these pre-signed urls.
  Negative
I have found many examples of JS, but not a single one with python also in the client side (not web based).
  Negative
I have tried to use boto on the client side, but there seems there is no simple way to take advantage of the boto API, but sign the requests with the remote signature.
  Negative
Is there a way I can use boto to handle the transfers from the client side?
  Negative
So far it seems the best way is to build my own client with python-requests, but I think it pretty much sounds to reinvent the wheel.
  Positive
So far I have been able to monkey-patch HmacKeys (from boto.auth, in boto2), so the provider.secret_key is no longer required (and doesn't raise NotReadyToAuthenticate()) and I can override the signing, injecting an API call for remote signing.
  Very negative
But this seems very tricky, fragile and difficult to maintain.
  Negative
Is there any other way with boto to achieve this?
  Neutral
563e332d61a8013065267cd7	X	Rather than signing URLs (which is typically used when making calls via web browser), you should generate temporary credentials via the AWS Security Token Service (STS).
  Very negative
From your server, issue the GetFederationToken API call to generate temporary credentials: Your Python app would then use these credentials when calling boto.
  Negative
The user will only be allowed to make APIs that you have permitted within your policy, for the time-frame specified.
  Negative
563e332d61a8013065267cd8	X	Boto3 gives the ability to create a pre-signed URL for any method call: From the Boto3 documentation: generate_presigned_url(ClientMethod, Params=None, ExpiresIn=3600, HttpMethod=None) Generate a presigned url given a client, its method, and arguments Parameters: Returns: The presigned url I just used it to sign a list_buckets() call and it returned a big URL, eg: Pasting it into a browser returned the bucket list in XML.
  Negative
563e332d61a8013065267cd9	X	we've been using SimpleSavant as ORM for SimpleDB and it works really well but so far I haven't seen a project of that quality for S3 or other services like SQS, will go through the list and check them out, thanks
563e332e61a8013065267cda	X	Please avoid adding signatures to your posts, as per the FAQ - stackoverflow.com/faq#signatures
563e332e61a8013065267cdb	X	Does anyone know of a high-level SDK for interacting with AWS?
  Negative
The SDK provided by Amazon is good and the REST/SOAP API well documented but I often find that I still end up having to write common, high level operations myself.
  Positive
Take for instance, the S3 client, it gives you the ability to put/get/list objects, etc. but it's sadly missing the ability to do high level operations such as create folder, move file to a different folder, etc.
  Negative
You could use tools like Cloud Berry or the Amazon web console to do this sort of things manually, but sometimes you will want to build some automation into your app like periodically backing up some data into a backup folder with time stamp.
  Negative
Cheers, UPDATE: sorry if I left the question a little too open, seeing as AWS covers so many different things, but in particular I'm looking for a high-level library for S3.
  Negative
563e332e61a8013065267cdc	X	There are some open source projects on CodePlex.
  Negative
The entire list is at http://www.codeplex.com/site/search?query=AWS&ac=8
563e332e61a8013065267cdd	X	The AWS SDK for .
  Negative
NET provides some high-level interfaces for S3.
  Neutral
The Amazon.S3.IO namespace contains FileInfo and DirectoryInfo abstractions and the Amazon.S3.Transfer utilities allow for simple upload and download, including for large files.
  Negative
563e332e61a8013065267cde	X	Depending on what exactly you need you can find CloudBlackbox package of our SecureBlackbox product useful.
  Negative
CloudBlackbox provides an almost-uniform API for accessing different cloud storages, and offers built-in encryption mechanisms.
  Negative
CloudBlackbox offers high-level API for .
  Positive
NET.
  Neutral
563e332e61a8013065267cdf	X	A newer version of AWS sdk implements S3FileInfo and S3DirectoryInfo.
  Negative
You can use it like .
  Neutral
net FileInfo and DirectoryInfo.
  Positive
The problem is that it does not support large files.
  Negative
563e332e61a8013065267ce0	X	I went to the repo, looked around, didn't see anything pertaining to the REST API for Elastic-Beanstalk.
  Very negative
Sorry Obi-Wan, your mind has been clouded...lol
563e332e61a8013065267ce1	X	Oh, now I understood (silly: why you just don't use the AWS SDK anyway?)
  Negative
563e332e61a8013065267ce2	X	Last time I checked the docs, ElasticBeanstalk only had a REST API...
563e332f61a8013065267ce3	X	No, the AWS SDK supports that since 2011.
  Negative
I know because thats what we've already had when I decided to write my Maven Plugin for it (beanstalker.ingenieux.com.br) ;)
563e332f61a8013065267ce4	X	I've just started working with the AWS Java SDK and need to deploy/update an elastic beanstalk application from my application.
  Negative
Currently, I have only found documentation for a REST api that allows the creation of an application version.
  Negative
As with the rest of Amazon's REST api, authentication is needed with a menagerie of params.
  Negative
The docs on ELB don't have specifics on authentication, whereas the docs for S3/EC2 have plenty of explanation.
  Negative
I am specifically asking about the "Signature Version" parameter?
  Neutral
Does anybody have an idea of what that would be for the ELB REST api?
  Neutral
Could anybody that has successfully worked with the ELB API point in the right direction for authentication?
  Positive
Thanks in advance!
  Neutral
563e332f61a8013065267ce5	X	Use the source, luke.
  Positive
In particular, those versions are bound to a specific version (see this announcement for an idea) All the AWS Services keep a common core.
  Positive
In particular, besides the Beanstalk-API specific part, you need to know the common part of the AWS apis, which are described in this document)
563e332f61a8013065267ce6	X	Thanks for answering sir, Actually I am new to AWS SDK, I need to get book details from its ISBN, to be more simple, I want that when I give ISBN number to service, it should give me detail of book (if present on amazon server).
  Very negative
I am not getting the point that which will I have to use either S3, EC2 or DynamoDB.
  Negative
Can you please help me....?
  Positive
563e332f61a8013065267ce7	X	@azeem S3 is a storage service you can use to store files, EC2 a virtual server environment that could be used for any number of purposes, and DynamoDB is a NoSQL database.
  Negative
If you are interested in learning more about these services, I recommend you read up on their appropriate pages on aws.amazon.com.
  Positive
You can also have a look at the samples included with the AWS SDK for iOS to see examples of how a number of the services could be used in a mobile application.
  Negative
563e332f61a8013065267ce8	X	@azeem: More specifically, Amazon Web Services is the name of Amazon's infrastructure services.
  Negative
The web service used to look up information about books is called the Product Advertising API and falls under Amazon Associates.
  Negative
As such, since the Advertising API is not part of AWS, the SDK does not have support for it.
  Negative
563e332f61a8013065267ce9	X	@RyanParman :: Does your answer means that using AWS iOS SDK, I cann't get book information from Amazon server if I have ISBN number of that book?
  Negative
563e332f61a8013065267cea	X	@BobKinney : thanks for answering and clearing my concepts about AWS.
  Negative
Hope you'll not mind if I ask something more.
  Neutral
I am usedto with google api of book family.
  Negative
As it uses only a simple url address and its response is the complete book detail.
  Negative
Is Amazon not offering any url/web service likewise google?
  Negative
563e332f61a8013065267ceb	X	I am new to AWS iOS SDK.
  Negative
I have installed AWS iOS SDK and also look the sample project given by AWS iOS SDK.
  Negative
But unfortunately I didn't get anything suitable for me.
  Negative
My simple task is to search books with their ISBN number on Amazon server.
  Negative
Can I have sample Xcode project that is requesting "itemlookup" from AWS?
  Negative
OR Can I have sample Xcode project that is making request to get item from AWS?
  Neutral
I have spent more then 8 hours on this but didn't find anything.
  Negative
Now atlast I come to SO.
  Positive
I hope that I'll get suitable help from here.
  Positive
Thank you in anticipation.
  Positive
563e332f61a8013065267cec	X	The AWS SDK for iOS is for accessing Amazon Web Services like S3, EC2, and DynamoDB.
  Negative
What you are looking for is the Advertising API.
  Negative
There are a number of questions on SO that should give you some starting ideas.
  Positive
563e332f61a8013065267ced	X	After a great discussion and help of Bob Kinney and Ryan Parman.
  Positive
I went to right path and found this on SO.
  Positive
In this question the last answer has a link of Sorceforge sample code of 'amazon advertising api' Sample Code Here Although this code has some errors, which I solved by googling them, but at last I got the desired results.
  Negative
Hope this will help you greatly...
563e332f61a8013065267cee	X	Just trying to understand this because I am trying to do something like this.../user/update?email=new@example.com&userid=123&sig=some_generated_string now couldn't someone hijack some_generated_string and send that as if it were from themself?
  Negative
(I'm sure they can't but maybe you can tell me why?)
  Negative
563e333061a8013065267cef	X	@jasondavis - yes, someone could sniff that particular request, it is true.
  Negative
But because the signature is going to be different for every single request, sniffing them does no good.
  Negative
At best, all they can do is send the exact same request!
  Neutral
They can't generate a malicious request, because to do that, you need the secret API key.
  Negative
And if they try to muck with the data in the request they sniffed, say changing the email to evil@example.com, the signature will no longer match, and the REST server will reject the request when it attempts to validate it.
  Negative
563e333061a8013065267cf0	X	Isn't this method insecure if the user is making the request from javascript and thus exposing their API key to the whole world?
  Negative
Someone could view source, get the API key and use the same methods to generate a valid request to make call to the server.
  Negative
Or am I missing something.
  Neutral
563e333061a8013065267cf1	X	@zombat (I don't think so, but anyway.
  Negative
.)
  Neutral
Is there a method to do that when user fills html form?
  Neutral
when I say 'user' I mean a registered user, not a developer who sends his request via his locally PHP file where the secret api key is stored.
  Negative
If there isn't, what should I do?
  Negative
563e333061a8013065267cf2	X	@zombat that is a very good question baris-usakli has asked.
  Positive
How to hide the API key in a javascript web app?
  Neutral
For the phone app market that is using the same API, even there how to create a private API for each downloaded phone app and let the API know about it?
  Negative
563e333061a8013065267cf3	X	This method is simple, but inherently insecure.
  Negative
Someone could easily sniff your API Key and start using it for themselves.
  Negative
If you want your API keys to be private, you don't want to be sending them as part of the request.
  Negative
563e333061a8013065267cf4	X	Works for Campaign Monitor: campaignmonitor.com/api/getting-started/#authentication
563e333061a8013065267cf5	X	Campaign Monitor will have a lot of explaining to do the first time one of their customers gets their API key hijacked because they were working on an open wireless connection at Starbucks.
  Negative
563e333061a8013065267cf6	X	Lots of services authenticate in that manner.
  Negative
Campaign Monitor, Freshbooks; even Facebook uses access tokens (although a token can be limited in what it can access).
  Negative
It's a pretty common way to authenticate an API.
  Negative
563e333061a8013065267cf7	X	Access tokens are a common method, but API Keys are not.
  Negative
Access tokens expire, so it's not as big a security hole if one gets leaked.
  Negative
But if I have your API Key, I can generate as many access tokens as I want.
  Negative
There's a good reason that all these services give you a "secret" key... it's supposed to be a secret.
  Positive
Any service that is having you pass your secret key is doing it wrong.
  Negative
563e333061a8013065267cf8	X	Does that not then go against REST conventions?
  Negative
563e333061a8013065267cf9	X	@MartinBean what do suggest?
  Negative
, I dont want to use the REST API to login users, i only want them to have a api key to grant them access to the API when they are logged in
563e333061a8013065267cfa	X	I think Martin Beans comment is only word poking instead of adding anything.
  Negative
His suggested answer looks even worser than mine, although I have not outlined what a session is, zombat describes this way better, check zombat's answer, it has many valid points: stackoverflow.com/a/8567909/367456
563e333061a8013065267cfb	X	It has valid points because RESTful APIs assume the use of a secure server.
  Very negative
563e333161a8013065267cfc	X	Am working with phil sturgeon REST_Controller for codeigniter to create a REST api, so far i've been able to create a simple library for generating api keys for the users.
  Negative
My problem is now sending the api key to the API for each request, how i do this without having to manually send it for every request.
  Negative
563e333161a8013065267cfd	X	You should look into request signing.
  Negative
A great example is Amazon's S3 REST API.
  Positive
The overview is actually pretty straightforward.
  Positive
The user has two important pieces of information to use your API, a public user id and a private API Key.
  Negative
They send the public id with the request, and use the private key to sign the request.
  Negative
The receiving server looks up the user's key and decides if the signed request is valid.
  Neutral
The flow is something like this: This methodology ensures the API key is never sent as part of the communication.
  Positive
Take a look at PHP's hash_hmac() function, it's popular for sending signed requests.
  Positive
Generally you get the user to do something like put all the parameters into an array, sort alphabetically, concatenate into a string and then hash_hmac that string to get the sig.
  Negative
In this example you might do: Then add that $sig onto the REST url as mentioned above.
  Neutral
563e333161a8013065267cfe	X	The idea of REST is that it's stateless—so no sessions or anything.
  Negative
If you want to authenticate, then this is where keys come in, and keys must be passed for every request, and each request authenticates the user (as REST is stateless, did I mention that?)
  Negative
.
  Neutral
There are various ways you can pass a key.
  Positive
You could pass it as a parameter (i.e. http://example.com/api/resource/id?key=api_key) or you can pass it as part of the HTTP headers.
  Negative
I've seen APIs that specify you send your username, and an API key as the password portion of the HTTP basic access authorization header.
  Negative
An example request: Where martinbean would be my account username on your website, and 4eefab4111b2a would be my API key.
  Negative
563e333161a8013065267cff	X	You need a session.
  Negative
First the user authenticates via REST.
  Neutral
For the rest of the session, the user is then authenticated.
  Negative
563e333161a8013065267d00	X	Have any of these answers been helpful?
  Neutral
563e333161a8013065267d01	X	@Kylar: Yes, several.
  Negative
Difficult to pick the right one to accept.
  Positive
563e333161a8013065267d02	X	"An origin server MUST NOT send a validator header field (Section 7.2), such as an ETag or Last-Modified field, in a successful response to PUT unless the request's representation data was saved without any transformation applied to the body (i.e., the resource's new representation data is identical to the representation data received in the PUT request) and the validator field value reflects the new representation."
  Negative
In other words, return an ETag only if the PUT entity exactly matches that which would be returned from a subsequent GET?
  Negative
563e333161a8013065267d03	X	@dkarp that's correct.
  Neutral
563e333161a8013065267d04	X	I have found neither suggestion nor prohibition to return the representation of a resource in the response to PUT request.
  Negative
Otherwise, the answer is fine.
  Positive
563e333261a8013065267d05	X	Disagree about 201.
  Negative
Assuming every item has a default ACL, then the PUT correctly returns 200.
  Negative
563e333261a8013065267d06	X	Both of those are quite wrong.
  Negative
202 is for continuing processing.
  Neutral
205 is to tell the UI to clear a form.
  Positive
563e333261a8013065267d07	X	Well, you're correct, maybe I interpret them wrong.
  Negative
However, I continue to see no point in introducing separate logic and differentiate this situation from the normal case when the user removes another user.
  Neutral
This can be handled on UI level if it is so important with appropriate warning message.
  Positive
563e333261a8013065267d08	X	My RESTful service includes a resource representing an item ACL.
  Negative
To update this ACL, a client does a PUT request with the new ACL as its entity.
  Negative
On success, the PUT response entity contains the sanitized, canonical version of the new ACL.
  Negative
In most cases, the HTTP response status code is fairly obvious.
  Negative
200 on success, 403 if the user isn't permitted to edit the ACL, 400 if the new ACL is malformed, 404 if they try to set an ACL on a nonexistent item, 412 if the If-Match header doesn't match, and the like.
  Negative
There is one case, however, where the correct HTTP status code isn't obvious.
  Negative
What if the authenticated user uses PUT to remove themselves from the ACL?
  Neutral
We need to indicate that the request has succeeded but that they no longer have access to the resource.
  Negative
I've considered returning 200 with the new ACL in the PUT entity, but this lacks any indication that they no longer have the ability to GET the resource.
  Negative
I've considered directly returning 403, but this doesn't indicate that the PUT was successful.
  Negative
I've considered returning 303 with the Location pointing back to the same resource (where a subsequent GET will give a 403), but this seems like a misuse of 303 given that the resource hasn't moved.
  Very negative
So what's the right REST HTTP status code for "success, and thus you no longer have access"?
  Neutral
563e333261a8013065267d09	X	200 is the appropriate response, because it indicates success (as any 2xx code implies).
  Positive
You may distinguish the user's lack of permission in the response (or, if you don't wish to, 204 is fine).
  Negative
Status codes make no contract that future requests will return the same code: a 200 response to the PUT does not mean a subsequent GET can't return 403.
  Negative
In general, servers should never try to tell clients what will happen if they issue a particular request.
  Negative
HTTP clients should almost always leap before they look and be prepared to handle almost any response code.
  Negative
You should read the updated description of the PUT method in httpbis; it discusses not only the use of 200/204 but indicates on a careful reading that returning a transformed representation in immediate response to the PUT is not appropriate; instead, use an ETag or Last-Modified header to indicate whether the entity the client sent was transformed or not.
  Very negative
If it was, the client should issue a subsequent GET rather than expecting the new representation to be sent in response to the PUT, if for no other reason than to update any caches along the way (because the response to a PUT is not cacheable).
  Negative
Section 6.3.1 agrees: the response to a PUT should represent the status of the action, not the resource itself.
  Negative
Note also that, for a new ACL, you MUST return 201, not 200.
  Negative
563e333261a8013065267d0a	X	You're confusing two semantic ideas, and trying to combine them into a single response code.
  Negative
The first: That you successfully created an ACL at the location that you were attempting to.
  Positive
The correct semantic response (in either a RESTful or non-RESTful scenario) is a 201 Created.
  Negative
From the RFC: "The request has been fulfilled and resulted in a new resource being created."
  Negative
The second: That the user who executed the PUT does not have access to this resource any more.
  Negative
This is a transient idea - what if the ACL is updated, or something changes before the next request?
  Neutral
The idea that a user does not have access to a resource of any kind (and this includes an ACL resource) only matters for the scope of that request.
  Negative
Before the next request is executed, something could change.
  Negative
On a single request where a user does not have access to something you should return a 403 Forbidden.
  Negative
Your PUT method should return a 201.
  Negative
If the client is worried about whether it has access any more, it should make a subsequent request to determine it's status.
  Negative
563e333261a8013065267d0b	X	You might want to take a look at HTTP response code "204 No Content" (http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html), indicating that the "server has fulfilled the request [to be removed from the ACL] but does not need to return an entity-body, and might want to return updated metainformation" (here, as a result of the successful removal).
  Negative
Although you're not allowed to return a message body with 204, you can return entity headers indicating changes to the user's access to the resource.
  Negative
I got the idea from Amazon S3 - they return a 204 on a successful DELETE request (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html), which seems to resemble your situation since by removing yourself from an ACL, you've blocked access to that resource in the future.
  Negative
563e333261a8013065267d0c	X	Very interesting question :-) This is why I love REST, sometimes it might get you crazy.
  Negative
Reading w3 http status code definitions I would choose (this of course is just my humble opinion) one of those: On the other hand (just popped-up in my mind), why should you introduce a separate logic and differentiate that case and not using 200 ?
  Negative
Is this rest going to be used from some client application that has an UI?
  Negative
And the user of the rest should show a pop-up to the end-user "Are you sure you want to remove yourself from the ACL?"
  Negative
well here the case can be handled if your rest returns 200 and just show a pop-up "Are you sure you want to remove user with name from the ACL?"
  Neutral
, no need to differentiate the two cases.
  Negative
If this rest will be used for some service-to-service communication(i.e. invoked only from another program) again why should you differentiate the cases here the program wouldn't care which user will be removed from the ACL.
  Negative
Hope that helps.
  Neutral
563e333261a8013065267d0d	X	Best recommendation here.
  Positive
I too second that you employ a module for your httpd.
  Negative
Much easier than coding a full blown abstraction in your app.
  Neutral
563e333261a8013065267d0e	X	I'm having problems when I perform a seek operation.
  Negative
Any clue on how I could implement a seek using this method?
  Negative
563e333261a8013065267d0f	X	No, it's not.
  Negative
The X-Sendfile header is removed, the file appears to be located at the requested location.
  Negative
Furthermore, x-sendfile can read files that are not normally web-accesible
563e333261a8013065267d10	X	The second solution will work and is secure.
  Negative
It is however not a very good idea to pass large files (video) trough php.
  Negative
563e333261a8013065267d11	X	What is the best way to password protect quicktime streaming videos using php/.
  Positive
htaccess.
  Neutral
They are being streamed using rtsp, but I can use other formats if necessary.
  Neutral
I know how to do authentication with php, but I'm not sure how to setup authentication so that will protect the streaming files urls so that a user can't just copy the url and share it.
  Negative
Or am I overthinking this and I can just use a normal authentication scheme and place the files in a protected directory?
  Neutral
563e333261a8013065267d12	X	Both nginx and lighttpd web servers have X-Send-File headers you can return from PHP.
  Negative
So you can do your checks in PHP and then conditionally server out the file.
  Negative
Lighttpd also has a neat module called mod_secure_download that allows you to programatically generate a URL that will only be valid for a short time period.
  Positive
Nginx, and possibly lighttpd, allow you to cap the download speed, so you're not sending out streaming data faster than it can be consumed.
  Negative
Either way, you want to use your web server for serving files.
  Negative
Serving them through PHP is possible, but slow.
  Negative
563e333261a8013065267d13	X	Try to use Amazon S3 service, it got it's quirks but it makes sense once you get familiar with it.
  Positive
There are hooks in their API to achieve temporally URL's that are active for specified time, so you can freely show url to visitor because it won't work 10 minutes or so later.
  Negative
It's almost trivial thing to do with php (around 15 lines of code), there are a lot of examples on their forums so you dont need to go from scratch and read full documentation on how to achieve this.
  Negative
What kind of authorization you will do before generate and show links it's up to you.
  Positive
You can also have it look like it's served from your domain like video.yourdomain.com instead of standard s3 URL's.
  Negative
Last thing, it's cheap - we payed around 2 US$ for the month of testing and deployment when I uploaded 8 GB and downloaded it 3 times completely and initialized download for around 100 times.
  Very negative
The person I was doing this for is so satisfied by price that he wants to move all of his downloadable media to s3.
  Positive
Now, re reading everything I wrote it looks like commercial/spam but I'm so satisfied with service because I coded everything for audio files earlier, and it took days until everything worked just fine and this took couple of hours to implement (mostly getting familiar with service).
  Negative
563e333361a8013065267d14	X	You might want to take a look at: mod_xsendfile (for apache) It enables you to internally redirect to a file.
  Negative
So you could point your download link to checkCredentials.php This module is also available for other webservers.
  Negative
If I remember correctly, the idea originally comes from lighttpd, but - as Josh states- is also available for nginx.
  Negative
563e333361a8013065267d15	X	In Jacco's solution is it possible to inspect the headers, and find the url for the file and download it without authentication?
  Negative
A possible solution I can think of would be to put the files somplace that is inaccessible to anyone but the browser, i.e. using a .
  Negative
htaccess deny all.
  Neutral
Would that work?
  Neutral
563e333361a8013065267d16	X	Try to get and display the body of the 400 Bad Request response (sorry, not a C# person so I can't tell you specifically how).
  Negative
The 400 response has multiple possible meanings, 2 of which are related to the signature itself, and the others indicate other problems.
  Negative
There should be a body on that response, that gives more information.
  Positive
563e333361a8013065267d17	X	Thanks for the reply.
  Neutral
I have solved it finally by myself.
  Positive
(edited my post and provided the answer)
563e333361a8013065267d18	X	I was actually writing this for windows phone 7 when I got stuck above, which didn't have any SDK till now.
  Negative
But the GA release link seems encouraging as it says 'support for windows phone 8', let me see if this makes my life easier.
  Positive
Nevertheless, thanks for the quick reply and pointers.
  Positive
I am still keeping this question open to see if the above can be fixed somehow
563e333361a8013065267d19	X	I tried with looking at the SDK but it was little too much for me to tweak and/or adopt it.
  Negative
Got my problem solved myself though.
  Negative
Thanks for the help nevertheless.
  Positive
563e333361a8013065267d1a	X	I am trying follow the example given at AWS documentation Signing AWS requests and make a ListUsers call in C#.
  Negative
I have arrived till the last stage of generating the signature (i.e ready to submit the signed request given at signature-4 request examples).
  Neutral
But the code I pasted below is throwing 'bad request' exception when submitted.
  Negative
Output i get is: Can some one help me what wrong I am doing here?
  Negative
Edit: I solved this finally by myself.
  Positive
I had to transform it to below.
  Negative
(answer is for RDS though I believe it's visible what the differences are).
  Neutral
563e333361a8013065267d1b	X	Can some one help me what wrong I am doing here?
  Negative
Maybe, but I strongly suggest to skip this endeavor all together and just use the excellent AWS SDK for .
  Negative
NET for all your AWS API interactions instead, because it indeed helps take the complexity out of coding by providing .
  Negative
NET APIs for many AWS services including Amazon S3, Amazon EC2, DynamoDB and more.
  Negative
But if you really need or want to do it yourself, I suggest to simply take a look at the source code of these very SDKs, which are all available at GitHub, including the one for the AWS SDK for .
  Negative
NET - regarding the issue at hand, you might want to start looking into AWS4Signer.cs for example.
  Neutral
563e333361a8013065267d1c	X	Does ContentProvider pattern lack any required feature ?
  Negative
563e333361a8013065267d1d	X	I want to develop a application where part of the data is dynamic like picture , show timing etc.Their are many content management system that use HTML5 and CSS but i want to also use the native iOS or Android Ui like the UISplitView for iPad.How is this possible ?
  Negative
whats the best way to manage and use dynamic data ?
  Positive
563e333361a8013065267d1e	X	I have been digging into this very exact answer.
  Negative
The best answer I can come up with is called parse.com.
  Positive
Which may not be 100% of what you are looking for.
  Negative
However.
  Neutral
What it does is serve as a central database that talks to multiple platforms(windows 8, iOS, Android) and offers up an api for use with every platform with lots of documentation to make programming super easy.
  Negative
http://deployd.com/ also This site is something Ill be looking into which uses a simplified node.js desktop for programming easy objective based functions with a database.
  Negative
Definitely am still looking.
  Positive
Either way the bast thing is to call your view...bring in a few objects...and have these databases feed your objects to specifically answer your question.
  Negative
As a developer Im used to Joomla and magento.
  Negative
These arent necessarily ios friendly.
  Negative
Anyway, best of luck.
  Positive
563e333361a8013065267d1f	X	I'd suggest taking a look at Cloud CMS (http://www.cloudcms.com).
  Negative
Cloud CMS is a cloud content management system that is built around JSON schema.
  Negative
Unlike traditional web content systems, Cloud CMS works with JSON and binary files (either through MongoDB GridFS or Amazon S3).
  Negative
It provides full-text search, structured query and an entire suite of enterprise features for things like workflow, analytics, users and groups and more.
  Positive
From an iOS or Android viewpoint, you really only need to interact with the REST API.
  Negative
You can do that directly or use one of the client libraries.
  Neutral
Disclaimer: I'm one of the founders of the company.
  Neutral
Would love to find out what you think and learn what we can do to improve things.
  Neutral
We're having a great time reinventing CMS for mobile.
  Positive
563e333461a8013065267d20	X	Stackoverflow isn't a recommendation nor code-handout site, and you haven't given any requirements as to the actual needs and requirements for a storage system (aside from having to be free).
  Negative
If free, maybe try Dropbox.
  Negative
563e333461a8013065267d21	X	is it possible to access dropbox from java code?????
  Neutral
563e333461a8013065267d22	X	Yes.
  Neutral
They even have an official SDK for Java.
  Negative
563e333461a8013065267d23	X	Am using CloudBees to deploy my Java EE application.
  Negative
In that I need to write and read files and I wont find any cloud file system from CloudBees.
  Negative
Please suggest me any free cloud file system storage and java code to access that file system.
  Negative
563e333461a8013065267d24	X	Using jclouds you can store stuff in several different clouds while using a consistent API.
  Negative
http://www.jclouds.org/
563e333461a8013065267d25	X	You can store files - however they will be ephemeral and not shared in the cluster.
  Negative
To achieve that, you would need to store in a DB or s3 or similar (there is also an option of webdav).
  Negative
563e333461a8013065267d26	X	file system on RUN@Cloud is indeed not persistent neither distributed.
  Negative
File stored there will "disappear" when application is redeployed/restarted and will not be replicated if application scale out on multiple nodes in cluster.
  Negative
Best option afaik is to use a storage service (amazon s3 to benefit from low network latency from your RUN instance) using jclouds neutral API (http://www.jclouds.org/documentation/quickstart/aws/), that can be configured to use filsystem storage (http://www.jclouds.org/documentation/quickstart/filesystem/) so that you can test on you own computer, and cache filestore content in temp directory - as defined by System.getProperty("java.io.temp") - to get best performances.
  Negative
This will require a "warm-up" phase for this cache to get populated from filestore, but you'll then get best of both words.
  Positive
563e333461a8013065267d27	X	Does the IBM DataWorks Data Load API support CSV files as input source?
  Negative
563e333461a8013065267d28	X	The answer is yes.
  Neutral
To accomplish this, you have provide the structure of the file in the request payload.
  Negative
This is explained in the API documentation Creating a Data Load Activity.
  Negative
This an excerpt of the documentation: Within the columns array, specify the columns to provision data from.
  Negative
If Analytics for Hadoop, Amazon S3, or SoftLayer Object Storage is the source, you must specify the columns.
  Negative
If you specify columns, only the columns that you specify are provisioned to the target... The Data Load application included in DataWorks is provided just as an example and assumes the input file has 2 columns, the first being an INTEGER and the second one a VARCHAR.
  Very negative
Note: This question was answered on dW Answers by user emalaga.
  Negative
563e333661a8013065267d29	X	You should do this on the server - it requires user intervention to download it locally, and that just seems hacky and unfriendly.
  Very negative
563e333661a8013065267d2a	X	@Archer thanks for your thoughts.
  Negative
The real scenario is to use Dropbox Chooser so there would be explicit user intervention to "choose" the files from one's own Dropbox.
  Negative
Also, jQueryFileUpload already uses FileReader() to give a preview of files selected by <label for='myComputerFiles'>.
  Negative
Obviously, I don't want to be hacky and unfriendly but it kinda seems to me that handling this client-side is everybody's (Dropbox's/jQueryFileUpload/FileReader()) intention as well as the way of the future.
  Negative
563e333661a8013065267d2b	X	I understand what you're asking, but I honestly believe this is best done on the server.
  Positive
Reason?
  Neutral
You would have to first download the file (which is an indeterminate process), and then upload it to the server.
  Negative
If you pass the URL to the server then it can perform the whole process in 1 action - a download (which is effectively the same as you uploading it).
  Positive
Also, the ability to read local files, which is what FileReader is for, does not mean you should download files to just upload them again.
  Negative
That's bad logic and your users will not appreciate it.
  Very negative
563e333661a8013065267d2c	X	Also, Dropbox Chooser is not meant to be a way to download files.
  Negative
It's meant to be a replacement for downloading file, or uploading them to other servers... ...without having to worry about the complexities of implementing a file browser, authentication, or managing uploads and storage
563e333661a8013065267d2d	X	If there is an API call on S3 that allows you to specify a URL then that would be the most obvious thing to use.
  Negative
If you can't do that then you either need to download the file for the user (onto your server) and then upload the file to S3, or you're back to the original idea of downloading at the client and uploading from there.
  Negative
Either way, the introduction of S3 obviously adds another layer of complication, but I'd initially look at getting a URL from the client and getting that file on my server so I could do anything I wanted after that.
  Negative
563e333661a8013065267d2e	X	I'd like to use jQueryFileUpload to upload a file that is not on my computer but rather is at an external website so all I have is its URL, e.g., https://dl.dropboxusercontent.com/s/wkfr8d04dhgbd86/onarborLogo64.png.
  Negative
I'm at a total loss on how to do this but I think it involves adding the file data programmatically rather than using the traditional <label for='myComputerFiles'>-based selection of files.
  Negative
If this is correct, what next FileReader()?
  Negative
Any thoughts would be appreciated.
  Negative
563e333761a8013065267d2f	X	You should do this on the server - it requires user intervention to download it locally, and that just seems hacky and unfriendly.
  Very negative
Reason?
  Neutral
You would have to first download the file (which is an indeterminate process), and then upload it to the server.
  Negative
If you pass the URL to the server then it can perform the whole process in 1 action - a download (which is effectively the same as you uploading it).
  Positive
Also, the ability to read local files, which is what FileReader is for, does not mean you should download files to just upload them again.
  Negative
That's bad logic and your users will not appreciate it.
  Very negative
Also, Dropbox Chooser is not meant to be a way to download files.
  Negative
It's meant to be a replacement for downloading file, or uploading them to other servers... ...without having to worry about the complexities of implementing a file browser, authentication, or managing uploads and storage.
  Negative
Since you're using S3, if there is an API call on S3 that allows you to specify a URL then that would be the most obvious thing to use.
  Negative
If you can't do that then you either need to download the file for the user (onto your server) and then upload the file to S3, or you're back to the original idea of downloading at the client and uploading from there.
  Negative
Either way, the introduction of S3 obviously adds another layer of complication, but I'd initially look at getting a URL from the client and getting that file on my server so I could do anything I wanted after that.
  Negative
This previous question may be of some help in this area... How to upload files directly to Amazon S3 from a remote server?
  Negative
563e333761a8013065267d30	X	I think you're right about Parse, I have just had some doubts because I can't find any documentation or tutorials related specifically to what I'm doing.
  Negative
Could you possibly point me in the right direction?
  Positive
563e333761a8013065267d31	X	Yeah, definitely.
  Positive
This is images: parse.com/docs/ios_guide#files/iOS This is how to save objects (such as text): parse.com/docs/ios_guide#objects/iOS This is how to retrieve them intelligently (parse.com/docs/ios_guide#queries/iOS) I really recommend just reading through the parse documentation.
  Very negative
It's really well written and you will be able to adapt it to what you are doing.
  Positive
563e333761a8013065267d32	X	Awesome I really appreciate it!
  Very positive
I had looked through the parse documentation before but I couldn't find any of this for some reason.
  Negative
That's exactly what I needed.
  Positive
563e333761a8013065267d33	X	I have a news application that i am in the process of building, and of course news updates a lot, so I have to constantly update my stories, so I need a backend of some sort that will let me update my stories over the air without updating the actual app.
  Negative
I found Parse.com and they have some awesome stuff, but with the way my app is built I don't think I can use them.
  Negative
I will have to update UIImageviews, UItextviews, and the names of Buttons.
  Negative
With Parse I can only seem to find help regarding the PFQueryTableViewController, which I could use this, but that requires completely recoding and some redesigning of my app to fit into that.
  Negative
So unless there's another way, I guess I will suck it up and get to work.
  Negative
So is there a simpler way to do this, or maybe a better service that works more towards what I'm describing?
  Negative
563e333761a8013065267d34	X	What you are asking basically is how to do network communication.
  Positive
It sounds like to me you can do what you want with just Parse.
  Negative
You just store the images and text and then call the information from the parse backend when you are loading.
  Neutral
From there you can update the UITextViews, button names, and UIImageViews however you want dynamically (using the .
  Negative
text, setTitle, and .
  Positive
image properties and methods respectively).
  Positive
You could also use Amazon S3 for image storage... but the API is less well documented for that.
  Negative
This is probably your best bet, unless you really want to delve in more deeply and learn how to use NSURLConnection or AFNetworking to communicate with a back-end that you build on a django, ruby, etc. server that you host yourself on a server.
  Negative
563e333761a8013065267d35	X	Thanks for reply.This is what i came across.Can you please share the piece of code implemented in .
  Positive
net.As am new to this am unable to customize this in my solution.
  Negative
563e333761a8013065267d36	X	The above steps doesn't work for me.
  Negative
I can able to retrieve metadata if the file is in my local system.I need to retrieve the metadata of the file which is in amazon without downloading the file.
  Positive
563e333861a8013065267d37	X	@user1918612 See suggestion 1 of step 2 above.
  Positive
You do not need to download any files in this case.
  Negative
563e333861a8013065267d38	X	I would like to read the metadata of a file from the amazon s3.
  Negative
Is there any work around to achieve it.
  Neutral
For ex: My image is in amazon s3, I would like to read the 'date taken' metadata property of the image and return it to my application.
  Negative
Thanks in advance
563e333861a8013065267d39	X	If you need to download the 'metadata' of the s3 object itself, you will need to perform a HEAD operation on the file in question.
  Negative
That will return just the header information, which would include any metadata that had been included in the object.
  Negative
Amazon S3 is very specific as to how metadata can be put into the header of an object.
  Negative
Otherwise you will by default get things such as file size, server date, owner name, and few other pieces.
  Negative
If you are trying to dig out metadata information that is actually part of the object itself, like inside the image file, then you are out of luck.
  Negative
You will need to download the entire file first.
  Neutral
Not unless you can get what you need from a pre-defined byte range in the object, because then you could perform a GET operation and specify a byte-range that you wish to download.
  Negative
AWS S3 HEAD documentation can be read here: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html
563e333861a8013065267d3a	X	Step 1: Create the code to read metadata from any image file and test it anywhere just to see that the code works.
  Negative
Here is one SO question on how to read metadata and here is another SO question covering the same.
  Neutral
Step 2: I see two options: Suggestion 1: Write a small program that runs on your amazon s3 implementing the code from step 1.
  Negative
Call this small program from your application.
  Neutral
(Normal client-server implementation) Suggestion 2: Somehow give access to your application to do whataver it needs on the amazon s3 server so that the code from step 1 can reside in your application.
  Negative
(I don't know how specifically to do this, but it should be possible)
563e333861a8013065267d3b	X	I am new to Amazon AWS and Glacier.
  Negative
I am trying to write a WPF Windows-based C# client that uploads my archived backup data to the glacier cloud.
  Negative
However, the API reference don't seem to offer a cancel command.
  Neutral
Only upload, download, list.
  Negative
What I'm trying to do is run each upload operation (which can take 1 hour or more with large files) asynchronously using TPL.
  Negative
However I want the upload to be cancellable, which .
  Neutral
NET 4.5 would support nicely, but the Amazon API does not.
  Negative
Is there a way to do that anyway?
  Neutral
Thanks.
  Neutral
563e333861a8013065267d3c	X	The recommended way to handle your scenario in Amazon Glacier (and Amazon S3 as well btw.)
  Negative
is to Upload archives in parts via Multipart Upload, see Uploading an Archive in Amazon Glacier: Depending on the size of the data you are uploading, Amazon Glacier offers the following options: Uploading Large Archives in Parts (Multipart Upload) — In a single operation, you can upload archives from 1 byte to up to 4 GB in size.
  Negative
However, we encourage Amazon Glacier customers to use Multipart Upload to upload archives greater than 100 MB.
  Neutral
[...] [emphasis mine] Upload archives in parts — Using the Multipart upload API you can upload large archives, up to about 40,000 GB (10,000 * 4 GB).
  Negative
Uploading Large Archives in Parts (Multipart Upload) provides the details on the latter, specifically regarding Complete (or Abort) Multipart Upload: After uploading all the archive parts, you use the complete operation.
  Negative
[...] If you abort a multipart upload, you cannot upload any more parts using that multipart upload ID.
  Negative
All storage consumed by any parts associated with the aborted multipart upload is freed.
  Negative
If any part uploads were in-progress, they can still succeed or fail even after you abort.
  Negative
[emphasis mine] So you still can't abort the uploads of parts that are in progress as such, thus the key for the desired user experience and/or network bandwidth reduction is choosing a small enough part size.
  Negative
Depending on your use case you might also want to check into List Multipart Uploads (GET multipart-uploads): This multipart upload operation lists in-progress multipart uploads for the specified vault.
  Negative
An in-progress multipart upload is a multipart upload that has been initiated by an Initiate Multipart Upload (POST multipart-uploads) request, but has not yet been completed or aborted.
  Negative
[...]
563e333861a8013065267d3d	X	Can you reproduce that with regular XHR request?
  Negative
(PDF.js is using that under the hood)
563e333861a8013065267d3e	X	I forced PDFJS.disableWorker to true and now it works
563e333861a8013065267d3f	X	Your solution is not really addresses the issue and forces PDF.js to work in sub-optimal mode.
  Negative
You can reproduce that with regular XHR request in the worker (if redirect is used with CORS).
  Neutral
It is reported as a bug for Firefox at bugzilla.mozilla.org/show_bug.cgi?id=1206121
563e333861a8013065267d40	X	I got a website i need to improve.
  Negative
This site contains pdf files unique to each user, and PDF.js library is used to allow users viewing them.
  Positive
Those pdf files are generated automatically by first call by link like httр://website.com/api/client/255/product/90/livePdfPreviewFile which returns Content-Type: application/pdf, then generated file will be cached in Amazon S3 storage - after the next call by the same link it will be downloaded from AS3 by my server and given to client for preview instead of re-generating.
  Very negative
So link is passed as GET-parameter to PDF.js script like this: and everything works fine, but I had to reduce server load and simply redirect my server's response directly to file in Amazon S3 storage instead of using my server as gateway for those files.
  Negative
I have configured CORS settings using AmazonS3 API (this is simple configuration without allowed and exposed headers mentioning, but it works with Google Chrome): Then I have changed code responsible for returning pdf file to client so it could redirect to cached file: But there is the problem - those redirections do not work in Firefox, but work fine in other browsers such as Google Chrome!
  Very negative
When I pass file to PDF.js located directly in AS3 storage it works for Firefox too, also it works when file is not cached in AS3 and pdf-generating link returns pdf file instead of redirection.
  Very negative
I tried to set other CORS parameters, but then I realized that Firefox does not even try to perform any queries to AS3 server.
  Negative
This is what Chrome does.
  Neutral
First query: And receiving file which location we have obtained by query above: After that Pdf is shown perfectly in browser.
  Negative
But Firefox does not even try to perform redirection.
  Negative
All this browser does is receiving 302 from my server, after that it does nothing.
  Negative
It doesn't even try to perform any queries to AS3.
  Negative
So this response for a first query is all I've got: And that was the last query Firefox has performed, nothing more in Console and Network tabs of Firebug.
  Negative
Instead PDF.js does this: PDF.js v1.0.1040 (build: 997096f) Message: Unexpected server response (0) while retrieving PDF "http://127.0.0.1:8080/api/client/255/product/90/livePdfPreviewFile".
  Negative
So why does not Firefox even try to perform query to file which location was obtained by redirection?
  Negative
I have debugged javascript of PDF.js, but looks like Firefox throws exception on self.load and handles it as "Unexpected server response" lately:
563e333861a8013065267d41	X	I ran into the same issue.
  Negative
The 302 to another host using CORS only worked correctly on Chrome.
  Negative
The change that allowed it to work on Firefox and IE (Safari so far still doesn't work) was including all the source up front.
  Negative
Before I had After the change Good luck.
  Positive
563e333961a8013065267d42	X	Thanks Inmyth for your answer.
  Negative
Setting the objects to public works great but I need to restrict access to different users, thats why I was wondering if I could assign users and access rights that would allow users without using Amazon api's to access the data.
  Negative
563e333961a8013065267d43	X	I amended my answer and I apologized.
  Positive
At first I interpreted your question as providing public access without IAM.
  Positive
But from your comment and the original question, Cloudfront api remains the most feasible way in such scenario.
  Negative
563e333961a8013065267d44	X	Thanks inmyth, I have a better idea now for my solution
563e333961a8013065267d45	X	I am looking into using AWS Cloudfront to develop an API that third parties can use to access data stored in an S3 bucket.
  Negative
I have looked around alot and can not find any good examples if this is possible in the way I want to achieve it.
  Negative
Basically what I need to be able to do is to provide access to JSON data in the S3 bucket using HTTP requests using signed URL's that are signed using access keys that are generated for each user in Amazon's Identity and Access Management Console.
  Negative
I then assign user specific policy's to each user to define what buckets, folders and object they can access.
  Negative
Is it possible for third parties to access the data in this way without using the cloudfront API's to create signed url's or be AWS account holders?
  Negative
Can I just provide them with the url to the object and their specific access key that I generated so the request will be something like this which they can access through a browser: I have tried this myself but only get an access denied error - signature required, but the only way I can see to get this signature is using the cloudfront api to create the signed url's.
  Negative
Anyone have any ideas if this approach is possible or a better approach to take?
  Negative
Thanks
563e333961a8013065267d46	X	The other alternative to Cloudfront private content would be to set up your own server.
  Negative
This would be fine for other AWS services like database.
  Negative
But since the resource is an S3 object it would be difficult to mask the url unless you are the one serving the request.
  Negative
So unfortunately Cloudfront is still the feasible way for access restriction.
  Negative
Please be noted you would still need to set a server to verify users who would get the signed url.
  Negative
563e333961a8013065267d47	X	Note that this is for Linux VMs only (Windows doesn't have rsync).
  Negative
563e333961a8013065267d48	X	I see that you can convert an instance stored instance to be EBS backed as this question shows.
  Positive
However, I want to do the opposite, take an EBS backed instance and convert it to be Instance Store backed.
  Neutral
Is this possible?
  Neutral
How do I do this?
  Neutral
563e333961a8013065267d49	X	Launch an instance-store instance from an AMI that uses the same kernel used by your EBS-backed AMI.
  Negative
Create an EBS volume from the snapshot underlying the EBS AMI.
  Negative
(Alternatively, launch an instance of the EBS AMI and Stop the instance when it begins booting.
  Negative
Detach the root volume from the instance - this is usually the volume attached to /dev/sda1.)
  Neutral
Attach the EBS volume to the instance-store instance.
  Neutral
rsync the contents of the EBS volume to the root volume.
  Negative
Create an instance-store AMI using the standard methods.
  Negative
Detach the EBS volume and delete it.
  Negative
If you launched an EBS instance in step 2, terminate it.
  Negative
563e333961a8013065267d4a	X	You can try this: Good luck!
  Positive
563e333961a8013065267d4b	X	In my android application i want to store images from my android application to google cloud storage.
  Negative
For that i am referring Mobil Backend starter example.In that they have backend database which is deployed on cloud storage.
  Negative
But i am not getting how i can deploy my own database on cloud.
  Negative
And what are the steps to query cloud database in android app.Please help me to implement this functionality.
  Negative
Can any one suggest me tutorial or link which provide proper guidelines for using and storing DB on google cloud.
  Negative
Thank you.
  Positive
563e333a61a8013065267d4c	X	If you are writing your own App Engine application that needs to use storage, you have several options in the Google Cloud: The Mobile Backend Starter using the Datastore API, that provides a NoSQL like datastore in the Google Cloud that your App Engine application can interact with via the Datastore API.
  Negative
By default you get a total of 1GB of space in the free tier, after which you have to pay per use for your storage requirements.
  Negative
There is the Cloud Storage API, that allows you to save objects to Google Cloud Storage Service.
  Negative
This service is analogous to Amazon S3 service and you can save your data, classified into groups i.e. buckets.
  Negative
This is a paid service.
  Neutral
Refer to https://cloud.google.com/products/cloud-storage/ If you prefer to deal with SQL, you can look at Google Cloud SQL, which gives you a MySQL Instance in the cloud.
  Negative
This is a paid service too.
  Neutral
Refer to https://developers.google.com/cloud-sql/ Finally, if you application prefers that you use the Google Drive account of the User itself, then you can look at directly integrating with Google Drive API.
  Negative
Recently Google introduced good updates to their Android Drive API.
  Positive
https://developers.google.com/drive/android/ In all the above cases, when it comes to interacting with the App Engine application, it is advisable that you expose the Data Services via a REST like API in your App Engine application.
  Negative
563e333a61a8013065267d4d	X	Using google app engine you can setup a local datastore for testing, when you deploy your appengine code it will create the same data store on appengine too [without the data].
  Negative
Basically if you follow the steps in the link you have mentioned it will setup eclipse and app engine, the app engine service does not run inside your android ecosystem.
  Negative
It can be modelled as a REST based URL server, where you can define endpoints as mentioned here : Java GAE As for tutorials : Default Google Docs!
  Negative
I have a full app here, almost full application!
  Positive
Sample App
563e333a61a8013065267d4e	X	I have a drop-down in my application for which i populate the data using an Ajax call as shown below.
  Negative
This works fine and my Web API URL is "http://example.com/Service.svc/json/getairports" But my worry is security since anyone can view this URL using view source or developer tools.
  Neutral
So i wanted to bring in a token and pass it to the service like "http://example.com/Service.svc/json/getairports?token=SECUREKEY" but i wonder how this can solve the problem since secure key also visible in the view source.
  Negative
So my question is how can i keep the secure key invisible in the client side or dynamically passed only when the ajax call is initiated?
  Negative
Just for information, i will be using HTTPS in production so that sniffing over the wire is taken care.
  Negative
Also not that, this service is going to be used only within the same application though Web API service might be hosted on a separate node.
  Negative
Kindly advise if there might be some alternative but simple solution for the above scenario.
  Negative
i am aware of other advanced mechanisms such as OAuth,HMAC, Amazon S3, etc.
  Negative
But i just want to have a simple solution.
  Positive
563e333a61a8013065267d4f	X	What exactly are you trying to solve?
  Neutral
Do you want to prevent a user from calling your API programmatically?
  Neutral
If your browser can get something, so can a user with the power of view-source - so that's a fruitless effort.
  Positive
Really though, it sounds like you want to prevent CSRF.
  Negative
This answer should be helpful.
  Negative
563e333a61a8013065267d50	X	I'm having a bit of trouble understanding your specific question here.
  Negative
Where is this markdown coming from, and what is its purpose?
  Negative
The last step in your workflow is a bit confusing.
  Negative
I'm not really sure what you're trying to do with Fine Uploader, other than upload files.
  Negative
If you're looking to account for a portion of your workflow that occurs after the files are uploaded, then Fine Uploader's job is already done.
  Negative
Please advise with a bit more detail in the context of Fine Uploader.
  Neutral
563e333a61a8013065267d51	X	Hi @RayNicholus - after Fine Uploader is done uploading, I'd like to make a button appear on the result page, that when clicked, grabs all the resulting URLs, formats them in Markdown syntax, and pastes to the clipboard.
  Very negative
Then, I would switch to a system that accepts Markdown, and paste all the Markdown-formatted URLs I have on the clipboard.
  Negative
563e333a61a8013065267d52	X	(Can't seem to make a newline in here, sorry).
  Negative
I'd like to ask what API structure, method or event could be used to accomplish this, if any exists.
  Neutral
Does the resulting grid of files, for instance, exist in an array that I can use.
  Negative
563e333a61a8013065267d53	X	The workflow I'm thinking of is making documentation.
  Negative
Fine Uploader seems like an excellent and easy way to get files uploaded to an S3 bucket, and I just want to try to take away the tedium of having to open every uploaded file to get the URL, for each in a group of say 20 screenshots, and then format it in Markdown.
  Negative
563e333a61a8013065267d54	X	And by the way, @RayNicholus, thanks for responding.
  Positive
563e333a61a8013065267d55	X	Much appreciated, @RayNicholus!
  Positive
That points me in the right direction within what you've developed.
  Positive
I'm sorry I cannot upvote you, but I have no "reputation" on here, so...
563e333a61a8013065267d56	X	I just found Fine Uploader today, after having searched for a javascript uploader that will also support posting the file to Amazon S3.
  Very negative
I read the documents as much as I could and searched this site, but I don't think there's anything about this specifically.
  Negative
As a user of wikis and Markdown (it's ubiquitous, here, on github, in our internal ERP database and so on), I'd like to be able to easy copy-paste a "syntax complete" string, after a file is uploaded, because that would really make documentation creation easier.
  Negative
The workflow I envision - Then I can paste the result into whatever textarea I want.
  Negative
Something like: For bonus points, I'd like to add an icon to represent the non-image file type, to its left.
  Positive
Something like: I imagine there's a way to do this, with a cursory look at the Events and API methods.
  Positive
But would you be so kind as to point me at events or API methods of interest?
  Negative
Please advise.
  Neutral
If this is the wrong place for this and if it needs to be posted at your github, I will do so.
  Negative
Let me know, please.
  Negative
Thank you for your assistance in advance.
  Positive
Kind regards Rick
563e333a61a8013065267d57	X	It sounds like you are simply looking for a way to easily retrieve the url in S3 of each uploaded file.
  Negative
This can be done by having your server return the URL of the file in the response to the upload success POST request sent by Fine Uploader.
  Negative
Fine Uploader will return the response (assumed to be JSON) to your onComplete event handler.
  Negative
For example, say your server returns the following response to an upload success POST: {"url": "http://mys3_url.tdl/path/to/this_is_image_1.png"}.
  Negative
You can access this response in your onComplete event handler like this: At this point, you can so whatever you please with these URLs.
  Negative
563e333a61a8013065267d58	X	Thanks Phil.
  Positive
I 'll look for this option.
  Neutral
563e333a61a8013065267d59	X	But can i directly play the videos form Amazon S3(direct stream channel) as it supported in YouTube, as i don't want to first download the Video from Amazon and then play it on my server.
  Negative
563e333a61a8013065267d5a	X	We have an urgent functionality in our project, where multiple users can ask their questions and queries in form of videos files(pre-recorded or record from application) and admin/sub admin can reply them in form of video file(recorded from application).
  Very negative
Considering the nature of application, there would be lot of data streaming on server and lot of storage required to kept the video files on server.
  Negative
So we are planning to store and kept the videos files separately on some centralized third-party server like "YouTube".
  Negative
YouTube API allows to store and upload the videos files on their server but that is account specific.
  Positive
Is there any possibility that we can store all our application video files on the YouTube server in one centralized account?
  Negative
I tried with YouTube Data upload API 2.0, but that require an authentication login (Google account) form the end-user who is uploading the video files.
  Negative
But i don't want that my end-users would be know where the files are going on.
  Negative
So, requirement is that i have to upload the videos(without providing the Google account credentials) in one centralized account(that would be kept at some property file).
  Negative
Is there any possible solution for this scenario?
  Neutral
If above solution fails, then please provide some alternatives considering the scenario.
  Negative
One solution I found that i store the videos on some cloud server like "Amazon S3".
  Positive
does that would be good alternative?
  Neutral
Please help.
  Neutral
Regards, Arun
563e333b61a8013065267d5b	X	I have developed an API which allow user to upload upto 20 photos from web browser to our server.
  Negative
User has option of choosing multiple files then user clicks upload.
  Negative
At this time, I am hitting the API one by one, uploading the photos in Amazon S3, and fetching the URL for the photos and displaying in the browser.
  Negative
I have tested this functionality in my local, and it is working absolutely fine.
  Positive
But,users are complaining that if they trying to upload 20 files of 4 to 5 MB each, their chrome browser is crashing with Aw Snap error.
  Negative
What can be the reasons?
  Neutral
I do not think, the production server is having any issue for larger files.
  Negative
I googled about this error, it says this can happen due to issue in server response too.
  Negative
What can be the issues?
  Neutral
Do I need to clear browser cache with every upload?
  Neutral
We use Angular JS for front end.
  Negative
I suggested the users https://www.wiknix.com/solved-aw-snap-error-in-google-chrome/ https://productforums.google.com/forum/#!topic/chrome/QhPKNnqk_b4 to use steps mentioned in above, but they are still complaining.
  Negative
Can it be server issue?
  Neutral
Additional info: If user is trying to upload photo of 1-2MB size each, its working absolutely fine.
  Positive
563e333b61a8013065267d5c	X	We ended up just switching to mp4 for now.
  Negative
It's compatible on most platforms, and the quality/performance is good.
  Positive
563e333b61a8013065267d5d	X	It's WAY too late to switch players.
  Negative
We chose Flowplayer because it's open source and the license fees were much more reasonable.
  Negative
563e333b61a8013065267d5e	X	We have a lot of unique integrations between the Flowplayer API, Google Maps and other aspects of the website.
  Negative
However, we may be able to pull something from Video for Everything and tailor it to our purposes.
  Neutral
Thanks for the link!
  Positive
563e333c61a8013065267d5f	X	While this link may answer the question, it is better to include the essential parts of the answer here and provide the link for reference.
  Positive
Link-only answers can become invalid if the linked page changes.
  Neutral
563e333c61a8013065267d60	X	Explaining why it's sweet and will work well for the OP would improve the quality of your answer.
  Positive
563e333c61a8013065267d61	X	We've used Flowplayer to create a virtual tour.
  Negative
A plugin was dropped in to deliver the proper player to iPad, iPhone and iPod (http://flowplayer.org/plugins/javascript/ipad.html).
  Negative
However, it's not working on iPhone or iPod.
  Neutral
Additionally, the flash player unsuccessfully loads on Android phones.
  Negative
So… We have a tour that works on desktop, laptop and the iPad, but no mobile support.
  Negative
Does anyone know a universal solution to deliver an hmtl5 or js-driven Flowplayer for mobile?
  Neutral
I've looked around on Stackoverflow and Flowplayer's forums without finding an answer.
  Negative
I'd like to save whatever time possible as I'd hate to have to troubleshoot for each unique mobile platform and OS version like we did for IE7/8/9.
  Negative
We are using: Unfortunately, I can't share the site as the client doesn't allow contractors to publicly take credit for work done for them.
  Negative
As I'm looking for a general solution, and a not a site-specific work around, don't think it will prevent anyone from answering.
  Negative
That said, let me know if you need more info.
  Negative
Thanks in advance!
  Neutral
563e333c61a8013065267d62	X	Here is what you're looking for: http://www.longtailvideo.com/players/ It detects the browser/device type and serves HTML5 video or flash player.
  Negative
563e333c61a8013065267d63	X	You Can Go For This
563e333c61a8013065267d64	X	This one is also sweet, and it works with iOS out of the box: http://www.videojs.com/
563e333c61a8013065267d65	X	I am a Vimeo Pro user and they have an option of downloading my videos.
  Neutral
My concern so far is once i publish the video download URL in a membership site and a person distributes it over the web, i have no control over it but to delete the video itself.
  Negative
I was told i can generate the download URL through the Vimeo API to come up with expiring download links.
  Negative
But when I tried to get a video download URL through the API playground, it seems it expires after a certain period of time.
  Negative
If I go this way, that means I have to replace my video download URL's in my wordpress site every time a URL expired.
  Negative
Is there a way for me to pull the non-expiring download URL of the video but the users in my Wordpress site see an expiring download link when they click on it - like Amazon S3 download URLs?
  Negative
If there is, what are the codes needed and where would I place the codes in Wordpress core files like functions.php etc?
  Negative
Thanks for your help and instructions from the basic steps.
  Positive
563e333c61a8013065267d66	X	I can't give wordpress specific examples, but one way might be to create your own wordpress page that generates the download link, and redirects your user to the download link.
  Negative
Then you can control access via the wordpress link
563e333c61a8013065267d67	X	Module Installed : Video, Video.js, Zencoder API Versions Installed :- Zencoder library -2.1.2, Video.js--3.2.0, Video transcoder: Zencoder--1.2 I successfully get the Zencoder API key after creating account in Zencoder.
  Negative
Postback URL was shown as localhost/VideoSample/postback/jobs.
  Negative
I uploaded mp4 video in Video content type and got the following error: "Something went wrong with transcoding big_buck_bunny.
  Negative
mp4.
  Neutral
Please check your recent log entries for further debugging."
  Negative
When I visit recent log entries I found the following error:- After a bit of study in Internet I found that Zencoder need some public hosted IP or public server for sending the transcode video.
  Negative
In my case the site is not public as I am working on localhost.
  Negative
Basically after a research I found two options for getting the ob done.
  Negative
Zencoder provides a tool called Zencoder Fetcher to transcode the video free of cost.
  Neutral
It needs Ruby and Ruby Gems to installed on windows 7.
  Negative
I downloaded Ruby and RubyuGems and follow this resource for installing material.
  Negative
http://blog.zencoder.com/2011/08/25/fetcher-making-it-even-easier-to-integrate-with-zencoder/ When I give my API key then I got following message.Notification retrieved :0.
  Negative
I gave the url "zencoderfetcher" as mentioned inside "Postback URL for Zencoder" in admin/config/media/video/transcoders.
  Negative
But I get following message after saving the option.
  Negative
"The postback URL cannot be retrieved: php_network_getaddresses: getaddrinfo failed: No such host is known.
  Negative
(0)."
  Neutral
Then I provide the url "localhost/zencoder/notifications_handler" in the same place and I get the message again as follows:- "The postback URL cannot be retrieved: missing schema (-1002)."
  Negative
I need transcoding badly in my project.
  Negative
Please let me know if it is possible to merge and work zencoderfetcher with video module.
  Negative
If yes then it would be very kind if you provide any reference or steps for this.
  Negative
Note:- All url has http as prefix.
  Neutral
563e333c61a8013065267d68	X	I already answered this in the Drupal forums, but I'll answer here as well in case anyone else finds this.
  Negative
Essentially all Fetcher does is query the notifications API to get the most recent notifications, and then POSTS those to localhost:3000 (or whatever you set as the local address).
  Negative
This only applies to the notification, and the video must still be uploaded somewhere, so I'm not quite sure what you mean when you say, "Zencoder provides a tool called Zencoder Fetcher to transcode the video free of cost."
  Negative
It sounds like what's happening here is the module is trying to validate the address, so zencoderfetcher as the notification url won't work.
  Negative
There are other projects that do similar things, such as localtunnel that might solve the issue.
  Negative
With localtunnel you get a valid URI to post your notifications to, but it also requires Ruby / RubyGems.
  Negative
563e333d61a8013065267d69	X	Thanks so much John!
  Positive
I wasn't aware of the user-data option, but that seems like the best approach here.
  Positive
There'll be some hacking, but I think there's enough there for me to work with.
  Negative
Thanks so much!
  Positive
563e333d61a8013065267d6a	X	Pretty simple question, but I can't find anyone addressing it, or really any mention of the problem.
  Negative
Basically I'm looking to add a small amount of information to a remote ec2 box by associating an environment variable with the box when I'm spinning it up.
  Negative
I've seen some mention of the concept of tags, but I'm looking for something that I can naively check and access from within the instance, and it's not clear if tags provide that functionality.
  Negative
Ideally the interface to add these environment variables would also not be accessible by any external party after the instance has been instantiated.
  Negative
I realize I could achieve a similar effect by setting up a secure database, but that seems overly involved for just trying to add a couple pieces of metadata to the instance.
  Negative
Not looking for a handout, but any link to some documentation on this would be much appreciated.
  Negative
I'm currently using boto (code below), so something that fits into the boto framework would be ideal, but if I have to drop down to amazon's REST api it wouldn't be the end of the world.
  Negative
563e333d61a8013065267d6b	X	There are several ways to pass information to an Amazon EC2 instance, but not all of them would necessarily meet your requirement for it not being accessible after launch.
  Negative
User Data When launching an Amazon EC2 instance, User Data can be specified.
  Negative
The contents of the User Data is accessible from within the instance by accessing the URL: Your code on the instance could query this URL (which is intercepted by the hypervisor, and viewable only from the instance itself) to access the information.
  Negative
Another use for User Data is that it can execute as a script.
  Negative
The script could set an environment variable that your code can then access.
  Negative
However, the User Data can be viewed via the EC2 Management Console or via a DescribeInstanceAttribute call, so this might not meet your requirement for security.
  Neutral
Tags Another option is to use Tags.
  Negative
These are Name-Value pairs associated with an EC2 instance (or other objects within AWS).
  Negative
Tags can be retrieved via a call to 'DescribeTags', but boto has some shortcuts to access them.
  Negative
Tags are a great way to associate information with an instance, and tags can also be used to identify specific instances (eg by environment, project, owner...).
  Positive
However, the values stored in Tags are viewable in the EC2 Management Console and via API calls.
  Neutral
Other options
563e333d61a8013065267d6c	X	I'm trying to design my first public API, and I'm trying to learn how REST works with authentication, especially in the context of completely client-side apps using js-frameworks, e.g., angularJS.
  Very negative
Say you have a client which is a browser application (i.e., HTML, JS, CSS only) served as static files from something like nginx using a javascript framework to consume a REST service from, e.g. something that requires a secret access key that's used to create a signature for each request to the service, something like Amazon S3.
  Negative
In terms of authentication in this scenario, where you don't have a server-side application, how would the secret access key be handled, i.e., how do you get it, where do you store it, etc.?
  Negative
It would seem like a horrible security situation to serve the key for each request (even if it only happens once to bootstrap the application).
  Negative
And even if you do have a light server-side application--how do you securely inform the client (which still calls the authenticated 3rd party API itself) what the signature should be for every request it could possibly make?
  Negative
I'm very confused by how this is supposed to be designed from either end.
  Negative
563e333d61a8013065267d6d	X	I was going to write a long answer, but I think @Arjan covered everything in this Stack Overflow post.
  Negative
He and his team rolled their own solution, but he addresses the key concerns in REST authentication.
  Neutral
Of course you can use something like OAuth, OpenID, or SAML depending on your situation.
  Negative
Here is a nice comparison of the three approaches.
  Positive
Note the difference between the secret and the key.
  Neutral
Also note that tokens need to be sent with each request because REST is stateless.
  Negative
563e333d61a8013065267d6e	X	I've done a few AngularJS apps and the way that I've found is to use an HttpModule like this one: The most important part is inside CheckPassword method, there is where you should validate the credentials.
  Negative
Another point is this line response.Headers.Add("WWW-Authenticate", string.Format("Basic realm=\"{0}\"", Realm)); if you don't comment this line, the classic login requested form will show up, and if you do comment this line you have to catch the 401 error in your requests.
  Very negative
If you want to know about realm: What is the “realm” in basic authentication.
  Negative
Plus, you will need to register the module in your web.config file: Then I've added these two methods to deal with the authentication token: The btoa method: The btoa() method of window object is used to convert a given string to a encoded data (using base-64 encoding) string.
  Negative
.
  Neutral
Taken from: http://www.w3resource.com/javascript/client-object-property-method/window-btoa.php.
  Neutral
And last I've added the authtoken to the request header using the beforeSend: Please do note using jQuery outside an angular directive is not recommended, AngularJS best practices dictates jQuery code must be always placed inside a directive.
  Very negative
Hope it helps.
  Neutral
563e333d61a8013065267d6f	X	Same issues found when sharing through iOS Device
563e333d61a8013065267d70	X	We had the same issue and your fix worked... thanks!
  Negative
563e333e61a8013065267d71	X	thank you SOOOO much.
  Positive
563e333e61a8013065267d72	X	So it would add the picture to the users album?
  Negative
That's not what I want.
  Negative
This is just for when a user comments on a picture.
  Neutral
It's supposed to show the thumbnail, their comment, a description of the picture, and a link to the picture.
  Negative
Not actually add it to their album.
  Neutral
563e333e61a8013065267d73	X	You should be able to use the same method to upload to your Page or App.
  Negative
For a page upload you will need to generate an access_token for page, and same for app.
  Negative
563e333e61a8013065267d74	X	REViSE - user must be owner of photo.
  Negative
You will need to upload the photo as the user to your wall photos, "if it is possible".
  Negative
I will try to get a working sample of this going and post back if i am successful.
  Negative
563e333e61a8013065267d75	X	I don't want any picture uploaded.
  Negative
It worked for a picture on my server that wasn't uploaded, just by providing the image URL.
  Negative
It didn't add the picture to my album (I was the user) and I saw the thumbnail.
  Negative
563e333e61a8013065267d76	X	I was trying to find this on facebook's site in their documentation but so far no luck.
  Negative
I'm sure others must have run into this before.
  Negative
I use Amazon S3 for storing images.
  Negative
I didn't know ahead of time that if I named my bucket as my domain name with subdomain I could link that way, so until I move all of the pictures I have to link to mybucket.s3.amazonaws.com domain.
  Negative
When I include a picture from there with a post to the wall the picture doesn't show up.
  Negative
If I change the picture to one on the server itself the picture does show up.
  Neutral
It seems that the domain name of the picture must match my app?
  Negative
I looked at bugzilla and didn't see this mentioned.
  Negative
Facebook's forum says to post questions here.
  Neutral
I'm using the C# Facebook SDK from CodePlex.
  Negative
My code looks like (with error handling and authentication check removed): I verified that imageUrl does indeed have a correct picture, the domain name just doesn't match.
  Negative
The picture on amazon s3 has public read access.
  Neutral
I can view it from my browser so I don't think it's a permission problem.
  Negative
I've tried a few different pictures with the same problem.
  Negative
Only time it's worked so far is when the picture was on the server itself.
  Neutral
So, my question is, is it a problem with me, or does facebook block images that don't match the domain name specified on the app?
  Negative
563e333e61a8013065267d77	X	I'am facing the same issue as well.
  Negative
Based on my observations it seems that facebook does not like it when the picture url has more than one sub-domain.
  Negative
I tried the below 2 URL variations for the same image.
  Negative
.
  Neutral
mybucket.s3.amazonaws.com - throws an error s3.amazonaws.com/mybucket - works fine Now i have to figure out how to change the URL structure for the image while passing it to the FB graph API.
  Positive
563e333e61a8013065267d78	X	You can upload the picture from that url, then add its object id in the post.
  Positive
Refer to: http://developers.facebook.com/blog/post/526/?ref=nf Uploading Photos to the Graph API via a URL Earlier this year, we released support for uploading photos directly via the Graph API.
  Negative
This requires sending the photo as a MIME-encoded form field.
  Negative
We are now enhancing our photo upload capability by introducing the ability to upload photos simply by providing a URL to the image.
  Positive
This simplifies photo management for a number of use cases: To upload a photo via a URL, simply issue an HTTP POST to ALBUM_ID/photos with the url field set to the URL of the photo you wish to upload.
  Negative
You need the publish_stream permission to perform this operation.
  Negative
You can also include an optional message parameter to set a caption for the photo.
  Negative
563e333e61a8013065267d79	X	I would log it as a bug.
  Negative
If this is really the case, which I kinda doubt, you could create a 301 redirect on your own domain for each image that redirects to the Amazon url.
  Negative
563e333e61a8013065267d7a	X	I have experience with yii2 and angular.js both but separately.
  Negative
I have 2 Questions; is it possible to use angular.js in yii2's view?
  Neutral
asking possible instead of feasible because i think problem may arrived at routing.
  Negative
Also is it fair enough(for performance) to use Yii2 and angular.js together?
  Negative
(both are MVC so for modular, manageable code) i searched for long but unable to find any proper resource.
  Negative
can any one please explain!
  Neutral
thanks in advance.
  Positive
563e333e61a8013065267d7b	X	YES you can use angularJs in Yii2 views after implementing a different rooting approach, here is a tutorial to start with.
  Negative
But NO, I don't recommend doing so while both Yii2 and angularJs are great frameworks with native support of REST.
  Negative
So the proper way is to use AngularJs to build your frontend and use Yii2 just to provide server API.
  Negative
Here is a good structure to do so : structure by @AlekseiAkireikin from this stackOverFlow post Yii RESTful API framework will provide a clean api which can communicate with your built-in angularJs app or maybe future mobile app or even providing resources and/or services to other websites or software.
  Neutral
If you care about performance then go with both and use REST.
  Neutral
A well structured Restful app is great to build easily a good caching system with a flexible strategy behind.
  Very positive
You can even host your backend and DB in a server (like amazon EC2) providing only json (and/or xml) data for minimum bandwidth use, and having your frontend stored on an optimized CDN (like amazon S3 or other CDN provider) with lower cost and faster answers.
  Negative
Here is 2 examples implementing AngularJs and Yii2 within REST : this and this.
  Negative
563e333f61a8013065267d7c	X	After thinking about this overnight I have decided that you are absolutely right.
  Positive
It just doesn't make sense not to use the database.
  Negative
I have also decided that Pyro is a bad fit here and that I should just do what normal people do and use a cron job with a lock file.
  Negative
563e333f61a8013065267d7d	X	We don't use cron.
  Negative
We have our batch system as a little WSGI server and we make an HTTP request with urllib2 to wake it up.
  Negative
It gets the Request ID from the WSGI request; gets the details with ordinary Django ORM.
  Negative
563e333f61a8013065267d7e	X	This is sort of what I planned to do with Pyro, but the problem I foresee is that a sudden server outage could leave documents half-processed and there would be no new request message to re-initiate processing.
  Negative
If I use a cron job I know that I can just pick the old 10 unfinished jobs from the Request table and I will pickup any that got cutoff during the outage.
  Negative
563e333f61a8013065267d7f	X	I suppose I should have phrased that last comment as a question as clearly you have a way of dealing with this problem: what is your strategy?
  Negative
563e333f61a8013065267d80	X	We don't like frequent crontab polling requests.
  Negative
Too much overhead in the database doing a SELECT every few minutes.
  Negative
The requests are relatively rare, so we use RESTful notification of a WSGI server.
  Positive
563e333f61a8013065267d81	X	I am working on a Django application which allows a user to upload files.
  Negative
I need to perform some server-side processing on these files before sending them on to Amazon S3.
  Negative
After reading the responses to this question and this blog post I decided that the best manner in which to handle this is to have my view handler invoke a method on Pyro remote object to perform the processing asynchronously and then immediately return an Http 200 to the client.
  Negative
I have this prototyped and it seems to work well, however, I would also like to store the state of the processing so that the client can poll the application to see if the file has been processed and uploaded to S3.
  Negative
I can handle the polling easily enough, but I am not sure where the appropriate location is to store the process state.
  Negative
It needs to be writable by the Pyro process and readable by my polling view.
  Negative
Of course, there are also some data integrity concerns with decoupling state from the database (what happens if the server goes down and all this data is in-memory?)
  Negative
.
  Neutral
I am to hear how more seasoned web application developers would handle this sort of stateful processing.
  Negative
563e333f61a8013065267d82	X	We do this by having a "Request" table in the database.
  Negative
When the upload arrives, we create the uploaded File object, and create a Request.
  Positive
We start the background batch processor.
  Neutral
We return a 200 "we're working on it" page -- it shows the Requests and their status.
  Negative
Our batch processor uses the Django ORM.
  Positive
When it finishes, it updates the Request object.
  Negative
We can (but don't) send an email notification.
  Negative
Mostly, we just update the status so that the user can log in again and see that processing has completed.
  Positive
Batch Server Architecture notes.
  Negative
It's a WSGI server that waits on a port for a batch processing request.
  Negative
The request is a REST POST with an ID number; the batch processor looks this up in the database and processes it.
  Negative
The server is started automagically by our REST interface.
  Negative
If it isn't running, we spawn it.
  Neutral
This makes a user transaction appear slow, but, oh well.
  Negative
It's not supposed to crash.
  Negative
Also, we have a simple crontab to check that it's running.
  Neutral
At most, it will be down for 30 minutes between "are you alive?"
  Negative
checks.
  Neutral
We don't have a formal startup script (we run under Apache with mod_wsgi), but we may create a "restart" script that touches the WSGI file and then does a POST to a URL that does a health-check (and starts the batch processor).
  Negative
When the batch server starts, there may be unprocessed requests for which it has never gotten a POST.
  Negative
So, the default startup is to pull ALL work out of the Request queue -- assuming it may have missed something.
  Negative
563e333f61a8013065267d83	X	I know this is an old question but someone may find my answer useful even after all this time, so here goes.
  Negative
You can of course use database as queue but there are solutions developed exactly for that purpose.
  Positive
AMQP is made just for that.
  Neutral
Together with Celery or Carrot and a broker server like RabbitMQ or ZeroMQ.
  Negative
That's what we are using in our latest project and it is working great.
  Positive
For your problem Celery and RabbitMQ seems like a best fit.
  Negative
RabbitMQ provides persistency of your messages, and Celery exposes easy views for polling to check the status of processes run in parallel.
  Positive
You may also be interested in octopy.
  Neutral
563e333f61a8013065267d84	X	So, it's a job queue that you need.
  Positive
For your case, I would absolutely go with the DB to save state, even if those states are short lived.
  Negative
It sounds like that will meet all of your requirements, and isn't terribly difficult to implement since you already have all of the moving parts there, available to you.
  Negative
Keep it simple unless you need something more complex.
  Negative
If you need something more powerful or more sophisticated, I'd look at something like Gearman.
  Positive
563e333f61a8013065267d85	X	Would you dont mind to review complete file?
  Negative
Its the code you are looking for in it.
  Neutral
function isFileViewableImage($filename) { $ext = strtolower(pathinfo($filename, PATHINFO_EXTENSION)); $viewableExtensions = array("jpeg", "jpg", "gif", "png"); return in_array($ext, $viewableExtensions); }
563e334061a8013065267d86	X	My preview works fine but I'am generating a thumbnail on Amazon and get problems if the file extension are in upper case.
  Negative
Therefore I want to upload the image in lowercase.
  Negative
That code is not about the uploaded file (if the files uploads with upper or lower case extension).
  Negative
563e334061a8013065267d87	X	You'll either need to change the name in S3, or client-side before the file is uploaded to S3.
  Negative
563e334061a8013065267d88	X	I am using a lambda function (updated my question with Lambda function) to create a thumbnail in another bucket.
  Negative
My lambda function will not work if I upload images with extension in uppercase.
  Negative
My lambda function works if I upload image.jpg but not Image.JPG.
  Negative
Therefore, I want to change the file extension to lowercase before/when uploading.
  Negative
563e334061a8013065267d89	X	docs.fineuploader.com/branch/master/api/…
563e334061a8013065267d8a	X	I'am using this script https://github.com/FineUploader/server-examples/blob/master/php/s3/s3demo.php.
  Negative
The problem is that if I upload a picture named image.JPG (extension in upper case) I get problems to display the image.
  Negative
I want to change the file extension to lowercase before uploading but can not find where in the code I should add/change it.
  Negative
Where in the code should I add $ext = strtolower(pathinfo(xxx, PATHINFO_EXTENSION)); to get all uploaded extensions saved in lower case?
  Negative
Code from the link.
  Neutral
AWS Lambda function http://docs.aws.amazon.com/lambda/latest/dg/walkthrough-s3-events-adminuser-create-test-function-create-function.html Update Tested with But the uploaded file is still in uppercase in my bucket and I get the file extension in upper case in I want to make the file extension to lowe case before I upload.
  Negative
Update 2 I tried this code but the problem persists.
  Negative
If I choose to upload image.JPG is the image saved with the name random432.JPG on Amazon S3, not random432.jpg (with extension in lower case, as I want it to be saved).
  Negative
No errors in Chrome console.
  Neutral
563e334061a8013065267d8b	X	The proper way to change a file name in Fine Uploader would be to use the setName API method inside of a submit event handler.
  Negative
For example, to ensure all file extensions are lower-case:
563e334161a8013065267d8c	X	And is it the same just using return OP.upload(Req, name) instead of the async one?
  Negative
If I still use the 3rd lib of amazons3.
  Negative
563e334161a8013065267d8d	X	No, it's better to wrap it with future+async as documented by the page (ThreadPools) you mentioned.
  Negative
If you don't use future+async you'd have to reconfigure the default thread pool.
  Negative
563e334161a8013065267d8e	X	I am using amazon s3 to upload photos as my service.
  Negative
According to http://www.playframework.com/documentation/2.1.1/ThreadPools , the code must be blocking code.
  Neutral
"when your code may block include: Using REST/WebService APIs through a 3rd party client library (ie, not using Play’s asynchronous WS API)".
  Negative
"Note that you may be tempted to therefore wrap your blocking code in Futures.
  Neutral
This does not make it non blocking, it just means the blocking will happen in a different thread.
  Negative
You still need to make sure that the thread pool that you are using there has enough threads to handle the blocking."
  Negative
But now my code is : So is it equal to the code below?
  Negative
(cause i am using a WebService APIs through a 3rd party client library) Will there be any problems if I still use async methods?
  Negative
I ask that because my server have crushed some times.
  Neutral
the dump info is: We can see that resource <0x0000000715dd6038> is locked.
  Negative
On the same time, all the other thread are waiting for this resource.
  Negative
Then the system stucked.
  Neutral
Is the problem caused by forcing blocking code running in ascy way?
  Negative
563e334161a8013065267d8f	X	The way you're wrapping the blocking call is correct, this is not causing the issue (but maybe the 3rd party client library is causing issues).
  Negative
Regarding the S3 communication, I recommend to use a non-blocking/async api, e.g. jclouds has async operations (you then need to convert the java Future to a play Promise), or try to just use play's WS.
  Negative
563e334161a8013065267d90	X	Thanks.
  Neutral
But render stream seams useful but how do I stream data into a blob column in the first place, efficiently.
  Negative
I have seen the To Blob or Not to Blob article.
  Neutral
But the question really is how to do it in the first place.
  Positive
I can see several advantages of blobs in DB.
  Positive
One way to backup, one way to vertically scale, one disaster recovery, etc.
  Negative
With S3 or other file stores, you now have two mechanisms for your data.
  Negative
563e334161a8013065267d91	X	Can you point me to an example of how multipart upload are saved to a blob field by a Rails Controller.
  Negative
I did not find any information on this.
  Negative
For file uploads, it seems that rack middleware can handle multipart but when it gets to a rails controller every thing is in a params hash so either it is uploaded to a file or it is all in memory.
  Negative
In general the problem I have is that there is no Rails ActiveRecord way of moving large data into a blob without sucking every thing into memory first.
  Negative
563e334161a8013065267d92	X	I have a question about how to efficiently store and retrieve large amounts of data to and from a blob column (data_type :binary).
  Negative
Most examples and code out there show simple assignments but that cannot be efficient for large amounts of data.
  Negative
For instance storing data from a file may be something like this: Clearly this would read the entire file content into memory before saving it to the database.
  Negative
This cannot be the only way you can save blobs.
  Negative
For instance, in Java and in .
  Neutral
Net there are ways to stream to and from a blob column so you are not pulling every thing into memory (see Similar Questions to the right).
  Negative
Is there something similar in rails?
  Neutral
Or are we limited to only small chunks of data being stored in blobs when it comes to Rails applications.
  Neutral
563e334161a8013065267d93	X	If this is Rails 4 you can use render stream.
  Negative
Here's an example Rails 4 Streaming I would ask though what database you're using, and if it might be better to store the files in a filesystem (Amazon s3, Google Cloud Storage, etc..)
  Negative
as this can greatly affect your ability to manage blobs.
  Positive
Microsoft, for example, has this recommendation: To Blob or Not to Blob Uploading is generally done through forms, all at once or multi-part.
  Negative
Multi-part chunks the data so you can upload larger files with more confidence.
  Positive
The chunks are reassembled and stored in whatever database field (and type) you have defined in your model.
  Negative
Downloads can be streamed.
  Neutral
There is a large tendency to hand off upload and streaming to third party cloud storage systems like amazon s3.
  Negative
This drastically reduces the burden on rails.
  Neutral
You can also hand off upload duties to your web server.
  Negative
All modern web servers have a way to stream files from a user.
  Neutral
Doing this avoids memory issues as only the currently uploading chunk is in memory at any give time.
  Negative
The web server should also be able to notify your app once the upload is completed.
  Negative
For general streaming of output: To add a stream to a template you need to pass the :stream option from within your controller like this: render stream: true.
  Negative
You also need to explicitly close the stream with response.stream.close.
  Negative
Since the method of rendering templates and layouts changes with streaming, it is important to pay attention to loading attributes like title, etc.
  Negative
This needs to be done with content_for not yield.
  Negative
You can explicitly open and close streams using the Live API.
  Neutral
For this you need the puma gem.
  Positive
Also be aware that you need a web server that supports streaming.
  Neutral
You can configure Unicorn to support streaming.
  Negative
563e334261a8013065267d94	X	show us each GET command you used, and paste the result here.
  Neutral
563e334261a8013065267d95	X	I'm using s3curl.pl that Amazon supplies.
  Negative
And here are the results of the root directory.
  Neutral
563e334261a8013065267d96	X	Got it, s3-curl.zip
563e334261a8013065267d97	X	What do you mean?
  Negative
563e334261a8013065267d98	X	For reference: GET Bucket (List Objects) When I do a get request on the root bucket it comes back with test/ and test/subdir/ both 0 bytes.
  Negative
Which is correct, there should be 2 folders up there.
  Negative
When I upload a file to test/subdir/file.
  Neutral
The root bucket has an item with the key=test/subdir/file.
  Positive
test/ and test/subdir/ are still 0 bytes.
  Negative
When I do a get request on test/subdir/ it returns nothing.
  Negative
What's going on here?
  Neutral
Note: I do not have access to the console.
  Negative
563e334261a8013065267d99	X	Greg, this might sound confusing at first, but the truth is that there's no such thing as "a folder" in Amazon S3.
  Negative
I'll explain.
  Neutral
The data structure of S3 is like a flat list of objects -- not like a tree.
  Negative
When you think you have a "file" called puppy.jpg inside a "folder" called pics, what you actually have is an object which key is pics/puppy.
  Negative
jpg.
  Neutral
Note that the / character is not any more special than the .
  Negative
character, or the p characters.
  Neutral
You might be thinking, Bruno is nuts, I see folders in the AWS Management Console.
  Negative
True, you see the folders.
  Positive
But they are actually emulated by the GUI.
  Positive
When you create a folder through the AWS Management Console, what it will actually do is create an object which name is the full path of the "folder", with a trailing slash, and 0 bytes.
  Positive
Just like the test/ object (not "folder") and the test/subdir/ object (not "folder") you mention in your question.
  Negative
To actually identify and draw "folders", the AWS Management Console (as well as many other S3 browsing tools) is doing is some API magic with the parameters delimiter and prefix.
  Negative
Now, knowing the fact that there's no such thing as a folder, and that they are emulated through the use of those 0-byte, trailing-/ objects, it should be easy to understand why you see the test/ object as a 0-byte object... The same reasoning would explain why you see nothing when you do a GET on a "folder" -- you are actually downloading a 0-byte object!
  Negative
Finally, as a conclusion, there's no easy way to obtain from S3 the size of "a folder" (they don't exist...).
  Neutral
The only way would be for you to list all the objects with that prefix and add their sizes.
  Negative
Or keep an index of your object ("files" and "folders") in some kind of database with more advanced querying capabilities.
  Neutral
563e334261a8013065267d9a	X	I need off help regarding amazon s3 with folders, The problem i get with the Amazon s3 class by undesigned is it doesnt support folders, it will only show you the full file name it gives you these three options out off the array.
  Negative
so as you can see it gives you the options name time size and hash no folders options so i am trying to find a work around.
  Positive
from above as you can see Cocaine VIP_Shufunk_192.mp3 is in the Music folder and their is also a folder Music/dnb/ which contains lots off files.
  Negative
What i am looking to do is find a what just to show files that are within a certain folder.
  Negative
so far ive tried.
  Neutral
ok so if i have a folder called Music i can have the following.
  Negative
ok so this will show all my files within music but the problem with this is it shows everything including folders within the music folder.
  Negative
I dont want it to show files that are within a folder within the music folder say Music/Dnb i dont want it to show these files only files within the Music folder not the Music/dnb folder???
  Negative
i have tried the following.
  Neutral
can anyone think off a solution to this???
  Neutral
Thanks
563e334261a8013065267d9b	X	Amazon S3 is a flat file system.
  Negative
There is no such thing as folders.
  Negative
There are simply really long file names with slashes in them.
  Negative
Most S3 tools will visually display these as folders, but that's not how S3 works under the hood.
  Negative
I've never used the Undesigned Amazon S3 class before, so I'm not sure about the specifics of that library.
  Negative
I used CloudFusion's S3 class for a long time until Amazon forked it to create the official PHP SDK (which is what I use now).
  Negative
In the PHP SDK, you can do: That will list all objects in your S3 bucket that have a file name (no real folders, remember?)
  Negative
that begins with Music/dnb/.
  Neutral
563e334261a8013065267d9c	X	
563e334361a8013065267d9d	X	There is no way around that in the Amazon API.
  Negative
You need to filter your amazon request using the prefix as you are and then filter out the 'subfolders' on the client.
  Negative
The best solution actually, is not to try to navigate your S3 storage.
  Negative
Instead you should maintain an 'index' of your files in a proper database (MySql, SimpleDb etc) that you search and query against, and then just retrieve the file from S3 when needed.
  Negative
563e334361a8013065267d9e	X	So, Here's the work around.
  Negative
To list your directories you do this.
  Negative
This will list all the folders in MYBUCKET/Music.
  Negative
This is a away you get aroung with having a flat file store.
  Negative
(This is written in scala)
563e334361a8013065267d9f	X	I've been asked to create a music streaming website and mobile app for a non-profit organization.
  Negative
The organization wants to upload all their music somewhere.
  Positive
Then users can stream and listen to the music from a website (desktop and mobile), iphone app and android app.
  Neutral
Although I've worked with video cloud platforms like brightcove to store and serve videos, I've never worked with audio cloud platforms before to store and serve audio.
  Negative
So my questions are: Do I really need an audio cloud platform?
  Negative
Can I get buy with just a web server and amazon S3?
  Negative
(Assume "light" traffic).
  Neutral
What are some popular audio cloud platforms that I should research?
  Neutral
Platforms must offer APIs that will let me build client side apps in IOS, Android and Web.
  Negative
Additional I tried uploading MP3s to soundcloud (an audio sharing platform).
  Negative
Then I used their javascript API to play a track This played ok on desktop browsers, but didn't work in android browsers or ios safari browsers.
  Negative
Are there comparable services to Soundcloud that have these issues resolved?
  Negative
Or would I save more time serving music via web server?
  Neutral
563e334761a8013065267da0	X	You could try using brightcove, vimeo, youtube etc..
  Negative
.
  Neutral
Then on the webpage, you can use css to make your player invisible to humans.
  Neutral
Then write your own player controls.
  Neutral
that could be a suitable replacement for soundcloud.
  Neutral
I posted an answer to your other question, which i'll repeat here because it's relevant Reasons to use audio cloud platforms: you need to transcode your audio files to other formats you want a more robust content delivery network (eg.
  Negative
serve content to a user from a server that's closest to them) pre-built CMS system Reasons to use your own web server
563e334761a8013065267da1	X	I've got a amazon S3 server that is connected to Simple DB.
  Very negative
in this server I've got different buckets, now, since i'm limited in space i need to delete some content from this buckets from time to time.
  Negative
This deletion needs to be done for specific buckets and based to date (nothing older then a week), of course the deletion needs to be done in both of the server and the DB, and to run as a scheduled task in the server (the server is Windows server 2008 + SQL 2008 R2) Can anyone suggest a script (Any language will be ok) for doing this task ?
  Negative
563e334761a8013065267da2	X	For S3 objects, you can use the S3 lifecycle feature to "expire" (delete) the objects when it crosses a specific age: Object Expiration - Amazon Simple Storage Service I am not aware of such a convenient way to do this on Simple DB.
  Negative
You might have to write a periodic script using Simple DB API do delete stuff on interval.
  Negative
563e334761a8013065267da3	X	Be aware that this configuration WILL NOT work if you specify port 465!
  Negative
(Because it uses different authentication mechanism).
  Positive
587 Seems to work just fine!
  Negative
.... Just lost 2 hours figuring out that...
563e334861a8013065267da4	X	Thanks for the mention to Mailjet.
  Negative
Since 2014, Mailjet has released a new major version of its platform.
  Negative
The V1 is still working but it's better to start with the V3.
  Positive
The SMTP address is now in-v3.
  Neutral
mailjet.com
563e334861a8013065267da5	X	I have moved an application to amazon builded in symfony2 and using swiftmailer for sending emails, I am not able to send emails from the application.
  Negative
So searching around the solution for sending emails.
  Negative
Please let me know if any solutions for sending email SES or configuring SMTP for symfony2.
  Negative
563e334861a8013065267da6	X	I got mine to work with the following details: And lastly, make sure the send FROM email address is verified.
  Negative
563e334861a8013065267da7	X	I never played with AWS SES but you can use mailjet to send email.
  Negative
You just need to configure Swiftmailer Transport to use their SMTP and you're done.
  Negative
They also ensure that your email are well sending (ie: not in spam) by providing several technique.
  Negative
You will have to setup some of them.
  Neutral
They do not provide example for Swiftmailer, but here a good one for Zend (you will see how easy it is):
563e334861a8013065267da8	X	Amazon SES has an excellent Php API from the official AWS Php SDK: http://docs.aws.amazon.com/aws-sdk-php/guide/latest/service-ses.html As you can see, SES is not used as a SMTP gateway (like Mailjet does) but as a HTTP API (via Guzzle, a Php library for HTTP queries).
  Negative
It's always an important decision to rely or not on Amazon services such as S3/SeS/SNS etc..
  Positive
.
  Neutral
maybe you don't want to depend on no Amazon technology?
  Negative
563e334861a8013065267da9	X	Thanks for the reply but I still want to hear about EC2.
  Negative
563e334861a8013065267daa	X	AppHarbor is hosted on AWS if you're asking for latency or other reasons.
  Negative
563e334861a8013065267dab	X	I thought AppHarbor was hosted on Azure?
  Negative
563e334861a8013065267dac	X	I want to move my projects to EC2 and need suggestion if EC2 support my project.
  Negative
Basic Requirement: 1 - MVC3 2 - EntityFramework 3 - SQLServer R2 4 - FullTrust 5 - .
  Negative
NET 3.5/4 and which service should I go for?
  Negative
563e334961a8013065267dad	X	I would use AppHarbor.com.
  Negative
It has all of the features you mentioned (it's also cloud based) and it's free.
  Positive
https://appharbor.com/ You can also integrate this with Amazon via the Amazon SDK API.
  Negative
(You can acquire this .
  Positive
dll via Nuget Package Manager) - This way you can manage all your files in S3 and deliver javascript via a Content Delivery Network if you wanted.
  Positive
563e334961a8013065267dae	X	You could also use a BaaS provider that offers API extensions out of the box so you can host your custom logic on their infrastructure (kind of makes them like a PaaS provider as well).
  Negative
CloudMine, for one, offers this.
  Positive
You can write your custom app in JavaScript or any JVM language (such as JRuby or Java).
  Negative
Might save you even more money and provide you what you need.
  Neutral
See cloudmine.me/docs/custom-code
563e334961a8013065267daf	X	@MarcW Yes, that's an excellent way to go about it too.
  Positive
563e334961a8013065267db0	X	Thanks for the answer!
  Neutral
Here is another relevant question: stackoverflow.com/questions/12047815/…
563e334961a8013065267db1	X	We are developing an application which requires the client (mobile device) to send files of size 5MB or more to the server component for processing and we would like some advice on the following: Is there any way to combine a Backend-as-a-Service (BaaS) platform with our own data-storage (hosted in our particular case in AWS)?
  Negative
We essentially would prefer if the files from the client are sent directly to our own database in the cloud rather than be stored in the BaaS servers.
  Negative
In other words, we need a BaaS platform or a solution that allows unbundling/bypassing its data-storage feature so that we can use the BaaS only for the rest of its facilities (such as the client authentication, the REST API etc).
  Negative
We have our own servers in EC2 which are needed for the main processing part of the files and only need the BaaS platform for conveniences that will kick-start our application in a short amount of time.
  Negative
Pulling the files from the BaaS platform's own data-storage to the EC2 servers would induce overall latency overhead as well as extra bandwidth cost in most cases.
  Negative
563e334961a8013065267db2	X	I'd faced a similar dilemma while building my app.
  Negative
In my case, I had to upload and store photos uploaded by users somewhere AND I didn't want to build a backend myself.
  Negative
So, I decided to use Amazon S3 to store the photos uploaded by the user and used SimpleDB as it offered me greater flexibility and ease of use than using a MySQL backend.
  Negative
Now, obviously, SimpleDB is not a Backend-as-a-Service platform but I was looking for the same convenience as you are.
  Negative
So what I'm suggesting is that you use a Backend-as-a-Service platform like Parse (which has an excellent freemium model), CloudMine (another great service but with tight limitations on the freemium model i.e only 500 free users/month) or Kinvey (which markets itself as the first BaaS platform, I don't have much information about it but it's definitely worth a look).
  Negative
And use S3 for your data storage.
  Negative
This way you can use the BaaS for client authentication, the REST API etc as you mentioned and you can continue using S3.
  Negative
All you need to do is create an appropriate naming scheme for your S3 buckets and objects such that you can easily identify which object belongs to which user, this can be done easily using a prefix-based naming scheme (seeing as S3 doesn't offer the ability to create sub-folders in buckets).
  Negative
Now whenever you need to pull some client information you can make a call to your BaaS with the client authenticated details and whenever you need access to your data-storage you can make a call to S3 using the Android SDK provided by AWS to retrieve the objects that belong to that particular user.
  Negative
Seeing as you plan on using EC2 to process those files transferring those files from S3 to EC2 should not cost you any extra bandwidth (I might be wrong here because I haven't looked into it but as far as I can remember transferring data within AWS is free).
  Negative
Do let me know if you have additional questions.
  Negative
563e334961a8013065267db3	X	I have a Rails app that catalogues recorded music products with metadata & wav files.
  Negative
Previously, my users had the option to send me files via ftp, which i'd monitor with a cron task for new .
  Negative
complete files and then pick it's associated .
  Negative
xml file and a perform metadata import and audio file transfer to S3.
  Negative
I regularly hit capacity limits on the prior FTP so decided to move the user 'dropbox' to S3, with an FTP gateway to allow users to send me their files.
  Negative
Now it's on S3 and due to S3 not storing the object in folders i'm struggling to get my head around how to navigate the bucket, find the .
  Neutral
complete files and then perform my imports as usual.
  Negative
Can anyway recommend how to 'scan' a bucket for new .
  Positive
complete files.....read the filename and then pass back to my app so that I can then pick up it's xml, wav and jpg files?
  Negative
The structure of the files in my bucket is like this.
  Positive
As you can see there are two products here.
  Neutral
I would need to find both and import their associated xml data and wavs/jpg
563e334961a8013065267db4	X	Though Amazon S3 does not formally have the concept of folders, you can actually simulate folders through the GET Bucket API, using the delimiter and prefix parameters.
  Negative
You'd get a result similar to what you see in the AWS Management Console interface.
  Neutral
Using this, you could list the top-level directories, and scan through them.
  Negative
After finding the names of the top-level directories, you could change the parameters and issue a new GET Bucket request, to list the "files" inside the "directory", and check for the existence of the .
  Negative
complete file as well as your .
  Neutral
xml and other relevant files.
  Positive
However, there might be a different approach to your problem: did you consider using SQS?
  Neutral
You could make the process that receives the uploads post a message to a queue in SQS, say, completed-uploads, with the name of the folder of the upload that just completed.
  Negative
Another process would then consume the queue and process the finished uploads.
  Negative
No need to scan through the directories in S3.
  Neutral
Just note that, if you try the SQS approach, you might need to be prepared for the possibility of being notified more than once of a finished upload: SQS guarantees that it will eventually deliver posted messages at least once; you might receive duplicated messages!
  Positive
(you can identify a duplicated message by saving the id of the received message on, say, a consistent database, and checking newly received messages against the same database).
  Negative
Also, remember that, if you use the US Standard Region for S3, then you don't have read-after-write consistency, you have only eventual-consistency, which means that the process receiving messages from SQS might try to GET the object from S3 and get nothing back -- just try again until it sees the object.
  Negative
563e334961a8013065267db5	X	Thanks jspcal.
  Neutral
I'd vote you up, but I don't have the rep yet.
  Negative
I was wondering about Cloudfront.
  Neutral
This is for integration with an ecommerce store, so perhaps Cloudfront will provide the programmatic ability to control number of downloads and not just time limit like S3.
  Negative
I had looked at Cloudfront but thought it was just a CDN for caching.
  Negative
Does it provide the api to control number of downloads?
  Neutral
563e334a61a8013065267db6	X	yeah cloudfront gives you access to the # of dl's.
  Positive
it's pretty advanced, fetchapp is probably easier to get started with
563e334a61a8013065267db7	X	Thanks for the info, Cloudfront needs a better marketing page that lists that as that is a clear shortcoming of S3.
  Negative
Glad you answered.
  Positive
I had written them off and I didn't know about fetchapp at all.
  Negative
563e334a61a8013065267db8	X	Is there a way to control number of downloads of digital content on Amazon S3 or via some middle man software that talks to S3?
  Negative
I already use their timed links, but I would like to control number of downloads also.
  Negative
Any ideas of how to accomplish this using S3 or suggestions about alternative services that could?
  Negative
Thanks!
  Positive
563e335b61a8013065267db9	X	couple solutions: Amazon CloudFront is a content delivery system that has an api and integrates with Amazon's other web services.
  Positive
that's probably what you want.
  Neutral
fetchapp is another service that is very nice... they actually use S3 on the back-end... you could roll your own digital download protector pretty easily with a script as well...
563e335b61a8013065267dba	X	Can you clarify what you mean by secure.
  Positive
Is this paid for content that you don't want anyone to get for free, personal/private info or are there other reasons?
  Negative
563e335b61a8013065267dbb	X	It could be paid or free content, but definitely don't want anyone who doesn't have the app to have access to the content.
  Negative
Just would like to know how is done on production apps, because I couldn't find any detailed info about it.
  Negative
Mostly the reason is paid for content.
  Neutral
563e335b61a8013065267dbc	X	In order to download and save a file on the external SD card (could be a zip file with some media files or a database file), what is the best practice of doing it?
  Negative
How do I secure this zip file from public access on the web server?
  Neutral
How do I access the folder from the app in order to download the file?
  Neutral
Is it better to use Amazon S3 cloud storage or Google Cloud Storage and their APIs?
  Negative
How usually the apps on the market secure and access their downloads?
  Neutral
Could anybody share sources and knowledge how can I do that, where I can find full documentation or which services I should use?
  Negative
Thank you!
  Positive
563e335c61a8013065267dbd	X	So I now see that you must compute the signature hash by encrypting with your key.
  Negative
So that answers my first question (I was wrong in my initial assessment).
  Negative
Azure lets you create unlimited number of containers, so I will have a container per customer.
  Positive
The customer will call my web service that will then generate the signature that is unique to the customer.
  Positive
This way any encryption keys, container names, etc. can be protected on my server and not on the client system.
  Negative
563e335c61a8013065267dbe	X	I will need to allow reads also, for a Restore feature that I have yet to implement.
  Negative
I think I will use a web service to generate the SAS any time the client needs to do an operation on the cloud.
  Negative
563e335c61a8013065267dbf	X	I am writing a backup service to backup a SQL database and save it in cloud storage.
  Negative
I have actually implemented this already with Amazon S3 but as the application will be distributed to customers, I cannot have the API keys stored with the application.
  Negative
I considered using a web service to provide the keys, but it's not my best option at this time (Because it still leaves the possibility of keys being stolen.)
  Negative
So I was looking into Windows Azure, the Blob Service and saw they have these Shared Access Signatures that can be used to provide access to resources in the cloud.
  Negative
You can also use the signature to put blobs in the cloud.
  Negative
But reading through the MSDN docs, I can't help but think this is an insecure system.
  Negative
Anyone who knows 1.
  Positive
the exact names of containers for my account and 2.
  Neutral
how to form the signature will be able to access the objects.
  Positive
You do not need a secret key when using this signature.
  Negative
At least that is my impression reading the docs.
  Neutral
So finally to my question.
  Neutral
Am I correct in my assessment of the shared access signatures with Azure, if not, why?
  Negative
And can anyone suggest an alternative way of doing what I am trying to accomplish.
  Negative
563e335c61a8013065267dc0	X	Shared Access Signatures can be scoped at either a specific container or a specific blob.
  Negative
They can then specify what permissions they give (read, write, list blobs), and they can specify how long they're valid.
  Negative
The only way to create a SAS is to have the storage key, but anyone who has the SAS can use it to do what it allows them to.
  Negative
It sounds like you want to allow all your customers to write blobs but not read them?
  Neutral
If so, a SAS that only specifies write permissions should do the trick.
  Negative
But I assume you also want to limit (or meter) usage by individually customers?
  Negative
If so, you'll probably need something active on the server (a web service?)
  Negative
that authorizes each use and generates a specific, short-expiry SAS to allow that operation.
  Negative
Then you can track and bill for each use.
  Neutral
563e335c61a8013065267dc1	X	I'm been experimenting with Fine Uploader.
  Negative
I am really interested in the chunking and resume features, but I'm experiencing difficulties putting the files back together server side; What I've found is that I have to allow for a blank file extension on the server side to allow the upload of the chunks, otherwise the upload will fail with unknown file type.
  Negative
It uploads the chunks fine with file names such as "blob" and "blob63" (no file extension) however is does not merge them back at completion of upload.
  Negative
Any help or pointers would be appreciated.
  Negative
And this is the server side script (PHP):
563e335c61a8013065267dc2	X	In order to handle chunked requests, you MUST store each chunk separately in your filesystem.
  Negative
How you name these chunks or where you store them is up to you, but I suggest you name them using the UUID provided by Fine Uploader and append the part number parameter included with each chunked request.
  Negative
After the last chunk has been sent, combine all chunks into one file, with the proper name, and return a standard success response as described in the Fine Uploader documentation.
  Negative
The original name of the file is, by default, passed in a qqfilename parameter with each request.
  Negative
This is also discussed in the docs and the blog.
  Neutral
It doesn't look like you've made any attempt to handle chunks server-side.
  Negative
There is a PHP example in the Widen/fine-uploader-server repo that you can use.
  Positive
Also, the documentation has a "server-side" section that explains how to handle chunking in detail.
  Negative
I'm guessing you did not read this.
  Neutral
Have a look.)
  Neutral
in the Widen/fine-uploader-server repo that you can use.
  Neutral
Also, the documentation has a "server-side" section that explains how to handle chunking in detail.
  Negative
I'm guessing you did not read this.
  Neutral
Have a look.
  Neutral
Note that, starting with Fine Uploader 3.8 (set to release VERY soon) you will be able to delegate all server-side upload handling to Amazon S3, as Fine Uploader will provide tight integration with S3 that sends all of your files directly to your bucket from the browser without you having to worry about constructing a policy document, making REST API calls, handling responses from S3, etc.
  Negative
I mention this as using S3 means that you never have to worry about handling things like chunked requests on your server again.
  Negative
563e335c61a8013065267dc3	X	Did you ever solve this ?
  Neutral
563e335c61a8013065267dc4	X	I'm implementing a REST service using WCF which will be used to upload very large files.
  Negative
The HTTP headers in this request will communicate information which will be validated prior to allowing the upload to proceed (things like permissions, available disk space, etc).
  Negative
It's possible this validation will fail resulting in an error response.
  Negative
I'd like to do this validation prior to the client sending the body of the request, so it has a chance to detect failure before uploading potentially gigabytes of data.
  Very negative
RESTful web services use the HTTP 1.1 Expect: 100-continue in the request to implement this.
  Negative
For example Amazon S3's REST API can validate your key and ACLs in response to an object PUT operation, returning 100 Continue if all is well, indicating you may proceed to send your data.
  Negative
I've rummaged around the WCF documentation and I just can't see a way to accomplish this without doing some pretty low-level hooking into the HTTP request processing pipeline.
  Negative
How would you suggest I solve this problem?
  Neutral
563e335d61a8013065267dc5	X	is that all they have reported?
  Neutral
have they given wordpress version, plugin version, browser vendor and version etc??
  Negative
somethign must have changed, has your plugin been updated?
  Negative
hae they updated wordpress?
  Neutral
the video file?
  Neutral
have they moved servers/domain??
  Positive
has anyone else reported this?
  Neutral
563e335d61a8013065267dc6	X	I am having issues with some of our users saying they have upgraded or changed theme and then they can no longer access our api.
  Negative
For instance this user.
  Neutral
http://mindfulnessexercises.com/nature-sounds-woodland-bridalway/ See the spinning audio player this is their question.
  Positive
We recently changed our theme to “Headline News,” and found out that the audio shortcode seemed to be not working any more.
  Negative
We are sure that the file from Amazon S3 is working and is in public just like how it’s working using your plugin before.
  Negative
I cannot understand what has changed and why this may no longer be working for them has Wordpress changed anything with the admin-ajax.php file with the new update.
  Negative
With my plugin i am doing the following.
  Neutral
PHP Has anything changed i am so stuck on how to bug test this because i can set it up on another host provider and i will work fine no issue at all.
  Negative
Can anyone suggest how i can bug test this, would really really appreciate some help.
  Negative
Thanks
563e335d61a8013065267dc7	X	Thanks for the tip!
  Positive
Just FYI, I think you have a type here.
  Negative
{"Content-Disposition": "attachment"} threw an error but {"Content-Disposition"=> "attachment"} worked properly.
  Negative
563e335d61a8013065267dc8	X	{"Content-Disposition": "attachment"} is Ruby 1.9 only.
  Negative
Use {"Content-Disposition"=> "attachment"} if you're still on 1.8.
  Negative
563e335d61a8013065267dc9	X	My application is using Rails 2 backend, Heroku for hosting, Paperclip for file uploads, and Amazon S3 for file storage.
  Negative
Right now users can upload files with paperclip + s3 - this works flawlessly.
  Positive
After upload, an icon appears on their dashboard, linked to the file location (in s3 bucket).
  Negative
When the icon is clicked, the browser opens the file in a new window (for most file types - PDF, MP3, img, etc).
  Negative
Instead of opening, I want the file to be automatically downloaded when the user clicks the file's icon (like Gmail attachments).
  Negative
The solution should be able to work for any file type and cross-browser.
  Positive
Is there a helper to do this in rails, or is javascript needed?
  Negative
I'm really stuck on this one so anything to point me in the right direction would be greatly appreciated.
  Negative
Thanks!
  Positive
563e335d61a8013065267dca	X	Please try the following: This should tell the Paperclip Gem to set the "Content-Disposition" header to the value "attachment" for newly uploaded files.
  Negative
Note that you have to manually edit the already uploaded file, e.g. with Cyberduck or another FTP Client.
  Negative
563e335e61a8013065267dcb	X	When you transfer the file, you need to set a Content-Disposition header with a value of attachment; filename=yourfilename.pdf.
  Negative
If it's transfered directly from S3, you'll need to tell S3 to set the Content-Disposition headers as well.
  Negative
Possibly also Content-Type.
  Neutral
Note that if you tell S3 to associate a Content-Disposition header, it will always transmit this header.
  Negative
FWIW, here's Amazon's documentation on doing a PUT for an Object: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectPUT.html
563e335e61a8013065267dcc	X	I am planing to develop an VoIP iOS app and use Twilios SDK.
  Negative
I am making the choice to either use LiveCode, Appery.io, PhoneGap or build a native Objective C app.
  Negative
I am going to build the app for iOS, Android and HTML5 so the ideal would be to develope in JavaScript for all platforms, but as I understand the support for WebRTC is laking on the iPhone so the alternativ for iOS is the native twilio SDK.
  Negative
My requirements is: I have seen several Twilio projects that use PhoneGap but none that are using LiveCode.
  Negative
I have already built an iOS VoIP app in Objective C, but I want to be able to release it on several platforms also such as for Android and build a HTML5 app, without redoing everything.
  Negative
563e335e61a8013065267dcd	X	This isn't really a programming question and should perhaps not be asked here.
  Negative
You can create an external for LiveCode and quickly create an interface using the LiveCode IDE.
  Negative
This is probably a quick and easy way to make a working app.
  Positive
If you're starting with LiveCode but are experienced in Objective-C, creating an external won't be a problem for you.
  Negative
LiveCode doesn't contain native iOS controls, which means that you have to emulate the GUI.
  Negative
If you use PhoneGap, you also will need to compile a plugin for PhoneGap using Objective-C, but you can use a framework, such as JQuery, to get the right GUI.
  Negative
Either way, you will have to compile the SDK and you'll need to be quite profound in Objective-C.
  Positive
LiveCode will meet all your requirements.
  Neutral
However, Apple will deny your app if you use PayPal for in-app purchases.
  Neutral
You'll have to use Apple's in-app purchasing feature.
  Neutral
I believe this is possible in LiveCode now.
  Neutral
I'm not sure how easy it is.
  Negative
I'm not sure about file listings either.
  Negative
On iOS, you won't have complete access to all files on the phone.
  Negative
This isn't a LiveCode limation but a limitation of the OS.
  Negative
563e335e61a8013065267dce	X	I’ve implemented the graph API POST /me/photos in my iOS app and its working fine.
  Negative
Same way I implemented the /me/videos with host graph-video.
  Negative
facebook.com suggested in Facebook documents and this link : [cURL - is there a way for upload a video to facebook without form?
  Negative
I get success response for this too like below but the video is not showing up on my Facebook account.
  Negative
Here is the code I have written : if I use the URL as "/me/videos" like I use /me/photos instead of https://graph-video.facebook.com/me/videos, I get the below error: I tried with both .
  Negative
mp4 and .
  Neutral
mov which are in supported videos.
  Neutral
I'm sure there is no issue with video because the same video I upload to amazon S3 before posting to FB and I can play the uploaded video.
  Very negative
Here is one sample: https://tagg-social-staging.s3.amazonaws.com/uploads/posts/videos/36/post-video.mp4 Note: I'm not posting the video using the above URL, but as multipart /form-data
563e335e61a8013065267dcf	X	I have fixed the posting video issue using the sample code provided here: https://developers.facebook.com/blog/post/2011/08/04/how-to--use-the-graph-api-to-upload-a-video--ios/ I don’t know how long it will work as it’s a deprecated code but I used the method ([FBRequestConnection startWithGraphPath:@"/me/videos") of latest v2.2 by changing API parameters : “source” to “video.mov” for video “message” to “description” for caption Now I have one more issue: I need to tag friends from my app.
  Negative
I use /{photo-id}/tags for tagging a photo which is working fine and I tried the same API to tag a video as I’m not able to get any other API from FB docs.
  Negative
I get the below error for while tagging friends for a video: Is there any API to tag a video from mobile app?
  Negative
563e335e61a8013065267dd0	X	Have you tried the suggestion in the answer of the first question you link ?
  Neutral
stackoverflow.com/a/3871531/428236
563e335e61a8013065267dd1	X	I want to use the Amazon.S3.IO API because it's a more simplier implementation and matches perfecty with existing interfaces.
  Negative
563e335e61a8013065267dd2	X	Thanks for the answer.
  Neutral
May i suggest to think about changing/providing the S3FileStream class, which could use a configurable range to download a file.
  Negative
I think Azure did implement it with the class BlobStream.
  Negative
563e335e61a8013065267dd3	X	Possible Duplicate: How to upload files to Amazon S3 (official SDK) that are larger than 5 MB (approx)?
  Negative
I try to use the Amazon.S3.IO API.
  Negative
If i write 10mb there is no problem.
  Negative
If i write 21mb i get an exception: The request was aborted: The request was canceled.
  Negative
StackTRace: at System.Net.ConnectStream.CloseInternal(Boolean internalCall, Boolean aborting) at System.Net.ConnectStream.System.Net.ICloseEx.CloseEx(CloseExState closeState) at System.Net.ConnectStream.Dispose(Boolean disposing) at System.IO.Stream.Close() at System.IO.Stream.Dispose() at Amazon.S3.AmazonS3Client.getRequestStreamCallback[T](IAsyncResult result) at Amazon.S3.AmazonS3Client.endOperation[T](IAsyncResult result) at Amazon.S3.AmazonS3Client.EndPutObject(IAsyncResult asyncResult) at Amazon.S3.AmazonS3Client.PutObject(PutObjectRequest request) at Amazon.S3.IO.S3FileStream.Flush(Boolean flushToS3) at Amazon.S3.IO.S3FileStream.Dispose(Boolean disposing) at System.IO.Stream.Close() at System.IO.StreamWriter.Dispose(Boolean disposing) at System.IO.TextWriter.Dispose() at S3FileSystem_Sample.
  Very negative
Program.createFile(S3DirectoryInfo rootDirectory, String filename) in c:\Program Files (x86)\AWS SDK for .
  Negative
NET\Samples\S3FileSystem_Sample \S3FileSystem_Sample\Program.cs:line 106 at S3FileSystem_Sample.
  Negative
Program.Main(String[] args) in c:\Program Files (x86)\AWS SDK for .
  Negative
NET\Samples\S3FileSystem_Sample\S3FileSystem_Sample\Program.cs:line 59 at System.AppDomain.
  Negative
_nExecuteAssembly(RuntimeAssembly assembly, String[] args) at System.AppDomain.ExecuteAssembly(String assemblyFile, Evidence assemblySecurity, String[] args) at Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly() at System.Threading.ThreadHelper.ThreadStart_Context(Object state) at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx) at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx) at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state) at System.Threading.ThreadHelper.ThreadStart() Any idea i could set the timeout?
  Very negative
Click here and here for more related questions.
  Negative
563e335e61a8013065267dd4	X	Unfortunately the Amazon.S3.IO API does not support setting a timeout but as a Developer on the SDK I will add that feature request to our backlog.
  Very negative
Keep in mind the Amazon.S3.IO API is a really easy interface to work with S3 but it is not well suited for large files because it buffers them into memory until you have passed the entire contents of the file to the API and then it writes it to S3.
  Negative
For large files you should use the TransferUtility found in the the Amazon.S3.Transfer namespace.
  Negative
563e335f61a8013065267dd5	X	what size are the images currently?
  Neutral
563e335f61a8013065267dd6	X	360x480 for youtube, I need to reduce it down to 140x220.
  Negative
So about 50% that I need to reduce in CSS.
  Negative
563e335f61a8013065267dd7	X	Here's my situation, I'm doing some basic Sinatra application hosted on heroku.
  Negative
The app is requesting the 25 top most popular videos on youtube by youtube RESTful api, but it only shows thumbnail images on the app not the videos (user has to click it to see the video).
  Negative
Also, the app is requesting my 10 most recent photos from my flickr account by flickraw gem, also show that on the app.
  Negative
When I run google chrome pagespeed to test the performance, I get 56/100.
  Negative
It suggests me that I should serve scaled images rather than reduce it by css or html (I got the smallest images I could get but I somehow need to reduce them down a bit to fit my page), which obviously I cannot do that, those files are on youtube and flickr.
  Very negative
Should I scale them down on the fly and cache that on Amazon S3?
  Negative
or what would be the best strategies to boost my pagespeed?
  Neutral
Right now it takes about 8 seconds to load the entire page.
  Negative
563e335f61a8013065267dd8	X	The most important factor is how often do you request YouTube and Flickr.
  Positive
You should make the request at a given time interval and cache the result / store it in memory.
  Negative
563e335f61a8013065267dd9	X	You want to read about share nothing architecture.
  Negative
Your user data needs to move to s3 or other storage mechanism.
  Neutral
563e335f61a8013065267dda	X	@talai : Thanks Talai .
  Negative
what is fast to access by PHP(codeignitor) S3 or EBS?
  Negative
and is there any link to guide how to store images on S3 via Codeignitor.
  Negative
563e335f61a8013065267ddb	X	I don't know about Codeignitor, but there is a PHP SDK for Amazon AWS that you can use.
  Negative
And in my opinion, EBS is faster than S3.
  Negative
The difference is that you can make the files stored on S3 accessible to anybody and from anywhere (if you want to, you can configure the security settings), but for EBS, you must pass through an EC2 instance.
  Negative
563e335f61a8013065267ddc	X	Recently My Website shifted on Amazon.
  Negative
Codeignitor folder I have folder name 'UPLOAD'.
  Negative
this folder is used for uploaded images and files.
  Negative
I make AMI image from EC2 Instance.
  Neutral
I have setup Auto scaling of Ec2 instances.
  Neutral
When my old ec2 instance is failed then automatically new instance is created.
  Negative
But My all data from "UPLOAD" of folder on old ec2 instance has lost.
  Negative
I want to separate "UPLOAD" folder in codeignitor from ec2 instance.
  Neutral
So whenever new instance is create it will get UPLOAD folder and its contents without loss.
  Negative
I want to separate this upload folder.
  Neutral
so when new instance is create then it will get this data.
  Negative
how to do this.
  Neutral
Thanks for Advance.
  Neutral
Note .
  Neutral
I have used MYSQL on Amazon RDS.
  Negative
563e335f61a8013065267ddd	X	You can use a shared Elastic Block Storage mounted directory.
  Negative
If you manually configure your stack using the AWS Console, go to the EC2 Service in the console, then go to Elastic Block Storage -> Volumes -> Create Volume.
  Negative
And in your launch configuration you can bind to this storage device.
  Negative
If you are using the command line tool as-create-launch-config to create your launch config, you need the argument --block-device-mapping "key1=value1,key2=value2..." If you are using Cloudformation to provision your stack, refer to this template for guidance.
  Negative
This assumes Codeignitor can be configured to state where its UPLOAD directory is.
  Negative
563e336061a8013065267dde	X	As said by Mike, you can use EBS, but you can also use Amazon Simple Storage Service (S3) to store your images.
  Negative
This way, whenever an instance starts, it can access all the previously uploaded images from S3.
  Negative
Of course, this means that you must change your code for the upload, to use the AWS API and not the filesystem to store your images to S3.
  Negative
563e336061a8013065267ddf	X	I could not figure this out.
  Negative
I ended up setting permisions manually using S3Hub, a s3 manager for the mac
563e336061a8013065267de0	X	Is there a way to change the permission of every single file in a S3 bucket using either aws-s3 gem or right_aws gem?
  Negative
I can't find it in the documentation.
  Negative
Do I have to do each file individually?
  Neutral
I would like to grant "everyone" view permission.
  Negative
563e336061a8013065267de1	X	I do not believe these gems are supposed to set permissions, that might be the reason you do not find this feature in the docs.
  Negative
Set your permissions in AWS console or through their API, amazon also has command line tools for setting S3 permissions.
  Negative
The gem aws-s3 (and probably right_aws as well) is for reading and storing files in S3 from ruby.
  Negative
Setting permissions is a bit different discipline.
  Positive
563e336061a8013065267de2	X	RightAws::S3::Grantee.new(key, "http://acs.amazonaws.com/groups/global/AllUsers", ["READ"], :apply, "AllUsers") works for me.
  Positive
563e336061a8013065267de3	X	According to the docs, the gem provides an acl= method for S3Object.
  Positive
You can pretty easily iterate through each object in the bucket and programmatically set the acl on each object.
  Neutral
http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/S3/S3Object.html#acl%3D-instance_method
563e336061a8013065267de4	X	I have Amazon S3 where all of my files are stored.
  Negative
Currently my users can go to a link where they can stream, but not download, audio and video files.
  Negative
How can I set up a link through either Amazon S3 or perhaps Amazon CloudFront that will allow someone to download an MP3 file or something of that nature?
  Neutral
Thanks for any advice!
  Positive
563e336061a8013065267de5	X	You must set the file's content header to something other than the media type the browser understands.
  Negative
For example: This used to be a big issue if you wanted to have both features (ability to display/view and ability to download) and you used to have to proxy the file download through your EC2 or other annoying ways.
  Negative
Now S3 has it built in: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html You can override values for a set of response headers using the query parameters listed in the following table.
  Negative
These response header values are only sent on a successful request, that is, when status code 200 OK is returned.
  Negative
The set of headers you can override using these parameters is a subset of the headers that Amazon S3 accepts when you create an object.
  Negative
The response headers that you can override for the GET response are Content-Type, Content-Language, Expires, Cache-Control, Content-Disposition, and Content-Encoding.
  Negative
To override these header values in the GET response, you use the request parameters described in the following table.
  Negative
(linke above)
563e336061a8013065267de6	X	check these out http://css-tricks.com/snippets/php/generate-expiring-amazon-s3-link/ http://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html hope it helps :)
563e336061a8013065267de7	X	So value_as_string contains a string representation of the encrypted file?
  Negative
Why can't you just decrypt the string after retrieving it from S3?
  Negative
563e336161a8013065267de8	X	You're correct I'm using boto, not boto3, but isn't boto3 the recommended library?
  Negative
It looks like that's the Amazon recommended API aws.amazon.com/sdk-for-python
563e336161a8013065267de9	X	Seems like your example relies on getObject being automatically decrypted on the fly... but that's in Java.
  Negative
Is this true for Python's boto3 as well?
  Neutral
563e336161a8013065267dea	X	Have edited the answer to state that no version of Boto supports client-side encryption.
  Neutral
There does exist this utility (I have not used it) that wraps Boto, providing the ability to specifcy client-side keys.
  Negative
pypi.python.org/pypi/s3-encryption/0.1.0
563e336161a8013065267deb	X	It's not.
  Negative
Client side encrypted, and I know the key.
  Negative
563e336161a8013065267dec	X	Did you write the code to encrypt it?
  Neutral
If so, it should be easy to write the decryption code.
  Positive
563e336161a8013065267ded	X	With an unencrypted file, I can do the following: But if the file's encrypted, I need to change something about that.
  Negative
I can't figure out what from reading the docs.
  Negative
What do I change?
  Neutral
I know the master symmetric key, which is a string like 30 chars or so long.
  Negative
563e336161a8013065267dee	X	At this time, no version of Boto supports client supplied keys in it's API.
  Negative
Instead, you could use the AWS SDK.
  Neutral
The general process is this: When downloading an object – The client first downloads the encrypted object from Amazon S3 along with the metadata.
  Negative
Using the material description in the metadata, the client first determines which master key to use to decrypt the encrypted data key.
  Negative
Using that master key, the client decrypts the data key and uses it to decrypt the object.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html Here is an example, in Java, that shows creation of a key, upload of a file that gets encrypted with the client key, and retrieval of the file and decrypted with the client key: Source used/adapted from here: http://aws.amazon.com/articles/2850096021478074 In Python, no AWS "supported" .
  Negative
e.g. Boto option exists.
  Neutral
However, there does exist a wrapper library for Boto that provides the ability to encrypt/decrypt using a client side-key is this library: https://pypi.python.org/pypi/s3-encryption/0.1.0
563e336161a8013065267def	X	If this is a file that AWS encrypted for you (by your setting the flag in boto to turn encryption on) then you don't have to do anything extra.
  Neutral
S3 transparently encrypts and decrypts the contents.
  Negative
They're encrypted inside of S3, but in the clear in your program.
  Neutral
So the same code works either way.
  Positive
563e336161a8013065267df0	X	If there any specific problem with using urllib and a bunch of threads/greenlets?
  Negative
Or is this a general `what's the best possible solution' question?
  Neutral
563e336161a8013065267df1	X	@Carpetsmoker It seems I asked the wrong question.
  Negative
I have clarified description: "What solution allows me to read chunks from upload content and starts streaming this chunks to internal storages befor user will have uploaded whole file.
  Negative
All solutions that I know wait for a whole content before give management to the wsgi applications/python web server"
563e336161a8013065267df2	X	Thanks!
  Negative
I take note of this solution with twisted.
  Positive
As you wish you may see my own solution with gevents below
563e336161a8013065267df3	X	Thanks for sharing your solution!
  Negative
563e336161a8013065267df4	X	I am trying to create python intellectual proxy-server that should be able for streaming large request body content from client to the some internal storages (that may be amazon s3, swift, ftp or something like this).
  Negative
Before streaming server should requests some internal API server that determines parameters for uploading to internal storages.
  Negative
The main restriction is that it should be done in one HTTP operation with method PUT.
  Negative
Also it should work asynchronously because there will be a lot of file uploads.
  Negative
What solution allows me to read chunks from upload content and starts streaming this chunks to internal storages befor user will have uploaded whole file?
  Negative
All python web applications that I know wait for a whole content will be received before give management to the wsgi applications/python web server.
  Negative
One of the solutions that I found is tornado fork https://github.com/nephics/tornado .
  Positive
But it is unofficial and tornado developers don't hurry to include it into the main branch.
  Negative
So may be you know some existing solutions for my problem?
  Negative
Tornado?
  Neutral
Twisted?
  Neutral
gevents?
  Neutral
563e336161a8013065267df5	X	Here's an example of a server that does streaming upload handling written with Twisted: This is a tac file (put it in streamingserver.tac and run twistd -ny streamingserver.tac).
  Negative
Because of the need to use self.channel.
  Neutral
_path this isn't a completely supported approach.
  Negative
The API overall is pretty clunky as well so this is more an example that it's possible than that it's good.
  Positive
There has long been an intent to make this sort of thing easier (http://tm.tl/288) but it will probably be a long while yet before this is accomplished.
  Positive
563e336261a8013065267df6	X	It seems I have a solution using gevent library and monkey patching: It works well when I try to upload huge file:
563e336261a8013065267df7	X	Is this mean that Filepicker will copy my files from S3 again or it just creates a Filepicker URL for them?
  Negative
I need the second option :)
563e336261a8013065267df8	X	Yes, second option creates new file and store it under your s3.
  Negative
The response will be similar to store method response.
  Neutral
Using GET request filepicker.io/api/file/EmlSqNgR0CcgiKJQ70aV/… return converted file from Filepicker conversion servers.
  Negative
Once you request file with specific parameters it is cached for one month.
  Positive
And the file is hosted on our S3 buckets.
  Negative
563e336261a8013065267df9	X	I have old files prior filepicker what I now copied to the S3 bucket.
  Negative
Can I access them with the filepicker API to get them cropped?
  Negative
I didn't find any relevant info in the documentation.
  Negative
According to the Stackoverflow threads, it seems I should store them again.
  Negative
Is it right?
  Neutral
563e336261a8013065267dfa	X	To use filepicker.io conversion feature file has to be available via filepicker API.
  Negative
So first store amazon url: https://s3.amazonaws.com/your_own_bucket/ZynOv436QOirPYbJIr3Y_5qYoopVTsixCJJiqSWSE.png Using Filepicker REST API: Sample response: Now you can convert filepicker url https://www.filepicker.io/api/file/EmlSqNgR0CcgiKJQ70aV Using GET request https://www.filepicker.io/api/file/EmlSqNgR0CcgiKJQ70aV/convert?w=200&h=250 Or using POST request to store converted file
563e336261a8013065267dfb	X	will the time to upload files go back down to "5 or 6 seconds" eventually even if you don't restart the app?
  Negative
563e336261a8013065267dfc	X	No, it doesn't go back.
  Negative
I need to restart the app server
563e336261a8013065267dfd	X	If you're using SSL for S3, you may look into AES-NI if you've already taken other steps to optimizing S3 performance.
  Negative
563e336261a8013065267dfe	X	@CleversonSchmidt It sounds like you have a resource-leak of some sort
563e336261a8013065267dff	X	@HyperAnthony If AES-NI is the problem, shouldn't happen every time?
  Negative
563e336261a8013065267e00	X	Thanks a lot!
  Positive
I'm passing an Inputstream to AmazonS3.putObject and setting the content lenght.
  Negative
I will check the TransferManager API and see if it helps.
  Positive
563e336261a8013065267e01	X	I'm currently working on a server app (JEE) and getting some problems to upload files to AWS S3.
  Negative
I'm using the Java SDK (S3client.putObject) to upload these files.
  Negative
When the server starts, everything happens as expected.
  Negative
Files are generated in the server (EC2 instance) and uploaded to S3 in a few seconds.
  Negative
But after some days, the performance degrades a lot.
  Negative
Files that usually took 5 or 6 seconds to be uploaded need now 10 to 30 minutes (yes, minutes).
  Negative
I profiled the app and the culprit here is the section that does the upload using the AWS Java SDK.
  Negative
Strangely the CPU utilization goes near 100% and stays there for minutes.
  Negative
As this is basically an IO operation, I don't understand why it may need so many CPU cyles to run.
  Negative
Has anyone eve experienced this behavior?
  Negative
Any tips on where to look?
  Neutral
PS: file size goes from 1 to 50 MB.
  Negative
Thanks a lot!
  Positive
Updates: The EC2 instance that creates the files and uploads them to S3 is m1.large.
  Negative
I'm using the 1.6.4 AWS SDK version .
  Negative
563e336261a8013065267e02	X	I can't think of any reason why the SDK code would cause your CPU to go so high.
  Negative
My first guess would be some sort of garbage collection issue.
  Negative
When you upload your data, are you passing in a File object to AmazonS3.putObject, or some sort of stream (including FileInputStream)?
  Negative
Streams can be a little tricky to deal with, since they aren't guaranteed to be repeatable and you have to explicitly provide the Content-Length in the ObjectMetadata as part of your upload, otherwise the SDK has to buffer your upload in memory to calculate the total length.
  Negative
That'd be the very first thing I'd recommend checking out.
  Negative
On a side note.
  Neutral
.
  Neutral
you should check out the TransferManager API in the SDK.
  Negative
It gives you a nice simple interface to uploading and downloading files to/from Amazon S3, and have several optimizations built in.
  Positive
If that still doesn't turn up a clue, then I'd recommend making a dead simple repro case for this.
  Negative
Write a single class file that simply uploads a random File to the same S3 key, and leave that running for the same duration as your application code.
  Negative
If you're able to reproduce the problem in that simple setup, then we can take a look at the code and help get it debugged, but with all the other variables involved in your full application code, we can't do much more than guess at what could be happening.
  Negative
563e336361a8013065267e03	X	Why do you have a .
  Negative
json at the end of the URLs?
  Neutral
563e336361a8013065267e04	X	That is the response format.
  Positive
.
  Neutral
json tells the server to respond with json, .
  Negative
xml tells the server to respond with xml format.
  Neutral
Rather that making it an optional parameter behind the ?
  Negative
.
  Neutral
blog.apigee.com/detail/…
563e336361a8013065267e05	X	Never seen content negotiation done on the URL, only in headers.
  Negative
On the URL it means you lose benefits of caching and more.
  Positive
563e336361a8013065267e06	X	@ScottRoepnack then you should consider the Accept HTTP header.
  Negative
563e336361a8013065267e07	X	@Oded If you used an Accept header, you'd also have a Vary: Accept, so caching wouldn't be affected.
  Negative
Conneg in extension has been discussed before; I'd agree with Shonzilla's answer there though.
  Negative
563e336361a8013065267e08	X	A MAC is meant to prove message authencity and protect against tampering with - it has nothing to do with user authentication
563e336361a8013065267e09	X	Added one of examples, how to handle user/client authentication without knowing of "login URL" beforehand
563e336361a8013065267e0a	X	Here is another two nice articles with stateless auth examples for REST services: blog.jdriven.com/2014/10/… technicalrex.com/2015/02/20/…
563e336361a8013065267e0b	X	"since each request can include credentials without impacting a human user" 3-way authentication and OAuth were invented because the thing in the quotes is bad.
  Negative
If you supply credentials with each request without a mechanism on the server to revoke them, that would be unsecure if used w/o SSL.
  Negative
563e336461a8013065267e0c	X	Whenever there is a concept of users, something has to get passed from client to server to identify which user.
  Negative
An OAuth token can certainly serve as the "credentials" here, instead of an actual user/password combination.
  Negative
Securing the channel with TLS is certainly always a good thing, but that's almost beside the point.
  Positive
Even if you use a cookie, some sort of token still gets sent to the server with every request, just with a cookie header instead of an authentication header.
  Negative
563e336461a8013065267e0d	X	And if you're not using TLS or OAuth for whatever reason, is sending a user/password every time really worse than sending it only once?
  Negative
If the attacker can obtain the user/password, the attacker can likely also obtain the session cookie.
  Negative
563e336461a8013065267e0e	X	The difference between a cookie and an authentication header being credentials is that cookies are always associated to a particular domain.
  Negative
This means that when the API receives a cookie, it knows where it came from (was written by the same domain earlier).
  Neutral
With a header, you never know and you have to implement specific checks for this.
  Negative
In general I agree, they are both credentials, but I think that passing credentials is not login.
  Negative
Login is the active action of opening the door.
  Positive
In the case of 3-way auth, only the first approval of the client would be login.
  Negative
563e336461a8013065267e0f	X	I am creating a REST api, closely following apigee suggestions, using nouns not verbs, api version baked into the url, two api paths per collection, GET POST PUT DELETE usage, etc.
  Negative
I am working on the login system, but unsure of the proper REST way to login users.
  Negative
I am not working on security at this point, just the login pattern or flow.
  Negative
(Later we will be adding 2 step oAuth, with an HMAC, etc) Possible Options What is the proper REST style for logging in users?
  Negative
563e336461a8013065267e10	X	Principled Design of the Modern Web Architecture by Roy T. Fielding and Richard N. Taylor, i.e. sequence of works from all REST terminology came from, contains definition of client-server interaction: All REST interactions are stateless.
  Negative
That is, each request contains all of the information necessary for a connector to understand the request, independent of any requests that may have preceded it.
  Negative
This restriction accomplishes four functions, 1st and 3rd is important in this particular case: And now lets go back to your security case.
  Positive
Every single request should contains all required information, and authorization/authentication is not an exception.
  Negative
How to achieve this?
  Neutral
Literally send all required information over wires with every request.
  Negative
One of examples how to archeive this is hash-based message authentication code or HMAC.
  Negative
In practice this means adding a hash code of current message to every request.
  Positive
Hash code calculated by cryptographic hash function in combination with a secret cryptographic key.
  Negative
Cryptographic hash function is either predefined or part of code-on-demand REST conception (for example JavaScript).
  Negative
Secret cryptographic key should be provided by server to client as resource, and client uses it to calculate hash code for every request.
  Negative
There are a lot of examples of HMAC implementations, but I'd like you to pay attention to the following three: If client knows the secret key, then it's ready to operate with resources.
  Negative
Otherwise he will be temporarily redirected (status code 307 Temporary Redirect) to authorize and to get secret key, and then redirected back to the original resource.
  Negative
In this case there is no need to know beforehand (i.e. hardcode somewhere) what the URL to authorize the client is, and it possible to adjust this schema with time.
  Negative
Hope this will helps you to find the proper solution!
  Positive
563e336461a8013065267e11	X	TL;DR Login for each request is not a required component to implement API security, authentication is.
  Negative
It is hard to answer your question about login without talking about security in general.
  Negative
With some authentication schemes, there's no traditional login.
  Negative
REST does not dictate any security rules, but the most common implementation in practice is OAuth with 3-way authentication (as you've mentioned in your question).
  Negative
There is no log-in per se, at least not with each API request.
  Negative
With 3-way auth, you just use tokens.
  Negative
This scheme gives the user the option to revoke access at any time.
  Neutral
Practially all publicly available RESTful APIs I've seen use OAuth to implement this.
  Negative
I just don't think you should frame your problem (and question) in terms of login, but rather think about securing the API in general.
  Negative
For further info on authentication of REST APIs in general, you can look at the following resources:
563e336461a8013065267e12	X	A big part of the REST philosophy is to exploit as many standard features of the HTTP protocol as possible when designing your API.
  Negative
Applying that philosophy to authentication, client and server would utilize standard HTTP authentication features in the API.
  Negative
Login screens are great for human user use cases: visit a login screen, provide user/password, set a cookie, client provides that cookie in all future requests.
  Negative
Humans using web browsers can't be expected to provide a user id and password with each individual HTTP request.
  Negative
But for a REST API, a login screen and session cookies are not strictly necessary, since each request can include credentials without impacting a human user; and if the client does not cooperate at any time, a 401 "unauthorized" response can be given.
  Negative
RFC 2617 describes authentication support in HTTP.
  Negative
TLS (HTTPS) would also be an option, and would allow authentication of the client to the server (and vice versa) in every request by verifying the public key of the other party.
  Negative
Additionally this secures the channel for a bonus.
  Negative
Of course, a keypair exchange prior to communication is necessary to do this.
  Neutral
(Note, this is specifically about identifying/authenticating the user with TLS.
  Negative
Securing the channel by using TLS / Diffie-Hellman is always a good idea, even if you don't identify the user by its public key.)
  Neutral
An example: suppose that an OAuth token is your complete login credentials.
  Negative
Once the client has the OAuth token, it could be provided as the user id in standard HTTP authentication with each request.
  Negative
The server could verify the token on first use and cache the result of the check with a time-to-live that gets renewed with each request.
  Negative
Any request requiring authentication returns 401 if not provided.
  Negative
563e336461a8013065267e13	X	I don't get why someone would close this as not constructive?
  Negative
563e336461a8013065267e14	X	Not sure either.
  Negative
Seems like a useful question, should be improved if anything.
  Negative
563e336461a8013065267e15	X	But should files be stored in databases?
  Negative
563e336461a8013065267e16	X	most DBs have very decent blob storage capabilities that can scale up to multiple TBs of data.
  Positive
unless youre doing something really drastic i dont see any reason why not
563e336561a8013065267e17	X	Right, maybe I should reconsider database storage then.
  Negative
So you won't get problems with some thousands of jpg/png images of size 40 KB to a couple of MB?
  Negative
And it is not a bad alternative to file system storage?
  Negative
563e336561a8013065267e18	X	Storing millions of images is actually one of the scenarios that MS Sql Server was designed against.
  Negative
I'd say most engines will handle it just fine.
  Negative
Every CMS I've met saves images in a database.
  Negative
563e336561a8013065267e19	X	absolutely no issues for several thousands of files.
  Negative
the DB might be bigger than the size of the files due to BLOB storage overhead (see stackoverflow.com/questions/4659441/…) but nothing significant in absolute terms.
  Negative
also, the large the files, the less the overhead is felt
563e336561a8013065267e1a	X	lucene stores its index separately from the actual files.
  Negative
you could have a lucene index for files stored in a DB as well.
  Negative
563e336561a8013065267e1b	X	As the title says, what is the prefered way of saving an uploaded file in a Java EE web application?
  Negative
I read some answers on other questions that saving the file to the filesystem of the servlet container is not recommended without further explanation.
  Negative
Some say you should save it to a database (but I doubt that from what I have read earlier) and some say that you should use JCR where the only implementation I can find is Apache JackRabbit, which doesn't seem to be very active?
  Negative
What would be the best option?
  Neutral
Are there other than those mentioned?
  Neutral
Reasons why you would choose one over the other is appriciated.
  Negative
563e336561a8013065267e1c	X	Depending on your environment you'll probably want to do one of a few things: Your server is in the cloud.
  Negative
You'll want to use a shared cloud store service such as Amazon S3 (which has a nice API btw) You are hosted on a traditional server.
  Negative
In this case the best practice would be to use a shared NAS, but cloud storage is also an option unless your client has regulatory concerns You are primarily dealing with many small(er) files and you want them to be searchable.
  Negative
For this scenario you'd choose a BLOB database column.
  Neutral
If you're handling large files (like video) you'll probably want to look into NAS/cloud storage instead and use the database as just a reference to the NAS/S3 location The reason for these options is because you don't want to sandbox your data to a running instance.
  Negative
This architecture allows for either additional instances of your application to be brought online or for a simple server migration and still have access to the shared data.
  Neutral
563e336561a8013065267e1d	X	JCR, as you've already seen, isnt all that popular.
  Negative
using the filesystem is not a very good idea both from a platform perspective (windows, for example has limits on max file path length, constraints on legal file names, and issues with >~100K files in a directory before it slows down to a crawl) and an architecture perspective - think about clustering your application: if you use any form of local storage you wont be able to cluster easily (as not all files are easily accessible tfrom all nodes), so you need to choose something accessible from all cluster nodes.
  Negative
DB is a good fit for that.
  Positive
some sort of cluster cache (or hadoop) might also be a good fit, depending on the specifics of your problem.
  Negative
563e336561a8013065267e1e	X	In my opinion this message this question depends on what you want to save.
  Neutral
Big files like HD video is are much faster accessed via filesystem.
  Negative
Using a database on the other hand makes it easier because you don't have to know the file are actually saved.
  Negative
Small amount and small files > database Otherwise filesystem Another pro for using filesystem as storage is the ability to implement a full text search framework like apache luscene.
  Negative
563e336561a8013065267e1f	X	I'm writing a Perl script to transfer files from Amazon S3 to Google Cloud Storage.
  Negative
The files I want to transfer have custom metadata on them, so I'm using the Multipart Upload API (https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload), where I specify the metadata as json in the first part, and the file data in the second part.
  Negative
I'm constructing the upload request with the following code: This is all working absolutely correctly for files with a 'binaryish' Content-type (image/png, for example).
  Negative
But for files with a 'textish' Content-type (text/vtt, application/json etc), the upload fails with the error "You must specify the content type of the destination object".
  Negative
If I hack the Content-type to be 'application/octet-stream', the upload works, although the file is then stored with the wrong Content-type in Google Storage.
  Negative
The upload also works correctly if I change to doing a simple (non-multipart) upload, but of course I'm then not able to upload the metadata.
  Neutral
Given that my current process works just fine for most files, I don't really want to have a separate process for text data of uploading it then adding the metadata separately.
  Negative
So, any ideas what I might be doing wrong?
  Negative
563e336561a8013065267e20	X	Do you have to use AMF for the upload?
  Neutral
You certainly can use it to do the upload but neither RemoteObject nor NetConnection dispatches any kind of progress event so the best you could do is have an indeterminate progress bar.
  Negative
563e336561a8013065267e21	X	Hy!
  Positive
.
  Neutral
Thanks ;)
563e336561a8013065267e22	X	I had to tackle a similar problem (uploading single photo from Flex to Django) while working on captionmash.com, maybe it can help you.
  Negative
I was using PyAMF for normal messaging but FileReference class had a built in upload method, so I chose the easy way.
  Negative
Basically system allows you to upload a single file from Flex to Google App Engine, then it uses App Engine's Image API to create thumbnail and also convert image to JPEG, then upload it to S3 bucket.
  Positive
boto library is used for Amazon S3 connection, you can view the whole code of the project here on github.
  Negative
This code is for single file upload only, but you should be able to do multi-file uploads by creating an array of FileReference objects and calling upload method on all of them.
  Negative
The code I'm posting here is a bit cleaned up, if you still have problems you should check the repo out.
  Negative
Client Side (Flex): Server side (Django on App Engine): Urls: Views: UploadService class
563e336561a8013065267e23	X	Is it possible to add a key to s3 with an utf-8 encoded name like "åøæ.jpg"?
  Negative
I'm getting the following error when uploading with boto:
563e336661a8013065267e24	X	From AWS FAQ: A key is a sequence of Unicode characters whose UTF-8 encoding is at most 1024 bytes long.
  Negative
From my experience, use ASCII.
  Negative
563e336661a8013065267e25	X	@2083: This is a bit of an old question, but if you haven't found the solution, and for everyone else that comes here like me looking for an answer: From the official documentation (http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html): Although you can use any UTF-8 characters in an object key name, the following key naming best practices help ensure maximum compatibility with other applications.
  Negative
Each application may parse special characters differently.
  Neutral
The following guidelines help you maximize compliance with DNS, web safe characters, XML parsers, and other APIs.
  Negative
Safe Characters The following character sets are generally safe for use in key names: Alphanumeric characters [0-9a-zA-Z] Special characters !
  Negative
, -, _, .
  Negative
, *, ', (, and ) The following are examples of valid object key names: 4my-organization my.great_photos-2014/jan/myvacation.
  Negative
jpg videos/2014/birthday/video1.wmv However, if what you really want, like me, is a filename that allows UTF-8 characters (note that this can be different from the key name).
  Negative
You have a way to do it!
  Neutral
From http://www.bennadel.com/blog/2591-embedding-foreign-characters-in-your-content-disposition-filename-header.htm and http://www.bennadel.com/blog/2696-overriding-content-type-and-content-disposition-headers-in-amazon-s3-pre-signed-urls.htm (Kudos to Ben Nadal) you can do that by making sure that when downloading the file, S3 will override the Content-Disposition header.
  Negative
As I have done it in java, I include here the code, I'm sure you'll be able to easily translate it to Python :) : It should help :)
563e336661a8013065267e26	X	Just to be clear, the first policy you included does not match the second.
  Positive
Are you sure they are the same?
  Neutral
Could you include the code you are using in your app?
  Negative
Are you using the transfer manager?
  Neutral
563e336661a8013065267e27	X	@BobKinney I've updated my question to show a simple node.js example to illustrate the problem.
  Negative
563e336661a8013065267e28	X	@ChrisH, Thanks for your detailed question.
  Positive
I just have a comment and a question.
  Neutral
My comment: this line var s3 = new AWS.S3(); should be after setting credentials (in third line), otherwise, errors will happen.
  Negative
And, the question is: What you wrote is for unauthenticated users, how would you do the same for Authenticated ones?
  Negative
Thanks!
  Positive
563e336661a8013065267e29	X	Thanks but the trailing '/' is not making any difference.
  Negative
The SO question you linked to confirms what I already thought...and suggests that my IAM policy should be working.
  Neutral
563e336661a8013065267e2a	X	I have created an IAM policy to allow Cognito users to write to my S3 bucket, but I would like to restrict them to folders based on their Cognito ID.
  Negative
I've followed Amazon's instructions here and created a policy that looks like this: But when I try to upload using the v2 of the AWS iOS SDK I get an access denied error.
  Negative
If I modify the last path component of the resource to replace ${cognito-identity.
  Negative
amazonaws.com:sub} with the explicit identityId value I am getting from the SDK's AWSCognitoCredentialsProvider it works.
  Negative
My understanding was that these should equate to the same thing.
  Neutral
Am I missing something in my policy, or should I be using a different path in my upload request?
  Negative
** Update ** I originally had this problem in iOS, so tonight I tried doing the same thing in node.js and the result is identical.
  Very negative
Here is the simple code I am using in node: And I get the same results that I get with iOS: unless I supply an explicit cognito ID in the IAM policy the API responds with 403.
  Negative
I've stripped my IAM policy down to the very bare minimum.
  Negative
This doesn't work: This does: I don't see what I'm missing here...the only documentation I've been able to find always shows the same example Resource value that I've been using.
  Negative
563e336661a8013065267e2b	X	Unfortunately there is currently an issue with the roles generated via the Cognito console in combination with policy variables.
  Negative
Please update your roles' access policy to include the following to ensure policy variables are evaluated correctly: 2014-09-16 Update: We have updated the Amazon Cognito console to correct this issue for new roles created via the Identity Pool creation wizard.
  Negative
Existing roles will still need to make the modification noted above.
  Neutral
563e336661a8013065267e2c	X	You are missing last slash.
  Negative
Also try to consider this article.
  Negative
563e336661a8013065267e2d	X	thank you for the reply , but this gives me a runtime exception since file might not have saved in s3 bucket ,
563e336661a8013065267e2e	X	exception AmazonServiceException { RequestId:E128351E5CB99880, ErrorCode:NoSuchKey, Message:The specified key does not exist. }
  Negative
563e336661a8013065267e2f	X	so handle this exception; this indicates that the S3 file doesn't exits.
  Negative
563e336661a8013065267e30	X	i thought handling exception anyway thanks
563e336761a8013065267e31	X	Thanks for the links @Naveen.
  Negative
Even thought 1.7.1 is depreciated but sometime old apps need it.
  Negative
That links helped.
  Positive
563e336761a8013065267e32	X	i have integrated aws v1 sdk to in my ios application to upload videos in to S3 bucket in background mode using NSURLSession But now i want to check file availability in bucket before start uploading for that , i managed to get link to V2 sdk How can I check the existence of a key/file on an Amazon S3 Bucket using AWS iOS SDK v2?
  Negative
what is the link used in V1 ??
  Neutral
563e336761a8013065267e33	X	AWS SDK for iOS is depreciated now; so I believe the documentation link also must have been taken out.
  Negative
Version 1 of the AWS Mobile SDK is deprecated as of September 29, 2014 and will continue to be available until December 31, 2014.
  Negative
If you are building new apps, we recommend you use Version 2.
  Positive
If you are working on existing apps that use Version 1 (1.7.x or lower) of the AWS Mobile SDK, you can download v1 for Android here and iOS here.
  Negative
The API reference guides are included in the respective downloads.
  Negative
Apps built using Version 1 will continue to function after December 31, 2014.
  Negative
However, we highly recommend that you update your apps to the latest version so you can take advantage of the latest features and bug fixes.
  Neutral
Source : http://aws.amazon.com/mobile/sdk/ I managed to find a sample code from AWS Mobile Blog [http://mobile.awsblog.com/post/Tx15F6J3B8B4YKK/Creating-Mobile-Apps-with-Dynamic-Content-Stored-in-Amazon-S3] to get the S3 object, you can extrapolate from there.
  Negative
Download Link for v1 iOS SDK : http://sdk-for-ios.amazonwebservices.com/aws-ios-sdk-1.7.1.zip
563e336761a8013065267e34	X	I'm trying to make a HTTP get request to https://elasticbeanstalk.us-east-1.amazonaws.com/?ApplicationName=MyApplicationName&Operation=DescribeEnvironments and getting I've tried setting my key and secret as username and password for basic HTTP auth, but clearly this doesn't work.
  Very negative
So how do I add my key and secret to my remote request?
  Negative
563e336761a8013065267e35	X	For most AWS usage scenarios it is highly recommended to use one of the many AWS SDKs to ease working with the APIs via higher level abstractions - these SDKs also take care of the required and slightly complex request signing, an explanation for the usually several options how to provide your AWS credentials can be found in the resp.
  Positive
SDK documentation: The AWS SDKs provide functions that wrap an API and take care of many of the connection details, such as calculating signatures, handling request retries, and error handling.
  Negative
The SDKs also contain sample code, tutorials, and other resources to help you get started writing applications that call AWS.
  Negative
Calling the wrapper functions in an SDK can greatly simplify the process of writing an AWS application.
  Negative
If you really have a need to use the AWS APIs via REST directly, Signing AWS API Requests will guide you through the required steps, see e.g. section Components of an AWS Signature 4 Request within Signature Version 4 Signing Process for the one that applies to AWS Elastic Beanstalk.
  Very negative
563e336761a8013065267e36	X	alestic.com/2012/01/ec2-ebs-boot-recommended
563e336761a8013065267e37	X	IMHO, this should be reopened, and then moved over to Server Fault.
  Negative
563e336761a8013065267e38	X	A question closed as "not constructive" has 144 upvotes (as of Mar 30 2013)?
  Negative
Shouldn't it be reopened?
  Negative
563e336761a8013065267e39	X	IMO this is a very constructive question and one of the first questions commonly asked when first learning about AWS -- look at the number of views.
  Negative
563e336761a8013065267e3a	X	Where's the button to call out the mods as "non-constructive" ?
  Negative
563e336761a8013065267e3b	X	Yes, the above were my thoughts as well... Hopefully somehow here writes about their preferences for instance-store as a comparison...
563e336861a8013065267e3c	X	@HelloWorldy: The comparison is really "An instance store can't do..." and list the things an EBS store can.
  Negative
There's no real benefit other than possibly a small cost savings (that can be offset by the convenience of stopping/starting EBS backed instances).
  Negative
563e336861a8013065267e3d	X	Instance store backed EC2 can also be set to not accidentally terminate.
  Negative
563e336861a8013065267e3e	X	I'm actually switching most of my EBS backed EC2 instances to using instance stores.
  Negative
It really depends on what you want to achieve.
  Neutral
I'm switching because of better IO and because I view each EC2 instance as disposable at all moments, or: it will break down any minute and I will lose everything that's on such an instance.
  Very negative
Architecting that way helps to get a real HA system.
  Positive
See also stu.mp/2011/04/the-cloud-is-not-a-silver-bullet.html
563e336861a8013065267e3f	X	@Jim: At least when I wrote the answer a year ago, you got much better IO by striping a number of EBS instances into a software RAID configuration than using instance storage.
  Very negative
It's also much faster to launch a replacement instance from EBS backing than from S3 backing (instance storage is loaded from S3, which can be slow).
  Negative
I have not done much on AWS the last 6 months or so; things may have changed.
  Negative
563e336861a8013065267e40	X	Is there any significant improvement of IO performance with EBS IOPS-kind of volumes compared to standard?
  Negative
Supposing, the above said holds for EBS IOPS volumes, as well.
  Positive
563e336861a8013065267e41	X	Both technologies evolve.
  Negative
I'm wirting this comment in 2014, when I have "Provisioned IOPS" EBS, but - the "instance store" is now SSD, which is even faster than before!!
  Negative
Ephemeral storage will always win in terms of speed.
  Positive
So I use both - keep the "persistent" stuff on EBS, having all the temp files, logs, "TempDB" database, swap-file and other stuff on Instance-store.
  Negative
BENEFIT FROM BOTH!
  Positive
563e336861a8013065267e42	X	What if you needed a distributed database which needs to store its data in a distributed and persistent manner.
  Negative
Wouldn't you need EBS because instance storage is not persistent?
  Negative
563e336861a8013065267e43	X	@CMCDragonkai Of course you do.
  Neutral
There are a lot of options these days, e.g. AWS started offering SSD-based storage.
  Negative
I would look into those and re-do the analysis (single vs. RAID, etc.).
  Negative
I would also look into getting the biggest instances possible because of network throughput.
  Negative
EBS is still an issue on instances like t1.micro.
  Negative
563e336861a8013065267e44	X	Netflix makes the same recommendations as well.
  Negative
563e336861a8013065267e45	X	So where do you store your block based persistent files?
  Negative
563e336861a8013065267e46	X	S3 has in-built redundancy.
  Negative
EBS has none, so you'll need to deploy redundancy software on top of it.
  Negative
563e336861a8013065267e47	X	I'm unclear as to what benefits I get from EBS vs. instance-store for my instances on Amazon EC2.
  Negative
If anything, it seems that EBS is way more useful (stop, start, persist + better speed) at relatively little difference in cost...?
  Negative
Also, is there any metric as to whether more people are using EBS now that it's available, considering it is still relatively new?
  Negative
563e336961a8013065267e48	X	The bottom line is you should almost always use EBS backed instances.
  Negative
Here's why I'm a heavy user of Amazon and switched all of my instances to EBS backed storage as soon as the technology came out of beta.
  Negative
I've been very happy with the result.
  Positive
Keep in mind that any piece of cloud-based infrastructure can fail at any time.
  Negative
Plan your infrastructure accordingly.
  Negative
While EBS-backed instances provide certain level of durability compared to ephemeral storage instances, they can and do fail.
  Negative
Have an AMI from which you can launch new instances as needed in any availability zone, back up your important data (e.g. databases), and if your budget allows it, run multiple instances of servers for load balancing and redundancy (ideally in multiple availability zones).
  Negative
563e336961a8013065267e49	X	99% of our AWS setup is recyclable.
  Negative
So for me it doesn't really matter if I terminate an instance -- nothing is lost ever.
  Negative
E.g. my application is automatically deployed on an instance from SVN, our logs are written to a central syslog server.
  Negative
The only benefit of instance storage that I see are cost-savings.
  Negative
Otherwise EBS-backed instances win.
  Neutral
Eric mentioned all the advantages.
  Neutral
[2012-07-16] I would phrase this answer a lot different today.
  Negative
I haven't had any good experience with EBS-backed instances in the past year or so.
  Negative
The last downtimes on AWS pretty much wrecked EBS as well.
  Positive
I am guessing that a service like RDS uses some kind of EBS as well and that seems to work for the most part.
  Positive
On the instances we manage ourselves, we have got rid off EBS where possible.
  Negative
Getting rid to an extend where we moved a database cluster back to iron (= real hardware).
  Negative
The only remaining piece in our infrastructure is a DB server where we stripe multiple EBS volumes into a software RAID and backup twice a day.
  Negative
Whatever would be lost in between backups, we can live with.
  Negative
EBS is a somewhat flakey technology since it's essentially a network volume: a volume attached to your server from remote.
  Neutral
I am not negating the work done with it – it is an amazing product since essentially unlimited persistent storage is just an API call away.
  Negative
But it's hardly fit for scenarios where I/O performance is key.
  Negative
And in addition to how network storage behaves, all network is shared on EC2 instances.
  Negative
The smaller an instance (e.g. t1.micro, m1.small) the worse it gets because your network interfaces on the actual host system are shared among multiple VMs (= your EC2 instance) which run on top of it.
  Negative
The larger instance you get, the better it gets of course.
  Positive
Better here means within reason.
  Positive
When persistence is required, I would always advice people to use something like S3 to centralize between instances.
  Negative
S3 is a very stable service.
  Positive
Then automate your instance setup to a point where you can boot a new server and it gets ready by itself.
  Negative
Then there is no need to have network storage which lives longer than the instance.
  Neutral
So all in all, I see no benefit to EBS-backed instances what so ever.
  Negative
I rather add a minute to bootstrap, then run with a potential SPOF.
  Negative
563e336961a8013065267e4a	X	We like instance-store.
  Negative
It forces us to make our instances completely recyclable, and we can easily automate the process of building a server from scratch on a given AMI.
  Negative
This also means we can easily swap out AMIs.
  Positive
Also, EBS still has performance problems from time to time.
  Negative
563e336961a8013065267e4b	X	Eric pretty much nailed it.
  Positive
We (Bitnami) are a popular provider of free AMIs for popular applications and development frameworks (PHP, Joomla, Drupal, you get the idea).
  Negative
I can tell you that EBS-backed AMIs are significantly more popular than S3-backed.
  Positive
In general I think s3-backed instances are used for distributed, time-limited jobs (for example, large scale processing of data) where if one machine fails, another one is simply spinned up.
  Negative
EBS-backed AMIS tend to be used for 'traditional' server tasks, such as web or database servers that keep state locally and thus require the data to be available in the case of crashing.
  Negative
One aspect I did not see mentioned is the fact that you can take snapshots of an EBS-backed instance while running, effectively allowing you to have very cost-effective backups of your infrastructure (the snapshots are block-based and incremental)
563e336961a8013065267e4c	X	I've had the exact same experience as Eric at my last position.
  Negative
Now in my new job, I'm going through the same process I performed at my last job... rebuilding all their AMIs for EBS backed instances - and possibly as 32bit machines (cheaper - but can't use same AMI on 32 and 64 machines).
  Negative
EBS backed instances launch quickly enough that you can begin to make use of the Amazon AutoScaling API which lets you use CloudWatch metrics to trigger the launch of additional instances and register them to the ELB (Elastic Load Balancer), and also to shut them down when no longer required.
  Negative
This kind of dynamic autoscaling is what AWS is all about - where the real savings in IT infrastructure can come into play.
  Negative
It's pretty much impossible to do autoscaling right with the old s3 "InstanceStore"-backed instances.
  Negative
563e336961a8013065267e4d	X	I'm just starting to use EC2 myself so not an expert, but Amazon's own documentation says: we recommend that you use the local instance store for temporary data and, for data requiring a higher level of durability, we recommend using Amazon EBS volumes or backing up the data to Amazon S3.
  Very negative
Emphasis mine.
  Neutral
I do more data analysis than web hosting, so persistence doesn't matter as much to me as it might for a web site.
  Negative
Given the distinction made by Amazon itself, I wouldn't assume that EBS is right for everyone.
  Negative
I'll try to remember to weigh in again after I've used both.
  Negative
563e336a61a8013065267e4e	X	I am writing a REST API and would like to implement an authentication system similar to AWS.
  Negative
http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html Basicly, on AWS the client encrypts the Authorization header with some request data using a secret key that is shared between client and server.
  Negative
(Authorization: AWS user: ) The server uses the key to decrypt the header using the shared key and compare to the request data.
  Negative
If successful, this means the client is legit (or at least is in possession of a legitimate key).
  Negative
The next step can be to execute the request or, preferrably, send the client a unique, time-based token (ex.: 30 minutes) that will be used on the actual request (added to a Token header, for example).
  Negative
This token cannot be decrypted by the client (uses a server-only key).
  Negative
On next requests, the server checks the token (not Authorization anymore) and authorizes the request to be executed.
  Negative
However, is it possible to have a man-in-the-middle, even on SSL-encrypted connections, that replays these token-authenticated requests?
  Neutral
Even if the MITM does not know what's inside the message, he/she could cause damage for example by ordering a product many times.
  Negative
If the server receives a replayed message and the token is still within the valid timestamp, the server will assume this is a valid request and execute it.
  Negative
AWS tries to solve this with a timestamp requirement: A valid time stamp (using either the HTTP Date header or an x-amz-date alternative) is mandatory for authenticated requests.
  Negative
Furthermore, the client timestamp included with an authenticated request must be within 15 minutes of the Amazon S3 system time when the request is received.
  Negative
If not, the request will fail with the RequestTimeTooSkewed error code.
  Negative
The intention of these restrictions is to limit the possibility that intercepted requests could be replayed by an adversary.
  Negative
For stronger protection against eavesdropping, use the HTTPS transport for authenticated requests.
  Negative
However, 15 minutes is still enough for requests to be replayed, isn't it?
  Neutral
What can be done to prevent replay attacks in this scenario?
  Neutral
Or am I overthinking and a certain degree of uncertainty is acceptable if you provide enough mechanisms?
  Neutral
I am thinking about requiring the client to add a unique string on each request body.
  Neutral
This string will be transport-encrypted and unavailable to MITM for modification.
  Neutral
On first receipt, the server will record this string and reject any new requests that contain the same string in the same context (example: two POSTS are rejected, but a POST and a DELETE are OK).
  Negative
EDIT Thanks for the info.
  Neutral
It seems the cnonce is what I need.
  Neutral
On the wikipedia diagram it seems the cnonce is only sent once, and then a token is generated, leaving it open to reuse.
  Negative
I guess it is necessary to send a new cnonce on every call with the same token.
  Negative
The cnonce should be included on the body (transport-protected) or shared-key-protected and included on a header.
  Negative
Body-protection seems the best (with obvious SSL) since it avoids some extra processing on both sides, but it could be shared-key-encrypted and included on a header (most likely prepended to the temp token).
  Neutral
The server would be able to read it directly on the body or decrypt it from the header (extra processing).
  Negative
563e336a61a8013065267e4f	X	A Cryptographic nonce, the unique string you mention, is indeed a good security practice.
  Positive
It will prevent requests to be reused.
  Neutral
It should be unique for each petition, independently of their nature.
  Positive
Including a timestamp and discarding all petitions made past a certain expiration date is also a good practice.
  Negative
Keeps the used nonce registry short and helps preventing collisions.
  Negative
The nonce registry should be associated to a user, to also prevent collisions.
  Negative
And consumers should use cryptographically secure pseudorandom number generators.
  Negative
If a predictable seed for the pseudorandom number generator is used, such as microtime, two nasty things can happen.
  Negative
563e336a61a8013065267e50	X	I'm developing a javascript application that is fully powered by a REST API.
  Negative
I have different stages of development (dev, stage, live) and I need to point to the correct REST API host depending on which environment it is hosted at.
  Negative
Currently the app gets built on each commit for each environment.
  Negative
By "built", I mean it I run a job (browserify) that starts from an entry point JS file and builds it into a single js file.
  Neutral
That means we make a dev, stage, and live build for each commit.
  Positive
We are trying to move to a traditional Continuous Integration solution of extracting the config and using one build where we can change the config.
  Negative
This is where the issue comes to play.
  Neutral
The build system (gulp) accepts REST API host configurations to be passed in as arguments to the build command.
  Neutral
The first step we accomplished to achieve Continuous Integration is removing the configurations from the being baked into the build.
  Negative
(That was the easy part.)
  Neutral
We end up with 2 files, config.js and app.js.
  Negative
We now need a way to manage a configuration that will be shipped with the static application.
  Negative
We are hosting on Amazon S3 and this is purely client side JS application so there is no way to set server environment variables that will contain the configurations.
  Very negative
We need to be able to deploy the software with the correct configuration file that is part of the source.
  Negative
How do we manage a single build with a configuration file for each environment per commit?
  Neutral
563e336a61a8013065267e51	X	This is helpful, but I didn't want the overhead of running my own server (I'd rather let Amazon do this).
  Negative
But it's a good suggestion.
  Positive
563e336b61a8013065267e52	X	So run your server on EC2 then...
563e336b61a8013065267e53	X	How do you hide the secret key?
  Negative
